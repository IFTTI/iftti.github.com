<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>PRML读书会第九章  Mixture Models and EM | IT技术干货</title>
    <meta name="viewport" content="width=device-width">
    <meta name="description" content="IT技术干货 KernelHacks 最好的技术站点 技术信息 纯干货">
    <link rel="canonical" href="http://iftti.com/posts/2015/2015-01-31-prml%25e8%25af%25bb%25e4%25b9%25a6%25e4%25bc%259a%25e7%25ac%25ac%25e4%25b9%259d%25e7%25ab%25a0-mixture-models-and-em-a7767c4f3.html">

    <!-- Custom CSS -->
    <link rel="stylesheet" href="/css/main.css">

</head>

    <body>

    <header class="site-header">

  <div class="wrap">

    <a class="site-title" href="/">IT技术干货</a>

    <nav class="site-nav">
      <a href="#" class="menu-icon">
        <svg version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
           viewBox="0 0 18 15" enable-background="new 0 0 18 15" xml:space="preserve">
          <path fill="#505050" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0
            h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
          <path fill="#505050" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484
            h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
          <path fill="#505050" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0
            c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
        </svg>
      </a>
      <div class="trigger">

          <a class="page-link" href="/about/">About</a>

      </div>
    </nav>

  </div>

</header>

    <div class="page-content">
      <div class="wrap">
      <div class="post">

  <header class="post-header">
    <h1>PRML读书会第九章  Mixture Models and EM</h1>
    <p class="meta"><span class="time">Jan 31, 2015</span><span class="source_url"> • 来自 52nlp.cn <a name="52nlp.cn" href="http://www.52nlp.cn/prml%e8%af%bb%e4%b9%a6%e4%bc%9a%e7%ac%ac%e4%b9%9d%e7%ab%a0-mixture-models-and-em" target="_blank">[原文链接]</a></span></p>
  </header>

  <article class="post-content">

						<p style="text-align: center"><span style="font-family: 微软雅黑;font-size: 15pt"><strong>PRML读书会第九章 Mixture Models and EM<br>
</strong></span></p>
<p style="text-align: center"><span style="font-family: 微软雅黑;font-size: 15pt"><strong>主讲人 网络上的尼采<br>
</strong></span></p>
<p style="text-align: center"><span style="font-family: 微软雅黑;font-size: 12pt"><strong>（新浪微博: <a href="http://weibo.com/dmalgorithms">@Nietzsche_复杂网络机器学习</a>）<br>
</strong></span></p>
<p><span style="font-size: 10pt"><span style="font-family: 微软雅黑">网络上的尼采(813394698) 9:10:56<br>
今天的主要内容有k-means、混合高斯模型、 EM算法。<br>
对于k-means大家都不会陌生，非常经典的一个聚类算法，已经50多年了，关于clustering推荐一篇不错的survey:</span><br>
<span style="font-family: 微软雅黑">Data clustering: 50 years beyond K-means。k-means表达的思想非常经典，就是对于复杂问题分解成两步不停的迭代进行逼近，并且每一步相对于前一步都是递减的。<br>
k-means有个目标函数 ：<br>
<img src="/images/52nlp.cn/46837cdb2db9de12cf0e58aa9828e496.jpg" alt=""></span><span style="font-family: 宋体"><br>
</span></span></p>
<p><span style="font-family: 微软雅黑;font-size: 10pt">假设有k个簇，<img src="/images/52nlp.cn/ec21b28673bf918acfc0ffb1a19f7d6a.jpg" alt="">是第k个簇的均值；每个数据点都有一个向量表示属于哪个簇，r<sub>nk</sub>是向量的元素，如果点x<sub>n</sub>属于第k个簇，则r<sub>nk</sub>是1，向量的其他元素是0。<br>
上面这个目标函数就是各个簇的点与簇均值的距离的总和，k-means要做的就是使这个目标函数最小。 这是个NP-hard问题，k-means只能收敛到局部最优。<br>
</span><span id="more-7668"></span></p>
<p><span style="font-size: 10pt"><span style="font-family: 微软雅黑">算法的步骤非常简单：<br>
先随机选k个中心点<br>
第一步也就是E步把离中心点近的数据点划分到这个簇里；<br>
第二步M步根据各个簇里的数据点重新确定均值，也就是中心点。<br>
然后就是迭代第一步和第二步，直到满足收敛条件为止。<br>
自强&lt;ccab4209211@qq.com&gt; 9:29:00<br>
收敛是怎么判断的呀？<br>
网络上的尼采(813394698) 9:30:16<br>
不再发生大的变化。大家思考下不难得出：无论E步还是M步,目标函数都比上一步是减少的。 下面是划分两个簇的过程 ：<br>
<img src="/images/52nlp.cn/7598cdf6a2910058717d1e31efb05bda.jpg" alt=""></span><span style="font-family: 宋体"><br>
</span></span></p>
<p><span style="font-family: 微软雅黑;font-size: 10pt">下面这个图说明聚类过程中目标函数单调递减，经过三轮迭代就收敛了，由于目标函数只减不增，并且有界，所以k-means是可以保证收敛的：<br>
</span></p>
<p><img src="/images/52nlp.cn/38ad63e87623715f4c20ecd77af98815.jpg" alt=""><span style="font-family: 宋体;font-size: 10pt"><br>
</span></p>
<p style="margin-left: 84pt"><span style="font-family: 微软雅黑;font-size: 10pt"> 书里还举例一个k-means对图像分割和压缩的例子：<br>
<img src="/images/52nlp.cn/e678f035703211290bed1a74c9ba371a.jpg" alt=""><br>
</span></p>
<p><span style="font-family: 微软雅黑;font-size: 10pt">图像分割后，每个簇由均值来表示，每个像素只存储它属于哪个簇就行了。压缩后图像的大小是k的函数 ：<br>
<img src="/images/52nlp.cn/85e721be3166977e2f1aa987f1cba99b.jpg" alt=""><br>
</span></p>
<p style="margin-left: 15pt"><span style="font-family: 微软雅黑;font-size: 10pt">现在讨论下k-means的性质和不足 ：<br>
</span></p>
<p style="margin-left: 15pt"><span style="font-family: 微软雅黑;font-size: 10pt">首先对初值敏感 ，由于只能收敛到局部最优，初值很大程度上决定它收敛到哪里；<br>
</span></p>
<p style="margin-left: 15pt"><span style="font-family: 微软雅黑;font-size: 10pt">从算法的过程可以看出，k-means对椭球形状的簇效果最好 ，不能发现任意形状的簇；<br>
</span></p>
<p><span style="font-family: 微软雅黑;font-size: 10pt">对孤立点干扰的鲁棒性差，孤立点是异质的，可以说是均值杀手，k-means又是围绕着均值展开的，试想下，原离簇的孤立点对簇的均值的拉动作用是非常大的。<br>
针对这些问题后来又有了基于密度的DBSCAN算法，最近Science上发了另一篇基于密度的方法：Clustering by fast search and find of density peaks。基于密度的方法无论对clustering还是outliers detection效果都不错，和k-means不同，这些算法已经没有了目标函数，但鲁棒性强，可以发现任意形状的簇。<br>
另外如何自动确定k-means的k是目前研究较多的一个问题。k-means就到这里，现在一块讨论下。<br>
</span></p>
<p><span style="font-family: 微软雅黑;font-size: 10pt">口水猫(465191936) 9:49:20<br>
如果对于这批数据想做k-mean 聚类，那么如果去换算距离？<br>
网络上的尼采(813394698) 9:49:53<br>
k-means一般基于欧式距离，关于距离度量是个专门的方向，点集有了度量才能有拓扑，有专门的度量学习这个方向。<br>
口水猫(465191936) 9:50:08<br>
嗯嗯 有没有一些参考意见了，k的选择可以参考coursera上的视频 选择sse下降最慢的那个拐点的k<br>
</span></p>
<p><span style="font-family: 微软雅黑;font-size: 10pt">网络上的尼采(813394698) 9:51:41<br>
</span></p>
<p><span style="font-size: 10pt"><span style="font-family: 微软雅黑">关于选哪个k是最优的比较主观，有从结果稳定性来考虑的，毕竟一个算法首先要保证的是多次运行后结果相差不大，关于这方面有一篇survey:</span><br>
<span style="font-family: 微软雅黑">Clustering stability an overview。另外还有其他自动选k的方法，DP、MDL什么的。MDL是最短描述长度，从压缩的角度来看k-means；DP是狄利克雷过程，一种贝叶斯无参方法，感兴趣可以看JORDAN小组的文章。<br>
</span></span></p>
<p> </p>
<p><span style="font-family: 微软雅黑;font-size: 10pt">我们下面讲混合高斯模型GMM，第二章我们说过，高斯分布有很多优点并且普遍存在，但是单峰函数，所以对于复杂的分布表达能力差，我们可以用多个高斯分布的线性组合来逼近这些复杂的分布。我们看下GMM的线性组合形式：<br>
</span></p>
<p><img src="/images/52nlp.cn/42e654495709a117c1093f0ac66a49e4.jpg" alt=""><span style="font-family: 宋体;font-size: 10pt"><br>
</span></p>
<p><span style="font-family: 微软雅黑;font-size: 10pt">对于每个数据点是哪个分布生成的，我们假设有个Z隐变量 ，和k-means类似，对于每个数据点都有一个向量z，如果是由第k个分布生成，元素z<sub>k</sub>=1,其他为0。<br>
z<sub>k</sub>=1的概率的先验就是高斯分布前的那个系数：<br>
</span></p>
<p><img src="/images/52nlp.cn/3377e6ba6f505ea07bb1e3e945251f39.jpg" alt=""><span style="font-size: 10pt"><br>
</span></p>
<p><span style="font-family: 微软雅黑;font-size: 10pt">下面是z<sub>k</sub>=1概率的后验，由贝叶斯公式推导的：<br>
</span></p>
<p><img src="/images/52nlp.cn/f90fba04fd491181c63ee115401aaf4b.jpg" alt=""><span style="font-family: 宋体;font-size: 10pt"><br>
</span></p>
<p><span style="font-family: 微软雅黑;font-size: 10pt">其实很好理解，如果没有<img src="/images/52nlp.cn/f555fa502a4ad8c0ba1e6e0a05cda131.jpg" alt="">限制，数据点由哪个分布得出的概率大z<sub>k</sub>=1的期望就大，但前面还有一个系数限制，所以期望形式是：<br>
</span></p>
<p><img src="/images/52nlp.cn/904a80fea2276aa374479de00127ed8f.jpg" alt=""><span style="font-size: 10pt"><span style="font-family: 微软雅黑"><br>
刚才有人问如何确定模型的参数，我们首先想到的就是log最大似然：<br>
<img src="/images/52nlp.cn/b105469cae1f0a59218f1218b98c6917.jpg" alt=""></span><span style="font-family: 宋体"><br>
</span></span></p>
<p><span style="font-size: 10pt"><span style="font-family: 微软雅黑">但是我们可以观察下这个目标函数，log里面有加和，求最优解是非常困难的，混合高斯和单个高斯的参数求法差别很大，如果里面有一个高斯分布坍缩成一个点，log似然函数会趋于无穷大。由于直接求解困难，这也是引入EM的原因。<br>
下面是GMM的图表示 ：<br>
<img src="/images/52nlp.cn/ca805a375bd0ed7af66d23a8ccc38f37.jpg" alt=""></span><span style="font-family: 宋体"><br>
</span></span></p>
<p><span style="font-size: 10pt"><span style="font-family: 微软雅黑">我们可以试想下，如果隐变量z<sub>n</sub>是可以观测的，也就是知道哪个数据点是由哪个分布生成的，那么我们求解就会很方便，可以利用高斯分布直接得到解析解。 但关键的是z<sub>n</sub>是隐变量，我们没法观测到。但是我们可以用它的期望来表示。 现在我们来看一下EM算法在GMM中的应用：<br>
<img src="/images/52nlp.cn/3e952447b6aff2f048828e89e7640b31.jpg" alt=""><br>
上面是EM对GMM的E步<br>
我们首先对模型的参数初始化<br>
E步就是我们利用这些参数得z<sub>nk</sub>的期望，这种形式我们前面已经提到了：<br>
<img src="/images/52nlp.cn/393125222a26a161318325c1342c9c23.jpg" alt=""></span><span style="font-family: 宋体"><br>
</span></span></p>
<p><span style="font-size: 10pt"><span style="font-family: 微软雅黑">现在我们有隐藏变量的期望了，由期望得新的模型参数也就是M步，高斯分布的好处就在这儿，可以推导出新参数的闭式解：<img src="/images/52nlp.cn/e868fc7713f28af916be0b413a18a9f8.jpg" alt=""></span><span style="font-family: 宋体"><br>
</span></span></p>
<p><span style="font-family: 微软雅黑;font-size: 10pt">然后不断迭代E步和M步直到满足收敛条件。<br>
</span></p>
<p><span style="font-family: 微软雅黑;font-size: 10pt">为什么要这么做，其实EM算法对我们前面提到的log最大似然目标函数：<img src="/images/52nlp.cn/8fc48c3f196073406ece346bd8582008.jpg" alt="">是单调递增的。<br>
karnon(447457116) 10:39:42<br>
用EM来解GMM其实是有问题的，解出来的解并不是最优的。。<br>
网络上的尼采(813394698) 10:40:14<br>
嗯，这个问题最后讲。<br>
</span></p>
<p><span style="font-family: 微软雅黑;font-size: 10pt">我们再来看EM更一般的形式：<br>
</span></p>
<p><img src="/images/52nlp.cn/b44ebca0a92fda2e432578bcca6a66ff.jpg" alt=""><span style="font-size: 10pt"><span style="font-family: 微软雅黑">是我们的目标函数，加入隐藏变量可以写成这种形式：<img src="/images/52nlp.cn/85a9b6d13d21ee510462edaf9d370390.jpg" alt=""></span><span style="font-family: 宋体"><br>
</span></span></p>
<p><span style="font-family: 微软雅黑;font-size: 10pt">先初始化模型的参数，由于隐变量无法观测到，我们用参数来得到它的后验<img src="/images/52nlp.cn/c4145dc79b970bc4da0511d536e839ad.jpg" alt=""><br>
</span></p>
<p><span style="font-family: 微软雅黑;font-size: 10pt">然后呢，我们通过隐藏变量的期望得到新的完整数据的最大似然函数：<br>
</span></p>
<p><img src="/images/52nlp.cn/683a3882cdac58430bfc03a110f6e148.jpg" alt=""><span style="font-family: 宋体;font-size: 10pt"><br>
</span></p>
<p><span style="font-size: 10pt"><span style="font-family: 微软雅黑">以上是E步</span><span style="font-family: 宋体">，</span><span style="font-family: 微软雅黑">M步是求这个似然函数的Q函数的最优解，也就是新的参数：<img src="/images/52nlp.cn/029f14c8859e95dd4a6baae4510b42ff.jpg" alt=""></span><span style="font-family: 宋体"><br>
</span></span></p>
<p><span style="font-size: 10pt"><span style="font-family: 微软雅黑">注意这个Q函数是包含隐变量的完整数据的似然函数，不是我们一开始的目标函数<img src="/images/52nlp.cn/3198a4df5965b0bdc03d8b1d2b1b1271.jpg" alt=""></span><span style="font-family: 宋体"><br>
</span></span></p>
<p><span style="font-family: 微软雅黑;font-size: 10pt">其实求完整数据的最大似然是在逼近我们目标函数的局部最优解，这个在后面讲。<br>
下面这个是一般化EM算法的步骤： <img src="/images/52nlp.cn/5d408efa7376e2982e2573bcc5b9f3d4.jpg" alt=""><br>
EM算法只所以用途广泛在于有潜在变量的场合都能用，并不局限于用在GMM上。现在我们回过头来看混合高斯模型的M步，这些得到的新参数就是Q函数的最优解：<br>
</span></p>
<p><img src="/images/52nlp.cn/7bb976ffbb06377117ecb49acfcdc7e7.jpg" alt=""><span style="font-family: 宋体;font-size: 10pt"><br>
</span></p>
<p><span style="font-size: 10pt"><span style="font-family: 微软雅黑">再思考下k-means，其实它是EM的特例，只不过是k-means对数据点的分配是硬性的，在E步每个数据点必须分配到一个簇，z里面只有一个1其他是0,而EM用的是z的期望：<img src="/images/52nlp.cn/417c395286d53e86de7c5bd8543d7de5.jpg" alt=""></span><span style="font-family: 宋体"><br>
</span></span></p>
<p><span style="font-size: 10pt"><span style="font-family: 微软雅黑">下面这个图说明k-means是EM算法特例，这与前面k-means聚类的过程的图对应：</span><span style="font-family: 宋体"><br>
</span></span></p>
<p><img src="/images/52nlp.cn/369c4c7aaa8e231d1879d3dc681018e1.jpg" alt=""><span style="font-family: 微软雅黑;font-size: 10pt"><br>
对于EM算法性质的证明最后讲，下面讲混合伯努利模型，高斯分布是针对连续的属性，伯努利是针对离散属性。混合形式：<br>
</span></p>
<p><img src="/images/52nlp.cn/a6983d91f6455e55e54452a870159ab7.jpg" alt=""><span style="font-family: 宋体;font-size: 10pt"><br>
</span></p>
<p><span style="font-family: 微软雅黑;font-size: 10pt"> 目标函数，log里面同样有加和：<br>
</span></p>
<p><img src="/images/52nlp.cn/46a87a121aedde9f18f311a15ca1b0fc.jpg" alt=""><span style="font-size: 10pt"><span style="font-family: 宋体">，</span><span style="font-family: 微软雅黑">。<br>
z<sub>nk</sub>=1的期望：<br>
<img src="/images/52nlp.cn/f996995179951169639a0a15e5dcef4a.jpg" alt=""></span><br>
</span></p>
<p><span style="font-family: 微软雅黑;font-size: 10pt">Q函数：<br>
</span></p>
<p><img src="/images/52nlp.cn/9c7307b5bcc431605d934b5dc784949c.jpg" alt=""><span style="font-family: 微软雅黑;font-size: 10pt"><br>
以上是E步，下面是M步的解，这两个：<br>
</span></p>
<p><img src="/images/52nlp.cn/5e93c35848ad64f7454b2507a7b78faf.jpg" alt=""><img src="/images/52nlp.cn/96fe6ca703ed64539952382065c4e591.jpg" alt=""><span style="font-family: 微软雅黑;font-size: 10pt"><br>
</span></p>
<p><span style="font-family: 微软雅黑;font-size: 10pt">其中：<br>
</span></p>
<p><img src="/images/52nlp.cn/e361a37822c7dfbffc5e5c5a93cc6062.jpg" alt=""><span style="font-family: 微软雅黑;font-size: 10pt"><br>
</span></p>
<p><span style="font-family: 微软雅黑;font-size: 10pt">书里举了一个手写字聚类的例子 ，先对像素二值化 ，然后聚成3个簇：<br>
<img src="/images/52nlp.cn/ada06663b0f148ef71d3b8792a8a36a6.jpg" alt=""><br>
这是3个簇的代表：<br>
</span></p>
<p><img src="/images/52nlp.cn/2db3ba32db5c8cadb786ac0fddfabbd8.jpg" alt=""><span style="font-family: 微软雅黑;font-size: 10pt"><br>
</span></p>
<p><span style="font-family: 微软雅黑;font-size: 10pt">k=1时簇的代表：<br>
<img src="/images/52nlp.cn/0ac12718dee9547a9fe6c4e2f1286d14.jpg" alt=""><br>
</span></p>
<p><span style="color: #333333;font-family: Arial;font-size: 10pt">=============================================================<br>
</span></p>
<p><span style="font-family: 微软雅黑;font-size: 10pt">最后说下EM算法为什么能收敛到似然函数的局部最优解:<br>
</span></p>
<p><span style="font-family: 微软雅黑;font-size: 10pt">我们的目标函数是分布<img src="/images/52nlp.cn/1414bf376650d7991084242f378aca24.jpg" alt="">的最大log似然<br>
</span></p>
<p><span style="font-size: 10pt"><span style="font-family: 微软雅黑">引入潜在变量，分布表示为：<img src="/images/52nlp.cn/f244d5d0eaabe1ce163db96988479a59.jpg" alt=""></span><span style="font-family: 宋体"><br>
</span></span></p>
<p><span style="font-family: 微软雅黑;font-size: 10pt">定义隐藏变量的分布为q(z)，目标函数可以表达为下面形式，这个地方是神来一笔：<br>
</span></p>
<p><img src="/images/52nlp.cn/0ab8563bdfb9c7320e15ca59c948db9a.jpg" alt=""><span style="font-family: 宋体;font-size: 10pt"><br>
</span></p>
<p><span style="font-size: 10pt"><span style="font-family: 微软雅黑">其中<img src="/images/52nlp.cn/2a6f7fa880f92e0af9997c915665ec2e.jpg" alt="">是<img src="/images/52nlp.cn/6d84d3f5481ef52a5df3648ff85f18fc.jpg" alt="">与q(z)的KL散度；<img src="/images/52nlp.cn/c30e96d7892982975ffacadeebc3afcf.jpg" alt="">是q(z)的泛函形式。<br>
我们原来讲过根据Jensen不等式来证明KL散度是非负的，这个性质在这里发挥了作用。由于<img src="/images/52nlp.cn/fa4f4f3b02293115dac180d475e553c1.jpg" alt="">是大于等于零的，所以<img src="/images/52nlp.cn/b45384983a6c3f39edeb181aa6c481ba.jpg" alt="">是目标函数<img src="/images/52nlp.cn/d2a8b72a1a991c24e843619168652dcc.jpg" alt="">的下界。<img src="/images/52nlp.cn/9284d99061fd7b10adc0cdb1c73a5912.jpg" alt="">与目标函数什么时候相等呢？其实就是<img src="/images/52nlp.cn/8ab6be979e26dde23bb1f5571d315cb4.jpg" alt="">等于0的时候，也就是q(z)与z的后验分布<img src="/images/52nlp.cn/e58cf86ef6e20f15298e720e16245974.jpg" alt="">相同时，这个时候就是E步z取它的期望时候。<img src="/images/52nlp.cn/ed9fe9cd163a25c7015d7d2c576abf6c.jpg" alt="">与目标函数相等是为了取一个比较紧的bound。<br>
M步就是最大化<img src="/images/52nlp.cn/50c80aed6038d7de0dd64f67c310a8a6.jpg" alt="">，随着<img src="/images/52nlp.cn/70edc6f0495ceac5a10da1dd7b7382b3.jpg" alt="">的增大，<img src="/images/52nlp.cn/067af07d7fb1cbd0dadcc0ce7165b7d6.jpg" alt="">开始大于0，也就是目标函数比<img src="/images/52nlp.cn/2083ed7c66c75b0a75f86ebced03a246.jpg" alt="">增大的幅度更大。这个过程是使目标函数一直单调递增的。下面是<img src="/images/52nlp.cn/489009f24edc0393ae3e99a5de227554.jpg" alt="">与Q函数的关系，这也是为什么M步新参数取完整数据的最大似然解的原因：<br>
<img src="/images/52nlp.cn/ec93cb053b7fe2ccdd0e154e54c8f46f.jpg" alt=""></span><span style="font-family: 宋体"><br>
</span></span></p>
<p style="margin-left: 15pt"><span style="font-family: 微软雅黑;font-size: 10pt">最后上一张非常形象的图，解释为什么EM能收敛到目标函数的局部最优：<br>
<img src="/images/52nlp.cn/be83534220e9ff3f7423041e7e437345.jpg" alt=""><br>
</span></p>
<p><span style="font-size: 10pt"><span style="font-family: 微软雅黑">红的曲线是目标函数；蓝的绿的曲线是两步迭代。<br>
咱们先看蓝的E步和M步：<br>
<img src="/images/52nlp.cn/0772ad51bdbc2259e5de5d05d424c298.jpg" alt=""></span><span style="font-family: 宋体"><br>
</span></span></p>
<p><span style="font-family: 微软雅黑;font-size: 10pt">E步时就是取z的期望的时候，这时目标函数与<img src="/images/52nlp.cn/0d6b36f529709da422275ec9c84ba62d.jpg" alt="">相同；<br>
M步就是最大化<img src="/images/52nlp.cn/768d61ba74934368420071b414838037.jpg" alt=""><br>
绿线是下一轮的迭代，EM过程中，目标函数一直是单调上升的，并且有界，所以EM能够保证收敛。但不一定能收敛到最优解，这与初始值有很大关系，试想一下，目标函数的曲线变动下，EM就有可能收敛到局部最优了。</span></p>
<p>注：PRML读书会系列文章由 <a href="http://weibo.com/dmalgorithms">@Nietzsche_复杂网络机器学习</a> 同学授权发布，转载请注明原作者和相关的主讲人，谢谢。</p>
<p>PRML读书会讲稿PDF版本以及更多资源下载地址：<a href="http://vdisk.weibo.com/u/1841149974">http://vdisk.weibo.com/u/1841149974</a></p>
<p>本文链接地址：<a href="http://www.52nlp.cn/prml%E8%AF%BB%E4%B9%A6%E4%BC%9A%E7%AC%AC%E4%B9%9D%E7%AB%A0-mixture-models-and-em">http://www.52nlp.cn/prml读书会第九章-mixture-models-and-em</a></p>

  </article>

  <div class="meta">

    <a class="basic-alignment left" href="/posts/2015/2015-01-30-prml%25e8%25af%25bb%25e4%25b9%25a6%25e4%25bc%259a%25e7%25ac%25ac%25e4%25b8%2583%25e7%25ab%25a0-sparse-kernel-machines-9458d994d.html" title="Previous Post: PRML读书会第七章 Sparse Kernel Machines" data-instant>&laquo; PRML读书会第七章 Sparse Kernel Machines</a>

    <a class="basic-alignment right" href="/posts/2015/2015-01-31-prml%25e8%25af%25bb%25e4%25b9%25a6%25e4%25bc%259a%25e7%25ac%25ac%25e5%2585%25ab%25e7%25ab%25a0-graphical-models-7cc076ef3.html" title="Next Post: PRML读书会第八章  Graphical Models" data-instant>PRML读书会第八章  Graphical Models &raquo;</a>

</div>
  <div id="related">
  <h2 class="subheader">Related Posts <small>They might be useful</small></h2>
  <ul class="posts">

      <li><span>07 Feb 2017</span> &raquo; <a href="http://iftti.com/posts/2017/2017-02-07-110052-111b13f41.html">研发团队 GIT 开发流程新人学习指南</a></li>

      <li><span>06 Feb 2017</span> &raquo; <a href="http://iftti.com/posts/2017/2017-02-06-110076-fcd8bd876.html">垃圾回收原来是这么回事</a></li>

      <li><span>06 Feb 2017</span> &raquo; <a href="http://iftti.com/posts/2017/2017-02-06-110072-cea0048e7.html">每天一个 Linux 命令（59）： rcp 命令</a></li>

  </ul>
</div>

<comments>

<!-- UY BEGIN -->
<div id="uyan_frame"></div>
<script type="text/javascript" src="http://v2.uyan.cc/code/uyan.js?uid=1936498"></script>
<!-- UY END -->

</comments>

</div>
      <!-- JiaThis Button BEGIN -->
<div class="jiathis_share_slide jiathis_share_24x24" id="jiathis_share_slide">
<div class="jiathis_share_slide_top" id="jiathis_share_title"></div>
<div class="jiathis_share_slide_inner">
<div class="jiathis_style_24x24">
<a class="jiathis_button_tsina"></a>
<a class="jiathis_button_googleplus"></a>
<a class="jiathis_button_twitter"></a>
<a class="jiathis_button_linkedin"></a>
<a class="jiathis_button_weixin"></a>
<a class="jiathis_button_cqq"></a>
<a class="jiathis_button_renren"></a>
<a class="jiathis_button_evernote"></a>
<a class="jiathis_button_pocket"></a>
<a href="http://www.jiathis.com/share" class="jiathis jiathis_txt jtico jtico_jiathis" target="_blank"></a>
<script type="text/javascript">
var jiathis_config = {data_track_clickback:'true'
	,slide:{
		divid:'wrap',
		pos:'left',
		gt:'true'
	}
};
</script>
<script type="text/javascript" src="http://v3.jiathis.com/code/jia.js?uid=1936498" charset="utf-8"></script>
<script type="text/javascript" src="http://v3.jiathis.com/code/jiathis_slide.js" charset="utf-8"></script>
</div></div></div>
<!-- JiaThis Button END -->
      </div>
    </div>

    <footer class="site-footer">

  <div class="wrap">

    <h2 class="footer-heading">IT技术干货</h2>

    <div class="footer-col-1 column">
      <p class="text">IT技术干货 KernelHacks 最好的技术站点 技术信息 纯干货</p>
      <ul>
        <li>汇集最好的科技与互联网信息</li>
        <li>Liu Lantao</li>
        <li><a href="mailto:iftti@iftti.com">iftti@iftti.com</a></li>
      </ul>
    </div>

    <div class="footer-col-2 column">
      <ul>
        <li>
          <a href="https://github.com/Lax">
            <span class="icon github">
              <svg version="1.1" class="github-icon-svg" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
                 viewBox="0 0 16 16" enable-background="new 0 0 16 16" xml:space="preserve">
                <path fill-rule="evenodd" clip-rule="evenodd" fill="#C2C2C2" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761
                c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32
                c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472
                c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037
                C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65
                c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261
                c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082
                c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129
                c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/>
              </svg>
            </span>
            <span class="username">Lax</span>
          </a>
        </li>
        <li>
          <a href="https://twitter.com/liulantao">
            <span class="icon twitter">
              <svg version="1.1" class="twitter-icon-svg" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
                 viewBox="0 0 16 16" enable-background="new 0 0 16 16" xml:space="preserve">
                <path fill="#C2C2C2" d="M15.969,3.058c-0.586,0.26-1.217,0.436-1.878,0.515c0.675-0.405,1.194-1.045,1.438-1.809
                c-0.632,0.375-1.332,0.647-2.076,0.793c-0.596-0.636-1.446-1.033-2.387-1.033c-1.806,0-3.27,1.464-3.27,3.27
                c0,0.256,0.029,0.506,0.085,0.745C5.163,5.404,2.753,4.102,1.14,2.124C0.859,2.607,0.698,3.168,0.698,3.767
                c0,1.134,0.577,2.135,1.455,2.722C1.616,6.472,1.112,6.325,0.671,6.08c0,0.014,0,0.027,0,0.041c0,1.584,1.127,2.906,2.623,3.206
                C3.02,9.402,2.731,9.442,2.433,9.442c-0.211,0-0.416-0.021-0.615-0.059c0.416,1.299,1.624,2.245,3.055,2.271
                c-1.119,0.877-2.529,1.4-4.061,1.4c-0.264,0-0.524-0.015-0.78-0.046c1.447,0.928,3.166,1.469,5.013,1.469
                c6.015,0,9.304-4.983,9.304-9.304c0-0.142-0.003-0.283-0.009-0.423C14.976,4.29,15.531,3.714,15.969,3.058z"/>
              </svg>
            </span>
            <span class="username">@liulantao</span>
          </a>
        </li>
        <li>
          <a href="https://plus.google.com/+LiuLantao">
            <span class="icon googleplus">
              <svg version="1.1" id="Layer_1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
                width="16px" height="16px" viewBox="0 0 134.658 131.646" enable-background="new 0 0 134.658 131.646"
                xml:space="preserve">
                <g>
                  <path fill="#C2C2C2" d="M126.515,4.109H8.144c-2.177,0-3.94,1.763-3.94,3.938v115.546c0,2.179,1.763,3.942,3.94,3.942h118.371
                  c2.177,0,3.94-1.764,3.94-3.942V8.048C130.455,5.872,128.691,4.109,126.515,4.109z"/>
                  <g>
                    <path fill="#FFFFFF" d="M70.479,71.845l-3.983-3.093c-1.213-1.006-2.872-2.334-2.872-4.765c0-2.441,1.659-3.993,3.099-5.43
                    c4.64-3.652,9.276-7.539,9.276-15.73c0-8.423-5.3-12.854-7.84-14.956h6.849l7.189-4.517H60.418
                    c-5.976,0-14.588,1.414-20.893,6.619c-4.752,4.1-7.07,9.753-7.07,14.842c0,8.639,6.633,17.396,18.346,17.396
                    c1.106,0,2.316-0.109,3.534-0.222c-0.547,1.331-1.1,2.439-1.1,4.32c0,3.431,1.763,5.535,3.317,7.528
                    c-4.977,0.342-14.268,0.893-21.117,5.103c-6.523,3.879-8.508,9.525-8.508,13.51c0,8.202,7.731,15.842,23.762,15.842
                    c19.01,0,29.074-10.519,29.074-20.932C79.764,79.709,75.344,75.943,70.479,71.845z M56,59.107
                    c-9.51,0-13.818-12.294-13.818-19.712c0-2.888,0.547-5.87,2.428-8.199c1.773-2.218,4.861-3.657,7.744-3.657
                    c9.168,0,13.923,12.404,13.923,20.382c0,1.996-0.22,5.533-2.762,8.09C61.737,57.785,58.762,59.107,56,59.107z M56.109,103.65
                    c-11.826,0-19.452-5.657-19.452-13.523c0-7.864,7.071-10.524,9.504-11.405c4.64-1.561,10.611-1.779,11.607-1.779
                    c1.105,0,1.658,0,2.538,0.111c8.407,5.983,12.056,8.965,12.056,14.629C72.362,98.542,66.723,103.65,56.109,103.65z"/>
                    <polygon fill="#FFFFFF" points="98.393,58.938 98.393,47.863 92.923,47.863 92.923,58.938 81.866,58.938 81.866,64.469
                    92.923,64.469 92.923,75.612 98.393,75.612 98.393,64.469 109.506,64.469 109.506,58.938"/>
                  </g>
                </g>
              </svg>
            </span>
            <span class="username">+LiuLantao</span>
          </a>
        </li>
      </ul>
    </div>

    <div class="footer-col-3 column">

<!--以下是QQ邮件列表订阅嵌入代码--><script >var nId = "6be92ef3590ee662cd5e6381ab2044c328716364f684cf3e",nWidth="auto",sColor="light",sText="填写您的邮件地址，订阅我们的精彩内容：" ;</script><script src="http://list.qq.com/zh_CN/htmledition/js/qf/page/qfcode.js" charset="gb18030"></script>

    </div>

  </div>

  <div class="wrap">
    <div>
      <a href="http://blog.liulantao.com">Blog</a> | <a href="http://1000bit.com">铅笔特评 1000bit</a> | <a href="http://visplanet.com">VisPlanet</a> | <a href="http://iftti.com">IT技术干货</a> | <a href="http://relax.org.cn">Relax</a> | <a href="http://hangzhou.io">杭州城市指南</a>
    </div>
  </div>

<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-1658815-7', 'iftti.com');
  ga('require', 'displayfeatures');
  ga('send', 'pageview');

</script>

</footer>

    </body>
</html>