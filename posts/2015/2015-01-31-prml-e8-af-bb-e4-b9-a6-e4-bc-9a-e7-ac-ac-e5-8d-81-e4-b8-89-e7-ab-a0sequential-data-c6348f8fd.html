<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>PRML读书会第十三章 Sequential Data | IT技术干货</title>
    <meta name="viewport" content="width=device-width">
    <meta name="description" content="IT技术干货 KernelHacks 最好的技术站点 技术信息 纯干货">
    <link rel="canonical" href="http://iftti.com/posts/2015/2015-01-31-prml-e8-af-bb-e4-b9-a6-e4-bc-9a-e7-ac-ac-e5-8d-81-e4-b8-89-e7-ab-a0sequential-data-c6348f8fd.html">

    <!-- Custom CSS -->
    <link rel="stylesheet" href="/css/main.css">

</head>


    <body>

    <header class="site-header">

  <div class="wrap">

    <a class="site-title" href="/">IT技术干货</a>

    <nav class="site-nav">
      <a href="#" class="menu-icon">
        <svg version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
           viewBox="0 0 18 15" enable-background="new 0 0 18 15" xml:space="preserve">
          <path fill="#505050" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0
            h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
          <path fill="#505050" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484
            h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
          <path fill="#505050" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0
            c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
        </svg>
      </a>
      <div class="trigger">
        
          
        
          <a class="page-link" href="/about/">About</a>
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
      </div>
    </nav>

  </div>

</header>


    <div class="page-content">
      <div class="wrap">
      <div class="post">

  <header class="post-header">
    <h1>PRML读书会第十三章 Sequential Data</h1>
    <p class="meta"><span class="time">Jan 31, 2015</span><span class="source_url"> • 来自 52nlp.cn <a name="52nlp.cn" href="http://www.52nlp.cn/prml%e8%af%bb%e4%b9%a6%e4%bc%9a%e7%ac%ac%e5%8d%81%e4%b8%89%e7%ab%a0sequential-data" target="_blank">[原文链接]</a></span></p>
  </header>

  <article class="post-content">
  

						<p style="text-align: center"><span style="font-family: 微软雅黑;font-size: 15pt"><strong>PRML读书会第十三章 Sequential Data<br>
</strong></span></p>
<p style="text-align: center"><span style="font-family: 微软雅黑;font-size: 15pt"><strong>主讲人 张巍<br>
</strong></span></p>
<p style="text-align: center"><span style="font-family: 微软雅黑;font-size: 12pt"><strong>（新浪微博: <a href="http://weibo.com/zh3f">@张巍_ISCAS</a>）<br>
</strong></span></p>
<p><span style="font-family: 微软雅黑;font-size: 12pt">软件所-张巍&lt;zh3f@qq.com&gt; 19:01:27<br>
<img src="/images/52nlp.cn/b2d833e8575fcbb5b4f1ec58406c77b6.jpg" alt="">我们开始吧，十三章是关于序列数据，现实中很多数据是有前后关系的，例如语音或者DNA序列，例子就不多举了，对于这类数据我们很自然会想到用马尔科夫链来建模：<br>
<img src="/images/52nlp.cn/2e5c659d66f02ae49b31b82411631439.jpg" alt=""><br>
</span></p>
<p><span style="font-family: 微软雅黑;font-size: 12pt">例如直接假设观测数据之间服从一阶马尔科夫链，这个假设显然太简单了，因为很多数据时明显有高阶相关性的，一个解决方法是用高阶马尔科夫链建模：<br>
<img src="/images/52nlp.cn/bd5dfce990fa9c0635b437cc6fdea541.jpg" alt=""><br>
</span></p>
<p><span style="font-family: 微软雅黑;font-size: 12pt">但这样并不能完全解决问题 ：1、高阶马尔科夫模型参数太多；2、数据间的相关性仍然受阶数限制。一个好的解决方法，是引入一层隐变量，建立如下的模型：<br>
<img src="/images/52nlp.cn/bfafcf38a00ed52d7690969f5d4c040a.jpg" alt=""><br>
</span><span id="more-8012"></span></p>
<p><span style="font-family: 微软雅黑;font-size: 12pt">这里我们假设隐变量之间服从一阶马尔科夫链，观测变量由其对应的隐变量生成。从上图可以看出，隐变量是一阶的，但是观测变量之间是全相关的，今天我们主要讨论的就是上图中的模型。如果隐变量是离散的，我们称之为Hidden Markov Models；如果是连续的，我们称之为: Linear Dynamical Systems。现在我们先来看一下HMM ，从图中可以看出，要完成建模，我们需要指定一下几个分布：<br>
1、转移概率：<br>
<img src="/images/52nlp.cn/724c6c884c2aa14cbb92c0b680bcbc82.jpg" alt=""><br>
2、马尔科夫链的初始概率：<br>
<img src="/images/52nlp.cn/2e57507f0cba5e24f26a8f49110e4e29.jpg" alt=""><br>
3、生成观测变量的概率(emission probabilities)：<br>
<img src="/images/52nlp.cn/cad416f3063209bb94d0537f9da27f2b.jpg" alt=""><br>
对于HMM， 这里1和2我们已经假设成了离散分布，由隐变量Zn生成观测数据可以用混合高斯模型或者神经网络，书上的Zn是一个k维的布尔变量，由此再看隐变量转移概率公式、观测数据的生成公式就容易理解了。模型建好了，我们接下来主要讨论下面三个问题：<br>
1、学习问题：就是学习模型中的参数；<br>
2、预测问题：即<img src="/images/52nlp.cn/bd3a3423f0fadb3cf704b61875848e91.jpg" alt="">,给定当前序列预测下一个观测变量；<br>
3、解码问题：即p(Z|X)，给定观测变量求隐变量，例如语音识别；<br>
</span></p>
<p><span style="font-family: 微软雅黑;font-size: 12pt">游侠(419504839) 19:24:21<br>
什么是解码问题？<br>
软件所-张巍&lt;zh3f@qq.com&gt; 19:25:18<br>
例如观测到了一段语音，要求识别其对应的句子。@游侠 我前面没怎么举例子，不知道这样说清楚没？<br>
游侠(419504839) 19:27:20<br>
这个和一般说的”推断”一样不<br>
软件所-张巍&lt;zh3f@qq.com&gt; 19:28:27<br>
这个也可以叫推断，只是推断是个比较一般的词汇。<br>
我们来看一下HMM有多少参数要学，对应于刚才说到的三个分布，我们也有三组参数要学。<br>
球猫(250992259) 19:30:46<br>
其实就是假设东西是一个马尔科夫模型生成的。。然后把参数用某种方法弄出来，最后根据模型的输出来给答案……是这样吧？<br>
</span></p>
<p><span style="font-family: 微软雅黑;font-size: 12pt">软件所-张巍&lt;zh3f@qq.com&gt; 19:32:45<br>
@球猫 对，都是这个思路，先把参数学出来，然后就可以做任何想要的推断了，在这里所谓的解码问题只是大家比较关心。<br>
软件所-张巍&lt;zh3f@qq.com&gt; 19:32:51<br>
好，继续，我们先来看1、学习问题。这里我们用EM算法来学习HMM的参数：<br>
1、是转移概率对应的转移矩阵；<br>
2、初始概率对应的离散分布参数；<br>
3、观测变量对应的分布参数（这里暂不指定）。<br>
用EM我们要做的就是：<br>
E步里根据当前参数估计隐变量的后验：<br>
</span></p>
<p><img src="/images/52nlp.cn/736a2e2a12b64dc79340bae7a36e93b2.jpg" alt=""><span style="font-family: 微软雅黑;font-size: 12pt"><br>
M步里最大化下面的期望：<br>
<img src="/images/52nlp.cn/edc13aa9704330a6b35cee1c45f8dfac.jpg" alt=""><br>
先来看M步，这里相对简单一点，整个模型的全概率展开为：<br>
<img src="/images/52nlp.cn/49a48650a8c699e258264e3e1fc9f41f.jpg" alt=""><br>
把13.10代入13.12,我们会发现计算时需要下面两个式子：<br>
<img src="/images/52nlp.cn/2cc983dfd30f3a84fd820ef3cc01e2ac.jpg" alt="">和<img src="/images/52nlp.cn/655a0b47e54ec5ba75b34946b2a52566.jpg" alt=""><br>
为了方便，我们就定义：<br>
<img src="/images/52nlp.cn/2ffb982cf3a17d0f6bb3a0c45d7eadbe.jpg" alt=""><br>
这样我们在E步就主要求出这两个式子就行了，当然这也就意味着求出了整个后验<img src="/images/52nlp.cn/fa5dff7cd3c680b2ba3955977f2eb213.jpg" alt="">，利用这两个式子，13.12可以化为：<br>
<img src="/images/52nlp.cn/30e09c02fac5b09c9474893e0e923f4f.jpg" alt=""><br>
这个时候就可以用一些通用方法，例如Lagrange来求解了，结果也很简单：<br>
<img src="/images/52nlp.cn/25d0a9dd4c0635c6729c565a2c1ede6a.jpg" alt=""><br>
对于观测变量的分布参数，与其具体分布形式相关，如果是高斯：<img src="/images/52nlp.cn/2e3bcd1fff2e331832a6a8ff2025cdb9.jpg" alt="">，对应的最优解为：<br>
<img src="/images/52nlp.cn/d70bf206853be75c5a78a7ecfd950521.jpg" alt=""></span></p>
<p>如果是离散：<img src="/images/52nlp.cn/abc79c3994b7f946fc5cdd08971202eb.jpg" alt="">对应的最优解为：<br>
<img src="/images/52nlp.cn/0688d4a65e4c1980384f99fb062fd6da.jpg" alt=""><br>
好，M步就这样， 现在来看E步，也是HMM比较核心的地方。刚才我们看到，E步需要求的是：<img src="/images/52nlp.cn/98b7d691808cb81f5d285f813cf737f9.jpg" alt=""><br>
由马尔科夫的性质，我们可以推出：<br>
<img src="/images/52nlp.cn/861783585412686f6dd662e1623f6fd2.jpg" alt=""></p>
<p><span style="font-family: 微软雅黑;font-size: 12pt">其中：<br>
</span></p>
<p><img src="/images/52nlp.cn/2d4f6a26ff07fbf58b31f1030a0a93f7.jpg" alt=""><span style="font-family: 微软雅黑;font-size: 12pt"><br>
接下来我们就建立<img src="/images/52nlp.cn/2f6b06a9c1f4c46ffc7927fc97bed61f.jpg" alt="">和<img src="/images/52nlp.cn/a6665b323030957ae42dd9831458c5a6.jpg" alt="">的递推公式<br>
<img src="/images/52nlp.cn/745f4de9d42f2d7a7614952d11d0abe3.jpg" alt=""><br>
其中：<br>
<img src="/images/52nlp.cn/87c6ad87d323a62678deb0dcde410129.jpg" alt=""><br>
这样我们从<img src="/images/52nlp.cn/48480bd6d6f051edcadeac0e151ee3fd.jpg" alt="">开始，可以递推出所有的<img src="/images/52nlp.cn/c9e0ebb93282120bdfe5c3621cbfabf7.jpg" alt="">，对于<img src="/images/52nlp.cn/72582463aa548d1997b0c374ff19b0ca.jpg" alt="">，也进行类似的推导：<br>
<img src="/images/52nlp.cn/f5cf1a95b03270463b50d95197da6576.jpg" alt=""><br>
</span></p>
<p><img src="/images/52nlp.cn/d45a4ae8080f65a83944c0916cfcc763.jpg" alt=""><span style="font-family: 微软雅黑;font-size: 12pt"><br>
从上式可以看到<img src="/images/52nlp.cn/928a4ff5354d175201f0199c8d1a8ad6.jpg" alt="">是一个逆推过程，所以我们需要初始值<img src="/images/52nlp.cn/c822f0a47b1d789a0ad6f9c91d85681a.jpg" alt="">，定义13.35并没有明确<img src="/images/52nlp.cn/c246542550e79656ad3e3126a2b0ae73.jpg" alt="">的定义：<br>
<img src="/images/52nlp.cn/38658037963f1aa9adcef132498ba581.jpg" alt=""><br>
因为z_N后没有观测数据，不过我们可以从<img src="/images/52nlp.cn/d57ab1a808c48c51dd22321d8dcfd55e.jpg" alt="">，得出:<br>
<img src="/images/52nlp.cn/58f44a00f48d59b728995fda22b9fcd6.jpg" alt=""><br>
这样<img src="/images/52nlp.cn/c75f6ae6ac230447236788b2309bd3f5.jpg" alt="">就等于1，现在我们可以方便的求出所有的<img src="/images/52nlp.cn/6a4a993e1090f69f4e3ceed436971b2a.jpg" alt="">和<img src="/images/52nlp.cn/4de686ea532dc33aa70d2783f9978db8.jpg" alt="">了，利用13.13也就可以求出所有的<img src="/images/52nlp.cn/f6a0e3e535c29cf0b07f88793d188a62.jpg" alt="">。类似的，我们可以求出<img src="/images/52nlp.cn/8b9c67e0d48e355537f650bd7a80ec11.jpg" alt="">：<br>
<img src="/images/52nlp.cn/6cff3d49ac9484aeb5eeef2bd4be2e67.jpg" alt=""><br>
这样在M步里求解所需要的分布就都求出来了，也就可以用EM来学习HMM的参数了，这里式子比较多，大家自己推一下会比较好理解。<br>
</span></p>
<p><span style="font-family: 微软雅黑;font-size: 12pt">第一个学习问题就这样了，接下来是预测问题，预测问题可以直接推导：<br>
<img src="/images/52nlp.cn/eca6ce061f900bfec03e84261f76aaa7.jpg" alt=""><br>
现在就剩最后一个解码问题，也就是argmax_Z{p(Z|X)}，刚才我们在E步已经求出了：<br>
<img src="/images/52nlp.cn/cf9adba69ca23e34bf5cac068fa17a22.jpg" alt=""><br>
但是现在的问题要复杂一点，因为我们要求概率最大的隐变量序列，用13.13可以求出单个隐变量，但是他们连在一起形成的整个序列可能概率很小，这个问题可以归结为一个动态规划：<br>
<img src="/images/52nlp.cn/06e01ca9d2702eeb138a498396f80666.jpg" alt=""><br>
</span></p>
<p><span style="font-family: 微软雅黑;font-size: 12pt">我们把HMM化成如上图的样子，最大化后验等价于最大化全概率，对于上图中的边，我们赋值为：<br>
log（<img src="/images/52nlp.cn/de3fdc919b228a823017f3f142235fe1.jpg" alt="">）<br>
初始节点赋值为：<br>
log(<img src="/images/52nlp.cn/e5271c8f4eab4d4457c02faad2afe4ef.jpg" alt="">*p(x_1|z_1)）<br>
其余节点赋值为：<br>
log(<img src="/images/52nlp.cn/2e47bc456f485948b0ad7a1ea2944c80.jpg" alt="">)<br>
这样任何一个序列Z，其全概率等于exp（Z对应路径上节点和边的值求和），这样，解码问题就转化为一个最长路径问题，用动态规划可以直接求解。大家看这里有没有问题，HMM的主要内容就是这些<img src="/images/52nlp.cn/c13ee5623f49d3491b6f040561f82f15.jpg" alt=""><br>
</span></p>
<p><span style="font-family: 微软雅黑;font-size: 12pt">接下来的Linear Dynamical Systems 其实和HMM大同小异，只是把离散分布换成了高斯，然后就是第二章公式的反复应用，都是细节问题，就不在这里讲了，大家看看有问题我们可以讨论。这一章还是式子主导的，略过了不少式子，大家推的时候有问题我们可以随时讨论。<br>
</span></p>
<p><span style="font-family: 微软雅黑;font-size: 12pt">天涯游(872352128) 21:09:21<br>
我对hmm 的理解，觉得这麻烦的是概率的理解的了，概率分解才是hmm的核心，当然了还有动态规划了。概率分解其实是实验事件的分解，如前向 和后向了，还有就是EM算法了。<br>
</span></p>
<p>注：PRML读书会系列文章由 <a href="http://weibo.com/dmalgorithms">@Nietzsche_复杂网络机器学习</a> 同学授权发布，转载请注明原作者和相关的主讲人，谢谢。</p>
<p>PRML读书会讲稿PDF版本以及更多资源下载地址：<a href="http://vdisk.weibo.com/u/1841149974">http://vdisk.weibo.com/u/1841149974</a></p>
<p>本文链接地址：<a href="prml%E8%AF%BB%E4%B9%A6%E4%BC%9A%E7%AC%AC%E5%8D%81%E4%B8%89%E7%AB%A0sequential-data">http://www.52nlp.cn/prml读书会第十三章sequential-data</a></p>
<p><span style="font-family: 微软雅黑;font-size: 12pt"><br>
</span></p>

											


  </article>

  <div class="meta">
  
    <a class="basic-alignment left" href="/posts/2015/2015-01-31-prml-e8-af-bb-e4-b9-a6-e4-bc-9a-e7-ac-ac-e5-8d-81-e4-b8-80-e7-ab-a0-sampling-methods-ed6b661c1.html" title="Previous Post: PRML读书会第十一章  Sampling Methods" data-instant>&laquo; PRML读书会第十一章  Sampling Methods</a>
  
  
    <a class="basic-alignment right" href="/posts/2015/2015-01-31-prml-e8-af-bb-e4-b9-a6-e4-bc-9a-e7-ac-ac-e5-8d-81-e4-ba-8c-e7-ab-a0-continuous-latent-variables-cad03f970.html" title="Next Post: PRML读书会第十二章 Continuous Latent Variables" data-instant>PRML读书会第十二章 Continuous Latent Variables &raquo;</a>
  
</div>
  <div id="related">
  <h2 class="subheader">Related Posts <small>They might be useful</small></h2>
  <ul class="posts">
    
      <li><span>22 May 2018</span> &raquo; <a href="http://iftti.com/posts/2018/2018-05-22-114031-4382c32a2.html">详解 Linux 文档属性、拥有者、群组、权限、差异</a></li>
    
      <li><span>21 May 2018</span> &raquo; <a href="http://iftti.com/posts/2018/2018-05-21-114016-1b3612333.html">分布式之消息队列复习精讲</a></li>
    
      <li><span>20 May 2018</span> &raquo; <a href="http://iftti.com/posts/2018/2018-05-20-114009-9d318dfaa.html">分布式之延时任务方案解析</a></li>
    
  </ul>
</div>

  
<comments>

<!-- UY BEGIN -->
<div id="uyan_frame"></div>
<script type="text/javascript" src="http://v2.uyan.cc/code/uyan.js?uid=1936498"></script>
<!-- UY END -->

</comments>

</div>
      <!-- JiaThis Button BEGIN -->
<div class="jiathis_share_slide jiathis_share_24x24" id="jiathis_share_slide">
<div class="jiathis_share_slide_top" id="jiathis_share_title"></div>
<div class="jiathis_share_slide_inner">
<div class="jiathis_style_24x24">
<a class="jiathis_button_tsina"></a>
<a class="jiathis_button_googleplus"></a>
<a class="jiathis_button_twitter"></a>
<a class="jiathis_button_linkedin"></a>
<a class="jiathis_button_weixin"></a>
<a class="jiathis_button_cqq"></a>
<a class="jiathis_button_renren"></a>
<a class="jiathis_button_evernote"></a>
<a class="jiathis_button_pocket"></a>
<a href="http://www.jiathis.com/share" class="jiathis jiathis_txt jtico jtico_jiathis" target="_blank"></a>
<script type="text/javascript">
var jiathis_config = {data_track_clickback:'true'
	,slide:{
		divid:'wrap',
		pos:'left',
		gt:'true'
	}
};
</script>
<script type="text/javascript" src="http://v3.jiathis.com/code/jia.js?uid=1936498" charset="utf-8"></script>
<script type="text/javascript" src="http://v3.jiathis.com/code/jiathis_slide.js" charset="utf-8"></script>
</div></div></div>
<!-- JiaThis Button END -->
      </div>
    </div>

    <footer class="site-footer">

  <div class="wrap">

    <h2 class="footer-heading">IT技术干货</h2>

    <div class="footer-col-1 column">
      <p class="text">IT技术干货 KernelHacks 最好的技术站点 技术信息 纯干货</p>
      <ul>
        <li>汇集最好的科技与互联网信息</li>
        <li>Liu Lantao</li>
        <li><a href="mailto:iftti@iftti.com">iftti@iftti.com</a></li>
      </ul>
    </div>

    <div class="footer-col-2 column">
      <ul>
        <li>
          <a href="https://github.com/Lax">
            <span class="icon github">
              <svg version="1.1" class="github-icon-svg" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
                 viewBox="0 0 16 16" enable-background="new 0 0 16 16" xml:space="preserve">
                <path fill-rule="evenodd" clip-rule="evenodd" fill="#C2C2C2" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761
                c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32
                c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472
                c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037
                C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65
                c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261
                c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082
                c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129
                c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/>
              </svg>
            </span>
            <span class="username">Lax</span>
          </a>
        </li>
        <li>
          <a href="https://twitter.com/liulantao">
            <span class="icon twitter">
              <svg version="1.1" class="twitter-icon-svg" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
                 viewBox="0 0 16 16" enable-background="new 0 0 16 16" xml:space="preserve">
                <path fill="#C2C2C2" d="M15.969,3.058c-0.586,0.26-1.217,0.436-1.878,0.515c0.675-0.405,1.194-1.045,1.438-1.809
                c-0.632,0.375-1.332,0.647-2.076,0.793c-0.596-0.636-1.446-1.033-2.387-1.033c-1.806,0-3.27,1.464-3.27,3.27
                c0,0.256,0.029,0.506,0.085,0.745C5.163,5.404,2.753,4.102,1.14,2.124C0.859,2.607,0.698,3.168,0.698,3.767
                c0,1.134,0.577,2.135,1.455,2.722C1.616,6.472,1.112,6.325,0.671,6.08c0,0.014,0,0.027,0,0.041c0,1.584,1.127,2.906,2.623,3.206
                C3.02,9.402,2.731,9.442,2.433,9.442c-0.211,0-0.416-0.021-0.615-0.059c0.416,1.299,1.624,2.245,3.055,2.271
                c-1.119,0.877-2.529,1.4-4.061,1.4c-0.264,0-0.524-0.015-0.78-0.046c1.447,0.928,3.166,1.469,5.013,1.469
                c6.015,0,9.304-4.983,9.304-9.304c0-0.142-0.003-0.283-0.009-0.423C14.976,4.29,15.531,3.714,15.969,3.058z"/>
              </svg>
            </span>
            <span class="username">@liulantao</span>
          </a>
        </li>
        <li>
          <a href="https://plus.google.com/+LiuLantao">
            <span class="icon googleplus">
              <svg version="1.1" id="Layer_1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
                width="16px" height="16px" viewBox="0 0 134.658 131.646" enable-background="new 0 0 134.658 131.646"
                xml:space="preserve">
                <g>
                  <path fill="#C2C2C2" d="M126.515,4.109H8.144c-2.177,0-3.94,1.763-3.94,3.938v115.546c0,2.179,1.763,3.942,3.94,3.942h118.371
                  c2.177,0,3.94-1.764,3.94-3.942V8.048C130.455,5.872,128.691,4.109,126.515,4.109z"/>
                  <g>
                    <path fill="#FFFFFF" d="M70.479,71.845l-3.983-3.093c-1.213-1.006-2.872-2.334-2.872-4.765c0-2.441,1.659-3.993,3.099-5.43
                    c4.64-3.652,9.276-7.539,9.276-15.73c0-8.423-5.3-12.854-7.84-14.956h6.849l7.189-4.517H60.418
                    c-5.976,0-14.588,1.414-20.893,6.619c-4.752,4.1-7.07,9.753-7.07,14.842c0,8.639,6.633,17.396,18.346,17.396
                    c1.106,0,2.316-0.109,3.534-0.222c-0.547,1.331-1.1,2.439-1.1,4.32c0,3.431,1.763,5.535,3.317,7.528
                    c-4.977,0.342-14.268,0.893-21.117,5.103c-6.523,3.879-8.508,9.525-8.508,13.51c0,8.202,7.731,15.842,23.762,15.842
                    c19.01,0,29.074-10.519,29.074-20.932C79.764,79.709,75.344,75.943,70.479,71.845z M56,59.107
                    c-9.51,0-13.818-12.294-13.818-19.712c0-2.888,0.547-5.87,2.428-8.199c1.773-2.218,4.861-3.657,7.744-3.657
                    c9.168,0,13.923,12.404,13.923,20.382c0,1.996-0.22,5.533-2.762,8.09C61.737,57.785,58.762,59.107,56,59.107z M56.109,103.65
                    c-11.826,0-19.452-5.657-19.452-13.523c0-7.864,7.071-10.524,9.504-11.405c4.64-1.561,10.611-1.779,11.607-1.779
                    c1.105,0,1.658,0,2.538,0.111c8.407,5.983,12.056,8.965,12.056,14.629C72.362,98.542,66.723,103.65,56.109,103.65z"/>
                    <polygon fill="#FFFFFF" points="98.393,58.938 98.393,47.863 92.923,47.863 92.923,58.938 81.866,58.938 81.866,64.469
                    92.923,64.469 92.923,75.612 98.393,75.612 98.393,64.469 109.506,64.469 109.506,58.938"/>
                  </g>
                </g>
              </svg>
            </span>
            <span class="username">+LiuLantao</span>
          </a>
        </li>
      </ul>
    </div>

    <div class="footer-col-3 column">
      
<!--以下是QQ邮件列表订阅嵌入代码--><script >var nId = "6be92ef3590ee662cd5e6381ab2044c328716364f684cf3e",nWidth="auto",sColor="light",sText="填写您的邮件地址，订阅我们的精彩内容：" ;</script><script src="http://list.qq.com/zh_CN/htmledition/js/qf/page/qfcode.js" charset="gb18030"></script>

    </div>

  </div>

  <div class="wrap">
    <div>
      <a href="http://blog.liulantao.com">Blog</a> | <a href="http://1000bit.com">铅笔特评 1000bit</a> | <a href="http://visplanet.com">VisPlanet</a> | <a href="http://iftti.com">IT技术干货</a> | <a href="http://relax.org.cn">Relax</a> | <a href="http://hangzhou.io">杭州城市指南</a>
    </div>
  </div>

  
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-1658815-7', 'iftti.com');
  ga('require', 'displayfeatures');
  ga('send', 'pageview');

</script>


</footer>


    </body>
</html>