<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>PRML读书会第七章 Sparse Kernel Machines | IT技术干货</title>
    <meta name="viewport" content="width=device-width">
    <meta name="description" content="IT技术干货 KernelHacks 最好的技术站点 技术信息 纯干货">
    <link rel="canonical" href="http://iftti.com/posts/2015/2015-01-30-prml-e8-af-bb-e4-b9-a6-e4-bc-9a-e7-ac-ac-e4-b8-83-e7-ab-a0-sparse-kernel-machines-9458d994d.html">

    <!-- Custom CSS -->
    <link rel="stylesheet" href="/css/main.css">

</head>


    <body>

    <header class="site-header">

  <div class="wrap">

    <a class="site-title" href="/">IT技术干货</a>

    <nav class="site-nav">
      <a href="#" class="menu-icon">
        <svg version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
           viewBox="0 0 18 15" enable-background="new 0 0 18 15" xml:space="preserve">
          <path fill="#505050" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0
            h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
          <path fill="#505050" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484
            h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
          <path fill="#505050" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0
            c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
        </svg>
      </a>
      <div class="trigger">
        
          
        
          <a class="page-link" href="/about/">About</a>
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
      </div>
    </nav>

  </div>

</header>


    <div class="page-content">
      <div class="wrap">
      <div class="post">

  <header class="post-header">
    <h1>PRML读书会第七章 Sparse Kernel Machines</h1>
    <p class="meta"><span class="time">Jan 30, 2015</span><span class="source_url"> • 来自 52nlp.cn <a name="52nlp.cn" href="http://www.52nlp.cn/prml%e8%af%bb%e4%b9%a6%e4%bc%9a%e7%ac%ac%e4%b8%83%e7%ab%a0-sparse-kernel-machines" target="_blank">[原文链接]</a></span></p>
  </header>

  <article class="post-content">
  

						<p style="text-align: center"><strong>PRML</strong><strong>读书会第七章 Sparse Kernel Machines</strong></p>
<p style="text-align: center"><strong>主讲人 网神</strong></p>
<p style="text-align: center"><strong>（新浪微博: <a href="http://weibo.com/ghtimaq">@豆角茄子麻酱凉面</a>）</strong></p>
<p>网神(66707180) 18:59:22<br>
大家好，今天一起交流下PRML第7章。第六章核函数里提到，有一类机器学习算法，不是对参数做点估计或求其分布，而是保留训练样本，在预测阶段，计算待预测样本跟训练样本的相似性来做预测，例如KNN方法。<br>
将线性模型转换成对偶形式，就可以利用核函数来计算相似性，同时避免了直接做高维度的向量内积运算。本章是稀疏向量机，同样基于核函数，用训练样本直接对新样本做预测，而且只使用了少量训练样本，所以具有稀疏性，叫sparse kernel machine。<br>
本章包括SVM和RVM(revelance vector machine)两部分，首先讲SVM，支持向量机。首先看SVM用于二元分类，并先假设两类数据是线性可分的。<br>
二元分类线性模型可以用这个式子表示：<a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-0.png"><img class="alignnone wp-image-7374" src="/images/52nlp.cn/59499bfd873ade8f22a64988c0dea4fd.jpg" alt="prml7-0" width="150" height="20"></a>。其中<a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-1.png"><img class="alignnone wp-image-7375" src="/images/52nlp.cn/9065666d1d1c75ddf12b4478768ca4f0.jpg" alt="prml7-1" width="50" height="20"></a>是基函数，这些都跟第三章和第四章是一样的。<br>
两类数据线性可分，当<a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-2.png"><img class="alignnone wp-image-7376" src="/images/52nlp.cn/62a94e5fa5586f24b2af2b33cdc595c4.jpg" alt="prml7-2" width="59" height="23"></a>时,分类结果是<a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-3.png"><img class="alignnone wp-image-7377" src="/images/52nlp.cn/865ae1ac29c14312a242a71e0941856b.jpg" alt="prml7-3" width="48" height="24"></a>; <a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-4.png"><img class="alignnone wp-image-7378" src="/images/52nlp.cn/3ca788b51e2d7807ed8d22e9964f9e3f.jpg" alt="prml7-4" width="57" height="22"></a>时,分类结果<a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-5.png"><img class="alignnone wp-image-7379" src="/images/52nlp.cn/ed5c3bbafaf081ccdeeefdf846a00604.jpg" alt="prml7-5" width="45" height="25"></a>;也就是对所有训练样本总是有<a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-6.png"><img class="alignnone wp-image-7380" src="/images/52nlp.cn/50c2e172008f76f4e4b32fc028486cb7.jpg" alt="prml7-6" width="64" height="20"></a>.要做的就是确定决策边界y(x)=0<br>
为了确定决策边界<a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-7.png"><img class="alignnone wp-image-7382" src="/images/52nlp.cn/a74a09c269c26009d290ae095e28252e.jpg" alt="prml7-7" width="99" height="19"></a>，SVM引入margin的概念。margin定义为决策边界y(x)到最近的样本的垂直距离。如下图所示：<span id="more-7371"></span></p>
<p><a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-8.png"><img class="aligncenter wp-image-7383" src="/images/52nlp.cn/abd000d9eec61f9646d15e0c67628a4a.jpg" alt="prml7-8" width="400" height="280"></a></p>
<p>SVM的目标是寻找一个margin最大的决策边界。 我们来看如何确定目标函数：<br>
首先给出一个样本点x到决策边界<a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-9.png"><img class="alignnone wp-image-7384" src="/images/52nlp.cn/075264ffb4abb01638be86e9a1dafaf4.jpg" alt="prml7-9" width="87" height="21"></a>的垂直距离公式是什么，先给出答案：|y(x)|/||w||<br>
这个距离怎么来的，在第四章有具体介绍。看下图:</p>
<p><a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-10.png"><img class="aligncenter wp-image-7385" src="/images/52nlp.cn/e506e3b1c96b4b2827230dab7b899b19.jpg" alt="prml7-10" width="400" height="316"></a></p>
<p>图例，我们看点x到y=0的距离r是多少：</p>
<p><a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-11.png"><img class="aligncenter wp-image-7386" src="/images/52nlp.cn/bff6d35612a26344477e268c3afff903.jpg" alt="prml7-11" width="450" height="135"></a></p>
<p>上面我们得到了任意样本点x到y(x)=0的距离，要做的是最大化这个距离。<br>
同时，要满足条件 <a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-12.png"><img class="alignnone wp-image-7387" src="/images/52nlp.cn/27075e97b252ba28a0ddbe0e59d21e79.jpg" alt="prml7-12" width="65" height="22"></a><br>
所以目标函数是:<a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-13.png"><img class="alignnone wp-image-7388" src="/images/52nlp.cn/ca03bad2ab7e36c084d2fd4cff86ceed.jpg" alt="prml7-13" width="242" height="51"></a><br>
求w和b，使所有样本中，与y=0距离最小的距离 最大化，整个式子就是最小距离最大化</p>
<p>这个函数优化很复杂，需要做一个转换</p>
<p>可以看到，对w和b进行缩放 <a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-14.png"><img class="alignnone wp-image-7389" src="/images/52nlp.cn/9415b8de671116a2bd53ee247cbac413.jpg" alt="prml7-14" width="231" height="16"></a>，距离 <a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-15.png"><img class="alignnone wp-image-7390" src="/images/52nlp.cn/9953879babb90d559046baf5ab94eb98.jpg" alt="prml7-15" width="66" height="44"></a>并不会变化<br>
根据这个属性，调整w和b,使到决策面最近的点满足：<a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-16.png"><img class="alignnone wp-image-7391" src="/images/52nlp.cn/87cc33b3d14b495163c72f1cfddd9682.jpg" alt="prml7-16" width="150" height="31"></a><br>
从而左右样本点都满足 <a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-17.png"><img class="alignnone wp-image-7392" src="/images/52nlp.cn/46519d67b80ea6bb0f62f90f388ab49b.jpg" alt="prml7-17" width="157" height="26"></a><br>
这样，前面的目标函数可以变为：<a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-18.png"><img class="alignnone wp-image-7393" src="/images/52nlp.cn/c3b915490c7ceeae753a731329fa59d7.jpg" alt="prml7-18" width="200" height="38"></a><br>
同时满足约束条件：<a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-19.png"><img class="alignnone wp-image-7394" src="/images/52nlp.cn/8c1512eb3d8806b77c75301c22a0f7c5.jpg" alt="prml7-19" width="150" height="30"></a><br>
这是一个不等式约束的二次规划问题，用拉格朗日乘子法来求解<br>
构造如下的拉格朗日函数：<a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-20.png"><img class="alignnone wp-image-7395" src="/images/52nlp.cn/1022359da65c76cf2205affcc2927a8f.jpg" alt="prml7-20" width="312" height="46"></a><br>
<a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-21.png"><img class="alignnone wp-image-7397" src="/images/52nlp.cn/e89b2f43b408c06cafa1a174dcc1e925.jpg" alt="prml7-21" width="25" height="23"></a>是拉格朗日乘子 ，这个函数分别对w和b求导，令导数等于0，可以得到w和b的表达式：</p>
<p><a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-22.png"><img class="aligncenter wp-image-7398" src="/images/52nlp.cn/f8e89966e267ab1a04b3820677410e2f.jpg" alt="prml7-22" width="237" height="137"></a></p>
<p>将w带入前面的拉格朗如函数L(w,b,a)，就可以消去w和b，变成a的函数<a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-23.png"><img class="alignnone wp-image-7399" src="/images/52nlp.cn/f53659a449efa280275a648b0969cef2.jpg" alt="prml7-23" width="35" height="21"></a>，这个函数是拉格朗日函数的对偶函数：</p>
<p><a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-24.png"><img class="aligncenter wp-image-7400" src="/images/52nlp.cn/3f9d3931bdfea21d53288a111f08c37d.jpg" alt="prml7-24" width="400" height="60"></a></p>
<p>为什么要转换成对偶函数，主要是变形后可以借助核函数，来解决线性不可分的问题，尤其是基函数的维度特别高的情况。求解这个对偶函数，得到参数<a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-21.png"><img class="alignnone wp-image-7397" src="/images/52nlp.cn/e89b2f43b408c06cafa1a174dcc1e925.jpg" alt="prml7-21" width="25" height="23"></a>，就确定了分类模型<br>
把 <a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-25.png"><img class="alignnone wp-image-7401" src="/images/52nlp.cn/eb54ddf34ba921b2c8a00e9ceafb3c3c.jpg" alt="prml7-25" width="150" height="45"></a> 带入 <a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-26.png"><img class="alignnone wp-image-7402" src="/images/52nlp.cn/fc839196371d02c6158bbe4eafd8e2b7.jpg" alt="prml7-26" width="200" height="35"></a>，就是用核函数表示的分类模型：</p>
<p><a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-27.png"><img class="aligncenter wp-image-7403" src="/images/52nlp.cn/c0583ddaa7807f4397f5480ed5573d5d.jpg" alt="prml7-27" width="236" height="54"></a></p>
<p>这就是最终的分类模型，完全由训练样本 <a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-28.png"><img class="alignnone wp-image-7404" src="/images/52nlp.cn/22fc880c382e2c140db96790fcead64b.jpg" alt="prml7-28" width="22" height="18"></a>，n=1…N决定。<br>
SVM具有稀疏性，这里面对大部分训练样本，<a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-29.png"><img class="alignnone wp-image-7405" src="/images/52nlp.cn/88189307048654b24e85a33cabdf44a7.jpg" alt="prml7-29" width="28" height="24"></a>都等于0，从而大部分样本在新样本预测时都不起作用。<br>
我们来看看为什么大部分训练样本，<a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-29.png"><img class="alignnone wp-image-7405" src="/images/52nlp.cn/88189307048654b24e85a33cabdf44a7.jpg" alt="prml7-29" width="28" height="24"></a>都等于0。这主要是由KKT条件决定的。我们从直观上看下KKT条件是怎么回事：</p>
<p>KKT是对拉格朗日乘子法的扩展，将其从约束为等式的情况扩展为约束为不等式的情况。所以先看下约束为等式的情况：例如求函数<a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-30.png"><img class="alignnone wp-image-7406" src="/images/52nlp.cn/c3640ffd1ec7c2073078ff78d971e3b8.jpg" alt="prml7-30" width="66" height="22"></a>的极大值，同时满足约束 <a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-31.png"><img class="alignnone wp-image-7407" src="/images/52nlp.cn/a07eabf0b88fc6364eb1b1a76193843e.jpg" alt="prml7-31" width="78" height="20"></a>，拉格朗日乘子法前面已经介绍，引入拉式乘子，构造拉式函数，然后求导，解除的值就是极值。这里从直观上看一下，为什么这个值就是满足条件的极值。设想取不同的z值，使<a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-32.png"><img class="alignnone wp-image-7408" src="/images/52nlp.cn/8f5459a9ee72efc02904561a35914af1.jpg" alt="prml7-32" width="74" height="21"></a>，就可以得到f(x1,x2)的不同等高线，如图:</p>
<p><a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-33.png"><img class="aligncenter wp-image-7409" src="/images/52nlp.cn/63018771852e659a67719d5adeff95d9.jpg" alt="prml7-33" width="352" height="299"></a></p>
<p>构成图中的曲线，图中标记的g=c，对于这种情况，改成g-c=0就可以了.假设g与f的某些等高线相交，交点就是同时满足约束条件和目标函数的值，但不一定是极大值。。有两种相交形式，一种是穿过，一种是相切。因为穿过意味着在该条等高线内部还存在着其他等高线与g相交，新等高线与目标函数的交点的值更大。只有相切时，才可能取得最大值。因此，在极大值处，f的梯度与g的梯度是平行的，因为梯度都垂直于g或f曲线，也就是存在lamda，使得 <a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-36.png"><img class="alignnone wp-image-7411" src="/images/52nlp.cn/c478796cb2fb8f0c194f28c281a2d46a.jpg" alt="prml7-36" width="84" height="25"></a>，这个式子正是拉格朗日函数 <a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-37.png"><img class="alignnone wp-image-7412" src="/images/52nlp.cn/d209fb106a0fc35e385536ae708c53d8.jpg" alt="prml7-37" width="127" height="20"></a> 对x求导的结果。<br>
接下来看看约束条件为不等式的情况，例如约束为 <a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-38.png"><img class="alignnone wp-image-7413" src="/images/52nlp.cn/3c1231e0cbc887bc8672be508765474b.jpg" alt="prml7-38" width="60" height="27"></a>，先看个图：</p>
<p><a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-39.png"><img class="aligncenter wp-image-7414" src="/images/52nlp.cn/c1305c8cf34c0c2f93fceb97ce9d9646.jpg" alt="prml7-39" width="397" height="304"></a></p>
<p>图里的约束是g&lt;0，不影响解释KKT条件。不等式约束分两种情况，假设极值点是x` ，当<a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-40.png"><img class="alignnone wp-image-7415" src="/images/52nlp.cn/c4bb0c8151a3292020b94669617402e4.jpg" alt="prml7-40" width="61" height="18"></a>时，也就是图中左边那部分，此时该约束条件是inactive的，对于极值点的确定不起作用。因此拉格朗日函数<a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-37.png"><img class="alignnone wp-image-7412" src="/images/52nlp.cn/d209fb106a0fc35e385536ae708c53d8.jpg" alt="prml7-37" width="140" height="22"></a>中，lamda等于0，极值完全由f一个人确定，相当于lamda等于0.当<a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-42.png"><img class="alignnone wp-image-7416" src="/images/52nlp.cn/be527f6fe36ee6a2f5b931c0204a6b1a.jpg" alt="prml7-42" width="58" height="23"></a>时，也就是图中右边部分，极值出现在g的边界处,这跟约束条件为等式时是一样的。<br>
总之，对于约束条件为不等式的拉格朗日乘子法，总有 <a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-43.png"><img class="alignnone wp-image-7417" src="/images/52nlp.cn/eb3415d9793de6c55328b15cf1d9cfbb.jpg" alt="prml7-43" width="77" height="20"></a>，不是lamda等于0，就是g=0<br>
这个结论叫KKT条件，总结起来就是:</p>
<p><a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-44.png"><img class="aligncenter wp-image-7418" src="/images/52nlp.cn/b6a979e79f6c4ddf0760e8394d5afd13.jpg" alt="prml7-44" width="124" height="117"></a></p>
<p>再返回来看SVM的目标函数构造的拉格朗日函数：</p>
<p><a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-45.png"><img class="aligncenter wp-image-7419" src="/images/52nlp.cn/ae4997a1ed1100ba6197111e0c6c87c4.jpg" alt="prml7-45" width="400" height="56"></a></p>
<p>根据KKT条件，有 <a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-46.png"><img class="alignnone wp-image-7420" src="/images/52nlp.cn/a2966e08f63c2648ae1b58060c6d1757.jpg" alt="prml7-46" width="300" height="27"></a>，所以对于<a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-47.png"><img class="alignnone wp-image-7421" src="/images/52nlp.cn/6fed8fc0a89aa5397e0c277aa0418419.jpg" alt="prml7-47" width="58" height="20"></a>大于1的那些样本点，其对应<a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-48.png"><img class="alignnone wp-image-7422" src="/images/52nlp.cn/09942740e0117eedc43b4f30bf2a2f43.jpg" alt="prml7-48" width="38" height="23"></a>的都等于0。只有<a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-47.png"><img class="alignnone wp-image-7421" src="/images/52nlp.cn/6fed8fc0a89aa5397e0c277aa0418419.jpg" alt="prml7-47" width="58" height="20"></a>等于1的那些样本点对保留下来，这些点就是支持向量。<br>
这部分大家有什么意见和问题吗？</p>
<p>============================讨论===============================</p>
<p>Fire(564122106) 20:01:56<br>
他为什么要符合KKT条件啊</p>
<p>网神(66707180) 20:02:33<br>
因为只有符合KKT条件，才能有解，否则拉格朗日函数没解，我的理解是这样的<br>
Fire(564122106) 20:03:54<br>
我上次看到一个版本说只有符合KKT条件    对偶解才和原始解才相同，不知道怎么解释。<br>
kxkr&lt;lxfkxkr@126.com&gt; 20:04:18<br>
貌似统计学习方法 附录里面 讲了这个<br>
Wolf      &lt;wuwja@foxmail.com&gt; 20:04:19<br>
an为0为什么和kkt条件相关<br>
kxkr&lt;lxfkxkr@126.com&gt; 20:04:20<br>
不过忘记了 ，我上次看到一个版本说只有符合KKT条件，对偶解才和原始解相同。<br>
YYKuaiXian(335015891) 20:04:40<br>
Ng的讲义就是用这种说法<br>
苦瓜炒鸡蛋(852383636) 20:04:47<br>
因为大部分的样本都不是sv</p>
<p>Wolf      &lt;wuwja@foxmail.com&gt; 20:05:05<br>
如果两类正好分布在margin上，那么所有的点都是sv<br>
YYKuaiXian(335015891) 20:05:40</p>
<p><a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-60.png"><img class="aligncenter wp-image-7424" src="/images/52nlp.cn/9272b84e4040957000ad940230cba486.jpg" alt="prml7-60" width="400" height="225"></a></p>
<p>Wolf      &lt;wuwja@foxmail.com&gt; 20:05:54<br>
只有符合kkt条件，primary问题和due问题的解才是一样的，否则胖子里面的瘦子总比瘦子里面的胖子大。<br>
kxkr&lt;lxfkxkr@126.com&gt; 20:07:03<br>
这个比喻 好！<br>
高老头(1316103319) 20:07:08<br>
对偶问题和原问题是什么关系，一个问题怎么找到它的对偶问题？<br>
Wolf      &lt;wuwja@foxmail.com&gt; 20:07:19<br>
所以kkt条件和sv为什么大部分为0没有直接关系，sv为0个人觉得是分界面的性质决定的，分界面是一个低维流形。<br>
Fire(564122106) 20:08:38</p>
<p>我也感觉sv是和样本数据性质有关的<br>
Wolf      &lt;wuwja@foxmail.com&gt; 20:09:26<br>
比如在二维的时候，分界面是一个线性函数，导致sv比较少，当投影到高维空间，分界面变成了一个超平面，导致sv变多了，另外，很多样本变成sv也是svm慢的一个原因。<br>
网神(66707180) 20:09:37<br>
sv本质上是svm选择的错误函数决定的，在正确一边分类边界以外的样本点，错误为0，在边界以内或在错误一边，错误大于0.<br>
苦瓜炒鸡蛋(852383636) 20:11:04<br>
sv确定的超平面 而非是超平面确定的sv<br>
Wolf      &lt;wuwja@foxmail.com&gt; 20:11:30<br>
sv确定的超平面 而非是超平面确定的sv，一样的，hinge为什么会导致稀疏？什么样的优化问题才有对偶问题，我也在疑问。对于一些规划问题（线性规划，二次规划）可以将求最大值（最小值）的问题转化为求最小值最大值的问题。</p>
<p>网神(66707180) 20:12:24<br>
kkt是从一个侧面解释稀疏，从另一个侧面，也就是错误函数是hinge函数，也可以得出稀疏的性质。svm跟逻辑回归做对比， hinge损失导致稀疏，我们先讲下这吧，svm的错误函数可以这么写:</p>
<p><a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-50.png"><img class="alignnone wp-image-7425" src="/images/52nlp.cn/58b400bb1ea52597c9c89e61a0478869.jpg" alt="prml7-50" width="224" height="78"></a></p>
<p>其中<a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-51.png"><img class="alignnone wp-image-7426" src="/images/52nlp.cn/630bda5cc934d8c6bc726f0b6d25d3e6.jpg" alt="prml7-51" width="186" height="31"></a><a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-52.png"><img class="alignnone wp-image-7427" src="/images/52nlp.cn/87fe05cfda35e5611d09b76659ca408e.jpg" alt="prml7-52" width="220" height="20"></a></p>
<p>这就是hinge错误函数，图形如图中的蓝色线</p>
<p><a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-54.png"><img class="aligncenter wp-image-7428" src="/images/52nlp.cn/74faa7c7b7c0dcea9390786a5f924e8e.jpg" alt="prml7-54" width="300" height="242"></a></p>
<p>而逻辑回归的错误函数是：</p>
<p><a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-55.png"><img class="alignleft wp-image-7429" src="/images/52nlp.cn/b16a03e3c24e112ee235c5784ad754a7.jpg" alt="prml7-55" width="188" height="59"></a></p>
<p> </p>
<p><a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-56.png"><img class="alignnone wp-image-7430" src="/images/52nlp.cn/4bcdc1bc37e27ad144ec193a14c5f70f.jpg" alt="prml7-56" width="200" height="27"></a></p>
<p>如图中的红色线，红色线跟蓝色线走势相近 ，区别是hinge函数在<a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-57.png"><img class="alignnone wp-image-7431" src="/images/52nlp.cn/7a24a34c5a297c86825dd42b3a16ec62.jpg" alt="prml7-57" width="226" height="38"></a>,图中z&gt;1时，错误等于0，也就是yt&gt;1的那些点都不产生损失. 这个性质可以带来稀疏的解。</p>
<p>========================讨论结束===============================</p>
<p>我接着讲了，后面还有挺多内容，刚才说的都是两类训练样本可以完全分开的情况，比如下面这个图，采用了高斯核函数的支持向量机，可以很清楚的看到决策边界，支持向量：</p>
<p><a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-61.png"><img class="aligncenter wp-image-7433" src="/images/52nlp.cn/da9c89c51ae9d65242e8864904b59d78.jpg" alt="prml7-61" width="400" height="300"></a></p>
<p>但实际中两类数据的分布会有重叠的情况，另外也有噪音的存在，导致两类训练数据如果一定要完全分开，泛化性能会很差。因此svm引入一些机制，允许训练时一些样本被误分类.我们要修改目标函数，允许样本点位于错误的一边，但会增加一个惩罚项，其大小随着数据点到边界的距离而增大这个惩罚项叫松弛变量, slack variables，记为 <a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-62.png"><img class="alignnone wp-image-7434" src="/images/52nlp.cn/14e411765a20156ddf4f7c929c63b25c.jpg" alt="prml7-62" width="25" height="26"></a>，并且大于等于0.其中下标n=1,..,N，也就是每个训练样本对应一个<a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-62.png"><img class="alignnone wp-image-7434" src="/images/52nlp.cn/14e411765a20156ddf4f7c929c63b25c.jpg" alt="prml7-62" width="25" height="26"></a>，对于位于正确的margin边界上或以内的数据点，其松弛变量 <a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-65.png"><img class="alignnone wp-image-7435" src="/images/52nlp.cn/e498d11c1fb025eaca5b81820ecab2f3.jpg" alt="prml7-65" width="47" height="21"></a>，其他样本点<a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-68.png"><img class="alignnone wp-image-7436" src="/images/52nlp.cn/6f7b57de480057c2b81ff08e10ee296c.jpg" alt="prml7-68" width="95" height="20"></a><br>
这样，如果样本点位于决策边界y(x)=0上, <a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-69.png"><img class="alignnone wp-image-7437" src="/images/52nlp.cn/a395e373c005b5ea620d8bc96d54c6eb.jpg" alt="prml7-69" width="48" height="21"></a><br>
如果被错分，位于错误的一边, <a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-70.png"><img class="alignnone wp-image-7438" src="/images/52nlp.cn/f4b5b7034c1467026c986e34910cb0bf.jpg" alt="prml7-70" width="34" height="20"></a> ，因此目标函数的限制条件由<a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-71.png"><img class="alignnone wp-image-7439" src="/images/52nlp.cn/841479b84e99082d0a673ab4670e1eee.jpg" alt="prml7-71" width="136" height="19"></a>修改为<a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-73.png"><img class="alignnone wp-image-7440" src="/images/52nlp.cn/337ff86f6a77f287bfc2db57b98c577c.jpg" alt="prml7-73" width="130" height="20"></a>，目标函数修从最小化<a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-751.png"><img class="alignnone wp-image-7441" src="/images/52nlp.cn/a7187b53eae4a460bb71ed0c9760ccb2.jpg" alt="prml7-75" width="46" height="36"></a>改为最小化 <a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-76.png"><img class="alignnone wp-image-7442" src="/images/52nlp.cn/ccdc27763fe43af22fa927e0f2ed67d1.jpg" alt="prml7-76" width="100" height="39"></a>，其中参数C用于控制松弛变量和margin之间的trade-off，因为对于错分的点，有<a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-77.png"><img class="alignnone wp-image-7443" src="/images/52nlp.cn/24f1d2c88b68dfd9c5d5b69e98887bfe.jpg" alt="prml7-77" width="45" height="25"></a> ，所以<a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-771.png"><img class="alignnone wp-image-7444" src="/images/52nlp.cn/8f6c678cbf360d8f3de03aa192ded234.jpg" alt="prml7-77" width="45" height="25"></a>是错分样本数的一个上限upper bound ，所以C相当于一个正则稀疏，控制着最小错分数和模型复杂度的trade-off.<br>
SVM在实际使用中，需要调整的参数很少，C是其中之一。<br>
看这个目标函数，可以看到，C越大，松弛变量就越倍惩罚，就会训练出越复杂的模型，来保证尽量少的样本被错分。当C趋于无穷时，每个样本点就会被模型正确分类。<br>
我们现在求解这个新的目标函数，加上约束条件，拉格朗日函数如下：</p>
<p><a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-80.png"><img class="aligncenter wp-image-7445" src="/images/52nlp.cn/222c43ac387bc14efed52cdbd5ce444d.jpg" alt="prml7-80" width="400" height="46"></a><br>
其中<a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-81.png"><img class="alignnone wp-image-7446" src="/images/52nlp.cn/2437b16f4196b5745efcd50bad44be3d.jpg" alt="prml7-81" width="22" height="24"></a>和<a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-82.png"><img class="alignnone wp-image-7447" src="/images/52nlp.cn/0d6e5a883e8476f677f5d6ec2cfa44ab.jpg" alt="prml7-82" width="24" height="24"></a>是拉式乘子，分别对w, b和{<a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-83.png"><img class="alignnone wp-image-7448" src="/images/52nlp.cn/483f19b23089cfbe2af9a2b253d634e9.jpg" alt="prml7-83" width="15" height="21"></a>}求导，令导数等于0，得到w, b,<a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-83.png"><img class="alignnone wp-image-7448" src="/images/52nlp.cn/483f19b23089cfbe2af9a2b253d634e9.jpg" alt="prml7-83" width="18" height="25"></a>的表示，带入L(w,b,a)，消去这些变量，得到以拉格朗日乘子为变量的对偶函数：</p>
<p><a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-84.png"><img class="aligncenter wp-image-7449" src="/images/52nlp.cn/07f3f2c0abd9376e0874389d3300122b.jpg" alt="prml7-84" width="350" height="62"></a></p>
<p>新的对偶函数跟前面的对偶函数形式相同，只有约束条件有不同。 这就是正则化的SVM。<br>
接下来提一下对偶函数的解法，对偶函数都是二次函数，而且是凸函数，这是svm的优势，具有全局最优解，该二次规划问题的求解难度是参数<a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-81.png"><img class="alignnone wp-image-7446" src="/images/52nlp.cn/2437b16f4196b5745efcd50bad44be3d.jpg" alt="prml7-81" width="22" height="24"></a>的数量很大，等于训练样本的数量 。书上回顾了一些方法，介绍不详细，主要思想是chunking，我总结一下，总结的不一定准确:<br>
1.去掉<a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-90.png"><img class="alignnone wp-image-7451" src="/images/52nlp.cn/8dcc0da742e7d1767a9f8928d7d52615.jpg" alt="prml7-90" width="22" height="28"></a>=0对应的核函数矩阵的行和列，将二次优化问题划分成多个小的优化问题；<br>
2.按固定大小划分成小的优化问题。<br>
3.SVM中最流行的是SMO, sequentialminimal optimization。每次只考虑两个拉格朗日乘子.<br>
SVM中维度灾难问题：核函数相当于高维(甚至无线维)的特征空间的内积，避免了显示的高维空间运算，貌似是避免了维度过高引起的维度灾难问题。 但实际上并没有避免。书上举了个例子 ，看这个二维多项式核函数：</p>
<p><a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-91.png"><img class="aligncenter wp-image-7452" src="/images/52nlp.cn/db43e9e78826635b7b4bf17d027b8aa4.jpg" alt="prml7-91" width="481" height="88"></a></p>
<p>这个核函数表示一个六维空间的内积。<a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-92.png"><img class="alignnone wp-image-7453" src="/images/52nlp.cn/823fa005a94902864ec3bddb104c6a3d.jpg" alt="prml7-92" width="46" height="21"></a>是从输入空间到六维空间的映射.映射后，六个维度每个维度的值是由固定参数的，也就是映射后，六维特征是有固定的形式。因此，原二维数据x都被限制到了六维空间的一个nonlinear manifold中。这个manifold之外就没有数据.<br>
网神(66707180) 20:48:59<br>
大家有什么问题吗？<br>
高老头(1316103319) 20:49:58<br>
manifold是什么意思？<br>
网神(66707180) 20:50:26<br>
我的理解是空间里一个特定的区域，原空间的数据，如果采样不够均匀，映射后的空间，仍然不会均匀，不会被打散到空间的各个角落，而只会聚集在某个区域。</p>
<p>接下来讲下SVM用于回归问题.</p>
<p>在线性回归中，一个正则化错误函数如下：<a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-93.png"><img class="alignnone wp-image-7454" src="/images/52nlp.cn/12fae1e7969dab323d1e2146f4019498.jpg" alt="prml7-93" width="169" height="47"></a><br>
为了获得稀疏解，将前面的二次错误函数用<a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-94.png"><img class="alignnone wp-image-7455" src="/images/52nlp.cn/d542f7cab050485e3b45266d978885ad.jpg" alt="prml7-94" width="90" height="14"></a>错误函数代替</p>
<p><a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-95.png"><img class="aligncenter wp-image-7456" src="/images/52nlp.cn/14493f70a42eb3fb8eebbee4c14e6f60.jpg" alt="prml7-95" width="347" height="44"></a></p>
<p>这个错误函数在y(x)和t的差小于时等于0.错误函数变为:</p>
<p><a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-97.png"><img class="aligncenter wp-image-7457" src="/images/52nlp.cn/658d5d59dcd734a6e34c685440ed3b73.jpg" alt="prml7-97" width="234" height="58"></a></p>
<p>我们再引入松弛变量，对每个样本，有两个松弛变量，分别对应 <a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-98.png"><img class="alignnone wp-image-7458" src="/images/52nlp.cn/988e4963a79f026fa496dc9183924a45.jpg" alt="prml7-98" width="180" height="21"></a><br>
如图:</p>
<p><a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-99.png"><img class="aligncenter wp-image-7459" src="/images/52nlp.cn/558c4d2ada4b168fea3b29235b5cf50b.jpg" alt="prml7-99" width="400" height="279"></a></p>
<p>没引入松弛变量前，样本值t预测正确的条件是<a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-100.png"><img class="alignnone wp-image-7461" src="/images/52nlp.cn/e1eca48537e2c625765782fc0ce6a0bb.jpg" alt="prml7-100" width="120" height="17"></a><br>
引入松弛变量后，变为:<a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-101.png"><img class="alignnone wp-image-7462" src="/images/52nlp.cn/619cb53c9553b67259feba85fea72cc7.jpg" alt="prml7-101" width="168" height="49"></a><br>
错误函数变为：</p>
<p><a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-102.png"><img class="alignnone wp-image-7463" src="/images/52nlp.cn/f5671fb35dc230cd86ba5023c234c1ea.jpg" alt="prml7-102" width="216" height="63"></a></p>
<p>加上约束条件<a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-103.png"><img class="alignnone wp-image-7464" src="/images/52nlp.cn/21616a41882cf5e1603edffe22e031b1.jpg" alt="prml7-103" width="99" height="21"></a>，就可以写出拉格朗日函数<br>
下面就跟前面的分类一样了.<br>
关于统计学习理论，书上简单提了一下PAC(probably approximately correct)和VC维，简单总结一下书上的内容：PAC的目的是理解多大的数据集可以给出好的泛化性，以及研究损失的上限。PAC里的一个关键概念是VC维，用于提供一个函数空间复杂度的度量，将PAC理论推广到了无限大的函数空间上。</p>
<p>============================讨论===============================</p>
<p>Fire(564122106) 21:05:56<br>
有哪位大神想过对svm提速的啊，svm在非线性大数据的情况下，速度还是比较慢的啊<br>
网神(66707180) 21:07:36<br>
svm分布式训练的方案研究过吗？<br>
Fire(564122106) 21:09:29<br>
没有，不过将来肯定要研究的！现在只是单机，现在有在单机的情况下，分布式进入内存的方案，有兴趣的可以看下：Selective Block Minimization for Faster Convergence of Limited Memory Large_Scale Linear Models 这个有介绍，我共享下啊。<br>
苦瓜炒鸡蛋(852383636) 21:11:05<br>
韩家炜的一个学生 提出了一个  仿照层次聚类的思想  改进的svm 速度好像挺快的</p>
<p>Making SVMs Scalable to Large Data Sets using Hierarchical Cluster Indexing    这个就是那篇论文的题目  发在  Data Mining and Knowledge Discovery</p>
<p>Fire(564122106) 21:16:29<br>
哦 我看下，我现在看的都是台湾林的<br>
Fire 分享文件 21:14:33<br>
“Selective Block Minimization for Faster Convergence of Limited Memory Large_Scale Linear Models.pdf” 下载<br>
苦瓜炒鸡蛋(852383636) 21:17:41<br>
有那个大神 在用svm做聚类，Support Vector Clustering   这篇能做  就是时间复杂度太高了<a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-105.png"><img class="alignnone wp-image-7465" src="/images/52nlp.cn/8b00c12ec4ef95c9a0f7c1bf57960f17.jpg" alt="prml7-105" width="47" height="28"></a></p>
<p>========================讨论结束===============================</p>
<p>网神(66707180) 8:54:01<br>
咱们开始讲RVM，前面讲了SVM，SVM有一些缺点，比如输出是decision而不是概率分布，SVM是为二元分类设计的，多类别分类不太实用，虽然有不少策略可以用于多元分类，但也各有问题参数C需要人工选择，通过多次训练来调整，感觉实际应用中这些缺点不算什么大缺点，但是RVM可以避免这些缺点。<br>
RVM是一种贝叶斯方式的稀疏核方法，可以用于回归和分类，除了避免SVM的主要缺点，还可以更稀疏，而泛化能力不会降低 。先看RVM回归，RVM回归的模型跟前面第三章形式相同，属于线性模型，但是参数w的先验分布有所不同。<br>
这个不同导致了稀疏性，等下再看这个不同<br>
线性回归模型如下：<a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-110.png"><img class="alignnone wp-image-7467" src="/images/52nlp.cn/f523f266a47763c867e9e792251615b5.jpg" alt="prml7-110" width="252" height="30"></a><br>
其中<a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-111.png"><img class="alignnone wp-image-7468" src="/images/52nlp.cn/76259bb20147c8220dfa577ab2669797.jpg" alt="prml7-111" width="45" height="22"></a>，是噪音的精度precision<br>
均值y(x)定义为：<a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-112.png"><img class="alignnone wp-image-7469" src="/images/52nlp.cn/f70baa9a0b60452b1fb9de7611e9a48d.jpg" alt="prml7-112" width="200" height="46"></a><br>
RVM作为一种稀疏核方法，它是如何跟核函数搭上边的，就是基函数 <a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-113.png"><img class="alignnone wp-image-7470" src="/images/52nlp.cn/70dabe8eb195b3b1f4408ff65be9958d.jpg" alt="prml7-113" width="45" height="22"></a>采用了核函数的形式<br>
每个核与一个训练样本对应，也就是：</p>
<p><a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-115.png"><img class="aligncenter wp-image-7471" src="/images/52nlp.cn/882e36bb526824c0ceaa8932563f40f3.jpg" alt="prml7-115" width="233" height="63"></a><br>
这个形式跟SVM用于回归的模型形式是相同的，看前面的式子(7.64)最后求得的SVM回归模型是：</p>
<p><a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-116.png"><img class="aligncenter wp-image-7472" src="/images/52nlp.cn/03d56aff60da5c23e3b63127c0900db0.jpg" alt="prml7-116" width="265" height="56"></a></p>
<p>可以看到，RVM回归和SVM回归模型相同，只是前面的系数从<a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-117.png"><img class="alignnone wp-image-7473" src="/images/52nlp.cn/e4f1a415ede76c01809c1cc27cf928e4.jpg" alt="prml7-117" width="87" height="23"></a>，接下来分析如何确定RVM模型中的参数w，下面的分析过程跟任何基函数都适用，不限于核函数。<br>
确定w的过程可以总结为：先假设w的先验分布，一般是高斯分布；然后给出似然函数，先验跟似然函数相乘的到w的后验分布，最大化后验分布，得到参数w 。<br>
先看w的先验，w的先验是以0为均值，以<a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-118.png"><img class="alignnone wp-image-7474" src="/images/52nlp.cn/ba0eee69afc7e38998e601db63213782.jpg" alt="prml7-118" width="18" height="18"></a>为精度的高斯分布，但是跟第三章线性回归的区别是，RVM为每个<a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-119.png"><img class="alignnone wp-image-7475" src="/images/52nlp.cn/087b5fe54ee0ecc2faec223bfc462c56.jpg" alt="prml7-119" width="29" height="23"></a>分别引入一个精度<a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-120.png"><img class="alignnone wp-image-7476" src="/images/52nlp.cn/0f16ebb822624ae14bbf7efe8bcbcc83.jpg" alt="prml7-120" width="25" height="20"></a>，而不是<a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-119.png"><img class="alignnone wp-image-7475" src="/images/52nlp.cn/087b5fe54ee0ecc2faec223bfc462c56.jpg" alt="prml7-119" width="29" height="23"></a>所有用一个的共享的精度<br>
所以w的先验是：</p>
<p><a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-121.png"><img class="aligncenter wp-image-7478" src="/images/52nlp.cn/b84f19aa02a01d9c1749cf6223f4d591.jpg" alt="prml7-121" width="200" height="44"></a></p>
<p>对于线性回归模型，根据这个先验和似然函数可以得到其后验分布：</p>
<p><a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-122.png"><img class="aligncenter wp-image-7479" src="/images/52nlp.cn/13c37468ba3ae5e1a73640e6ed23e451.jpg" alt="prml7-122" width="261" height="29"></a><br>
均值和方差分别是：</p>
<p><a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-130.png"><img class="aligncenter wp-image-7480" src="/images/52nlp.cn/f2653512e01330750b7d07214aa8d633.jpg" alt="prml7-130" width="200" height="58"></a></p>
<p>这是第三章的结论，推导过程就不说了<br>
其中<a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-132.png"><img class="alignnone wp-image-7481" src="/images/52nlp.cn/ffd96b0166c47644ea2b087bf19152c7.jpg" alt="prml7-132" width="22" height="20"></a>是NxM的矩阵，<a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-134.png"><img class="alignnone wp-image-7482" src="/images/52nlp.cn/627003b183a65c22431525dfd8d07444.jpg" alt="prml7-134" width="100" height="25"></a>，A是对角矩阵<a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-135.png"><img class="alignnone wp-image-7483" src="/images/52nlp.cn/0a055de07c3b81d693fc8315d73df92c.jpg" alt="prml7-135" width="100" height="21"></a><br>
对于RVM，因为基函数是核函数，所以<a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-136.png"><img class="alignnone wp-image-7484" src="/images/52nlp.cn/3963b0edc13de5261b43bd01d589e212.jpg" alt="prml7-136" width="49" height="20"></a>，K是NxN维的核矩阵，其元素是<a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-137.png"><img class="alignnone wp-image-7485" src="/images/52nlp.cn/956c79933e2575a0800b77c694e38cf2.jpg" alt="prml7-137" width="71" height="25"></a><br>
接下来需要确定超参数<a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-1381.png"><img class="alignnone wp-image-7487" src="/images/52nlp.cn/119817f3aecefed48c1e2967a3733314.jpg" alt="prml7-138" width="25" height="20"></a>和<a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-139.png"><img class="alignnone wp-image-7488" src="/images/52nlp.cn/a080a04d50fbfbe08d27bfaccbdd8fbf.jpg" alt="prml7-139" width="24" height="22"></a>。一个是w先验的精度，一个是线性模型p(t|x,w)的精度<br>
确定的方法叫做evidence approximation方法，又叫type-2 maximum likelihood，这在第三章有详细介绍，这里简单说一下思路：<br>
该方法基于一个假设，即两个参数是的后验分布<a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-140.png"><img class="alignnone wp-image-7489" src="/images/52nlp.cn/3ce874a70da30237da1e1f04958dbfc3.jpg" alt="prml7-140" width="67" height="16"></a>是sharply peaked的 ，其中心值是<a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-141.png"><img class="alignnone wp-image-7490" src="/images/52nlp.cn/01ab2d9a82b3439cc604d3c3ba504d98.jpg" alt="prml7-141" width="56" height="19"></a> ，根据贝叶斯定理，<a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-145.png"><img class="alignnone wp-image-7491" src="/images/52nlp.cn/36988089e341589716c18197a4e9b6fc.jpg" alt="prml7-145" width="200" height="22"></a>，先验<a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-146.png"><img class="alignnone wp-image-7492" src="/images/52nlp.cn/81e9bf85606d52d5e498f5fd0309a224.jpg" alt="prml7-146" width="54" height="17"></a>是relatively flat的，所以只要看<a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-147.png"><img class="alignnone wp-image-7493" src="/images/52nlp.cn/24c1df957dfec2c2a22e77217a53aca5.jpg" alt="prml7-147" width="60" height="18"></a>，<a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-141.png"><img class="alignnone wp-image-7490" src="/images/52nlp.cn/01ab2d9a82b3439cc604d3c3ba504d98.jpg" alt="prml7-141" width="56" height="19"></a>就是使的<a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-147.png"><img class="alignnone wp-image-7493" src="/images/52nlp.cn/24c1df957dfec2c2a22e77217a53aca5.jpg" alt="prml7-147" width="60" height="18"></a>最大的值。<br>
<a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-147.png"><img class="alignnone wp-image-7493" src="/images/52nlp.cn/24c1df957dfec2c2a22e77217a53aca5.jpg" alt="prml7-147" width="60" height="18"></a>是对w进行积分的边界分布：</p>
<p><a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-149.png"><img class="aligncenter wp-image-7494" src="/images/52nlp.cn/c31a6ca8846b2858059c7d668e338b8f.jpg" alt="prml7-149" width="300" height="44"></a></p>
<p>这个分布是两个高斯分布的卷积，其log最大似然函数是:</p>
<p><a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-150.png"><img class="aligncenter wp-image-7495" src="/images/52nlp.cn/6fc7a490aa3d156b7b6e02af6f953689.jpg" alt="prml7-150" width="400" height="61"></a></p>
<p>其中C是NxN矩阵，<a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-151.png"><img class="alignnone wp-image-7496" src="/images/52nlp.cn/ae86b2f1e612df3c802e51e360c75184.jpg" alt="prml7-151" width="120" height="20"></a><br>
<a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-153.png"><img class="alignnone wp-image-7497" src="/images/52nlp.cn/86529881251ba3be0ed87737638dd0fb.jpg" alt="prml7-153" width="184" height="22"></a>这一步，以及C的值，是第三章的内容，大家看前面吧。<br>
我们可以通过最大化似然函数，求得<a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-141.png"><img class="alignnone wp-image-7490" src="/images/52nlp.cn/01ab2d9a82b3439cc604d3c3ba504d98.jpg" alt="prml7-141" width="56" height="19"></a>，书上提到了两种方法，一种是EM，一种是直接求导迭代。前者第九章尼采已经讲了，这里看下后者。<br>
首先我们分别求这个log似然函数对所有参数<a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-1381.png"><img class="alignnone wp-image-7487" src="/images/52nlp.cn/119817f3aecefed48c1e2967a3733314.jpg" alt="prml7-138" width="25" height="20"></a>和<a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-139.png"><img class="alignnone wp-image-7488" src="/images/52nlp.cn/a080a04d50fbfbe08d27bfaccbdd8fbf.jpg" alt="prml7-139" width="25" height="23"></a>求偏导，并令偏导等于0，求得参数的表达式：</p>
<p><a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-155.png"><img class="aligncenter wp-image-7498" src="/images/52nlp.cn/8faf55d76c0fce515ecaa4a95a424c49.jpg" alt="prml7-155" width="352" height="148"></a></p>
<p>其中<a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-159.png"><img class="alignnone wp-image-7499" src="/images/52nlp.cn/bb4e0796cb82f0954e0d4b013b7ab4e7.jpg" alt="prml7-159" width="30" height="15"></a>是w的后验均值m的第i个元素,</p>
<p><a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-160.png"><img class="alignnone wp-image-7500" src="/images/52nlp.cn/ce72b635cd9a1f7768926bb94d22f463.jpg" alt="prml7-160" width="20" height="20"></a>是度量<a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-161.png"><img class="alignnone wp-image-7503" src="/images/52nlp.cn/9cfaeb20e6da4864e4feb584c969f993.jpg" alt="prml7-161" width="25" height="22"></a>被样本集合影响的程度<a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-162.png"><img class="alignnone wp-image-7501" src="/images/52nlp.cn/ad588c9949a2169d6b30247144be272c.jpg" alt="prml7-162" width="120" height="21"></a><br>
<a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-163.png"><img class="alignnone wp-image-7502" src="/images/52nlp.cn/aa7bcc00d0e4dd971e72b53aed127fd4.jpg" alt="prml7-163" width="30" height="19"></a>是w的后验方差<a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-164.png"><img class="alignnone wp-image-7504" src="/images/52nlp.cn/afab5e15f0ba7149697a3a301b4db262.jpg" alt="prml7-164" width="20" height="23"></a>的对角线上的元素。<br>
求<a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-141.png"><img class="alignnone wp-image-7490" src="/images/52nlp.cn/01ab2d9a82b3439cc604d3c3ba504d98.jpg" alt="prml7-141" width="56" height="19"></a>是一个迭代的过程：<br>
先选一个<a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-141.png"><img class="alignnone wp-image-7490" src="/images/52nlp.cn/01ab2d9a82b3439cc604d3c3ba504d98.jpg" alt="prml7-141" width="56" height="19"></a>的初值，然后用下面这个公式得到后验的均值和方差：</p>
<p><a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-171.png"><img class="aligncenter wp-image-7506" src="/images/52nlp.cn/e2885e40bb51bf30f5ecdb671bed9517.jpg" alt="prml7-171" width="239" height="70"></a><br>
然后再同这个这个公式重新计算<a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-141.png"><img class="alignnone wp-image-7490" src="/images/52nlp.cn/01ab2d9a82b3439cc604d3c3ba504d98.jpg" alt="prml7-141" width="76" height="25"></a></p>
<p> </p>
<p><a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-175.png"><img class="aligncenter wp-image-7507" src="/images/52nlp.cn/5a4a3bc4908f24d819384c8661e9f036.jpg" alt="prml7-175" width="300" height="126"></a></p>
<p>这样迭代计算，一直到到达一个人为确定的收敛条件，这就是确定<a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-141.png"><img class="alignnone wp-image-7490" src="/images/52nlp.cn/01ab2d9a82b3439cc604d3c3ba504d98.jpg" alt="prml7-141" width="56" height="19"></a>的过程。<br>
通过计算，最后的结果中，大部分参数<a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-177.png"><img class="alignnone wp-image-7508" src="/images/52nlp.cn/b30b11aae574a5d38603ff2c65b888b6.jpg" alt="prml7-177" width="35" height="24"></a>都是非常大甚至无穷大的值，从而根据w后验均值和方差的公式，其均值和方差都等于0，这样<a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-178.png"><img class="alignnone wp-image-7509" src="/images/52nlp.cn/675d3088d84f2995d9e3078543f689de.jpg" alt="prml7-178" width="28" height="23"></a>的值就是0，其对应的基函数就不起作用了 ，从而达到了稀疏的目的。这就是RVM稀疏的原因。<br>
需要实际推导一下整个过程，才能明白为什么大部分<a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-177.png"><img class="alignnone wp-image-7508" src="/images/52nlp.cn/b30b11aae574a5d38603ff2c65b888b6.jpg" alt="prml7-177" width="35" height="24"></a>都趋于无穷大。那些<a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-1781.png"><img class="alignnone wp-image-7510" src="/images/52nlp.cn/b7d8242b385352c42bbba280199fe7a9.jpg" alt="prml7-178" width="28" height="23"></a>不为0的<a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-180.png"><img class="alignnone wp-image-7511" src="/images/52nlp.cn/9498be8074018ba224b5a529ccb73c62.jpg" alt="prml7-180" width="25" height="22"></a>叫做relevance vectors，相当于SVM中的支持向量。需要强调，这种获得稀疏性的机制可以用于任何基函数的线性组合中。这种获得稀疏性的机制似乎非常普遍的。<br>
求得超参数，就可以通过下面式子得到新样本的分布：</p>
<p><a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-181.png"><img class="aligncenter wp-image-7512" src="/images/52nlp.cn/9108931dd540c9a70425bfe4b97bdee5.jpg" alt="prml7-181" width="400" height="50"></a></p>
<p>下面看一个图示：</p>
<p><a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-182.png"><img class="aligncenter wp-image-7513" src="/images/52nlp.cn/43915b79038ff79d36864219d57fa7a0.jpg" alt="prml7-182" width="350" height="255"></a></p>
<p>可以看到，其相关向量的数量比SVM少了很多，跟SVM相比的缺点是，RVM的优化函数不是凸函数，训练时间比SVM长，书上接下来专门对RVM的稀疏性进行分析，并且介绍了一种更快的求<a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-141.png"><img class="alignnone wp-image-7490" src="/images/52nlp.cn/01ab2d9a82b3439cc604d3c3ba504d98.jpg" alt="prml7-141" width="56" height="19"></a>的方法：</p>
<p><a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-185.png"><img class="wp-image-7514 aligncenter" src="/images/52nlp.cn/452b8b8bd8be4712b220ef4bc4ac6707.jpg" alt="prml7-185" width="489" height="273"></a></p>
<p>我接着讲RVM分类，我们看逻辑回归分类的模型:</p>
<p><a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-189.png"><img class="aligncenter wp-image-7516" src="/images/52nlp.cn/903b349deaf785d0a72b68b53467f7ee.jpg" alt="prml7-189" width="233" height="32"></a></p>
<p><a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-190.png"><img class="alignnone wp-image-7517" src="/images/52nlp.cn/916df9972541b9d2bf493165933952f2.jpg" alt="prml7-190" width="30" height="24"></a>是sigmoid函数，我们引入w的先验分布，跟RVM回归相同，每个<a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-191.png"><img class="alignnone wp-image-7518" src="/images/52nlp.cn/129a801cc3b69c2e26a824482e0edaed.jpg" alt="prml7-191" width="25" height="21"></a>对应一个不同的精度<br>
这种先验叫做ARD先验，跟RVM回归相比，在求<a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-147.png"><img class="alignnone wp-image-7493" src="/images/52nlp.cn/24c1df957dfec2c2a22e77217a53aca5.jpg" alt="prml7-147" width="60" height="18"></a>的分布时，不再对w进行积分。<br>
我们看在RVM回归时，是这么求<a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-147.png"><img class="alignnone wp-image-7493" src="/images/52nlp.cn/24c1df957dfec2c2a22e77217a53aca5.jpg" alt="prml7-147" width="60" height="18"></a>的：</p>
<p><a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-149.png"><img class="aligncenter wp-image-7494" src="/images/52nlp.cn/c31a6ca8846b2858059c7d668e338b8f.jpg" alt="prml7-149" width="300" height="44"></a></p>
<p>从而得到：</p>
<p><a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-150.png"><img class="aligncenter wp-image-7495" src="/images/52nlp.cn/6fc7a490aa3d156b7b6e02af6f953689.jpg" alt="prml7-150" width="400" height="61"></a></p>
<p>在RVM分类时，因为涉用到sigmod函数,计算积分很难，具体的为什么难，在第四章4.5节有更多的介绍，我们这里用Laplace approximation来求<a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-147.png"><img class="alignnone wp-image-7493" src="/images/52nlp.cn/24c1df957dfec2c2a22e77217a53aca5.jpg" alt="prml7-147" width="60" height="18"></a>的近似高斯分布，Laplace approximation我叫拉普拉斯近似，后面都写中文了。<br>
先看下拉普拉斯近似的原理，拉普拉斯近似的目的是找到连续变量的分布函数的高斯近似分布，也就是用高斯分布 近似模拟一个不是高斯分布的分布。<br>
假设一个单变量z，其分布是<a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-200.png"><img class="alignnone wp-image-7519" src="/images/52nlp.cn/13b0b6a4364a7fcd1cdc116c1193edc1.jpg" alt="prml7-200" width="89" height="39"></a>，分母上的Z是归一化系数<a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-201.png"><img class="alignnone wp-image-7520" src="/images/52nlp.cn/6a78ccd59f2bfb7dc0ec6c95d591b07b.jpg" alt="prml7-201" width="79" height="24"></a>，目标是找到一个可以近似p(z)的高斯分布q(z)。<br>
第一步是先找到p(z)的mode(众数) ，众数mode是一个统计学的概念，可以代表一组数据，不受极端数据的影响，比如可以选择中位数做一组实数的众数，对于高斯分布，众数就是其峰值。一组数据可能没有众数也可能有几个众数。<br>
拉普拉斯分布第一步要找到p(z)的众数<a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-203.png"><img class="alignnone wp-image-7521" src="/images/52nlp.cn/b51c879c43f7d4ba01f109d839882b98.jpg" alt="prml7-203" width="23" height="21"></a>，这是p(z)的一个极大值点，可能是局部的，因为p(z)可能有多个局部极大值。在该点，一阶导数等于0，<a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-205.png"><img class="alignnone wp-image-7523" src="/images/52nlp.cn/6d697339ae20468daf9bbef89ed88641.jpg" alt="prml7-205" width="73" height="26"></a>，后面再说怎么找<a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-203.png"><img class="alignnone wp-image-7521" src="/images/52nlp.cn/b51c879c43f7d4ba01f109d839882b98.jpg" alt="prml7-203" width="23" height="21"></a>。找到<a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-203.png"><img class="alignnone wp-image-7521" src="/images/52nlp.cn/b51c879c43f7d4ba01f109d839882b98.jpg" alt="prml7-203" width="23" height="21"></a>后，用<a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-208.png"><img class="alignnone wp-image-7524" src="/images/52nlp.cn/2ee0c0653701b0d445cc35b1742c30f0.jpg" alt="prml7-208" width="47" height="22"></a>的泰勒展开来构造一个二次函数：</p>
<p><a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-210.png"><img class="aligncenter wp-image-7525" src="/images/52nlp.cn/f203f571e5c4b9cfcafe2a5df6aede93.jpg" alt="prml7-210" width="246" height="47"></a></p>
<p>其中A是f(z)在<a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-203.png"><img class="alignnone wp-image-7521" src="/images/52nlp.cn/b51c879c43f7d4ba01f109d839882b98.jpg" alt="prml7-203" width="23" height="21"></a>的二阶导数再取负数。上式中，没有一阶导数部分，因为<a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-203.png"><img class="alignnone wp-image-7521" src="/images/52nlp.cn/b51c879c43f7d4ba01f109d839882b98.jpg" alt="prml7-203" width="23" height="21"></a>是局部极大值，一阶导数为0，把上式两边取指数，得到：</p>
<p><a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-221.png"><img class="wp-image-7526 aligncenter" src="/images/52nlp.cn/619f5fa128c995cb3ddf8e6023738306.jpg" alt="prml7-221" width="242" height="43"></a></p>
<p>把<a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-224.png"><img class="alignnone wp-image-7527" src="/images/52nlp.cn/ab463aa0251bc608748331788cce65cf.jpg" alt="prml7-224" width="30" height="26"></a>换成归一化系数，得到近似的高斯分布：</p>
<p><a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-225.png"><img class="aligncenter wp-image-7528" src="/images/52nlp.cn/ebec1a1327597aee0b66d316ba0a8f54.jpg" alt="prml7-225" width="258" height="49"></a></p>
<p>拉普拉斯分布得到的近似高斯分布的一个图示：</p>
<p><a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-226.png"><img class="aligncenter wp-image-7529" src="/images/52nlp.cn/8172709b41e6e3716ac7a7dd6d3f1ee1.jpg" alt="prml7-226" width="460" height="199"></a></p>
<p>注意，高斯近似存在的条件是，原分布的二阶导数取负数、也就是高斯近似的精确度A&gt;0，也就是驻点<a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-203.png"><img class="alignnone wp-image-7521" src="/images/52nlp.cn/b51c879c43f7d4ba01f109d839882b98.jpg" alt="prml7-203" width="23" height="21"></a>必须是局部极大值，f(x)在改点出的导数为负数。当z是一个M维向量时，近似方法跟单变量的不同只是二阶导数的负数A变成了MxM维的海森矩阵的负数。<br>
多维变量近似后的高斯分布如下：</p>
<p><a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-250.png"><img class="aligncenter wp-image-7530" src="/images/52nlp.cn/3297f5bbf4fb74ea4f95c105d2c749d6.jpg" alt="prml7-250" width="400" height="58"></a></p>
<p>A是海森矩阵的负数.mode众数<a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-203.png"><img class="alignnone wp-image-7521" src="/images/52nlp.cn/b51c879c43f7d4ba01f109d839882b98.jpg" alt="prml7-203" width="23" height="21"></a>一般是通过数值优化算法来寻找的，不讲了。<br>
再回来看用拉普拉斯分布来近似RVM分类中的<a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-147.png"><img class="alignnone wp-image-7493" src="/images/52nlp.cn/24c1df957dfec2c2a22e77217a53aca5.jpg" alt="prml7-147" width="60" height="18"></a>：<br>
刚才拉普拉斯分布忘了说一个公式，就是求得q(z)后，确定<a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-200.png"><img class="alignnone wp-image-7519" src="/images/52nlp.cn/13b0b6a4364a7fcd1cdc116c1193edc1.jpg" alt="prml7-200" width="89" height="39"></a>中的分母，也就是归一化系数的公式：</p>
<p><a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-252.png"><img class="aligncenter wp-image-7531" src="/images/52nlp.cn/2acdce5f24d6d6993653bb2e690425c3.jpg" alt="prml7-252" width="350" height="127"></a></p>
<p>这个一会有用。先看RVM中对w的后验分布的近似，先求后验分布的mode众数，通过最大化log后验分布<a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-253.png"><img class="alignnone wp-image-7532" src="/images/52nlp.cn/2e042045c196ecdf67584bede9703287.jpg" alt="prml7-253" width="95" height="22"></a>来求mode.先写出这个log后验分布：</p>
<p><a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-255.png"><img class="aligncenter wp-image-7533" src="/images/52nlp.cn/a892da69f67d1371b3be2be04c5de799.jpg" alt="prml7-255" width="461" height="82"></a></p>
<p>其中<a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-135.png"><img class="alignnone wp-image-7483" src="/images/52nlp.cn/0a055de07c3b81d693fc8315d73df92c.jpg" alt="prml7-135" width="98" height="20"></a><br>
最后求得的高斯近似的均值(也就是原分布的mode)和精度如下：</p>
<p><a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-256.png"><img class="aligncenter wp-image-7534" src="/images/52nlp.cn/69dc8c1687afafadbde2314ca9d1d02b.jpg" alt="prml7-256" width="227" height="66"></a></p>
<p>现在用这个w后验高斯近似来求边界似然<a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-258.png"><img class="alignnone wp-image-7535" src="/images/52nlp.cn/6267d7cf0db322c083dac5fb5bc370eb.jpg" alt="prml7-258" width="276" height="30"></a><br>
根据前面那个求归一化系数Z的公式Z=<a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-259.png"><img class="alignnone wp-image-7536" src="/images/52nlp.cn/d25835c1ffb0f0a38cb525a142741eda.jpg" alt="prml7-259" width="94" height="41"></a><br>
有:</p>
<p><a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-260.png"><img class="aligncenter wp-image-7537" src="/images/52nlp.cn/b50ffda19afb0e102300eef9c341a9ca.jpg" alt="prml7-260" width="350" height="75"></a></p>
<p>RVM这部分大量基于第二章高斯分布和第三、四两章，公式推导很多，需要前后关联才能看明白。</p>
<p>注：PRML读书会系列文章由 <a href="http://weibo.com/dmalgorithms">@Nietzsche_复杂网络机器学习</a> 同学授权发布，转载请注明原作者和相关的主讲人，谢谢。</p>
<p>PRML读书会讲稿PDF版本以及更多资源下载地址：<a href="http://vdisk.weibo.com/u/1841149974">http://vdisk.weibo.com/u/1841149974</a></p>
<p>本文链接地址：<a href="http://www.52nlp.cn/prml%E8%AF%BB%E4%B9%A6%E4%BC%9A%E7%AC%AC%E4%B8%83%E7%AB%A0-sparse-kernel-machines">http://www.52nlp.cn/prml读书会第七章-sparse-kernel-machines</a></p>

											


  </article>

  <div class="meta">
  
    <a class="basic-alignment left" href="/posts/2015/2015-01-28-83821-cb2ee3ffb.html" title="Previous Post: .NET中使用Redis" data-instant>&laquo; .NET中使用Redis</a>
  
  
    <a class="basic-alignment right" href="/posts/2015/2015-01-31-prml-e8-af-bb-e4-b9-a6-e4-bc-9a-e7-ac-ac-e4-b9-9d-e7-ab-a0-mixture-models-and-em-a7767c4f3.html" title="Next Post: PRML读书会第九章  Mixture Models and EM" data-instant>PRML读书会第九章  Mixture Models and EM &raquo;</a>
  
</div>
  <div id="related">
  <h2 class="subheader">Related Posts <small>They might be useful</small></h2>
  <ul class="posts">
    
      <li><span>14 May 2018</span> &raquo; <a href="http://iftti.com/posts/2018/2018-05-14-113985-6dc6e9281.html">给初学者看的 shuf 命令教程</a></li>
    
      <li><span>13 May 2018</span> &raquo; <a href="http://iftti.com/posts/2018/2018-05-13-113977-b56537dc0.html">常用排序算法总结（2）</a></li>
    
      <li><span>13 May 2018</span> &raquo; <a href="http://iftti.com/posts/2018/2018-05-13-113953-957e4ea5a.html">10 个常用的软件架构模式</a></li>
    
  </ul>
</div>

  
<comments>

<!-- UY BEGIN -->
<div id="uyan_frame"></div>
<script type="text/javascript" src="http://v2.uyan.cc/code/uyan.js?uid=1936498"></script>
<!-- UY END -->

</comments>

</div>
      <!-- JiaThis Button BEGIN -->
<div class="jiathis_share_slide jiathis_share_24x24" id="jiathis_share_slide">
<div class="jiathis_share_slide_top" id="jiathis_share_title"></div>
<div class="jiathis_share_slide_inner">
<div class="jiathis_style_24x24">
<a class="jiathis_button_tsina"></a>
<a class="jiathis_button_googleplus"></a>
<a class="jiathis_button_twitter"></a>
<a class="jiathis_button_linkedin"></a>
<a class="jiathis_button_weixin"></a>
<a class="jiathis_button_cqq"></a>
<a class="jiathis_button_renren"></a>
<a class="jiathis_button_evernote"></a>
<a class="jiathis_button_pocket"></a>
<a href="http://www.jiathis.com/share" class="jiathis jiathis_txt jtico jtico_jiathis" target="_blank"></a>
<script type="text/javascript">
var jiathis_config = {data_track_clickback:'true'
	,slide:{
		divid:'wrap',
		pos:'left',
		gt:'true'
	}
};
</script>
<script type="text/javascript" src="http://v3.jiathis.com/code/jia.js?uid=1936498" charset="utf-8"></script>
<script type="text/javascript" src="http://v3.jiathis.com/code/jiathis_slide.js" charset="utf-8"></script>
</div></div></div>
<!-- JiaThis Button END -->
      </div>
    </div>

    <footer class="site-footer">

  <div class="wrap">

    <h2 class="footer-heading">IT技术干货</h2>

    <div class="footer-col-1 column">
      <p class="text">IT技术干货 KernelHacks 最好的技术站点 技术信息 纯干货</p>
      <ul>
        <li>汇集最好的科技与互联网信息</li>
        <li>Liu Lantao</li>
        <li><a href="mailto:iftti@iftti.com">iftti@iftti.com</a></li>
      </ul>
    </div>

    <div class="footer-col-2 column">
      <ul>
        <li>
          <a href="https://github.com/Lax">
            <span class="icon github">
              <svg version="1.1" class="github-icon-svg" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
                 viewBox="0 0 16 16" enable-background="new 0 0 16 16" xml:space="preserve">
                <path fill-rule="evenodd" clip-rule="evenodd" fill="#C2C2C2" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761
                c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32
                c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472
                c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037
                C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65
                c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261
                c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082
                c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129
                c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/>
              </svg>
            </span>
            <span class="username">Lax</span>
          </a>
        </li>
        <li>
          <a href="https://twitter.com/liulantao">
            <span class="icon twitter">
              <svg version="1.1" class="twitter-icon-svg" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
                 viewBox="0 0 16 16" enable-background="new 0 0 16 16" xml:space="preserve">
                <path fill="#C2C2C2" d="M15.969,3.058c-0.586,0.26-1.217,0.436-1.878,0.515c0.675-0.405,1.194-1.045,1.438-1.809
                c-0.632,0.375-1.332,0.647-2.076,0.793c-0.596-0.636-1.446-1.033-2.387-1.033c-1.806,0-3.27,1.464-3.27,3.27
                c0,0.256,0.029,0.506,0.085,0.745C5.163,5.404,2.753,4.102,1.14,2.124C0.859,2.607,0.698,3.168,0.698,3.767
                c0,1.134,0.577,2.135,1.455,2.722C1.616,6.472,1.112,6.325,0.671,6.08c0,0.014,0,0.027,0,0.041c0,1.584,1.127,2.906,2.623,3.206
                C3.02,9.402,2.731,9.442,2.433,9.442c-0.211,0-0.416-0.021-0.615-0.059c0.416,1.299,1.624,2.245,3.055,2.271
                c-1.119,0.877-2.529,1.4-4.061,1.4c-0.264,0-0.524-0.015-0.78-0.046c1.447,0.928,3.166,1.469,5.013,1.469
                c6.015,0,9.304-4.983,9.304-9.304c0-0.142-0.003-0.283-0.009-0.423C14.976,4.29,15.531,3.714,15.969,3.058z"/>
              </svg>
            </span>
            <span class="username">@liulantao</span>
          </a>
        </li>
        <li>
          <a href="https://plus.google.com/+LiuLantao">
            <span class="icon googleplus">
              <svg version="1.1" id="Layer_1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
                width="16px" height="16px" viewBox="0 0 134.658 131.646" enable-background="new 0 0 134.658 131.646"
                xml:space="preserve">
                <g>
                  <path fill="#C2C2C2" d="M126.515,4.109H8.144c-2.177,0-3.94,1.763-3.94,3.938v115.546c0,2.179,1.763,3.942,3.94,3.942h118.371
                  c2.177,0,3.94-1.764,3.94-3.942V8.048C130.455,5.872,128.691,4.109,126.515,4.109z"/>
                  <g>
                    <path fill="#FFFFFF" d="M70.479,71.845l-3.983-3.093c-1.213-1.006-2.872-2.334-2.872-4.765c0-2.441,1.659-3.993,3.099-5.43
                    c4.64-3.652,9.276-7.539,9.276-15.73c0-8.423-5.3-12.854-7.84-14.956h6.849l7.189-4.517H60.418
                    c-5.976,0-14.588,1.414-20.893,6.619c-4.752,4.1-7.07,9.753-7.07,14.842c0,8.639,6.633,17.396,18.346,17.396
                    c1.106,0,2.316-0.109,3.534-0.222c-0.547,1.331-1.1,2.439-1.1,4.32c0,3.431,1.763,5.535,3.317,7.528
                    c-4.977,0.342-14.268,0.893-21.117,5.103c-6.523,3.879-8.508,9.525-8.508,13.51c0,8.202,7.731,15.842,23.762,15.842
                    c19.01,0,29.074-10.519,29.074-20.932C79.764,79.709,75.344,75.943,70.479,71.845z M56,59.107
                    c-9.51,0-13.818-12.294-13.818-19.712c0-2.888,0.547-5.87,2.428-8.199c1.773-2.218,4.861-3.657,7.744-3.657
                    c9.168,0,13.923,12.404,13.923,20.382c0,1.996-0.22,5.533-2.762,8.09C61.737,57.785,58.762,59.107,56,59.107z M56.109,103.65
                    c-11.826,0-19.452-5.657-19.452-13.523c0-7.864,7.071-10.524,9.504-11.405c4.64-1.561,10.611-1.779,11.607-1.779
                    c1.105,0,1.658,0,2.538,0.111c8.407,5.983,12.056,8.965,12.056,14.629C72.362,98.542,66.723,103.65,56.109,103.65z"/>
                    <polygon fill="#FFFFFF" points="98.393,58.938 98.393,47.863 92.923,47.863 92.923,58.938 81.866,58.938 81.866,64.469
                    92.923,64.469 92.923,75.612 98.393,75.612 98.393,64.469 109.506,64.469 109.506,58.938"/>
                  </g>
                </g>
              </svg>
            </span>
            <span class="username">+LiuLantao</span>
          </a>
        </li>
      </ul>
    </div>

    <div class="footer-col-3 column">
      
<!--以下是QQ邮件列表订阅嵌入代码--><script >var nId = "6be92ef3590ee662cd5e6381ab2044c328716364f684cf3e",nWidth="auto",sColor="light",sText="填写您的邮件地址，订阅我们的精彩内容：" ;</script><script src="http://list.qq.com/zh_CN/htmledition/js/qf/page/qfcode.js" charset="gb18030"></script>

    </div>

  </div>

  <div class="wrap">
    <div>
      <a href="http://blog.liulantao.com">Blog</a> | <a href="http://1000bit.com">铅笔特评 1000bit</a> | <a href="http://visplanet.com">VisPlanet</a> | <a href="http://iftti.com">IT技术干货</a> | <a href="http://relax.org.cn">Relax</a> | <a href="http://hangzhou.io">杭州城市指南</a>
    </div>
  </div>

  
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-1658815-7', 'iftti.com');
  ga('require', 'displayfeatures');
  ga('send', 'pageview');

</script>


</footer>


    </body>
</html>