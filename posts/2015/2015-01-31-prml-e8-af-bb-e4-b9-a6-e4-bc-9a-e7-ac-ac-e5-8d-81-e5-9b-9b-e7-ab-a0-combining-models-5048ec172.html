<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>PRML读书会第十四章 Combining Models | IT技术干货</title>
    <meta name="viewport" content="width=device-width">
    <meta name="description" content="IT技术干货 KernelHacks 最好的技术站点 技术信息 纯干货">
    <link rel="canonical" href="http://iftti.com/posts/2015/2015-01-31-prml-e8-af-bb-e4-b9-a6-e4-bc-9a-e7-ac-ac-e5-8d-81-e5-9b-9b-e7-ab-a0-combining-models-5048ec172.html">

    <!-- Custom CSS -->
    <link rel="stylesheet" href="/css/main.css">

</head>


    <body>

    <header class="site-header">

  <div class="wrap">

    <a class="site-title" href="/">IT技术干货</a>

    <nav class="site-nav">
      <a href="#" class="menu-icon">
        <svg version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
           viewBox="0 0 18 15" enable-background="new 0 0 18 15" xml:space="preserve">
          <path fill="#505050" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0
            h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
          <path fill="#505050" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484
            h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
          <path fill="#505050" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0
            c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
        </svg>
      </a>
      <div class="trigger">
        
          
        
          <a class="page-link" href="/about/">About</a>
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
      </div>
    </nav>

  </div>

</header>


    <div class="page-content">
      <div class="wrap">
      <div class="post">

  <header class="post-header">
    <h1>PRML读书会第十四章 Combining Models</h1>
    <p class="meta"><span class="time">Jan 31, 2015</span><span class="source_url"> • 来自 52nlp.cn <a name="52nlp.cn" href="http://www.52nlp.cn/prml%e8%af%bb%e4%b9%a6%e4%bc%9a%e7%ac%ac%e5%8d%81%e5%9b%9b%e7%ab%a0-combining-models" target="_blank">[原文链接]</a></span></p>
  </header>

  <article class="post-content">
  

						<p style="text-align: center"><span style="font-family: 微软雅黑;font-size: 15pt"><strong>PRML读书会第十四章 Combining Models<br>
</strong></span></p>
<p style="text-align: center"><span style="font-family: 微软雅黑;font-size: 15pt"><strong>主讲人 网神<br>
</strong></span></p>
<p style="text-align: center"><span style="font-family: 微软雅黑;font-size: 12pt"><strong>（新浪微博: <a href="http://weibo.com/ghtimaq">@豆角茄子麻酱凉面</a>）<br>
</strong></span></p>
<p><span style="font-family: 微软雅黑;font-size: 12pt">网神(66707180) 18:57:18<br>
</span></p>
<p><span style="font-family: 微软雅黑;font-size: 12pt">大家好，今天我们讲一下第14章combining models，这一章是联合模型，通过将多个模型以某种形式结合起来，可以获得比单个模型更好的预测效果。包括这几部分：<br>
committees, 训练多个不同的模型，取其平均值作为最终预测值。<br>
</span></p>
<p><span style="font-family: 微软雅黑;font-size: 12pt">boosting: 是committees的特殊形式，顺序训练L个模型，每个模型的训练依赖前一个模型的训练结果。<br>
决策树：不同模型负责输入变量的不同区间的预测，每个样本选择一个模型来预测，选择过程就像在树结构中从顶到叶子的遍历。<br>
conditional mixture model条件混合模型：引入概率机制来选择不同模型对某个样本做预测，相比决策树的硬性选择，要有很多优势。<br>
</span></p>
<p><span style="font-family: 微软雅黑;font-size: 12pt">本章主要介绍了这几种混合模型。讲之前，先明确一下混合模型与Bayesian model averaging的区别，贝叶斯模型平均是这样的：假设有H个不同模型h，每个模型的先验概率是p(h)，一个数据集的分布是： <img src="/images/52nlp.cn/34049e89a2b52fec00e21028537c8a95.jpg" alt=""><br>
整个数据集X是由一个模型生成的，关于h的概率仅仅表示是由哪个模型来生成的 这件事的不确定性。而本章要讲的混合模型是数据集中，不同的数据点可能由不同模型生成。看后面讲到的内容就明白了。</span><span id="more-8143"></span></p>
<p><span style="font-family: 微软雅黑;font-size: 12pt"> 首先看committes，committes是一大类，包括boosting，首先将最简单的形式，就是讲多个模型的预测的平均值作为最后的预测。主要讲这么做的合理性，为什么这么做会提高预测性能。从频率角度的概念，bias-variance trade-off可以解释，这个理论在3.5节讲过，我们把这个经典的图copy过来：<br>
<img src="/images/52nlp.cn/d7fd27da2d2a4078b55b51a962f362b5.jpg" alt=""><br>
这个图大家都记得吧，左边一列是对多组数据分别训练得到一个模型，对应一条sin曲线，看左下角这个图，正则参数lamda取得比较小，得到一个bias很小，variance很大的一个模型 。每条线的variance都很大，这样模型预测的错误就比较大，但是把这么多条曲线取一个平均值，得到右下角图上的红色线，红色线跟真实sin曲线也就是蓝色线 基本拟合。所以用平均之后模型来预测，variance准确率就提高了很多，这是直观上来看，接下里从数学公式推导看下：<br>
有一个数据集，用bootstrap方法构造M个不同的训练集bootstrap方法就是从数据集中随机选N个放到训练集中，做M次，就得到M个训练集，M个训练集训练的到M个模型，用<img src="/images/52nlp.cn/260f62f05c7137d87363d42d0b4e6b71.jpg" alt="">表示，那么用committees方法，对于某个x，最终预测值是:<br>
<img src="/images/52nlp.cn/d026fc480331d6ecd46b8d5c71d5fa5f.jpg" alt=""><br>
我们来看这个预测值是如何比单个<img src="/images/52nlp.cn/207b73339c50de814a5445bfb85e5e4e.jpg" alt="">预测值准确的，假设准确的预测模型是h(x)，那么训练得到的y(x)跟h(x)的关系是：<br>
<img src="/images/52nlp.cn/8d881215254c90eb202f718e704e257b.jpg" alt=""><br>
后面那一项是模型的error<br>
ZealotMaster(850458544) 19:24:34<br>
能使error趋近于0嘛？<br>
网神(66707180) 19:25:13<br>
模型越好越趋近于0，但很难等于0，这里committes方法就比单个模型更趋近于0<br>
ZealotMaster(850458544) 19:25:28<br>
求证明<br>
网神(66707180) 19:25:39<br>
正在证明，平均平方和错误如下：<br>
<img src="/images/52nlp.cn/9feca5e8a98966062b8a00a2a5ae3a78.jpg" alt=""><br>
也就是单个模型的期望error是：<br>
</span><!--more--></p>
<p><img src="/images/52nlp.cn/edfc885341d5e2aff7cb8e9ce2d8cc9e.jpg" alt=""><span style="font-family: 微软雅黑;font-size: 12pt"><br>
如果用M个模型分别做预测，其平均错误是:<br>
</span></p>
<p><img src="/images/52nlp.cn/706886a24a82934640f0ed3cb2c77de5.jpg" alt=""><span style="font-family: 微软雅黑;font-size: 12pt"><br>
而如果用committes的结果来做预测，其期望错误是：<br>
<img src="/images/52nlp.cn/31942c7f7dfe2621817f0c9865e4a9c2.jpg" alt=""><br>
这个<img src="/images/52nlp.cn/1a89b3a12117c64cd4ce4ec4e94a73e8.jpg" alt="">跑到了平方的里面，如果假设不同模型的error都是0均值，并且互不相关，也就是：<br>
<img src="/images/52nlp.cn/c4125acd5323dde10033a3fc4bba789b.jpg" alt=""><br>
就可以得到：<br>
<img src="/images/52nlp.cn/9ce6f5f44232139ae5cea112cb38a2e6.jpg" alt=""><br>
</span></p>
<p><span style="font-family: 微软雅黑;font-size: 12pt">在不同模型error互不相关的假设的下，committes错误是单个模型error的1/M，但实际上，不同模型的error通常是相关的，因此error不会减少这么多，但肯定是小于单个模型的error，接下来讲boosting，可以不考虑那个假设，取得实质的提高.boosting应该是有不同的变种，其中最出名的就是AdaBoost, adaptive boosting. 它可以将多个弱分类器结合，取得很好的预测效果，所谓弱分类器就是，只要比随即预测强一点，大概就是只要准确率超50%就行吧，这是我的理解。<br>
boosting的思想是依次预测每个分类器，每个分类器训练时，对每个数据点加一个权重。训练完一个分类器模型，该模型分错的，再下一个模型训练时，增大权重；分对的，减少权重，具体的算法如下，我把整个算法帖出来，再逐步解释：<br>
<img src="/images/52nlp.cn/74327fcd9031e4d559da041b2c53566c.jpg" alt=""><br>
<img src="/images/52nlp.cn/295f07ae732d0fffd28e8c3cc0c29568.jpg" alt=""><br>
</span></p>
<p><span style="font-family: 微软雅黑;font-size: 12pt">大家看下面这个图比较形象：<br>
</span></p>
<p><img src="/images/52nlp.cn/e590871e4c1161436b676afe1f724a66.jpg" alt=""><span style="font-family: 宋体;font-size: 12pt"><br>
</span></p>
<p><span style="font-family: 微软雅黑;font-size: 12pt">第一步，初始化每个数据点的权重为1/N.接下来依次训练M个分类器，每个分类器训练时，最小化加权的错误函数(14.15)，错误函数看上面贴的算法，从这个错误函数可以看出，权重相同时，尽量让更多的x分类正确，权重不同时，优先让权重大的x分类正确，训练完一个模型后，式(14.16)计算<img src="/images/52nlp.cn/807e0108a531b7e3541ef0f0f4f3ece1.jpg" alt="">，既分类错误的样本的加权比例. 然后式(14.17)计算:<br>
</span></p>
<p><img src="/images/52nlp.cn/2db3e0932ccb3ce7187014eed4a66bb5.jpg" alt=""><span style="font-family: 微软雅黑;font-size: 12pt"><br>
只要分类器准确率大于50%，<img src="/images/52nlp.cn/41cc5a635ecdc66bca26bc7f3701f079.jpg" alt="">就小于0.5, <img src="/images/52nlp.cn/74f10425ee28391e2e2f2bdb5fe0c54e.jpg" alt="">就大于0。而且<img src="/images/52nlp.cn/a19d93aa1bb729ab897d177389b86d75.jpg" alt="">越小(既对应的分类器准确率越高)，<img src="/images/52nlp.cn/2fe09f26a79caaead8801af51e4ebe94.jpg" alt="">就越大，然后用<img src="/images/52nlp.cn/ead1ea951290b8e893a4cba92085cd28.jpg" alt="">更新每个数据点的权重，即式(14.18)：<br>
<img src="/images/52nlp.cn/da9febec80ce6450d40a1e28f71df3c3.jpg" alt=""><br>
可以看出，对于分类错误的数据点，<img src="/images/52nlp.cn/41b20a6189947ac9baa3ff337c56e9c8.jpg" alt="">大于0，所以exp(a)就大于1，所以权重变大。但是从这个式子看不出，对于分类正确的样本，权重变小。这个式子表明，分类正确的样本，其权重不变 ，因为exp(0)=1.这是个疑问。如此循环，训练完所有模型，最后用式(14.19)做预测:<br>
</span></p>
<p><img src="/images/52nlp.cn/3c2a3346f113a9b81d3e5fe279875d95.jpg" alt=""><span style="font-family: 微软雅黑;font-size: 12pt"><br>
从上面过程可以看出，如果训练集合中某个样本在逐步训练每个模型时，一直被分错，他的权重就会一直变大，最后对应的<img src="/images/52nlp.cn/bb9d57b80044785da4fed52b76c63f11.jpg" alt="">也越来越大，下面看一个图例：<br>
<img src="/images/52nlp.cn/01c27eaff2ed90a28ea7acc4a30630da.jpg" alt=""><br>
图中有蓝红两类样本 ，分类器是单个的平行于轴线的阈值 ，第一个分类器(m=1)把大部分点分对了，但有2个蓝点，3个红点不对，m=2时，这几个错的就变大了，圈越大，对应其权重越大 ，后面的分类器似乎是专门为了这个几个错分点而在努力工作，经过150个分类器，右下角那个图的分割线已经很乱了，看不出到底对不对 ，应该是都已经分对了吧。<br>
网神(66707180) 19:59:59<br>
@ZealotMaster 不知道是否明白点了，大家有啥问题？<br>
ZealotMaster(850458544) 20:00:14<br>
嗯，清晰一些了，这个也涉及over fitting嘛？感觉m=150好乱……<br>
苦瓜炒鸡蛋(852383636) 20:02:23<br>
是过拟合<br>
网神(66707180) 20:02:25<br>
</span></p>
<p><span style="font-family: 微软雅黑;font-size: 12pt">不同的分割线，也就是不同的模型，是主要针对不同的点的，针对哪些点，由模型组合时的系数<img src="/images/52nlp.cn/aafecb351a1a31b0f825770387701584.jpg" alt="">来影响。<br>
苦瓜炒鸡蛋(852383636) 20:04:50<br>
</span></p>
<p><span style="font-family: 微软雅黑;font-size: 12pt">这是韩家炜 那个数据挖掘书的那一段：<br>
<img src="/images/52nlp.cn/f86f32740fb812fc6e23eb32b195e041.jpg" alt="">网神(66707180) 20:04:56<br>
嗯，这章后面也讲到了boosting对某些错分的样本反应太大，一些异常点会对模型造成很大的影响。<br>
</span></p>
<p><span style="color: #333333;font-family: Arial;font-size: 12pt">================================================================<br>
</span></p>
<p><span style="font-family: 微软雅黑;font-size: 12pt">接下来讲boosting的错误函数，我们仔细看下对boosting错误函数的分析，boosting最初用统计学习理论来分析器泛化错误的边界bound，但后来发现这个bound太松，没有意义。实际性能比这个理论边界好得多，后来用指数错误函数来表示。从优化指数损失函数来解释adaboost比较直观，每次固定其他分类器和系数将常量分出来，能推出单分类器的损失函数及系数，再根据常量的形式能得出下一步数据权重的更新方式。指数错误函数如下：<br>
<img src="/images/52nlp.cn/a832ff02b251a31145ec2978d68eff2c.jpg" alt=""><br>
</span></p>
<p><span style="font-family: 微软雅黑;font-size: 12pt">其中N是N个样本点，<img src="/images/52nlp.cn/b8c3a8bf62dc4c9e13218d8403823502.jpg" alt="">是多个线性分类器的线性组合:<br>
</span></p>
<p><img src="/images/52nlp.cn/d41bb2c0761dab497048dc45bfd4ffcb.jpg" alt=""><span style="font-family: 微软雅黑;font-size: 12pt"><br>
</span></p>
<p><span style="font-family: 微软雅黑;font-size: 12pt"><br>
<img src="/images/52nlp.cn/b8687ae74c4e3b9fed6b093363871e50.jpg" alt="">是分类的目标值。我们的目标是训练系数<img src="/images/52nlp.cn/eb49075d7a0a1f14bf88060f24fb4f29.jpg" alt="">和分类器<img src="/images/52nlp.cn/3eb90f63a61969bf3d35ef14ae7bdc84.jpg" alt="">的参数，使E最小。<br>
最小化E的方法，是先只针对一个分类器进行最小化，而固定其他分类器的参数，例如我们固定<img src="/images/52nlp.cn/811390986a918a4328a5279ae2e05ceb.jpg" alt="">和其系数<img src="/images/52nlp.cn/894dde0986ff93853f09a0c9dba2b9da.jpg" alt="">，只针对<img src="/images/52nlp.cn/f999ce74aa7649e48e050a5a3f981326.jpg" alt="">做优化，这样错误函数E可以改写为：<br>
<img src="/images/52nlp.cn/373a495db2fdbb519fbc8cb765c45dfa.jpg" alt=""><br>
也就是把固定的分类器的部分都当做一个常量：<img src="/images/52nlp.cn/cde9e250563b43d4d752d702013f1f15.jpg" alt="">，只保留<img src="/images/52nlp.cn/cebba926cd9548ae16514f83ac833c22.jpg" alt="">相关的部分。我们用<img src="/images/52nlp.cn/a7ca764158500d7e35c390dfea231508.jpg" alt="">表示<img src="/images/52nlp.cn/53ad4e58c492c5ef02d8b5dc4d2c43ef.jpg" alt="">分对的数据集，<img src="/images/52nlp.cn/02b058b549de0c8290c43ce15af9fa44.jpg" alt="">表示分错的数据集，E又可以写成如下形式：<br>
<img src="/images/52nlp.cn/d319b0ebe90b8f42d323cd8ca7058933.jpg" alt="">上式中，因为将数据分成两部分，也就确定了<img src="/images/52nlp.cn/669955e9a11c6efd6ee26a2cf76d36c4.jpg" alt="">个<img src="/images/52nlp.cn/7221debdbb3f59bbe487e43acfe790a5.jpg" alt="">是否相等，所以消去了这两个变量<br>
看起来清爽点了：<br>
<img src="/images/52nlp.cn/fec2e051f7afb0652921e4402e50ae60.jpg" alt=""><br>
</span></p>
<p><span style="font-family: 微软雅黑;font-size: 12pt">这里面后一项是常量，前一项就跟前面boosting的算法里所用的错误函数：<img src="/images/52nlp.cn/ea919b926a43c4de2201f4f3816060dc.jpg" alt="">形式一样了。<br>
</span></p>
<p><span style="font-family: 微软雅黑;font-size: 12pt">上面是将：<img src="/images/52nlp.cn/f149927c432b8f2592555df3a13249b6.jpg" alt="">对<img src="/images/52nlp.cn/c86fb57e89d291fce7a7634fc521d00a.jpg" alt="">做最小化得出的结论,即指数错误函数就是boosting里求单个模型时所用的错误函数.类似，也可以得到指数错误函数里的<img src="/images/52nlp.cn/1e2c14d1dae0e07b7c3659f94461b258.jpg" alt="">就是boosting里的<img src="/images/52nlp.cn/33f552c6b7bcfa63dd0ef3dd6ce33808.jpg" alt="">，确定了<img src="/images/52nlp.cn/36805bbb7eb128a7e8f8a39a8242ebfc.jpg" alt="">根据<img src="/images/52nlp.cn/68ecdd3a5231a2fb61c36b4e30f5b5a7.jpg" alt="">以及<img src="/images/52nlp.cn/2e56f39601ac51a6e978eae5248de331.jpg" alt=""><br>
</span></p>
<p><span style="font-family: 微软雅黑;font-size: 12pt">可以得到，更新w的方法：<br>
</span></p>
<p><img src="/images/52nlp.cn/99f557fc742c9d3238f5c52d4ac0d210.jpg" alt=""><span style="font-family: 微软雅黑;font-size: 12pt"><br>
又因为<img src="/images/52nlp.cn/0900b5f4e8d8c739a1803155b25afba4.jpg" alt="">有：<img src="/images/52nlp.cn/9f388e495a3cfacb5d0132ae5219df07.jpg" alt="">这又跟boosting里更新数据点权重的方法以一致。<br>
总之，就是想说明，用指数错误函数可以描述boosting的error分析,接下来看看指数错误函数的性质，再贴一下指数错误函数的形式：<br>
<img src="/images/52nlp.cn/67d2eb762baaf54f62c009489901ccc6.jpg" alt=""><br>
</span></p>
<p><img src="/images/52nlp.cn/9e2bd681a4be0fb754048de7ee7c7d77.jpg" alt=""><span style="font-family: 微软雅黑;font-size: 12pt"><br>
其期望error是：<br>
<img src="/images/52nlp.cn/4d1dedcacdfdcfc2d0decf2358bfecd3.jpg" alt=""><br>
</span></p>
<p><span style="font-family: 微软雅黑;font-size: 12pt">然后最所有的y(x)做variational minimization,得到：<img src="/images/52nlp.cn/bf904034bcd7eb6996e190b594af6fa2.jpg" alt=""><br>
这是half the log-odds ，看下指数错误函数的图形：</span></p>
<p><img src="/images/52nlp.cn/f8aa82ad988b68e84aa440c0e41d476a.jpg" alt=""><br>
绿色的线是指数错误函数<img src="/images/52nlp.cn/dd5d35e6b757c24a8b026353c11de82a.jpg" alt="">可以看到，对于分错的情况，既z&lt;0时，绿色的线呈指数趋势上升，所以异常点会对训练结果的影响很大。图中红色的线是corss-entropy 错误，是逻辑分类中用的错误函数，对分错的情况，随错误变大，其函数值基本是线性增加的，蓝色线是svm用的错误函数，叫hinge 函数。</p>
<p> </p>
<p><span style="color: #333333;font-family: Arial;font-size: 12pt">================================================================<br>
</span></p>
<p><span style="font-family: 微软雅黑;font-size: 12pt">大家有没有要讨论的？公式比较多，需要推导一下才能理解。接下来讲决策树和条件混合模型。决策树概念比较简单，容易想象是什么样子的，可以认为决策树是多个模型，每个模型负责x的一个区间的预测，通过树形结构来寻找某个x属于哪个区间，从而得到预测值。<br>
</span></p>
<p style="background: white"><span style="font-family: 微软雅黑;font-size: 12pt">决策树有多个算法比较出名，ID3, C4.5, CART，书上以CART为例讲的。<br>
</span></p>
<p style="background: white"><span style="font-family: 微软雅黑;font-size: 12pt">CART叫classification and regression trees先看一个图示：<br>
</span></p>
<p style="background: white"><img src="/images/52nlp.cn/b80d17c180021e8e8dbbc88e621bf8bf.jpg" alt=""><span style="font-family: 微软雅黑;font-size: 12pt"><br>
</span></p>
<p style="background: white"><span style="font-family: 微软雅黑;font-size: 12pt">这个二维输入空间，被划分成5个区间，每个区间的类别分别是A-E，它的决策树如下，很简单明了：<br>
</span></p>
<p style="background: white"><img src="/images/52nlp.cn/2dfa58b5ce59e7101c723d7e39d024c5.jpg" alt=""><span style="font-family: 微软雅黑;font-size: 12pt"><br>
</span></p>
<p style="background: white"><span style="font-family: 微软雅黑;font-size: 12pt">决策树在一些领域应用比较多，最主要的原因是，他有很好的可解释性。那如何训练得到一个合适的决策树呢？也就是如何决定需要哪些节点，节点上选择什么变量作为判断依据，以及判断的阈值是多大。<br>
</span></p>
<p style="background: white"><span style="font-family: 微软雅黑;font-size: 12pt">首先错误函数是残差的平方和，而树的结构则用贪心策略来演化。<br>
</span></p>
<p style="background: white"><span style="font-family: 微软雅黑;font-size: 12pt">开始只有一个节点，也就是根节点，对应整个输入空间，然后将空间划分为2，在多个划分选择之中，选择使残差最小的那个划分，书上提到这个区间划分以及划分点阈值的选择，是用穷举的方法。然后对不同的叶子节点再分别划分子区间。这样树不停长大，何时停止增加节点呢？简单的方法是当残差小于某个值的时候。但经验发现经常再往多做一些划分后，残差又可以大幅降低<br>
</span></p>
<p style="background: white"><span style="font-family: 微软雅黑;font-size: 12pt">所以通常的做法是，先构造一个足够大的树，使叶子节点多到某个数量，然后再剪枝，合并的原则是使叶子尽量减少，同时残差又不要增大。<br>
</span></p>
<p style="background: white"><span style="font-family: 微软雅黑;font-size: 12pt">也就是最小化这个值：<img src="/images/52nlp.cn/4da5f86c0a1ab4fb6fca505a12f20349.jpg" alt="">其中：<img src="/images/52nlp.cn/5d56a4269bfddba2754944fcb2b3feaf.jpg" alt=""><br>
</span></p>
<p style="background: white"><img src="/images/52nlp.cn/144bd69bdc28eab53dd7395630a3d811.jpg" alt=""><span style="font-family: 微软雅黑;font-size: 12pt">是某个叶子负责的那个区间里的所有数据点的预测值的平均值：<img src="/images/52nlp.cn/74f2951dbc2d33edd22645b390fce6f9.jpg" alt=""><br>
</span></p>
<p style="background: white"><img src="/images/52nlp.cn/c177d700b2e20742dedf4bd0b5a8e9ad.jpg" alt=""><span style="font-family: 微软雅黑;font-size: 12pt">是某个数据点的预测值，<img src="/images/52nlp.cn/f18645cad1e4c4abcbedc62fd6d792b2.jpg" alt="">是叶子的总数，<img src="/images/52nlp.cn/60fc4069a865e8f4b2222850b22ec6b6.jpg" alt="">控制残差和叶子数的trade-off，这是剪枝的依据。<br>
</span></p>
<p style="background: white"><span style="font-family: 微软雅黑;font-size: 12pt">以上是针对回归说的，对于分类，剪枝依据仍然是：<br>
<img src="/images/52nlp.cn/99b2f1337e51650c3c0ea1533d8216ef.jpg" alt=""><br>
</span></p>
<p style="background: white"><span style="font-family: 微软雅黑;font-size: 12pt">只是Q(T)不再是残差的平方和，而是cross-entropy或Gini index<br>
</span></p>
<p style="background: white"><span style="font-family: 微软雅黑;font-size: 12pt">交叉熵的计算是:<br>
</span></p>
<p style="background: white"><img src="/images/52nlp.cn/4b34d974a272161fe86ce3e4f3116a1c.jpg" alt=""><span style="font-family: 微软雅黑;font-size: 12pt"><br>
</span></p>
<p style="background: white"><img src="/images/52nlp.cn/82157131bc27d6d7bd242d728bbc78a5.jpg" alt=""><span style="font-family: 微软雅黑;font-size: 12pt">是第<img src="/images/52nlp.cn/a0f805dbf1793bb77d02538c702f707f.jpg" alt="">个叶子，也就是第<img src="/images/52nlp.cn/4c246d3eef0898fc02c7a1da7fc84b92.jpg" alt="">个区间里的数据点，被赋予类别k的比例。<br>
</span></p>
<p style="background: white"><span style="font-family: 微软雅黑;font-size: 12pt">Gini index计算是：<br>
<img src="/images/52nlp.cn/d939cde3d6bf1441594d67ec4167915e.jpg" alt=""><br>
</span></p>
<p style="background: white"><span style="font-family: 微软雅黑;font-size: 12pt">决策树的优点是直观，可解释性好。缺点是每个节点对输入空间的划分都是根据某个维度来划分的，在坐标空间里看，就是平行于某个轴来划分的，其次，决策树的划分是硬性的，在回归问题里，硬性划分更明显。<br>
</span></p>
<p><span style="color: #333333;font-family: Arial;font-size: 12pt">================================================================<br>
</span></p>
<p style="background: white"><span style="font-family: 微软雅黑;font-size: 12pt">决策树就讲这么多，接下来是conditional mixture models条件混合模型。条件混合模型，我的理解是，将不同的模型依概率来结合，这部分讲了线性回归的条件混合，逻辑回归的条件混合，和更一般性的混合方法mixture of experts，首先看线性回归的混合。<br>
</span></p>
<p style="background: white"><span style="font-family: 微软雅黑;font-size: 12pt">第9章讲过高斯混合模型，其模型是这样的：<img src="/images/52nlp.cn/becba5426d95be96d0a89031b3205dfc.jpg" alt=""><br>
</span></p>
<p style="background: white;margin-left: 78pt"><span style="font-family: 微软雅黑;font-size: 12pt">这是用多个高斯密度分布来拟合输入空间，就像这种x的分布：<br>
<img src="/images/52nlp.cn/277734c4a3303676c03408ca3690a285.jpg" alt=""><br>
</span></p>
<p style="background: white"><span style="font-family: 微软雅黑;font-size: 12pt">线性回归混合的实现，可以把这个高斯混合分布扩展成条件高斯分布，模型如下：<br>
</span></p>
<p style="background: white"><img src="/images/52nlp.cn/d028e0621c86b9a6ed597c8b26937419.jpg" alt=""><span style="font-family: 微软雅黑;font-size: 12pt"><br>
</span></p>
<p style="background: white"><span style="font-family: 微软雅黑;font-size: 12pt">这里面，有K个线性回归<img src="/images/52nlp.cn/4ce9bc7a20415f31a3b7b4a62f84a6f2.jpg" alt="">，其权重参数是<img src="/images/52nlp.cn/564f48cd6f45a53cfff3370e8d314050.jpg" alt="">，将多个线性回归的预测值确定的概率联合起来，得到最终预测值的概率分布。模型中的参数<img src="/images/52nlp.cn/c27406d7a7204d3deca0d739867e6f41.jpg" alt="">包括<img src="/images/52nlp.cn/0828741d5bf8791b66f8387c3ce47481.jpg" alt=""><img src="/images/52nlp.cn/613c6d88ba9211eeb571d2d53c1fa3a3.jpg" alt="">三部分，接下来看如何训练得到这些参数，总体思路是用第9章介绍的EM算法，引入一个隐藏变量<img src="/images/52nlp.cn/7f7ebb4a6fc07f14aebb8b0e3c33978d.jpg" alt="">，每个训练样本点对应一个<img src="/images/52nlp.cn/9667b64c7930238e14af4800ceab34b6.jpg" alt="">，<img src="/images/52nlp.cn/3ad9215828c6978c210396112dc30d3c.jpg" alt="">是一个K维的二元向量，<img src="/images/52nlp.cn/0ee4cbc7cef75aaab2f65c9121fcdae6.jpg" alt="">,如果第k个模型负责生成第n个数据点，则<img src="/images/52nlp.cn/983d4586b3b69b02c1a6f3885d81f173.jpg" alt="">等于1，否则等于0，这样，我们可以写出log似然函数：<br>
</span></p>
<p style="background: white"><img src="/images/52nlp.cn/7a312f3cfd352e39ac7dd82417cacc09.jpg" alt=""><span style="font-family: 微软雅黑;font-size: 12pt"><br>
</span></p>
<p style="background: white"><span style="font-family: 微软雅黑;font-size: 12pt">然后用EM算法结合最大似然估计来求各个参数，EM算法首先选择所有参数的初始值，假设是<img src="/images/52nlp.cn/cb4d067adcafafbe8a58e55a9f3ace4a.jpg" alt=""><br>
</span></p>
<p style="background: white"><span style="font-family: 微软雅黑;font-size: 12pt">在E步根据这些参数值，可以得到模型k相对于数据点n的后验概率：<br>
<img src="/images/52nlp.cn/468b966639ba4d611e15abeca48f24ef.jpg" alt=""><br>
</span></p>
<p style="background: white"><span style="font-family: 微软雅黑;font-size: 12pt">书上提高一个词，这个后验概率又叫做responsibilities，大概是这个数据点n由模型k 负责生成的概率吧，有了这个responsibilities，就可以计算似然函数的期望值了，如下：<img src="/images/52nlp.cn/15500fa0f8a9ca32c40084f27e8dc32b.jpg" alt=""><br>
</span></p>
<p style="background: white"><span style="font-family: 微软雅黑;font-size: 12pt">在EM的M步，我们令<img src="/images/52nlp.cn/4b83391f2c00113c5b147e5d2c1c13b5.jpg" alt="">为固定值，最大化这个期望值<img src="/images/52nlp.cn/e5a0b6b32795686f6e529d3f0ac08020.jpg" alt="">,从而求得新的参数<img src="/images/52nlp.cn/a5488cec20c3b3aab46088fbceaa3c46.jpg" alt=""><br>
</span></p>
<p style="background: white"><span style="font-family: 微软雅黑;font-size: 12pt">首先看<img src="/images/52nlp.cn/18a362abba3c845d778039c12d9d2b34.jpg" alt="">，<img src="/images/52nlp.cn/dcdce906dabd83bd34ad6c8b41b07af1.jpg" alt="">是各个模型的混合权重系数，满足<img src="/images/52nlp.cn/201664070d7ab0feca4e3ba72556da29.jpg" alt="">，用拉格朗日乘子法，可以求得:<br>
<img src="/images/52nlp.cn/5133c5765ef06bca503423ea6b86b5c2.jpg" alt=""><br>
</span></p>
<p style="background: white"><span style="font-family: 微软雅黑;font-size: 12pt">接下来求线性回归的参数<img src="/images/52nlp.cn/e62058614509d71fd623da7cb916d24d.jpg" alt="">，将似然函数期望值的式子里的高斯分布展开，可以得到如下式子：<br>
</span></p>
<p style="background: white"><img src="/images/52nlp.cn/a2d2f03dd14d2af8b5ce33874907703f.jpg" alt=""><span style="font-family: 微软雅黑;font-size: 12pt"><br>
</span></p>
<p style="background: white"><span style="font-family: 微软雅黑;font-size: 12pt">要求第k个模型的<img src="/images/52nlp.cn/8b3aeba8b9e7d59f95b893b43791a381.jpg" alt="">，其他模型的W都对其没有影响，可以统统归做后面的const，这是因为log似然函数，每个模型之间是相加的关系，一求导数，其他模型的项就都消去了。上面的式子是一个加权的最小平方损失函数，每个数据点对应一个权重<img src="/images/52nlp.cn/b60c2bcad7a820a58080ed40059a936d.jpg" alt="">，这个权重可以看做是数据点的有效精度，将这个式子对<img src="/images/52nlp.cn/f9cdbf22b6a12d269e824e87f536327c.jpg" alt="">求导，可以得到：<br>
</span></p>
<p style="background: white"><img src="/images/52nlp.cn/4d55ead19dc7202918743f57a0703c1d.jpg" alt=""><span style="font-family: 微软雅黑;font-size: 12pt"><br>
</span></p>
<p style="background: white"><span style="font-family: 微软雅黑;font-size: 12pt">最后求得<img src="/images/52nlp.cn/029b89bf4181821bf9b311f5870cca43.jpg" alt=""><br>
</span></p>
<p style="background: white"><span style="font-family: 微软雅黑;font-size: 12pt">其中<img src="/images/52nlp.cn/7407d67188a8ef4a59f1781280d69460.jpg" alt="">，同样，对<img src="/images/52nlp.cn/ebc3e3dcd784d2f0ed9c2fb87aa56da8.jpg" alt="">求导，得到：<img src="/images/52nlp.cn/c344b994c842620d11f1f1c540097684.jpg" alt=""><br>
</span></p>
<p style="background: white"><span style="font-family: 微软雅黑;font-size: 12pt">这样的到了所有的参数的新值，再重复E步和M步，迭代求得满意的最终的参数值。<br>
</span></p>
<p style="background: white"><span style="font-family: 微软雅黑;font-size: 12pt">接下来看一个线性回归混合模型EM求参数的图示，两条直线对应两个线性回归模型用EM方法迭代求参数，两条直线最终拟合两部分数据点，中间和右边分别是经过了30轮和50轮迭代，下面那一排，表示每一轮里，每个数据点对应的responsibilities，也就是类k对于数据点n的后验概率：<br>
</span></p>
<p style="background: white"><img src="/images/52nlp.cn/fab335c33f3f473d37d68f4cae5d8f1b.jpg" alt=""><span style="font-family: 微软雅黑;font-size: 12pt"><br>
</span></p>
<p style="background: white"><span style="font-family: 微软雅黑;font-size: 12pt">最终求得的混合模型图示：<br>
</span></p>
<p style="background: white"><img src="/images/52nlp.cn/ca096490972c57c8819c7f3a419424a8.jpg" alt=""><span style="font-family: 微软雅黑;font-size: 12pt"><br>
</span></p>
<p style="background: white"><span style="font-family: 微软雅黑;font-size: 12pt">接下来讲逻辑回归混合模型，逻辑回归模型本身就定义了一个目标值的概率，所以可以直接用逻辑回归本身作为概率混合模型的组件，其模型如下：<br>
</span></p>
<p style="background: white"><img src="/images/52nlp.cn/b6b7473fb2928f3833f4d43cf500f9d1.jpg" alt=""><span style="font-family: 微软雅黑;font-size: 12pt"><br>
</span></p>
<p style="background: white"><span style="font-family: 微软雅黑;font-size: 12pt">其中<img src="/images/52nlp.cn/7cd4693032b90dff67fecf2ca98880bd.jpg" alt="">，这里面的参数<img src="/images/52nlp.cn/134c3e48dd1cfc67d178d694613a5150.jpg" alt="">包括<img src="/images/52nlp.cn/48ffddd8b724787806eb9c01fd909824.jpg" alt="">两部分。求参数的方法也是用EM，这里就不细讲了，要提一下的是在M步，似然函数不是一个closed-form的形式，不能直接求导来的出参数，需要用IRLS(iterative reweighted least squares)等迭代方法来求，下图是一个逻辑回归混合模型的训练的图示，左图是两个类别的数据，蓝色和红色表示实际的分布，中间图是用一个逻辑回归来拟合的模型，右图是用两个逻辑回归混合模型拟合的图形：<br>
</span></p>
<p style="background: white"><img src="/images/52nlp.cn/dea49da5c750abf5102dc0e7b0d602f8.jpg" alt=""><span style="font-family: 微软雅黑;font-size: 12pt"><br>
</span></p>
<p style="background: white"><span style="font-family: 微软雅黑;font-size: 12pt">接下来讲mixtures of experts，mixture of experts是更一般化的混合模型，前面讲的两个混合模型，其混合系数是固定值，我们可以让混合系数是输入x的函数即：<br>
</span></p>
<p style="background: white"><img src="/images/52nlp.cn/24abfd5e8b55dbf7c29a3dcf0d74e48b.jpg" alt=""><span style="font-family: 微软雅黑;font-size: 12pt"><br>
</span></p>
<p style="background: white"><span style="font-family: 微软雅黑;font-size: 12pt">系数<img src="/images/52nlp.cn/0aac3223f1bd024d03fd61562af7a911.jpg" alt="">叫做gating函数，组成模型<img src="/images/52nlp.cn/1e67281c9e866be02aa564af464190c5.jpg" alt="">叫做experts，,每个experts可以对输入空间的不同区域建模，对不同区域的输入进行预测，而gating函数则决定一个experts该负责哪个区域。<br>
</span></p>
<p style="background: white"><span style="font-family: 微软雅黑;font-size: 12pt">gating函数满足<img src="/images/52nlp.cn/3d5acf4bcb4eb78019b74187c82a616e.jpg" alt="">，因此可以用softmax函数来作为gating函数，如果expert函数也是线性函数，则整个模型就可以用EM算法来确定。在M步，可能需要用IRLS，来迭代求解，这个模型仍然有局限性，更一般化的模型是hierarchical mixture of experts<br>
</span></p>
<p><span style="font-family: 微软雅黑;font-size: 12pt">也就是混合模型的每个组件又可以是一个混合模型。好了就讲这么多吧，书上第14章的内容都讲完了。</span></p>
<p>注：PRML读书会系列文章由 <a href="http://weibo.com/dmalgorithms">@Nietzsche_复杂网络机器学习</a> 同学授权发布，转载请注明原作者和相关的主讲人，谢谢。</p>
<p>PRML读书会讲稿PDF版本以及更多资源下载地址：<a href="http://vdisk.weibo.com/u/1841149974">http://vdisk.weibo.com/u/1841149974</a></p>
<p>本文链接地址：<a href="http://www.52nlp.cn/prml%E8%AF%BB%E4%B9%A6%E4%BC%9A%E7%AC%AC%E5%8D%81%E5%9B%9B%E7%AB%A0-combining-models">http://www.52nlp.cn/prml读书会第十四章-combining-models</a></p>

											


  </article>

  <div class="meta">
  
    <a class="basic-alignment left" href="/posts/2015/2015-01-31-prml-e8-af-bb-e4-b9-a6-e4-bc-9a-e7-ac-ac-e5-8d-81-e4-ba-8c-e7-ab-a0-continuous-latent-variables-cad03f970.html" title="Previous Post: PRML读书会第十二章 Continuous Latent Variables" data-instant>&laquo; PRML读书会第十二章 Continuous Latent Variables</a>
  
  
    <a class="basic-alignment right" href="/posts/2015/2015-01-31-prml-e8-af-bb-e4-b9-a6-e4-bc-9a-e7-ac-ac-e5-8d-81-e7-ab-a0-approximate-inference-c3b1e2e4b.html" title="Next Post: PRML读书会第十章  Approximate Inference" data-instant>PRML读书会第十章  Approximate Inference &raquo;</a>
  
</div>
  <div id="related">
  <h2 class="subheader">Related Posts <small>They might be useful</small></h2>
  <ul class="posts">
    
      <li><span>14 May 2018</span> &raquo; <a href="http://iftti.com/posts/2018/2018-05-14-113985-6dc6e9281.html">给初学者看的 shuf 命令教程</a></li>
    
      <li><span>13 May 2018</span> &raquo; <a href="http://iftti.com/posts/2018/2018-05-13-113977-b56537dc0.html">常用排序算法总结（2）</a></li>
    
      <li><span>13 May 2018</span> &raquo; <a href="http://iftti.com/posts/2018/2018-05-13-113953-957e4ea5a.html">10 个常用的软件架构模式</a></li>
    
  </ul>
</div>

  
<comments>

<!-- UY BEGIN -->
<div id="uyan_frame"></div>
<script type="text/javascript" src="http://v2.uyan.cc/code/uyan.js?uid=1936498"></script>
<!-- UY END -->

</comments>

</div>
      <!-- JiaThis Button BEGIN -->
<div class="jiathis_share_slide jiathis_share_24x24" id="jiathis_share_slide">
<div class="jiathis_share_slide_top" id="jiathis_share_title"></div>
<div class="jiathis_share_slide_inner">
<div class="jiathis_style_24x24">
<a class="jiathis_button_tsina"></a>
<a class="jiathis_button_googleplus"></a>
<a class="jiathis_button_twitter"></a>
<a class="jiathis_button_linkedin"></a>
<a class="jiathis_button_weixin"></a>
<a class="jiathis_button_cqq"></a>
<a class="jiathis_button_renren"></a>
<a class="jiathis_button_evernote"></a>
<a class="jiathis_button_pocket"></a>
<a href="http://www.jiathis.com/share" class="jiathis jiathis_txt jtico jtico_jiathis" target="_blank"></a>
<script type="text/javascript">
var jiathis_config = {data_track_clickback:'true'
	,slide:{
		divid:'wrap',
		pos:'left',
		gt:'true'
	}
};
</script>
<script type="text/javascript" src="http://v3.jiathis.com/code/jia.js?uid=1936498" charset="utf-8"></script>
<script type="text/javascript" src="http://v3.jiathis.com/code/jiathis_slide.js" charset="utf-8"></script>
</div></div></div>
<!-- JiaThis Button END -->
      </div>
    </div>

    <footer class="site-footer">

  <div class="wrap">

    <h2 class="footer-heading">IT技术干货</h2>

    <div class="footer-col-1 column">
      <p class="text">IT技术干货 KernelHacks 最好的技术站点 技术信息 纯干货</p>
      <ul>
        <li>汇集最好的科技与互联网信息</li>
        <li>Liu Lantao</li>
        <li><a href="mailto:iftti@iftti.com">iftti@iftti.com</a></li>
      </ul>
    </div>

    <div class="footer-col-2 column">
      <ul>
        <li>
          <a href="https://github.com/Lax">
            <span class="icon github">
              <svg version="1.1" class="github-icon-svg" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
                 viewBox="0 0 16 16" enable-background="new 0 0 16 16" xml:space="preserve">
                <path fill-rule="evenodd" clip-rule="evenodd" fill="#C2C2C2" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761
                c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32
                c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472
                c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037
                C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65
                c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261
                c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082
                c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129
                c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/>
              </svg>
            </span>
            <span class="username">Lax</span>
          </a>
        </li>
        <li>
          <a href="https://twitter.com/liulantao">
            <span class="icon twitter">
              <svg version="1.1" class="twitter-icon-svg" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
                 viewBox="0 0 16 16" enable-background="new 0 0 16 16" xml:space="preserve">
                <path fill="#C2C2C2" d="M15.969,3.058c-0.586,0.26-1.217,0.436-1.878,0.515c0.675-0.405,1.194-1.045,1.438-1.809
                c-0.632,0.375-1.332,0.647-2.076,0.793c-0.596-0.636-1.446-1.033-2.387-1.033c-1.806,0-3.27,1.464-3.27,3.27
                c0,0.256,0.029,0.506,0.085,0.745C5.163,5.404,2.753,4.102,1.14,2.124C0.859,2.607,0.698,3.168,0.698,3.767
                c0,1.134,0.577,2.135,1.455,2.722C1.616,6.472,1.112,6.325,0.671,6.08c0,0.014,0,0.027,0,0.041c0,1.584,1.127,2.906,2.623,3.206
                C3.02,9.402,2.731,9.442,2.433,9.442c-0.211,0-0.416-0.021-0.615-0.059c0.416,1.299,1.624,2.245,3.055,2.271
                c-1.119,0.877-2.529,1.4-4.061,1.4c-0.264,0-0.524-0.015-0.78-0.046c1.447,0.928,3.166,1.469,5.013,1.469
                c6.015,0,9.304-4.983,9.304-9.304c0-0.142-0.003-0.283-0.009-0.423C14.976,4.29,15.531,3.714,15.969,3.058z"/>
              </svg>
            </span>
            <span class="username">@liulantao</span>
          </a>
        </li>
        <li>
          <a href="https://plus.google.com/+LiuLantao">
            <span class="icon googleplus">
              <svg version="1.1" id="Layer_1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
                width="16px" height="16px" viewBox="0 0 134.658 131.646" enable-background="new 0 0 134.658 131.646"
                xml:space="preserve">
                <g>
                  <path fill="#C2C2C2" d="M126.515,4.109H8.144c-2.177,0-3.94,1.763-3.94,3.938v115.546c0,2.179,1.763,3.942,3.94,3.942h118.371
                  c2.177,0,3.94-1.764,3.94-3.942V8.048C130.455,5.872,128.691,4.109,126.515,4.109z"/>
                  <g>
                    <path fill="#FFFFFF" d="M70.479,71.845l-3.983-3.093c-1.213-1.006-2.872-2.334-2.872-4.765c0-2.441,1.659-3.993,3.099-5.43
                    c4.64-3.652,9.276-7.539,9.276-15.73c0-8.423-5.3-12.854-7.84-14.956h6.849l7.189-4.517H60.418
                    c-5.976,0-14.588,1.414-20.893,6.619c-4.752,4.1-7.07,9.753-7.07,14.842c0,8.639,6.633,17.396,18.346,17.396
                    c1.106,0,2.316-0.109,3.534-0.222c-0.547,1.331-1.1,2.439-1.1,4.32c0,3.431,1.763,5.535,3.317,7.528
                    c-4.977,0.342-14.268,0.893-21.117,5.103c-6.523,3.879-8.508,9.525-8.508,13.51c0,8.202,7.731,15.842,23.762,15.842
                    c19.01,0,29.074-10.519,29.074-20.932C79.764,79.709,75.344,75.943,70.479,71.845z M56,59.107
                    c-9.51,0-13.818-12.294-13.818-19.712c0-2.888,0.547-5.87,2.428-8.199c1.773-2.218,4.861-3.657,7.744-3.657
                    c9.168,0,13.923,12.404,13.923,20.382c0,1.996-0.22,5.533-2.762,8.09C61.737,57.785,58.762,59.107,56,59.107z M56.109,103.65
                    c-11.826,0-19.452-5.657-19.452-13.523c0-7.864,7.071-10.524,9.504-11.405c4.64-1.561,10.611-1.779,11.607-1.779
                    c1.105,0,1.658,0,2.538,0.111c8.407,5.983,12.056,8.965,12.056,14.629C72.362,98.542,66.723,103.65,56.109,103.65z"/>
                    <polygon fill="#FFFFFF" points="98.393,58.938 98.393,47.863 92.923,47.863 92.923,58.938 81.866,58.938 81.866,64.469
                    92.923,64.469 92.923,75.612 98.393,75.612 98.393,64.469 109.506,64.469 109.506,58.938"/>
                  </g>
                </g>
              </svg>
            </span>
            <span class="username">+LiuLantao</span>
          </a>
        </li>
      </ul>
    </div>

    <div class="footer-col-3 column">
      
<!--以下是QQ邮件列表订阅嵌入代码--><script >var nId = "6be92ef3590ee662cd5e6381ab2044c328716364f684cf3e",nWidth="auto",sColor="light",sText="填写您的邮件地址，订阅我们的精彩内容：" ;</script><script src="http://list.qq.com/zh_CN/htmledition/js/qf/page/qfcode.js" charset="gb18030"></script>

    </div>

  </div>

  <div class="wrap">
    <div>
      <a href="http://blog.liulantao.com">Blog</a> | <a href="http://1000bit.com">铅笔特评 1000bit</a> | <a href="http://visplanet.com">VisPlanet</a> | <a href="http://iftti.com">IT技术干货</a> | <a href="http://relax.org.cn">Relax</a> | <a href="http://hangzhou.io">杭州城市指南</a>
    </div>
  </div>

  
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-1658815-7', 'iftti.com');
  ga('require', 'displayfeatures');
  ga('send', 'pageview');

</script>


</footer>


    </body>
</html>