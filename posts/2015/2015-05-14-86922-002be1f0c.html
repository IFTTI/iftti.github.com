<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>机器学习实战ByMatlab（5）：Logistic Regression | IT技术干货</title>
    <meta name="viewport" content="width=device-width">
    <meta name="description" content="IT技术干货 KernelHacks 最好的技术站点 技术信息 纯干货">
    <link rel="canonical" href="http://iftti.com/posts/2015/2015-05-14-86922-002be1f0c.html">

    <!-- Custom CSS -->
    <link rel="stylesheet" href="/css/main.css">

</head>

    <body>

    <header class="site-header">

  <div class="wrap">

    <a class="site-title" href="/">IT技术干货</a>

    <nav class="site-nav">
      <a href="#" class="menu-icon">
        <svg version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
           viewBox="0 0 18 15" enable-background="new 0 0 18 15" xml:space="preserve">
          <path fill="#505050" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0
            h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
          <path fill="#505050" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484
            h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
          <path fill="#505050" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0
            c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
        </svg>
      </a>
      <div class="trigger">

          <a class="page-link" href="/about/">About</a>

      </div>
    </nav>

  </div>

</header>

    <div class="page-content">
      <div class="wrap">
      <div class="post">

  <header class="post-header">
    <h1>机器学习实战ByMatlab（5）：Logistic Regression</h1>
    <p class="meta"><span class="time">May 14, 2015</span><span class="source_url"> • 来自 jobbole.com <a name="jobbole.com" href="http://blog.jobbole.com/86922/" target="_blank">[原文链接]</a></span></p>
  </header>

  <article class="post-content">

        <!-- div style="margin-bottom: 10px;">
            <script language=javascript>
                var randomNumLuobo = Math.round(Math.random()*1);
                var imageLuobo=new Array(2);
                imageLuobo[0]="http://jbcdn2.b0.upaiyun.com/2014/11/luobo-620x60.png";
                imageLuobo[1]="http://jbcdn2.b0.upaiyun.com/2014/11/luobo-620x60.png";
                var imageUrlLuobo=imageLuobo[randomNumLuobo];
                var urlsLuobo=new Array(2);
                urlsLuobo[0]="http://www.luobo360.com";
                urlsLuobo[1]="http://www.luobo360.com";
                var urlLuobo = urlsLuobo[randomNumLuobo];
                var adHTML = "<a href='"+urlLuobo+"' target='_blank'><img src='"+imageUrlLuobo+"' border='0'></a>";
                document.write(adHTML);
            </script>
        </div -->

        <span style="display:block;margin-bottom:10px;"></span>

<p>什么叫做回归呢？举个例子，我们现在有一些数据点，然后我们打算用一条直线来对这些点进行拟合（该曲线称为最佳拟合曲线），这个拟合过程就被称为回归。</p>
<p>利用Logistic回归进行分类的主要思想是：</p>
<p>根据现有数据对分类边界线建立回归公式，以此进行分类。</p>
<p>这里的”回归“一词源于最佳拟合，表示要找到最佳拟合参数集。训练分类器时的嘴阀就是寻找最佳拟合曲线，使用的是最优化算法。</p>
<p><strong>基于Logistic回归和Sigmoid函数的分类</strong></p>
<p>优点：计算代价不高，易于理解和实现<br>
缺点：容易欠拟合，分类精度可能不高</p>
<p>使用数据类型：数值型和标称型数据</p>
<p>Sigmoid函数：</p>
<p><img class="alignnone" alt="" src="/images/jobbole.com/0db740d1a0ca36068e7dc61899ff88c2.jpg" width="165" height="70"></p>
<p>波形如下：</p>
<p><img class="alignnone" alt="" src="/images/jobbole.com/9a87ddecb06f0f3d75e42ce56164d63f.jpg" width="311" height="243"></p>
<p>当z为0时，值为0.5，当z增大时，g(z)逼近1，当z减小时，g(z)逼近0</p>
<p>Logistic回归分类器：</p>
<p>对每一个特征都乘以一个回归系数，然后把所有结果都相加，再讲这个总和代入Sigmoid函数中，从而得到一个范围在0-1之间的数值。任何大于0.5的数据被分为1，小于0.5的数据被分为0.因此Logistic回归也被看成是一种概率分布。</p>
<p>分类器的函数形式确定之后，现在的问题就是，如何确定回归系数？</p>
<p>基于最优化方法的最佳回归系数确定</p>
<p>Sigmoid函数的输入记为z，由下面公式得出：</p>
<p><img class="alignnone" alt="" src="/images/jobbole.com/14f57eeaf24ced43b001399de9fde048.jpg" width="655" height="70"></p>
<p>如果采用向量的写法，则上述公式可以写成：</p>
<p><img class="alignnone" alt="" src="/images/jobbole.com/e740330abd6ca66af4fb4b0d641f5a1d.jpg" width="231" height="70"></p>
<p>其中向量X就是分类器的输入数据，向量W也就是我们要找到的最佳参数，从而使分类器尽可能更加地精确。接下来将介绍几种需找最佳参数的方法。</p>
<p><strong>梯度上升法</strong></p>
<p>梯度上升法的基本思想：</p>
<p>要找到某函数的最大值，最好的方法是沿着该函数的梯度方向寻找</p>
<p>这里提一下梯度下降法，这个我们应该会更加熟悉，因为我们在很多代价函数J的优化的时候经常用到它，其基本思想是：</p>
<p>要找到某函数的最小值，最好的方法是沿着该函数的梯度方向的反方向寻找</p>
<p>函数的梯度表示方法如下：</p>
<p><img class="alignnone" alt="" src="/images/jobbole.com/08092aba19c17b70b7e4352c322c3deb.jpg" width="495" height="297"></p>
<p><img class="alignnone" alt="" src="/images/jobbole.com/bd1e8e8f7e1bf0030e61e96848faf2be.jpg" width="690" height="93"></p>
<p>移动方向确定了，移动的大小我们称之为步长，用α表示，用向量来表示的话，梯度下降算法的迭代公式如下：</p>
<p><img class="alignnone" alt="" src="/images/jobbole.com/e3091bb4b1ff91b26870ecc03dbfcbb8.jpg" width="417" height="79"></p>
<p>该公式已知被迭代执行，直到某个停止条件位置，比如迭代次数达到某个指定值或者算法的误差小到某个允许的误差范围内。</p>
<p>注：梯度下降算法中的迭代公式如下：</p>
<p><img class="alignnone" alt="" src="/images/jobbole.com/81cbfc30c3da7a117669a02e77577dcf.jpg" width="417" height="79"></p>
<p><strong>Matlab 实现</strong></p>
<pre class="brush: python; gutter: true">function weight = gradAscent
%%
clc
close all
clear
%%
data = load('testSet.txt');
[row , col] = size(data);
dataMat = data(:,1:col-1);
dataMat = [ones(row,1) dataMat] ;
labelMat = data(:,col);
alpha = 0.001;
maxCycle = 500;
weight = ones(col,1);
for i = 1:maxCycle
 h = sigmoid((dataMat * weight)');
 error = (labelMat - h');
 weight = weight + alpha * dataMat' * error;
end
figure
scatter(dataMat(find(labelMat(:) == 0),2),dataMat(find(labelMat(:) == 0),3),3);
hold on
scatter(dataMat(find(labelMat(:) == 1),2),dataMat(find(labelMat(:) == 1),3),5);
hold on
x = -3:0.1:3;
y = (-weight(1)-weight(2)*x)/weight(3);
plot(x,y)
hold off
end
function returnVals = sigmoid(inX)
 % 注意这里的sigmoid函数要用点除
 returnVals = 1.0./(1.0+exp(-inX));
end</pre>

<p><strong>效图如下：</strong></p>
<p><img class="alignnone" alt="" src="/images/jobbole.com/d9b27f9e65fdf4e6edab9e9aebac7524.jpg" width="560" height="420"></p>
<p>由上图可以看到，回归效果还是挺不错的，只有2-4个点分类错误。</p>
<p>其实这是的梯度上升算法是批量梯度上升算法，每一次更新参数的时候都要讲所有的数据集都代入训练，效果并不好，下面我们将介绍改进版本：随机梯度上升算法</p>
<p><strong>随机梯度上升</strong></p>
<p>梯度上升算法在每次更新回归系数时都要遍历整个数据集，该方法在处理100个左右的数据集时尚可，但如果有数十亿样本和成千上万的特征，那么该方法的复杂度就太高了。一种改进方法是一次仅用一个样本点来更新回归系数，该方法就称为随机梯度上升法。由于可以在新样本到来之前对分类器进行增量式更新，因此随机梯度算法是一个在线学习算法。与”在线学习“相对应，一次处理所有数据被称作是”批处理“</p>
<p>随机梯度上升算法可以写成如下的伪代码：</p>
<p>所有回归系数初始化为1<br>
对数据集中的每个样本<br>
计算该样本的梯度<br>
使用alpha x gradient 更新回归系数值<br>
返回回归系数值<br>
<strong></strong></p>
<p><strong>Matlab 代码实现</strong></p>
<pre class="brush: python; gutter: true">function stocGradAscent
%%
%
% Description : LogisticRegression using stocGradAsscent
% Author : Liulongpo
% Time：2015-4-18 10:57:25
%
%%
clc
clear 
close all
%%
data = load('testSet.txt');
[row , col] = size(data);
dataMat = [ones(row,1) data(:,1:col-1)];
alpha = 0.01;
labelMat = data(:,col);
weight = ones(col,1);
for i = 1:row
 h = sigmoid(dataMat(i,:)*weight);
 error = labelMat(i) - h;
 dataMat(i,:)
 weight
 weight = weight + alpha * error * dataMat(i,:)'
end
figure
scatter(dataMat(find(labelMat(:)==0),2),dataMat(find(labelMat(:)==0),3),5);
hold on
scatter(dataMat(find(labelMat(:) == 1),2),dataMat(find(labelMat(:) == 1),3),5);
hold on
x = -3:0.1:3;
y = -(weight(1)+weight(2)*x)/weight(3);
plot(x,y)
hold off

end
function returnVals = sigmoid(inX)
 % 注意这里的sigmoid函数要用点除
 returnVals = 1.0./(1.0+exp(-inX));
end</pre>

<p><strong>效果如下：</strong></p>
<p><img class="alignnone" alt="" src="/images/jobbole.com/b17298172a463bea826144325ace9f2c.jpg" width="560" height="420"></p>
<p>由上图可以看出，随机梯度上升算法分类效果并没有上面的的梯度上升算法分类效果好。</p>
<p>但是直接比较梯度上升算法和随机梯度上升算法是不公平的，前者是在整个数据集上迭代500次得到的结果，后者只是迭代了100次。一个判断算法优劣的可靠方法是看它是否收敛，也就是说求解的参数是否达到了稳定值，是否还会不断变化。</p>
<p>我们让随机梯度上升算法在整个数据集上运行200次，迭代过程中3个参数的变化如下图：</p>
<p><img class="alignnone" alt="" src="/images/jobbole.com/15b047844ad1b2cfc3df93a8c1ae0e90.jpg" width="560" height="420"></p>
<p>由上图可以看到，weight1 最先达到稳定，而weight0和weight2则还需要更多的迭代次数来达到稳定。</p>
<p>此时的分类器跟之前的梯度上升算法的分类效果差不多，如下：</p>
<p><img class="alignnone" alt="" src="/images/jobbole.com/c133b57418f958a863869b816986a2c0.jpg" width="560" height="420"></p>
<p>但同时我们也可以看到，三个参数都有不同程度的波动。产生这种现象的原因是存在一些不能被正确分类的样本点（数据集并非线性可分），在每次迭代的时候都会引起参数的剧烈变化。我们期望算法能避免来回波动，从而收敛到某个值。另外，算法收敛速度也要加快。</p>
<p><strong>改进的随机梯度上升算法</strong></p>
<p>改进的随机梯度上升算法的主要两个改进点如下：</p>
<p>1,每一步调整alpha的值，也就是alpha的值是不严格下降的<br>
2.随机采取样本来更新回归参数</p>
<p><strong>matlab代码如下：</strong></p>
<pre class="brush: python; gutter: true">function ImproveStocGradAscent
%%
%
% Description : LogisticRegression using stocGradAsscent
% Author : Liulongpo
% Time：2015-4-18 10:57:25
%
%%
clc
clear 
close all
%%
data = load('testSet.txt');
[row , col] = size(data);
dataMat = [ones(row,1) data(:,1:col-1)];
%alpha = 0.01;
numIter = 20;
labelMat = data(:,col);
weightVal = zeros(3,numIter*row);
weight = ones(col,1);
j = 0;
for k = 1:numIter
 randIndex = randperm(row);
 for i = 1:row
 % 改进点 1
 alpha = 4/(1.0+i+k)+0.01; 
 j = j+1;
 % 改进点 2 
 h = sigmoid(dataMat(randIndex(i),:)*weight);
 % 改进点 2
 error = labelMat(randIndex(i)) - h;
 % 改进点 2
 weight = weight + alpha * error * dataMat(randIndex(i),:)';
 weightVal(1,j) = weight(1);
 weightVal(2,j) = weight(2);
 weightVal(3,j) = weight(3);
 end
end
figure
i = 1:numIter*row;
subplot(3,1,1)
plot(i,weightVal(1,:)),title('weight0')%,axis([0 numIter*row 0.8 7])
j = 1:numIter*row;
subplot(3,1,2)
plot(j,weightVal(2,:)),title('weight1')%,axis([0 numIter*row 0.3 1.2])
k = 1:numIter*row;
subplot(3,1,3)
plot(k,weightVal(3,:)),title('weight2')%,axis([0 numIter*row -1.2 -0.1])
figure
scatter(dataMat(find(labelMat(:)==0),2),dataMat(find(labelMat(:)==0),3),5);
hold on
scatter(dataMat(find(labelMat(:) == 1),2),dataMat(find(labelMat(:) == 1),3),5);
hold on
x = -3:0.1:3;
y = -(weight(1)+weight(2)*x)/weight(3);
plot(x,y,'r')
hold off

end
function returnVals = sigmoid(inX)
 % 注意这里的sigmoid函数要用点除
 returnVals = 1.0./(1.0+exp(-inX));
end</pre>

<p>改进点 1 中的alpha会随着迭代次数的增加不断减小，但由于代码中常数0.01的存在，alpha不会减少到0。这样做是为了保证在多次迭代之后新数据对于参数的更新还有一定的影响。</p>
<p>另一点值得注意的就是，alpha每次减少 1/(k+i) ，k 是迭代次数，i是样本的下标。所以 alpha 不是严格下降的。避免参数的严格下降也常见于模拟退火算法等其他优化算法中。</p>
<p>第二个改进的地方如代码注释中标记的，这里通过随机采取样本来更新回归参数，这样能够减少参数的周期性的波动。</p>
<p>由于alpha的动态变化，我们可以在开始的时候设置比较大的值，代码中设置0.01，alpha也就是每一次迭代的步长，步长越大，越能够加快参数的收敛速度。然后ahpha会不严格下降，这样就避免了过拟合现象的发生。至于什么是过拟合已经alpha的选取问题将在下面描述。</p>
<p>迭代20次后效果如下：</p>
<p><img class="alignnone" alt="" src="/images/jobbole.com/a0e4ae063d45fd3473eb5814432b479a.jpg" width="560" height="420"></p>
<p><img class="alignnone" alt="" src="/images/jobbole.com/cf5a7e2c5c831fbb3f1a6e81aafe3689.jpg" width="560" height="420"></p>
<p>由上图可知，步长alpha动态变化之后，参数的收敛速度加快了很多，这里只是对所有样本数据集迭代20次，weight0 和 weight2很早就收敛。证明了该算法的优异性。</p>
<p><strong>学习率alpha的选取</strong></p>
<p>首先我们看一下梯度上升算法的核心代码，如下：</p>
<p>h = sigmoid(dataMat(i,:) * weight);<br>
error = labelMat(i) – h;<br>
weight = weight + alpha * error * dataMat(i,:)’;</p>
<p>第一行做的就是估计分类，第二行计算当前估计与正确分类之间的差error，第三行根据这个error来更新参数weight。</p>
<p>我们在迭代的时候，要做的目标就是最小化 error ，我们令 J 代表 error，令向量 θ 代表weight，则很显然，J是θ的函数。这里盗用Standfor 机器学习教程的图，如下：</p>
<p><img class="alignnone" alt="" src="/images/jobbole.com/34d404890cc220ccce93fe4236026792.jpg" width="436" height="542"></p>
<p>上图中的每个箭头就是每一次迭代的更新步长，第一幅图我们看到，在最小化 J(θ) 的时候迭代了很多次，这说明什么？说明我们要走很多步才能到达全局最优点，原因就是我们每一步走的距离太短，走得太慢，也就是我们的alpha设置得太小。但是当我们处于最优点附近的时候，这样有利我们向最优点靠近。</p>
<p>下图中的每个箭头也代表走一步，我们可以看到，迭代的时候，每一步都没有到达最优点，而是在最优点的附近波动。为什么呢？因为步长太大了嘛，明明就在眼前了，半步或者四分之三步就走到了，你却只能一跨而过，重新再来。但是学习率大的话，在刚开始迭代的时候有利于我们参数的快速收敛，也有利于我们避开局部最小值。</p>
<p>综合以上两种情况，我们就应该在开始的时候选取较大的学习率，然后不断不严格减小学习率，这样才是最优的选择。</p>

        <!-- BEGIN #author-bio -->

<!-- END #author-bio -->

  </article>

  <div class="meta">

    <a class="basic-alignment left" href="/posts/2015/2015-05-14-86914-0cb880bf3.html" title="Previous Post: 机器学习实战ByMatlab（4）：二分K-means算法" data-instant>&laquo; 机器学习实战ByMatlab（4）：二分K-means算法</a>

    <a class="basic-alignment right" href="/posts/2015/2015-05-14-86929-20e708697.html" title="Next Post: .NET高级工程师面试题之SQL篇" data-instant>.NET高级工程师面试题之SQL篇 &raquo;</a>

</div>
  <div id="related">
  <h2 class="subheader">Related Posts <small>They might be useful</small></h2>
  <ul class="posts">

      <li><span>25 Jan 2017</span> &raquo; <a href="http://iftti.com/posts/2017/2017-01-25-109980-586c9687f.html">每天一个 Linux 命令（47）： iostat 命令</a></li>

      <li><span>25 Jan 2017</span> &raquo; <a href="http://iftti.com/posts/2017/2017-01-25-109832-8f41a380e.html">照顾好应用的缓存，应付大流量</a></li>

      <li><span>23 Jan 2017</span> &raquo; <a href="http://iftti.com/posts/2017/2017-01-23-109970-be7a95583.html">每天一个 Linux 命令（46）： vmstat 命令</a></li>

  </ul>
</div>

<comments>

<!-- UY BEGIN -->
<div id="uyan_frame"></div>
<script type="text/javascript" src="http://v2.uyan.cc/code/uyan.js?uid=1936498"></script>
<!-- UY END -->

</comments>

</div>
      <!-- JiaThis Button BEGIN -->
<div class="jiathis_share_slide jiathis_share_24x24" id="jiathis_share_slide">
<div class="jiathis_share_slide_top" id="jiathis_share_title"></div>
<div class="jiathis_share_slide_inner">
<div class="jiathis_style_24x24">
<a class="jiathis_button_tsina"></a>
<a class="jiathis_button_googleplus"></a>
<a class="jiathis_button_twitter"></a>
<a class="jiathis_button_linkedin"></a>
<a class="jiathis_button_weixin"></a>
<a class="jiathis_button_cqq"></a>
<a class="jiathis_button_renren"></a>
<a class="jiathis_button_evernote"></a>
<a class="jiathis_button_pocket"></a>
<a href="http://www.jiathis.com/share" class="jiathis jiathis_txt jtico jtico_jiathis" target="_blank"></a>
<script type="text/javascript">
var jiathis_config = {data_track_clickback:'true'
	,slide:{
		divid:'wrap',
		pos:'left',
		gt:'true'
	}
};
</script>
<script type="text/javascript" src="http://v3.jiathis.com/code/jia.js?uid=1936498" charset="utf-8"></script>
<script type="text/javascript" src="http://v3.jiathis.com/code/jiathis_slide.js" charset="utf-8"></script>
</div></div></div>
<!-- JiaThis Button END -->
      </div>
    </div>

    <footer class="site-footer">

  <div class="wrap">

    <h2 class="footer-heading">IT技术干货</h2>

    <div class="footer-col-1 column">
      <p class="text">IT技术干货 KernelHacks 最好的技术站点 技术信息 纯干货</p>
      <ul>
        <li>汇集最好的科技与互联网信息</li>
        <li>Liu Lantao</li>
        <li><a href="mailto:iftti@iftti.com">iftti@iftti.com</a></li>
      </ul>
    </div>

    <div class="footer-col-2 column">
      <ul>
        <li>
          <a href="https://github.com/Lax">
            <span class="icon github">
              <svg version="1.1" class="github-icon-svg" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
                 viewBox="0 0 16 16" enable-background="new 0 0 16 16" xml:space="preserve">
                <path fill-rule="evenodd" clip-rule="evenodd" fill="#C2C2C2" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761
                c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32
                c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472
                c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037
                C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65
                c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261
                c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082
                c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129
                c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/>
              </svg>
            </span>
            <span class="username">Lax</span>
          </a>
        </li>
        <li>
          <a href="https://twitter.com/liulantao">
            <span class="icon twitter">
              <svg version="1.1" class="twitter-icon-svg" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
                 viewBox="0 0 16 16" enable-background="new 0 0 16 16" xml:space="preserve">
                <path fill="#C2C2C2" d="M15.969,3.058c-0.586,0.26-1.217,0.436-1.878,0.515c0.675-0.405,1.194-1.045,1.438-1.809
                c-0.632,0.375-1.332,0.647-2.076,0.793c-0.596-0.636-1.446-1.033-2.387-1.033c-1.806,0-3.27,1.464-3.27,3.27
                c0,0.256,0.029,0.506,0.085,0.745C5.163,5.404,2.753,4.102,1.14,2.124C0.859,2.607,0.698,3.168,0.698,3.767
                c0,1.134,0.577,2.135,1.455,2.722C1.616,6.472,1.112,6.325,0.671,6.08c0,0.014,0,0.027,0,0.041c0,1.584,1.127,2.906,2.623,3.206
                C3.02,9.402,2.731,9.442,2.433,9.442c-0.211,0-0.416-0.021-0.615-0.059c0.416,1.299,1.624,2.245,3.055,2.271
                c-1.119,0.877-2.529,1.4-4.061,1.4c-0.264,0-0.524-0.015-0.78-0.046c1.447,0.928,3.166,1.469,5.013,1.469
                c6.015,0,9.304-4.983,9.304-9.304c0-0.142-0.003-0.283-0.009-0.423C14.976,4.29,15.531,3.714,15.969,3.058z"/>
              </svg>
            </span>
            <span class="username">@liulantao</span>
          </a>
        </li>
        <li>
          <a href="https://plus.google.com/+LiuLantao">
            <span class="icon googleplus">
              <svg version="1.1" id="Layer_1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
                width="16px" height="16px" viewBox="0 0 134.658 131.646" enable-background="new 0 0 134.658 131.646"
                xml:space="preserve">
                <g>
                  <path fill="#C2C2C2" d="M126.515,4.109H8.144c-2.177,0-3.94,1.763-3.94,3.938v115.546c0,2.179,1.763,3.942,3.94,3.942h118.371
                  c2.177,0,3.94-1.764,3.94-3.942V8.048C130.455,5.872,128.691,4.109,126.515,4.109z"/>
                  <g>
                    <path fill="#FFFFFF" d="M70.479,71.845l-3.983-3.093c-1.213-1.006-2.872-2.334-2.872-4.765c0-2.441,1.659-3.993,3.099-5.43
                    c4.64-3.652,9.276-7.539,9.276-15.73c0-8.423-5.3-12.854-7.84-14.956h6.849l7.189-4.517H60.418
                    c-5.976,0-14.588,1.414-20.893,6.619c-4.752,4.1-7.07,9.753-7.07,14.842c0,8.639,6.633,17.396,18.346,17.396
                    c1.106,0,2.316-0.109,3.534-0.222c-0.547,1.331-1.1,2.439-1.1,4.32c0,3.431,1.763,5.535,3.317,7.528
                    c-4.977,0.342-14.268,0.893-21.117,5.103c-6.523,3.879-8.508,9.525-8.508,13.51c0,8.202,7.731,15.842,23.762,15.842
                    c19.01,0,29.074-10.519,29.074-20.932C79.764,79.709,75.344,75.943,70.479,71.845z M56,59.107
                    c-9.51,0-13.818-12.294-13.818-19.712c0-2.888,0.547-5.87,2.428-8.199c1.773-2.218,4.861-3.657,7.744-3.657
                    c9.168,0,13.923,12.404,13.923,20.382c0,1.996-0.22,5.533-2.762,8.09C61.737,57.785,58.762,59.107,56,59.107z M56.109,103.65
                    c-11.826,0-19.452-5.657-19.452-13.523c0-7.864,7.071-10.524,9.504-11.405c4.64-1.561,10.611-1.779,11.607-1.779
                    c1.105,0,1.658,0,2.538,0.111c8.407,5.983,12.056,8.965,12.056,14.629C72.362,98.542,66.723,103.65,56.109,103.65z"/>
                    <polygon fill="#FFFFFF" points="98.393,58.938 98.393,47.863 92.923,47.863 92.923,58.938 81.866,58.938 81.866,64.469
                    92.923,64.469 92.923,75.612 98.393,75.612 98.393,64.469 109.506,64.469 109.506,58.938"/>
                  </g>
                </g>
              </svg>
            </span>
            <span class="username">+LiuLantao</span>
          </a>
        </li>
      </ul>
    </div>

    <div class="footer-col-3 column">

<!--以下是QQ邮件列表订阅嵌入代码--><script >var nId = "6be92ef3590ee662cd5e6381ab2044c328716364f684cf3e",nWidth="auto",sColor="light",sText="填写您的邮件地址，订阅我们的精彩内容：" ;</script><script src="http://list.qq.com/zh_CN/htmledition/js/qf/page/qfcode.js" charset="gb18030"></script>

    </div>

  </div>

<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-1658815-7', 'iftti.com');
  ga('require', 'displayfeatures');
  ga('send', 'pageview');

</script>

</footer>

    </body>
</html>