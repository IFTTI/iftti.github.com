<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Python自然语言处理实践: 在NLTK中使用斯坦福中文分词器 | IT技术干货</title>
    <meta name="viewport" content="width=device-width">
    <meta name="description" content="IT技术干货 KernelHacks 最好的技术站点 技术信息 纯干货">
    <link rel="canonical" href="http://iftti.com/posts/2014/2014-09-24-python-e8-87-aa-e7-84-b6-e8-af-ad-e8-a8-80-e5-a4-84-e7-90-86-e5-ae-9e-e8-b7-b5-e5-9c-a8nltk-e4-b8-ad-e4-bd-bf-e7-94-a8-e6-96-af-e5-9d-a6-e7-a6-8f-e4-b8-ad-e6-96-87-e5-88-86-e8-af-8d-e5-99-a8-5dd26d0a5.html">

    <!-- Custom CSS -->
    <link rel="stylesheet" href="/css/main.css">

</head>


    <body>

    <header class="site-header">

  <div class="wrap">

    <a class="site-title" href="/">IT技术干货</a>

    <nav class="site-nav">
      <a href="#" class="menu-icon">
        <svg version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
           viewBox="0 0 18 15" enable-background="new 0 0 18 15" xml:space="preserve">
          <path fill="#505050" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0
            h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
          <path fill="#505050" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484
            h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
          <path fill="#505050" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0
            c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
        </svg>
      </a>
      <div class="trigger">
        
          
        
          <a class="page-link" href="/about/">About</a>
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
      </div>
    </nav>

  </div>

</header>


    <div class="page-content">
      <div class="wrap">
      <div class="post">

  <header class="post-header">
    <h1>Python自然语言处理实践: 在NLTK中使用斯坦福中文分词器</h1>
    <p class="meta"><span class="time">Sep 24, 2014</span><span class="source_url"> • 来自 52nlp.cn <a name="52nlp.cn" href="http://www.52nlp.cn/python%e8%87%aa%e7%84%b6%e8%af%ad%e8%a8%80%e5%a4%84%e7%90%86%e5%ae%9e%e8%b7%b5-%e5%9c%a8nltk%e4%b8%ad%e4%bd%bf%e7%94%a8%e6%96%af%e5%9d%a6%e7%a6%8f%e4%b8%ad%e6%96%87%e5%88%86%e8%af%8d%e5%99%a8" target="_blank">[原文链接]</a></span></p>
  </header>

  <article class="post-content">
  

						<p>斯坦福大学自然语言处理组是世界知名的NLP研究小组，他们提供了一系列开源的Java文本分析工具，包括分词器(<a href="http://nlp.stanford.edu/software/segmenter.shtml">Word Segmenter</a>)，词性标注工具（<a href="http://nlp.stanford.edu/software/tagger.shtml">Part-Of-Speech Tagger</a>），命名实体识别工具（<a href="http://nlp.stanford.edu/software/CRF-NER.shtml">Named Entity Recognizer</a>），句法分析器（<a href="http://nlp.stanford.edu/software/lex-parser.shtml">Parser</a>）等，可喜的事，他们还为这些工具训练了相应的中文模型，支持中文文本处理。在使用NLTK的过程中，发现当前版本的<a href="https://github.com/nltk/nltk">NLTK</a>已经提供了相应的斯坦福文本处理工具接口，包括词性标注，命名实体识别和句法分析器的接口，不过可惜的是，没有提供<a href="http://www.52nlp.cn/category/word-segmentation">分词</a>器的接口。在google无果和阅读了相应的代码后，我决定照猫画虎为NLTK写一个斯坦福<a href="http://www.52nlp.cn/category/word-segmentation">中文分词</a>器接口，这样可以方便的在Python中调用斯坦福文本处理工具。</p>
<p>首先需要做一些准备工作，第一步当然是安装NLTK，这个可以参考我们在gensim的相关文章中的介绍《<a href="http://www.52nlp.cn/%E5%A6%82%E4%BD%95%E8%AE%A1%E7%AE%97%E4%B8%A4%E4%B8%AA%E6%96%87%E6%A1%A3%E7%9A%84%E7%9B%B8%E4%BC%BC%E5%BA%A6%E4%B8%89">如何计算两个文档的相似度</a>》，不过这里建议check github上最新的NLTK源代码并用“python setup.py install”的方式安装这个版本：<a href="https://github.com/nltk/nltk">https://github.com/nltk/nltk</a>。这个版本新增了对于斯坦福句法分析器的接口，一些老的版本并没有，这个之后我们也许还会用来介绍。而我们也是在这个版本中添加的斯坦福分词器接口，其他版本也许会存在一些小问题。其次是安装Java运行环境，以Ubuntu 12.04为例，安装Java运行环境仅需要两步：</p>
<blockquote><p>sudo apt-get install default-jre<br>
sudo apt-get install default-jdk</p></blockquote>
<p>最后，当然是最重要的，你需要下载<a href="http://nlp.stanford.edu/software/segmenter.shtml">斯坦福分词器</a>的相应文件，包括源代码，模型文件，词典文件等。注意斯坦福分词器并不仅仅支持中文分词，还支持阿拉伯语的分词，需要下载的zip打包文件是这个: <a href="http://nlp.stanford.edu/software/stanford-segmenter-2014-08-27.zip">Download Stanford Word Segmenter version 2014-08-27</a>，下载后解压。</p>
<p>准备工作就绪后，我们首先考虑的是在nltk源代码里的什么地方来添加这个接口文件。在nltk源代码包下，斯坦福词性标注器和命名实体识别工具的接口文件是这个：nltk/tag/stanford.py ，而句法分析器的接口文件是这个：nltk/parse/stanford.py , 虽然在nltk/tokenize/目录下有一个stanford.py文件，但是仅仅提供了一个针对英文的tokenizer工具PTBTokenizer的接口，没有针对斯坦福分词器的接口，于是我决定在nltk/tokenize下添加一个stanford_segmenter.py文件，作为nltk斯坦福中文分词器的接口文件。NLTK中的这些接口利用了Linux 下的管道（PIPE）机制和subprocess模块，这里直接贴源代码了，感兴趣的同学可以自行阅读:<br>
<span id="more-6763"></span></p>
<div class="codecolorer-container text default" style="overflow:auto;white-space:nowrap;width:620px;height:450px;"><table cellspacing="0" cellpadding="0"><tbody><tr>
<td class="line-numbers"><div>1<br>2<br>3<br>4<br>5<br>6<br>7<br>8<br>9<br>10<br>11<br>12<br>13<br>14<br>15<br>16<br>17<br>18<br>19<br>20<br>21<br>22<br>23<br>24<br>25<br>26<br>27<br>28<br>29<br>30<br>31<br>32<br>33<br>34<br>35<br>36<br>37<br>38<br>39<br>40<br>41<br>42<br>43<br>44<br>45<br>46<br>47<br>48<br>49<br>50<br>51<br>52<br>53<br>54<br>55<br>56<br>57<br>58<br>59<br>60<br>61<br>62<br>63<br>64<br>65<br>66<br>67<br>68<br>69<br>70<br>71<br>72<br>73<br>74<br>75<br>76<br>77<br>78<br>79<br>80<br>81<br>82<br>83<br>84<br>85<br>86<br>87<br>88<br>89<br>90<br>91<br>92<br>93<br>94<br>95<br>96<br>97<br>98<br>99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>
</div></td>
<td><div class="text codecolorer">#!/usr/bin/env python<br>
# -*- coding: utf-8 -*-<br>
# Natural Language Toolkit: Interface to the Stanford Chinese Segmenter<br>
#<br>
# Copyright (C) 2001-2014 NLTK Project<br>
# Author: 52nlp &lt;52nlpcn@gmail.com&gt;<br>
#<br>
# URL: &lt;http://nltk.org/&gt;<br>
# For license information, see LICENSE.TXT<br>
<br>
from __future__ import unicode_literals, print_function<br>
<br>
import tempfile<br>
import os<br>
import json<br>
from subprocess import PIPE<br>
<br>
from nltk import compat<br>
from nltk.internals import find_jar, config_java, java, _java_options<br>
<br>
from nltk.tokenize.api import TokenizerI<br>
<br>
class StanfordSegmenter(TokenizerI):<br>
    r"""<br>
    Interface to the Stanford Segmenter<br>
<br>
    &gt;&gt;&gt; from nltk.tokenize.stanford_segmenter import StanfordSegmenter<br>
    &gt;&gt;&gt; segmenter = StanfordSegmenter(path_to_jar="stanford-segmenter-3.4.1.jar", path_to_sihan_corpora_dict="./data", path_to_model="./data/pku.gz", path_to_dict="./data/dict-chris6.ser.gz")<br>
    &gt;&gt;&gt; sentence = u"这是斯坦福中文分词器测试"<br>
    &gt;&gt;&gt; segmenter.segment(sentence)<br>
    &gt;&gt;&gt; u'\u8fd9 \u662f \u65af\u5766\u798f \u4e2d\u6587 \u5206\u8bcd\u5668 \u6d4b\u8bd5\n'<br>
    &gt;&gt;&gt; segmenter.segment_file("test.simp.utf8")<br>
    &gt;&gt;&gt; u'\u9762\u5bf9 \u65b0 \u4e16\u7eaa \uff0c \u4e16\u754c \u5404\u56fd ...<br>
    """<br>
<br>
    _JAR = 'stanford-segmenter.jar'<br>
<br>
    def __init__(self, path_to_jar=None,<br>
            path_to_sihan_corpora_dict=None,<br>
            path_to_model=None, path_to_dict=None,<br>
            encoding='UTF-8', options=None,<br>
            verbose=False, java_options='-mx2g'):<br>
        self._stanford_jar = find_jar(<br>
            self._JAR, path_to_jar,<br>
            env_vars=('STANFORD_SEGMENTER',),<br>
            searchpath=(),<br>
            verbose=verbose<br>
        )<br>
        self._sihan_corpora_dict = path_to_sihan_corpora_dict<br>
        self._model = path_to_model<br>
        self._dict = path_to_dict<br>
<br>
        self._encoding = encoding<br>
        self.java_options = java_options<br>
        options = {} if options is None else options<br>
        self._options_cmd = ','.join('{0}={1}'.format(key, json.dumps(val)) for key, val in options.items())<br>
<br>
    def segment_file(self, input_file_path):<br>
        """<br>
        """<br>
        cmd = [<br>
            'edu.stanford.nlp.ie.crf.CRFClassifier',<br>
            '-sighanCorporaDict', self._sihan_corpora_dict,<br>
            '-textFile', input_file_path,<br>
            '-sighanPostProcessing', 'true',<br>
            '-keepAllWhitespaces', 'false',<br>
            '-loadClassifier', self._model,<br>
            '-serDictionary', self._dict<br>
        ]<br>
<br>
        stdout = self._execute(cmd)<br>
<br>
        return stdout<br>
<br>
    def segment(self, tokens):<br>
        return self.segment_sents([tokens])<br>
<br>
    def segment_sents(self, sentences):<br>
        """<br>
        """<br>
        encoding = self._encoding<br>
        # Create a temporary input file<br>
        _input_fh, self._input_file_path = tempfile.mkstemp(text=True)<br>
<br>
        # Write the actural sentences to the temporary input file<br>
        _input_fh = os.fdopen(_input_fh, 'wb')<br>
        _input = '\n'.join((' '.join(x) for x in sentences))<br>
        if isinstance(_input, compat.text_type) and encoding:<br>
            _input = _input.encode(encoding)<br>
        _input_fh.write(_input)<br>
        _input_fh.close()<br>
<br>
        cmd = [<br>
            'edu.stanford.nlp.ie.crf.CRFClassifier',<br>
            '-sighanCorporaDict', self._sihan_corpora_dict,<br>
            '-textFile', self._input_file_path,<br>
            '-sighanPostProcessing', 'true',<br>
            '-keepAllWhitespaces', 'false',<br>
            '-loadClassifier', self._model,<br>
            '-serDictionary', self._dict<br>
        ]<br>
<br>
        stdout = self._execute(cmd)<br>
<br>
        # Delete the temporary file<br>
        os.unlink(self._input_file_path)<br>
<br>
        return stdout<br>
<br>
    def _execute(self, cmd, verbose=False):<br>
        encoding = self._encoding<br>
        cmd.extend(['-inputEncoding', encoding])<br>
        _options_cmd = self._options_cmd<br>
        if _options_cmd:<br>
            cmd.extend(['-options', self._options_cmd])<br>
<br>
        default_options = ' '.join(_java_options)<br>
<br>
        # Configure java.<br>
        config_java(options=self.java_options, verbose=verbose)<br>
<br>
        stdout, _stderr = java(cmd,classpath=self._stanford_jar, stdout=PIPE, stderr=PIPE)<br>
        stdout = stdout.decode(encoding)<br>
<br>
        # Return java configurations to their default values.<br>
        config_java(options=default_options, verbose=False)<br>
<br>
        return stdout<br>
<br>
def setup_module(module):<br>
    from nose import SkipTest<br>
<br>
    try:<br>
        StanfordSegmenter()<br>
    except LookupError:<br>
        raise SkipTest('doctests from nltk.tokenize.stanford_segmenter are skipped because the stanford segmenter jar doesn\'t exist')</div></td>
</tr></tbody></table></div>
<p>我在github上fork了一个最新的NLTK版本，然后在这个版本中添加了<a href="https://github.com/panyang/nltk/blob/develop/nltk/tokenize/stanford_segmenter.py">stanford_segmenter.py</a>，感兴趣的同学可以自行下载这个代码，放到nltk/tokenize/目录下，然后重新安装NLTK：sudo python setpy.py install. 或者直接clone我们的这个<a href="https://github.com/panyang/nltk">nltk</a>版本，安装后就可以使用斯坦福中文分词器了。</p>
<p>现在就可以在Python NLTK中调用这个斯坦福中文分词接口了。为了方便起见，建议首先进入到解压后的斯坦福分词工具目录下：cd stanford-segmenter-2014-08-27，然后在这个目录下启用ipython，当然默认python解释器也可：</p>
<p># 初始化斯坦福中文分词器<br>
In [1]: from nltk.tokenize.stanford_segmenter import StanfordSegmenter</p>
<p># 注意分词模型，词典等资源在data目录下，使用的是相对路径，<br>
# 在stanford-segmenter-2014-08-27的目录下执行有效<br>
# 注意，斯坦福中文分词器提供了两个中文分词模型：<br>
#     ctb.gz是基于宾州中文树库训练的模型<br>
#     pku.gz是基于北大在2005backoof上提供的人名日报语料库<br>
# 这里选用了pku.gz，方便最后的测试<br>
In [2]: segmenter = StanfordSegmenter(path_to_jar=”stanford-segmenter-3.4.1.jar”, path_to_sihan_corpora_dict=”./data”, path_to_model=”./data/pku.gz”, path_to_dict=”./data/dict-chris6.ser.gz”)</p>
<p># 测试一个中文句子，注意u<br>
In [3]: sentence = u”这是斯坦福中文分词器测试”</p>
<p># 调用segment方法来切分中文句子，这里隐藏了一个问题，我们最后来说明<br>
In [4]: segmenter.segment(sentence)<br>
Out[4]: u’\u8fd9 \u662f \u65af\u5766\u798f \u4e2d\u6587 \u5206\u8bcd\u5668 \u6d4b\u8bd5\n’</p>
<p># 由于分词后显示的是中文编码，我们把这个结果输出到文件中<br>
# 不知道有没有同学有在python解释器总显示中文的方法<br>
In [5]: outfile = open(‘outfile’, ‘w’)</p>
<p>In [6]: result = segmenter.segment(sentence)</p>
<p># 注意写入到文件的时候要encode 为 UTF-8编码<br>
In [7]: outfile.write(result.encode(‘UTF-8′))</p>
<p>In [8]: outfile.close()</p>
<p>打开这个outfile文件：</p>
<blockquote><p>这 是 斯坦福 中文 分词器 测试</p></blockquote>
<p>这里同时提供了一个segment_file的调用方法，方便直接对文件进行切分，让我们来测试《<a href="http://www.52nlp.cn/%E4%B8%AD%E6%96%87%E5%88%86%E8%AF%8D%E5%85%A5%E9%97%A8%E4%B9%8B%E8%B5%84%E6%BA%90">中文分词入门之资源</a>》中介绍的backoff2005的测试集pku_test.utf8，来看看斯坦福分词器的效果：</p>
<p>In [9]: result = segmenter.segment_file(‘pku_test.utf8′)</p>
<p>In [10]: outfile = open(‘pku_outfile’, ‘w’)</p>
<p>In [11]: outfile.write(result.encode(‘UTF-8′))</p>
<p>In [12]: outfile.close()</p>
<p>打开结果文件pku_outfile：</p>
<blockquote><p>共同 创造 美好 的 新 世纪 ——二○○一年 新年 贺词<br>
（ 二○○○年 十二月 三十一日 ） （ 附 图片 1 张 ）<br>
女士 们 ， 先生 们 ， 同志 们 ， 朋友 们 ：<br>
2001年 新年 钟声 即将 敲响 。 人类 社会 前进 的 航船 就要 驶入 21 世纪 的 新 航程 。 中国 人民 进入 了 向 现代化 建设 第三 步 战略 目标 迈进 的 新 征程 。<br>
在 这个 激动人心 的 时刻 ， 我 很 高兴 通过 中国 国际 广播 电台 、 中央 人民 广播 电台 和 中央 电视台 ， 向 全国 各族 人民 ， 向 香港 特别 行政区 同胞 、 澳门 特别 行政区 同胞 和 台湾 同胞 、 海外 侨胞 ， 向 世界 各国 的 朋友 们 ， 致以 新 世纪 第一 个 新年 的 祝贺 ！<br>
….
</p></blockquote>
<p>我们用backoff2005的测试脚本来测试一下斯坦福中文分词器在这份测试语料上的效果：</p>
<p>./icwb2-data/scripts/score ./icwb2-data/gold/pku_training_words.utf8 ./icwb2-data/gold/pku_test_gold.utf8 pku_outfile &gt; stanford_pku_test.score</p>
<p>结果如下：<br>
=== SUMMARY:<br>
=== TOTAL INSERTIONS:	1479<br>
=== TOTAL DELETIONS:	1974<br>
=== TOTAL SUBSTITUTIONS:	3638<br>
=== TOTAL NCHANGE:	7091<br>
=== TOTAL TRUE WORD COUNT:	104372<br>
=== TOTAL TEST WORD COUNT:	103877<br>
=== TOTAL TRUE WORDS RECALL:	0.946<br>
=== TOTAL TEST WORDS PRECISION:	0.951<br>
=== F MEASURE:	0.948<br>
=== OOV Rate:	0.058<br>
=== OOV Recall Rate:	0.769<br>
=== IV Recall Rate:	0.957<br>
###	pku_outfile	1479	1974	3638	7091	104372	103877	0.946	0.951	0.948	0.058	0.769	0.957</p>
<p>准确率是95.1%， 召回率是94.6%, F值是94.8%, 相当不错。感兴趣的同学可以测试一下其他测试集，或者用宾州中文树库的模型来测试一下结果。</p>
<p>最后我们再说明一下这个接口存在的问题，因为使用了Linux PIPE模式来调用斯坦福中文分词器，相当于在Python中执行相应的Java命令，导致每次在执行分词时会加载一遍分词所需的模型和词典，这个对文件操作时（segment_file)没有多大的问题，但是在对句子执行分词（segment)的时候会存在很大的问题，每次都加载数据，在实际产品中基本是不可用的。虽然发现斯坦福分词器提供了一个 –readStdin 的读入标准输入的参数，也尝试通过python subprocess中先load 文件，再用的communicate方法来读入标准输入，但是仍然没有解决问题，发现还是一次执行，一次结束。这个问题困扰了我很久，google了很多资料也没有解决问题，欢迎懂行的同学告知或者来解决这个问题，再此先谢过了。That’s all!</p>
<p>注：原创文章，转载请注明出处“<a href="http://www.52nlp.cn">我爱自然语言处理</a>”：<a href="http://www.52nlp.cn">www.52nlp.cn</a></p>
<p>本文链接地址：<a href="http://www.52nlp.cn/?p=6763">http://www.52nlp.cn/python自然语言处理实践-在nltk中使用斯坦福中文分词器</a></p>

											


  </article>

  <div class="meta">
  
    <a class="basic-alignment left" href="/posts/2014/2014-09-23-77531-61f37eb4a.html" title="Previous Post: 深度学习之浅见" data-instant>&laquo; 深度学习之浅见</a>
  
  
    <a class="basic-alignment right" href="/posts/2014/2014-09-24-howto-use-custom-rubygem-in-logstash-205f7f438.html" title="Next Post: 在 logstash 里使用其他 RubyGems 模块" data-instant>在 logstash 里使用其他 RubyGems 模块 &raquo;</a>
  
</div>
  <div id="related">
  <h2 class="subheader">Related Posts <small>They might be useful</small></h2>
  <ul class="posts">
    
      <li><span>14 May 2018</span> &raquo; <a href="http://iftti.com/posts/2018/2018-05-14-113985-6dc6e9281.html">给初学者看的 shuf 命令教程</a></li>
    
      <li><span>13 May 2018</span> &raquo; <a href="http://iftti.com/posts/2018/2018-05-13-113977-b56537dc0.html">常用排序算法总结（2）</a></li>
    
      <li><span>13 May 2018</span> &raquo; <a href="http://iftti.com/posts/2018/2018-05-13-113953-957e4ea5a.html">10 个常用的软件架构模式</a></li>
    
  </ul>
</div>

  
<comments>

<!-- UY BEGIN -->
<div id="uyan_frame"></div>
<script type="text/javascript" src="http://v2.uyan.cc/code/uyan.js?uid=1936498"></script>
<!-- UY END -->

</comments>

</div>
      <!-- JiaThis Button BEGIN -->
<div class="jiathis_share_slide jiathis_share_24x24" id="jiathis_share_slide">
<div class="jiathis_share_slide_top" id="jiathis_share_title"></div>
<div class="jiathis_share_slide_inner">
<div class="jiathis_style_24x24">
<a class="jiathis_button_tsina"></a>
<a class="jiathis_button_googleplus"></a>
<a class="jiathis_button_twitter"></a>
<a class="jiathis_button_linkedin"></a>
<a class="jiathis_button_weixin"></a>
<a class="jiathis_button_cqq"></a>
<a class="jiathis_button_renren"></a>
<a class="jiathis_button_evernote"></a>
<a class="jiathis_button_pocket"></a>
<a href="http://www.jiathis.com/share" class="jiathis jiathis_txt jtico jtico_jiathis" target="_blank"></a>
<script type="text/javascript">
var jiathis_config = {data_track_clickback:'true'
	,slide:{
		divid:'wrap',
		pos:'left',
		gt:'true'
	}
};
</script>
<script type="text/javascript" src="http://v3.jiathis.com/code/jia.js?uid=1936498" charset="utf-8"></script>
<script type="text/javascript" src="http://v3.jiathis.com/code/jiathis_slide.js" charset="utf-8"></script>
</div></div></div>
<!-- JiaThis Button END -->
      </div>
    </div>

    <footer class="site-footer">

  <div class="wrap">

    <h2 class="footer-heading">IT技术干货</h2>

    <div class="footer-col-1 column">
      <p class="text">IT技术干货 KernelHacks 最好的技术站点 技术信息 纯干货</p>
      <ul>
        <li>汇集最好的科技与互联网信息</li>
        <li>Liu Lantao</li>
        <li><a href="mailto:iftti@iftti.com">iftti@iftti.com</a></li>
      </ul>
    </div>

    <div class="footer-col-2 column">
      <ul>
        <li>
          <a href="https://github.com/Lax">
            <span class="icon github">
              <svg version="1.1" class="github-icon-svg" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
                 viewBox="0 0 16 16" enable-background="new 0 0 16 16" xml:space="preserve">
                <path fill-rule="evenodd" clip-rule="evenodd" fill="#C2C2C2" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761
                c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32
                c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472
                c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037
                C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65
                c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261
                c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082
                c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129
                c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/>
              </svg>
            </span>
            <span class="username">Lax</span>
          </a>
        </li>
        <li>
          <a href="https://twitter.com/liulantao">
            <span class="icon twitter">
              <svg version="1.1" class="twitter-icon-svg" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
                 viewBox="0 0 16 16" enable-background="new 0 0 16 16" xml:space="preserve">
                <path fill="#C2C2C2" d="M15.969,3.058c-0.586,0.26-1.217,0.436-1.878,0.515c0.675-0.405,1.194-1.045,1.438-1.809
                c-0.632,0.375-1.332,0.647-2.076,0.793c-0.596-0.636-1.446-1.033-2.387-1.033c-1.806,0-3.27,1.464-3.27,3.27
                c0,0.256,0.029,0.506,0.085,0.745C5.163,5.404,2.753,4.102,1.14,2.124C0.859,2.607,0.698,3.168,0.698,3.767
                c0,1.134,0.577,2.135,1.455,2.722C1.616,6.472,1.112,6.325,0.671,6.08c0,0.014,0,0.027,0,0.041c0,1.584,1.127,2.906,2.623,3.206
                C3.02,9.402,2.731,9.442,2.433,9.442c-0.211,0-0.416-0.021-0.615-0.059c0.416,1.299,1.624,2.245,3.055,2.271
                c-1.119,0.877-2.529,1.4-4.061,1.4c-0.264,0-0.524-0.015-0.78-0.046c1.447,0.928,3.166,1.469,5.013,1.469
                c6.015,0,9.304-4.983,9.304-9.304c0-0.142-0.003-0.283-0.009-0.423C14.976,4.29,15.531,3.714,15.969,3.058z"/>
              </svg>
            </span>
            <span class="username">@liulantao</span>
          </a>
        </li>
        <li>
          <a href="https://plus.google.com/+LiuLantao">
            <span class="icon googleplus">
              <svg version="1.1" id="Layer_1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
                width="16px" height="16px" viewBox="0 0 134.658 131.646" enable-background="new 0 0 134.658 131.646"
                xml:space="preserve">
                <g>
                  <path fill="#C2C2C2" d="M126.515,4.109H8.144c-2.177,0-3.94,1.763-3.94,3.938v115.546c0,2.179,1.763,3.942,3.94,3.942h118.371
                  c2.177,0,3.94-1.764,3.94-3.942V8.048C130.455,5.872,128.691,4.109,126.515,4.109z"/>
                  <g>
                    <path fill="#FFFFFF" d="M70.479,71.845l-3.983-3.093c-1.213-1.006-2.872-2.334-2.872-4.765c0-2.441,1.659-3.993,3.099-5.43
                    c4.64-3.652,9.276-7.539,9.276-15.73c0-8.423-5.3-12.854-7.84-14.956h6.849l7.189-4.517H60.418
                    c-5.976,0-14.588,1.414-20.893,6.619c-4.752,4.1-7.07,9.753-7.07,14.842c0,8.639,6.633,17.396,18.346,17.396
                    c1.106,0,2.316-0.109,3.534-0.222c-0.547,1.331-1.1,2.439-1.1,4.32c0,3.431,1.763,5.535,3.317,7.528
                    c-4.977,0.342-14.268,0.893-21.117,5.103c-6.523,3.879-8.508,9.525-8.508,13.51c0,8.202,7.731,15.842,23.762,15.842
                    c19.01,0,29.074-10.519,29.074-20.932C79.764,79.709,75.344,75.943,70.479,71.845z M56,59.107
                    c-9.51,0-13.818-12.294-13.818-19.712c0-2.888,0.547-5.87,2.428-8.199c1.773-2.218,4.861-3.657,7.744-3.657
                    c9.168,0,13.923,12.404,13.923,20.382c0,1.996-0.22,5.533-2.762,8.09C61.737,57.785,58.762,59.107,56,59.107z M56.109,103.65
                    c-11.826,0-19.452-5.657-19.452-13.523c0-7.864,7.071-10.524,9.504-11.405c4.64-1.561,10.611-1.779,11.607-1.779
                    c1.105,0,1.658,0,2.538,0.111c8.407,5.983,12.056,8.965,12.056,14.629C72.362,98.542,66.723,103.65,56.109,103.65z"/>
                    <polygon fill="#FFFFFF" points="98.393,58.938 98.393,47.863 92.923,47.863 92.923,58.938 81.866,58.938 81.866,64.469
                    92.923,64.469 92.923,75.612 98.393,75.612 98.393,64.469 109.506,64.469 109.506,58.938"/>
                  </g>
                </g>
              </svg>
            </span>
            <span class="username">+LiuLantao</span>
          </a>
        </li>
      </ul>
    </div>

    <div class="footer-col-3 column">
      
<!--以下是QQ邮件列表订阅嵌入代码--><script >var nId = "6be92ef3590ee662cd5e6381ab2044c328716364f684cf3e",nWidth="auto",sColor="light",sText="填写您的邮件地址，订阅我们的精彩内容：" ;</script><script src="http://list.qq.com/zh_CN/htmledition/js/qf/page/qfcode.js" charset="gb18030"></script>

    </div>

  </div>

  <div class="wrap">
    <div>
      <a href="http://blog.liulantao.com">Blog</a> | <a href="http://1000bit.com">铅笔特评 1000bit</a> | <a href="http://visplanet.com">VisPlanet</a> | <a href="http://iftti.com">IT技术干货</a> | <a href="http://relax.org.cn">Relax</a> | <a href="http://hangzhou.io">杭州城市指南</a>
    </div>
  </div>

  
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-1658815-7', 'iftti.com');
  ga('require', 'displayfeatures');
  ga('send', 'pageview');

</script>


</footer>


    </body>
</html>