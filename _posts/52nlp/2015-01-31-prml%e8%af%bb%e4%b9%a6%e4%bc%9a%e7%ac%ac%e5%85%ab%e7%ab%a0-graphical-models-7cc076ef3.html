---
layout: post
title: 'PRML读书会第八章  Graphical Models'
time: 2015-01-31
site_name: 52nlp.cn
source_url: http://www.52nlp.cn/prml%e8%af%bb%e4%b9%a6%e4%bc%9a%e7%ac%ac%e5%85%ab%e7%ab%a0-graphical-models
images:
  711dd31ad211f0e208691384f21bebbe: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0402_PRMLGra1.jpg
  240009b30917c75e6e8dea72da7c15f8: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0402_PRMLGra2.jpg
  74f8f5620ef90e699e0342640893699b: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0402_PRMLGra3.jpg
  81e64fe6e9f426a5b1fefd362bab6542: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0402_PRMLGra4.jpg
  dcb19345aa1d8d4667bee7cb594a98c9: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0402_PRMLGra5.jpg
  cdb88b875a38905cd10f8b30a9fdfe2e: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0402_PRMLGra6.jpg
  20331242043b319984d3d085b162a187: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0402_PRMLGra7.jpg
  4ee6105a10bd19978f490fdf3db351be: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0402_PRMLGra8.jpg
  59487f132a9f73a1f28adb3a4bb993d0: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0402_PRMLGra9.jpg
  167997121e6a661fe1e5c6f4494fbbfb: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0402_PRMLGra10.jpg
  e54196de84b7b77e7d95a53330fcacb5: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0402_PRMLGra11.jpg
  f615cf610671e1a7e35d0cc8113996d5: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0402_PRMLGra12.jpg
  9125f163f71810ae944df8a9838e0612: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0402_PRMLGra13.jpg
  8c54ccb844643a221887efc4f4d5c016: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0402_PRMLGra14.jpg
  08329d123764480718e95f9b24b560d2: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0402_PRMLGra15.jpg
  53312848ea6cc6fa4fe45ee72a517857: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0402_PRMLGra16.jpg
  078f0292ee0e167d5b57e39f2f33a3b9: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0402_PRMLGra17.jpg
  9e79837e987cd2dc6d65684894f97413: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0402_PRMLGra18.jpg
  1d5657c5f5a06a5bde0c9b9191c61124: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0402_PRMLGra19.jpg
  1a6025a85cd29be822854607d38f0843: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0402_PRMLGra20.jpg
  a323fb98bb812323c011f2b2580f21d1: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0402_PRMLGra21.jpg
  2710737c2857506f56039c1fafe3dcdc: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0402_PRMLGra22.jpg
  529938c377158d69ed2a5a35791dc4d4: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0402_PRMLGra23.jpg
  f3ef8cad75c98f0b2ee4d97a21d51902: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0402_PRMLGra24.jpg
  fbe5e6aad5e858efd74fc9405d7db1b4: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0402_PRMLGra25.jpg
  1bb01c9ebf347758bffb277296a3773a: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0402_PRMLGra26.jpg
  bea0d1d974cb1d1328298a50269effba: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0402_PRMLGra27.jpg
  e7573463095c637a611ffbdc9815e918: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0402_PRMLGra28.jpg
  aa64bd639bca57abb6382807b8c54fc6: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0402_PRMLGra29.jpg
  17fe077bd1cd61e2bcf88c8e3856f8e2: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0402_PRMLGra30.jpg
  02fb8032fb5a670e9c6ddb53977d247a: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0402_PRMLGra31.jpg
  767ab9ef9b394c9e6842bb0f6f9a8546: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0402_PRMLGra32.jpg
  f7a363d2fae2929c533b7eebf412dee5: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0402_PRMLGra33.gif
  1fda0c07f6f3fe74fdb1f5fd01ba140e: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0402_PRMLGra34.jpg
  777fee0478174419b50c2e7f2181b32b: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0402_PRMLGra35.jpg
  4d57459d061b12b5306943839c821a92: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0402_PRMLGra36.jpg
  cbe9bf98180a14cb5045dca1794dc84d: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0402_PRMLGra37.jpg
  7e71122e14cfaa88afdd58296b070111: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0402_PRMLGra38.jpg
  3635e536873f2f8fc72de50279ebdd3d: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0402_PRMLGra39.jpg
  8eb9c9545ad3c13c7fad9613b7a21b60: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0402_PRMLGra40.jpg
  780cb05565153f42f5c16af41ffb2745: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0402_PRMLGra41.jpg
  b73761b2e92ab0e5ee900c85c690fced: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0402_PRMLGra42.jpg
  bce8e43c96148fe947b45673d132458d: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0402_PRMLGra43.jpg
  ab7496f79bbd3a88c5efdc07bfc7f41e: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0402_PRMLGra44.jpg
  6abcc733a0eef13480d910dcbbf65917: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0402_PRMLGra45.jpg
  373711657bf8fc457039064b2160f500: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0402_PRMLGra46.jpg
  c3a6215edc71bf7e8f686093c75d3eb4: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0402_PRMLGra47.jpg
  76860163e1ac4afd0716cb30dc461b10: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0402_PRMLGra48.jpg
  6c1d66db3d4502367458e64206cbbeca: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0402_PRMLGra49.jpg
  5251644e28bcc5c37856419b42209fe7: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0402_PRMLGra50.jpg
  3763991ca8d8df08824cbe3afa03a122: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0402_PRMLGra51.jpg
  fb058e2e7185d55c95cdfd5caab7bccc: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0402_PRMLGra52.jpg
  1123302e409577b601ed6d165ec7ffa9: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0402_PRMLGra53.jpg
  a10834c216091c29e028a5dcefbcfb45: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0402_PRMLGra54.jpg
  eaae67fffb2d9a5038ead38c5fdbbc0b: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0402_PRMLGra55.jpg
  09d6d75acf7191132e8fc99768b0cc1e: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0402_PRMLGra56.jpg
  4dde39f1d7972ae3eede98f20eba946d: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0402_PRMLGra57.jpg
  6998144da89ee64b53db7ed2ef38f45a: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0402_PRMLGra58.gif
---
{% raw %}

						<p style="text-align: center;background: white"><span style="font-family: 微软雅黑;font-size: 15pt"><strong>PRML读书会<span style="color: black">第八章 Graphical Models<br>
</span></strong></span></p>
<p style="text-align: center;background: white"><span style="color: black;font-family: 微软雅黑;font-size: 15pt"><strong>主讲人 网神<br>
</strong></span></p>
<p style="text-align: center;background: white"><span style="color: black;font-family: 微软雅黑;font-size: 12pt"><strong>（新浪微博: <a href="http://weibo.com/ghtimaq">@豆角茄子麻酱凉面</a>）<br>
</strong></span></p>
<p style="background: white"><span style="color: black;font-family: 微软雅黑;font-size: 9pt">网神(66707180) 18:52:10<br>
</span></p>
<p style="background: white"><span style="color: black;font-family: 微软雅黑;font-size: 9pt">今天的内容主要是：<br>
</span></p>
<p style="background: white"><span style="color: black;font-family: 微软雅黑;font-size: 9pt">1.贝叶斯网络和马尔科夫随机场的概念，联合概率分解，条件独立表示；2.图的概率推断inference。<br>
</span></p>
<p style="background: white"><span style="color: black;font-family: 微软雅黑;font-size: 9pt">图模型是用图的方式表示概率推理 ，将概率模型可视化，方便展示变量之间的关系，概率图分为有向图和无向图。有向图主要是贝叶斯网络，无向图主要是马尔科夫随机场。对两类图，prml都讲了如何将联合概率分解为条件概率，以及如何表示和判断条件依赖。<br>
先说贝叶斯网络，贝叶斯网络是有向图，用节点表示随机变量，用箭头表示变量之间的依赖关系。一个例子:<br>
</span><span id="more-7603"></span></p>
<p style="background: white"><img src="/images/52nlp.cn/711dd31ad211f0e208691384f21bebbe.jpg" alt=""><span style="color: black;font-family: 微软雅黑;font-size: 9pt"><br>
这是一个有向无环图，这个图表示的概率模型如下:<br>
p(x1,x2,…x7)= <img src="/images/52nlp.cn/240009b30917c75e6e8dea72da7c15f8.jpg" alt="">形式化一下，贝叶斯网络表示的联合分布是：<br>
<img src="/images/52nlp.cn/74f8f5620ef90e699e0342640893699b.jpg" alt=""><br>
</span></p>
<p style="background: white"><span style="color: black;font-family: 微软雅黑;font-size: 9pt">其中<img src="/images/52nlp.cn/81e64fe6e9f426a5b1fefd362bab6542.jpg" alt="">是xk的所有父节点。<br>
</span></p>
<p style="background: white"><span style="color: black;font-family: 微软雅黑;font-size: 9pt">以上是贝叶斯网络将联合概率分解为条件概率的方法，比较直观易懂，就不多说了。下面说一下条件独立的表示和判断方法。条件独立是，给定a,b,c三个节点，如果p(a,b|c)=p(a|c)p(b|c)，则说给定c，a和b条件独立。当然 a, b, c也可以是三组节点，这里只以单个节点为例。用图表示，有三种情况 。<br>
第一种情况如图：<br>
<img src="/images/52nlp.cn/dcb19345aa1d8d4667bee7cb594a98c9.jpg" alt=""><br>
c位于两个箭头的尾部，称作tail-to-tail，这种情况，c未知的时候，a，b是不独立的。c已知的时候，a,b条件独立。来看为什么，首先，这个图联合概率如下：<br>
在c未知的时候,p(a,b)如下求解:<br>
<img src="/images/52nlp.cn/cdb88b875a38905cd10f8b30a9fdfe2e.jpg" alt=""><br>
可以看出，无法得出:p(a,b)=p(a)p(b)，所以a,b不独立。<br>
如果c已知，则：<br>
</span></p>
<p style="background: white"><img src="/images/52nlp.cn/20331242043b319984d3d085b162a187.jpg" alt=""><span style="color: black;font-family: 微软雅黑;font-size: 9pt"><br>
所以a,b条件独立于c。条件独立用以下符号表示：<br>
<img src="/images/52nlp.cn/4ee6105a10bd19978f490fdf3db351be.jpg" alt=""><br>
a,b不独立的符号表示:：<br>
<img src="/images/52nlp.cn/59487f132a9f73a1f28adb3a4bb993d0.jpg" alt=""><br>
这是图表示的条件独立的第一种形式，叫做tail-to-tail。第二种是tail-to-head，如图：<br>
<img src="/images/52nlp.cn/167997121e6a661fe1e5c6f4494fbbfb.jpg" alt=""><br>
这种情况也是c未知时，a和b不独立。c已知时，a和b条件独立于c，推导如下：<br>
<img src="/images/52nlp.cn/e54196de84b7b77e7d95a53330fcacb5.jpg" alt=""><br>
第三种情况是head-to-head，如图：<br>
<img src="/images/52nlp.cn/f615cf610671e1a7e35d0cc8113996d5.jpg" alt=""><br>
这种情况反过来了，c未知时，a和b是独立的；但当c已知时，a和b不满足条件独立，<br>
因为：<img src="/images/52nlp.cn/9125f163f71810ae944df8a9838e0612.jpg" alt=""><br>
计算该概率的边界概率，得<br>
<img src="/images/52nlp.cn/8c54ccb844643a221887efc4f4d5c016.jpg" alt=""><br>
所以a和b相互独立.<br>
但c已知时：<br>
<img src="/images/52nlp.cn/08329d123764480718e95f9b24b560d2.jpg" alt=""><br>
无法得到p(a,b|c)=p(a|c)p(b|c)<br>
将这三种情况总结，就是贝叶斯网络的一个重要概念，D-separation，这个概念的内容就是:<br>
A,B,C三组节点，如果A中的任意节点与B的任意节点的所有路径上，存在以下节点，就说A和B被C阻断:<br>
1, A到B的路径上存在tail-to-tail或head-to-tail形式的节点，并且该节点属于C<br>
2. 路径上存在head-to-head的节点，并且该节点不属于C<br>
举个例子：<br>
</span></p>
<p style="background: white"><img src="/images/52nlp.cn/53312848ea6cc6fa4fe45ee72a517857.jpg" alt=""><span style="color: black;font-family: 微软雅黑;font-size: 9pt"><br>
左边图上，节点f和节点e都不是d-separation.因为f是tail-to-tail，但f不是已知的，因此f不属于C.<br>
e是head-to-head，但e的子节点c是已知的，所以e也不属于C。<br>
speedmancs&lt;speedmancs@qq.com&gt; 19:23:05<br>
2漏了一点，该节点包括所有后继<br>
网神(66707180) 19:23:21<br>
右边图，f和e都是d-separation.理由与上面相反.对，是漏了这一点。看到这个例子才想起来，这部分大家有什么问题没？<br>
speedmancs&lt;speedmancs@qq.com&gt; 19:24:54<br>
这个还是抽象了一些，我之前看的prml这一章，没看懂，后来看了PGM前三章，主要看了那个学生成绩的那个例子，就明白了。姑妄记之，其实蛮不好记的。讲得挺好的，继续。<br>
网神(66707180) 19:27:14<br>
因为有了这些条件独立的规则，可以将图理解成一个filter。<br>
既给定一系列随机变量，其联合分布p(x1,x2…,xn)理论上可以分解成各种条件分布的乘积，但过一遍图，不满足图表示依赖关系和条件独立的分布就被过滤掉。所以图模型，用不同随机变量的连接表示各种关系，可以表示复杂的分布模型。<br>
接下来是马尔科夫随机场，是无向图，也叫马尔科夫网络，马尔科夫网络也有条件独立属性。<br>
用MRF(malkov random field)表示马尔科夫网络，MRF因为是无向的，所以不存在tail-to-tail这些概念。MRF的条件独立如图：<br>
<img src="/images/52nlp.cn/078f0292ee0e167d5b57e39f2f33a3b9.jpg" alt=""><br>
如果A的任意节点和B的任意节点的任意路径上，都存在至少一个节点属于C<br>
speedmancs&lt;speedmancs@qq.com&gt; 19:33:16<br>
无向图的条件独立 比有向图简单多了。<br>
网神(66707180) 19:33:18<br>
那么A和B条件独立于C，可以理解为，如果C的节点都是已知的，就阻断了A和B的所有路径。<br>
网神(66707180) 19:33:49<br>
嗯，MRF的 概率分解就概念比较多了，不像有向图那么直观，MRF联合概率分解成条件概率。用到了clique的概念，我翻译成”团”，就是图的一个子图，子图上两两节点都有连接. 例如这个图，最大团有两个，分别是(x1,x2,x3)和(x2,x3,x4)：<br>
<img src="/images/52nlp.cn/9e79837e987cd2dc6d65684894f97413.jpg" alt=""><br>
MRF的联合概率分解另一个概念是potential function，联合概率分解成一系列potential函数的乘积:<br>
<img src="/images/52nlp.cn/1d5657c5f5a06a5bde0c9b9191c61124.jpg" alt=""><br>
</span></p>
<p style="background: white"><img src="/images/52nlp.cn/1a6025a85cd29be822854607d38f0843.jpg" alt=""><span style="color: black;font-family: 微软雅黑;font-size: 9pt">是一个最大团的所有节点，一个potential函数<img src="/images/52nlp.cn/a323fb98bb812323c011f2b2580f21d1.jpg" alt="">，是最大团的一个函数.<br>
这个函数具体的定义是依赖具体应用的，一会举个例子.<br>
上面式子里那个Z是normalization常量：<br>
<img src="/images/52nlp.cn/2710737c2857506f56039c1fafe3dcdc.jpg" alt=""><br>
speedmancs&lt;speedmancs@qq.com&gt; 19:42:47<br>
这个Z很麻烦<br>
网神(66707180) 19:43:15<br>
在这个式子里<img src="/images/52nlp.cn/529938c377158d69ed2a5a35791dc4d4.jpg" alt="">，p(x)是一系列potential函数的乘积。换一种理解方式，定义将potential函数表示成指数函数:<br>
<img src="/images/52nlp.cn/f3ef8cad75c98f0b2ee4d97a21d51902.jpg" alt=""><br>
这样p(x)就可以表示成一系列E(Xc)的和的指数函数，E(Xc)叫做能量函数，这么转换之后，可以将图理解成一个能量的集合，他的值等于各个最大团的能量的和.先举个例子看看potential函数和能量函数在具体应用中是什么样的，大家再讨论。<br>
要把噪声图片尽量还原成 原图，用图的方式表示噪声图和还原后的图，每个像素点是一个节点:<br>
<img src="/images/52nlp.cn/fbe5e6aad5e858efd74fc9405d7db1b4.jpg" alt=""><br>
上面那层Yi，是噪声图，紫色表示这些是已知的，是观察值。下面那层Xi是未知的，要求出Xi，使Xi作为像素值得到的图，尽量接近无噪声图片。每个xi的值，与yi相关，也与相邻的xj相关。这里边，最大团是(xi, yi)和(xi, xj)，两类最大团。<br>
对于(xi, yi)，选择能量函数E(xi,yi)=<img src="/images/52nlp.cn/1bb01c9ebf347758bffb277296a3773a.jpg" alt="">；对于(xi,xj)，选择能量函数E(xi, xj)=<img src="/images/52nlp.cn/bea0d1d974cb1d1328298a50269effba.jpg" alt=""><br>
两个能量函数的意思都是，如果xi和yi (或xi和xj)的值相同，则能量小；如果不同，则能量大.<br>
整个图的能量如下：<br>
<img src="/images/52nlp.cn/e7573463095c637a611ffbdc9815e918.jpg" alt=""><br>
</span></p>
<p style="background: white"><img src="/images/52nlp.cn/aa64bd639bca57abb6382807b8c54fc6.jpg" alt=""><span style="color: black;font-family: 微软雅黑;font-size: 9pt">是一个偏置项<br>
speedmancs&lt;speedmancs@qq.com&gt; 19:56:55<br>
偏置项是一种先验吧<br>
网神(66707180) 19:57:28<br>
有了这个能量函数，接下来就是求出Xi，使得能量E(x,y)最小。求最小，书上简单说了一下，我的理解也是用梯度下降类似的方法。<br>
speedmancs&lt;speedmancs@qq.com&gt; 19:57:34<br>
这里表示-1的点更多吧。<br>
网神(66707180) 19:58:20<br>
对偏执项的作用，书上这么解释：Such a term has the effect of biasing the model towards pixel values that have one particular sign in preference to the other.<br>
speedmancs&lt;speedmancs@qq.com&gt; 19:59:29<br>
恩，对，让能量最小<br>
网神(66707180) 19:59:39<br>
</span></p>
<p style="background: white"><span style="color: black;font-family: 微软雅黑;font-size: 9pt">为了使E(x,y)尽量小，是尽量让xi选择-1，而不是1。E(x,y)越小，得到的图就越接近无噪声的图，因为：<br>
</span></p>
<p style="background: white"><span style="color: black;font-family: 微软雅黑;font-size: 9pt"><br>
<img src="/images/52nlp.cn/17fe077bd1cd61e2bcf88c8e3856f8e2.jpg" alt=""><br>
所以E(x,y)越小，p(x,y)就越大。<br>
η&lt;liyitan2144@163.com&gt; 20:01:13<br>
是不是可以这样看，<img src="/images/52nlp.cn/02fb8032fb5a670e9c6ddb53977d247a.jpg" alt="">表示平滑；<img src="/images/52nlp.cn/767ab9ef9b394c9e6842bb0f6f9a8546.jpg" alt="">表示似然。<br>
网神(66707180) 20:02:18<br>
liyitan2144说得好，<img src="/images/52nlp.cn/f7a363d2fae2929c533b7eebf412dee5.jpg" alt=""><br>
speedmancs&lt;speedmancs@qq.com&gt; 20:02:33<br>
恩，所以这是一个 产生式模型。<br>
网神(66707180) 20:02:59<br>
有向图和无向图的概率分解 和 条件独立 都说完了.有向图和无向图是可以互相转换的，有向图转换成无向图，如果每个节点都只有少于等于1个父节点，比较简单。如果有超过1个父节点，就需要在转换之后的无向图上增加一些边，来避免都是有向图上的一些关系，这部分就不细说了。<br>
下面要说图的inference了。前面大家有啥要讨论的？先讨论一下吧。<br>
</span></p>
<p><span style="color: #333333"><span style="font-family: Arial">============================</span><span style="font-family: 宋体">讨论</span><span style="font-family: Arial">=================================<br>
</span></span></p>
<p style="background: white"><span style="color: black;font-family: 微软雅黑;font-size: 9pt">speedmancs&lt;speedmancs@qq.com&gt; 20:06:45<br>
刚才那个例子中，那几个beta, h等参数如何得到？<br>
网神(66707180) 20:07:30<br>
那个是用迭代求解的方法，求得这几个参数，书上提到了两种方法，一种ICM,iterated conditional modes<br>
一种max-product方法，其中max-product方法效果比较好,在后面的inference一节里详细讲了这个方法，而ICM方法只是提了一下，原理没有细说.两种方法的效果看下图,左边是ICM的结果，右边是max-product的方法：<br>
<img src="/images/52nlp.cn/1fda0c07f6f3fe74fdb1f5fd01ba140e.jpg" alt=""><br>
</span></p>
<p style="background: white"><span style="color: black;font-family: 微软雅黑;font-size: 9pt">speedmancs&lt;speedmancs@qq.com&gt; 20:15:07<br>
稍微插一句，刚才那个denoise的例子，最好的那个结果是graph cut，而且那几个beta参数是事先固定了。<br>
η&lt;liyitan2144@163.com&gt; 20:16:12<br>
graph cut是指？<br>
kxkr&lt;lxfkxkr@126.com&gt; 20:16:30<br>
<img src="/images/52nlp.cn/777fee0478174419b50c2e7f2181b32b.jpg" alt="">kxkr&lt;lxfkxkr@126.com&gt; 20:17:18<br>
<img src="/images/52nlp.cn/4d57459d061b12b5306943839c821a92.jpg" alt=""><br>
</span></p>
<p style="background: white"><span style="color: black;font-family: 微软雅黑;font-size: 9pt">网神(66707180) 20:18:11<br>
这个例子只是为了说明能量函数和潜函数.所以不一定是最佳方法，这两段截屏是prml上的，咋没看到捏<br>
kxkr&lt;lxfkxkr@126.com&gt; 20:19:05<br>
那几个参数如何得到 文中好像并没有说，只是说了ICM、graph-cut能够得到最后的去噪图像，第一段 在 figure8.30，第二个截图在 section8.4，figure8.32下面。<br>
网神(66707180) 20:20:15<br>
看到了，先不管这个吧，主要知道能量函数是啥样的就行了<br>
kxkr&lt;lxfkxkr@126.com&gt; 20:21:25<br>
嗯<br>
</span></p>
<p><span style="color: #333333"><span style="font-family: Arial">========================</span><span style="font-family: 宋体">讨论结束</span><span style="font-family: Arial">==============f===================<br>
</span></span></p>
<p style="background: white"><span style="color: black;font-family: 微软雅黑;font-size: 9pt">网神(66707180) 20:22:24<br>
</span></p>
<p style="background: white"><span style="color: black;font-family: 微软雅黑;font-size: 9pt">接着说inference了，inference就是已知一些变量的值，求另一些变量的概率<br>
</span></p>
<p style="background: white"><img src="/images/52nlp.cn/cbe9bf98180a14cb5045dca1794dc84d.jpg" alt=""><span style="color: black;font-family: 微软雅黑;font-size: 9pt"><br>
比如上图，已知y，求x的概率<br>
这个简单的图可以用典型的贝叶斯法则来求<br>
<img src="/images/52nlp.cn/7e71122e14cfaa88afdd58296b070111.jpg" alt=""><br>
对于复杂点的情况，比如链式图：<br>
<img src="/images/52nlp.cn/3635e536873f2f8fc72de50279ebdd3d.jpg" alt=""><br>
为了求p(xn)，就是求xn的边界概率。这里都假设x的值是离散的。如果是连续的，就是积分，为了求这个边界概率，做这个累加动作，如果x的取值是k个，则要做k的(N-1)次方次计算，利用图结构，可以简化计算，这个简化方法就不讲了。<br>
下面讲通用的图inference的方法，就是factor graph方法，链式图或树形图，都比较好求边界概率。<br>
所以factor graph就是把复杂的图转换成树形图，针对树形图来求.先说一下什么是树形图，树形图有两种情况：<br>
1. 每个节点只有一个父节点。<br>
2. 如果有的节点有多个父节点，必须图上每个节点之间只有一条路径。<br>
这是树形图的三种情况：<br>
</span></p>
<p><span style="font-family: 宋体;font-size: 12pt"><br>
<img src="/images/52nlp.cn/8eb9c9545ad3c13c7fad9613b7a21b60.jpg" alt=""><br>
</span></p>
<p style="background: white"><span style="color: black;font-family: 微软雅黑;font-size: 9pt">对于无向图，只要无环，都可以看做树形图；对于有向图，必须每两个节点之间只有一条路径；中间那种是典型的树.右边那种多个有多个节点的叫polytree。<br>
这种书结构的图，都比较好求边界概率. 具体怎么求，就不说了。<br>
这里主要说怎么把复杂的图转换成树形图.，这种转换引入factor节点，从而将普通的图转换成factor图.<br>
先看个例子：<br>
<img src="/images/52nlp.cn/780cb05565153f42f5c16af41ffb2745.jpg" alt=""><br>
对于这个有向图，p(x1,x2,x3)=p(x1)p(x2|x1)p(x3|x1,x2)，等号右边的三个概率作为三个factor<br>
每个factor作为一个节点，加入新的factor图中：<br>
<img src="/images/52nlp.cn/b73761b2e92ab0e5ee900c85c690fced.jpg" alt=""><br>
上面的x1,x2,x3是原图的随机变量 ，下面的fa,fb,fc,fd是factor节点，只有随即变量节点和factor节点之间有连接，每类节点自身互不连接，每个factor连接的变量节点，是相互依赖的节点。<br>
kxkr&lt;lxfkxkr@126.com&gt; 20:42:10<br>
就是一个clique中的节点吧<br>
网神(66707180) 20:42:19<br>
对，严格的说，也不是。<br>
kxkr&lt;lxfkxkr@126.com&gt; 20:44:15<br>
对于有向图 ？<br>
</span></p>
<p style="background: white"><span style="color: black;font-family: 微软雅黑;font-size: 9pt">网神(66707180) 20:45:10<br>
x1,x2,x3是一个最大团. 可以只用一个factor节点，如中间那个图<br>
kxkr&lt;lxfkxkr@126.com&gt; 20:45:17<br>
嗯<br>
网神(66707180) 20:45:22<br>
也可以用多个factor节点，如右边图，但什么情况下用一个factor什么情况用多个factor，我没想明白<br>
kxkr&lt;lxfkxkr@126.com&gt; 20:47:10<br>
嗯，继续<br>
网神(66707180) 20:47:21<br>
转换成factor图后，就是树形图了，符合前面树形图的两种情况，这时候，要求一个节点或一组节点的边界概率，用一种叫做sum-product的方法，已求一个节点的边界概率为例.<br>
η&lt;liyitan2144@163.com&gt; 20:49:50<br>
<img src="/images/52nlp.cn/bce8e43c96148fe947b45673d132458d.jpg" alt="">虽然也对具体的应用情景不清楚，但是一个factor能表达的信息比使用多个factor能表达的信息多.<br>
网神(66707180) 20:51:06<br>
单个的factor表达的信息多，而且节点越少，计算越简单，所以是不是尽量用少的factor？max-product求单个节点的边界概率，其思想是以该节点为root。<br>
η&lt;liyitan2144@163.com&gt; 20:52:38<br>
但是，从设计模型的角度看，节点少了，一个节点设计的复杂程度就大了，不一定容易设计，拆解成多个factor，每个factor都很简单，设计方便。<br>
kxkr&lt;lxfkxkr@126.com&gt; 20:54:37<br>
文中貌似倾向于一个单个的factor<br>
<img src="/images/52nlp.cn/ab7496f79bbd3a88c5efdc07bfc7f41e.jpg" alt="">η&lt;liyitan2144@163.com&gt; 20:56:57<br>
这貌似是在表达一个通用的方法，和是把大的clique的factor拆解成小的多个factor没有关系。<br>
kxkr&lt;lxfkxkr@126.com&gt; 20:57:53<br>
拆成多个factor是不是会造成参数变多，不容易求呢，继续吧，这个有待实践。η&lt;liyitan2144@163.com&gt; 21:00:00<br>
参数应该会变多吧，不过模型其实简单了，确实有待实践，我觉得这和具体应用有关，比如在一副图像里面，如果仅仅表达”像素”的相似度，我们完全可以使用小factor。<br>
网神(66707180) 21:00:39<br>
我继续，prml这章图模型只讲了基础的概念，没讲常用的图模型实例，HMM和CRF这两大主流图模型方法还没讲，感觉不够直观.<br>
</span></p>
<p style="background: white"><span style="color: black;font-family: 微软雅黑;font-size: 9pt">我继续说inference. 转换成factor图后，求节点xi的边缘概率。把xi作为root节点,把求root边界概率理解成一个信息(message)传递的过程，从叶子节点传递概率信息到root节点.传递的规则是：<br>
从叶子节点开始，如果叶子是变量节点，发送1给父节点,如果叶子是factor节点，发送f(x)给父节点.<br>
对于非叶子节点，如果是变量节点，将其收到的message相乘，发给父节点，如果是factor节点，将其收到的message和自身f(x)相乘，然后做一个sum，发给父节点。<br>
举个例子：<br>
<img src="/images/52nlp.cn/6abcc733a0eef13480d910dcbbf65917.jpg" alt=""><br>
这个图中求x3的边缘概率，message传递的过程是:<br>
<img src="/images/52nlp.cn/373711657bf8fc457039064b2160f500.jpg" alt=""><br>
<img src="/images/52nlp.cn/c3a6215edc71bf7e8f686093c75d3eb4.jpg" alt="">中的<img src="/images/52nlp.cn/76860163e1ac4afd0716cb30dc461b10.jpg" alt="">表示从节点x1传递到fa，最后p(x3)是等于<img src="/images/52nlp.cn/6c1d66db3d4502367458e64206cbbeca.jpg" alt="">，因为只有一个fb节点向其传入信息，如果要求x2的边界概率,因为x2有三个节点出入信息,分别是：<br>
</span></p>
<p style="background: white"><img src="/images/52nlp.cn/5251644e28bcc5c37856419b42209fe7.jpg" alt=""><img src="/images/52nlp.cn/3763991ca8d8df08824cbe3afa03a122.jpg" alt=""><img src="/images/52nlp.cn/fb058e2e7185d55c95cdfd5caab7bccc.jpg" alt=""><span style="font-family: 微软雅黑;font-size: 9pt"><span style="color: black"><br>
所以p(x2)就等于这三个信息的乘积<br>
<img src="/images/52nlp.cn/1123302e409577b601ed6d165ec7ffa9.jpg" alt=""></span><span style="color: red"><br>
</span><span style="color: black">这个结果与边界分布的定义是相符的，x2的边界定义如下：<br>
<img src="/images/52nlp.cn/a10834c216091c29e028a5dcefbcfb45.jpg" alt=""><img src="/images/52nlp.cn/eaae67fffb2d9a5038ead38c5fdbbc0b.jpg" alt=""><br>
<img src="/images/52nlp.cn/09d6d75acf7191132e8fc99768b0cc1e.jpg" alt="">通过这个，可以推导出上面的边界分布. 推导如下：<br>
<img src="/images/52nlp.cn/4dde39f1d7972ae3eede98f20eba946d.jpg" alt=""><br>
</span></span></p>
<p style="background: white"><span style="color: black;font-family: 微软雅黑;font-size: 9pt">上面就是用sum-product来求边缘分布的方法。<br>
不知道讲的是否明白，不明白就看书吧，一起研究<img src="/images/52nlp.cn/6998144da89ee64b53db7ed2ef38f45a.jpg" alt="">今天就讲到这了，大家有啥问题讨论下。<br>
huajh7(284696304) 21:34:52<br>
补充几点，一是 temporal model ,如Dynamical Bayesian newtwork(DBN), plate models ，这是图模型的表达能力; 二是belief Bropagation ，包括exact 和approximation ，loopy时的收敛性; Inference包括MCMC,变分法。这是图模型在tree和graph的推理能力。三是Structure Learning ，BIC score等。这是图模型的学习能力。</span></p>
<p>注：PRML读书会系列文章由 <a href="http://weibo.com/dmalgorithms">@Nietzsche_复杂网络机器学习</a> 同学授权发布，转载请注明原作者和相关的主讲人，谢谢。</p>
<p>PRML读书会讲稿PDF版本以及更多资源下载地址：<a href="http://vdisk.weibo.com/u/1841149974">http://vdisk.weibo.com/u/1841149974</a></p>
<p>本文链接地址：<a href="http://www.52nlp.cn/prml%E8%AF%BB%E4%B9%A6%E4%BC%9A%E7%AC%AC%E5%85%AB%E7%AB%A0-graphical-models">http://www.52nlp.cn/prml读书会第八章-graphical-models</a></p>

											
{% endraw %}
