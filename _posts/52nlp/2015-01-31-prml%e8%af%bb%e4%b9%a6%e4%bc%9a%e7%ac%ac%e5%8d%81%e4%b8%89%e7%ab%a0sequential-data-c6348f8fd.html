---
layout: post
title: 'PRML读书会第十三章 Sequential Data'
time: 2015-01-31
site_name: 52nlp.cn
source_url: http://www.52nlp.cn/prml%e8%af%bb%e4%b9%a6%e4%bc%9a%e7%ac%ac%e5%8d%81%e4%b8%89%e7%ab%a0sequential-data
images:
  b2d833e8575fcbb5b4f1ec58406c77b6: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0529_PRMLSeq1.png
  2e5c659d66f02ae49b31b82411631439: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0529_PRMLSeq2.png
  bd5dfce990fa9c0635b437cc6fdea541: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0529_PRMLSeq3.png
  bfafcf38a00ed52d7690969f5d4c040a: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0529_PRMLSeq4.png
  724c6c884c2aa14cbb92c0b680bcbc82: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0529_PRMLSeq5.png
  2e57507f0cba5e24f26a8f49110e4e29: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0529_PRMLSeq6.png
  cad416f3063209bb94d0537f9da27f2b: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0529_PRMLSeq7.png
  bd3a3423f0fadb3cf704b61875848e91: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0529_PRMLSeq8.png
  736a2e2a12b64dc79340bae7a36e93b2: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0529_PRMLSeq9.png
  edc13aa9704330a6b35cee1c45f8dfac: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0529_PRMLSeq10.png
  49a48650a8c699e258264e3e1fc9f41f: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0529_PRMLSeq11.png
  2cc983dfd30f3a84fd820ef3cc01e2ac: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0529_PRMLSeq12.png
  655a0b47e54ec5ba75b34946b2a52566: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0529_PRMLSeq13.png
  2ffb982cf3a17d0f6bb3a0c45d7eadbe: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0529_PRMLSeq14.png
  fa5dff7cd3c680b2ba3955977f2eb213: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0529_PRMLSeq15.png
  30e09c02fac5b09c9474893e0e923f4f: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0529_PRMLSeq16.png
  25d0a9dd4c0635c6729c565a2c1ede6a: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0529_PRMLSeq17.png
  2e3bcd1fff2e331832a6a8ff2025cdb9: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0529_PRMLSeq18.png
  d70bf206853be75c5a78a7ecfd950521: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0529_PRMLSeq19.png
  abc79c3994b7f946fc5cdd08971202eb: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0529_PRMLSeq20.png
  0688d4a65e4c1980384f99fb062fd6da: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0529_PRMLSeq21.png
  98b7d691808cb81f5d285f813cf737f9: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0529_PRMLSeq22.png
  861783585412686f6dd662e1623f6fd2: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0529_PRMLSeq23.png
  2d4f6a26ff07fbf58b31f1030a0a93f7: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0529_PRMLSeq24.png
  2f6b06a9c1f4c46ffc7927fc97bed61f: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0529_PRMLSeq25.png
  a6665b323030957ae42dd9831458c5a6: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0529_PRMLSeq26.png
  745f4de9d42f2d7a7614952d11d0abe3: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0529_PRMLSeq27.png
  87c6ad87d323a62678deb0dcde410129: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0529_PRMLSeq28.png
  48480bd6d6f051edcadeac0e151ee3fd: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0529_PRMLSeq29.png
  c9e0ebb93282120bdfe5c3621cbfabf7: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0529_PRMLSeq30.png
  72582463aa548d1997b0c374ff19b0ca: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0529_PRMLSeq31.png
  f5cf1a95b03270463b50d95197da6576: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0529_PRMLSeq32.png
  d45a4ae8080f65a83944c0916cfcc763: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0529_PRMLSeq33.png
  928a4ff5354d175201f0199c8d1a8ad6: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0529_PRMLSeq34.png
  c822f0a47b1d789a0ad6f9c91d85681a: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0529_PRMLSeq35.png
  c246542550e79656ad3e3126a2b0ae73: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0529_PRMLSeq36.png
  38658037963f1aa9adcef132498ba581: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0529_PRMLSeq37.png
  d57ab1a808c48c51dd22321d8dcfd55e: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0529_PRMLSeq38.png
  58f44a00f48d59b728995fda22b9fcd6: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0529_PRMLSeq39.png
  c75f6ae6ac230447236788b2309bd3f5: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0529_PRMLSeq40.png
  6a4a993e1090f69f4e3ceed436971b2a: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0529_PRMLSeq41.png
  4de686ea532dc33aa70d2783f9978db8: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0529_PRMLSeq42.png
  f6a0e3e535c29cf0b07f88793d188a62: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0529_PRMLSeq43.png
  8b9c67e0d48e355537f650bd7a80ec11: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0529_PRMLSeq44.png
  6cff3d49ac9484aeb5eeef2bd4be2e67: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0529_PRMLSeq45.png
  eca6ce061f900bfec03e84261f76aaa7: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0529_PRMLSeq46.png
  cf9adba69ca23e34bf5cac068fa17a22: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0529_PRMLSeq47.png
  06e01ca9d2702eeb138a498396f80666: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0529_PRMLSeq48.png
  de3fdc919b228a823017f3f142235fe1: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0529_PRMLSeq49.png
  e5271c8f4eab4d4457c02faad2afe4ef: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0529_PRMLSeq50.png
  2e47bc456f485948b0ad7a1ea2944c80: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0529_PRMLSeq51.png
  c13ee5623f49d3491b6f040561f82f15: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0529_PRMLSeq52.png
---
{% raw %}

						<p style="text-align: center"><span style="font-family: 微软雅黑;font-size: 15pt"><strong>PRML读书会第十三章 Sequential Data<br>
</strong></span></p>
<p style="text-align: center"><span style="font-family: 微软雅黑;font-size: 15pt"><strong>主讲人 张巍<br>
</strong></span></p>
<p style="text-align: center"><span style="font-family: 微软雅黑;font-size: 12pt"><strong>（新浪微博: <a href="http://weibo.com/zh3f">@张巍_ISCAS</a>）<br>
</strong></span></p>
<p><span style="font-family: 微软雅黑;font-size: 12pt">软件所-张巍&lt;zh3f@qq.com&gt; 19:01:27<br>
<img src="/images/52nlp.cn/b2d833e8575fcbb5b4f1ec58406c77b6.jpg" alt="">我们开始吧，十三章是关于序列数据，现实中很多数据是有前后关系的，例如语音或者DNA序列，例子就不多举了，对于这类数据我们很自然会想到用马尔科夫链来建模：<br>
<img src="/images/52nlp.cn/2e5c659d66f02ae49b31b82411631439.jpg" alt=""><br>
</span></p>
<p><span style="font-family: 微软雅黑;font-size: 12pt">例如直接假设观测数据之间服从一阶马尔科夫链，这个假设显然太简单了，因为很多数据时明显有高阶相关性的，一个解决方法是用高阶马尔科夫链建模：<br>
<img src="/images/52nlp.cn/bd5dfce990fa9c0635b437cc6fdea541.jpg" alt=""><br>
</span></p>
<p><span style="font-family: 微软雅黑;font-size: 12pt">但这样并不能完全解决问题 ：1、高阶马尔科夫模型参数太多；2、数据间的相关性仍然受阶数限制。一个好的解决方法，是引入一层隐变量，建立如下的模型：<br>
<img src="/images/52nlp.cn/bfafcf38a00ed52d7690969f5d4c040a.jpg" alt=""><br>
</span><span id="more-8012"></span></p>
<p><span style="font-family: 微软雅黑;font-size: 12pt">这里我们假设隐变量之间服从一阶马尔科夫链，观测变量由其对应的隐变量生成。从上图可以看出，隐变量是一阶的，但是观测变量之间是全相关的，今天我们主要讨论的就是上图中的模型。如果隐变量是离散的，我们称之为Hidden Markov Models；如果是连续的，我们称之为: Linear Dynamical Systems。现在我们先来看一下HMM ，从图中可以看出，要完成建模，我们需要指定一下几个分布：<br>
1、转移概率：<br>
<img src="/images/52nlp.cn/724c6c884c2aa14cbb92c0b680bcbc82.jpg" alt=""><br>
2、马尔科夫链的初始概率：<br>
<img src="/images/52nlp.cn/2e57507f0cba5e24f26a8f49110e4e29.jpg" alt=""><br>
3、生成观测变量的概率(emission probabilities)：<br>
<img src="/images/52nlp.cn/cad416f3063209bb94d0537f9da27f2b.jpg" alt=""><br>
对于HMM， 这里1和2我们已经假设成了离散分布，由隐变量Zn生成观测数据可以用混合高斯模型或者神经网络，书上的Zn是一个k维的布尔变量，由此再看隐变量转移概率公式、观测数据的生成公式就容易理解了。模型建好了，我们接下来主要讨论下面三个问题：<br>
1、学习问题：就是学习模型中的参数；<br>
2、预测问题：即<img src="/images/52nlp.cn/bd3a3423f0fadb3cf704b61875848e91.jpg" alt="">,给定当前序列预测下一个观测变量；<br>
3、解码问题：即p(Z|X)，给定观测变量求隐变量，例如语音识别；<br>
</span></p>
<p><span style="font-family: 微软雅黑;font-size: 12pt">游侠(419504839) 19:24:21<br>
什么是解码问题？<br>
软件所-张巍&lt;zh3f@qq.com&gt; 19:25:18<br>
例如观测到了一段语音，要求识别其对应的句子。@游侠 我前面没怎么举例子，不知道这样说清楚没？<br>
游侠(419504839) 19:27:20<br>
这个和一般说的”推断”一样不<br>
软件所-张巍&lt;zh3f@qq.com&gt; 19:28:27<br>
这个也可以叫推断，只是推断是个比较一般的词汇。<br>
我们来看一下HMM有多少参数要学，对应于刚才说到的三个分布，我们也有三组参数要学。<br>
球猫(250992259) 19:30:46<br>
其实就是假设东西是一个马尔科夫模型生成的。。然后把参数用某种方法弄出来，最后根据模型的输出来给答案……是这样吧？<br>
</span></p>
<p><span style="font-family: 微软雅黑;font-size: 12pt">软件所-张巍&lt;zh3f@qq.com&gt; 19:32:45<br>
@球猫 对，都是这个思路，先把参数学出来，然后就可以做任何想要的推断了，在这里所谓的解码问题只是大家比较关心。<br>
软件所-张巍&lt;zh3f@qq.com&gt; 19:32:51<br>
好，继续，我们先来看1、学习问题。这里我们用EM算法来学习HMM的参数：<br>
1、是转移概率对应的转移矩阵；<br>
2、初始概率对应的离散分布参数；<br>
3、观测变量对应的分布参数（这里暂不指定）。<br>
用EM我们要做的就是：<br>
E步里根据当前参数估计隐变量的后验：<br>
</span></p>
<p><img src="/images/52nlp.cn/736a2e2a12b64dc79340bae7a36e93b2.jpg" alt=""><span style="font-family: 微软雅黑;font-size: 12pt"><br>
M步里最大化下面的期望：<br>
<img src="/images/52nlp.cn/edc13aa9704330a6b35cee1c45f8dfac.jpg" alt=""><br>
先来看M步，这里相对简单一点，整个模型的全概率展开为：<br>
<img src="/images/52nlp.cn/49a48650a8c699e258264e3e1fc9f41f.jpg" alt=""><br>
把13.10代入13.12,我们会发现计算时需要下面两个式子：<br>
<img src="/images/52nlp.cn/2cc983dfd30f3a84fd820ef3cc01e2ac.jpg" alt="">和<img src="/images/52nlp.cn/655a0b47e54ec5ba75b34946b2a52566.jpg" alt=""><br>
为了方便，我们就定义：<br>
<img src="/images/52nlp.cn/2ffb982cf3a17d0f6bb3a0c45d7eadbe.jpg" alt=""><br>
这样我们在E步就主要求出这两个式子就行了，当然这也就意味着求出了整个后验<img src="/images/52nlp.cn/fa5dff7cd3c680b2ba3955977f2eb213.jpg" alt="">，利用这两个式子，13.12可以化为：<br>
<img src="/images/52nlp.cn/30e09c02fac5b09c9474893e0e923f4f.jpg" alt=""><br>
这个时候就可以用一些通用方法，例如Lagrange来求解了，结果也很简单：<br>
<img src="/images/52nlp.cn/25d0a9dd4c0635c6729c565a2c1ede6a.jpg" alt=""><br>
对于观测变量的分布参数，与其具体分布形式相关，如果是高斯：<img src="/images/52nlp.cn/2e3bcd1fff2e331832a6a8ff2025cdb9.jpg" alt="">，对应的最优解为：<br>
<img src="/images/52nlp.cn/d70bf206853be75c5a78a7ecfd950521.jpg" alt=""></span></p>
<p>如果是离散：<img src="/images/52nlp.cn/abc79c3994b7f946fc5cdd08971202eb.jpg" alt="">对应的最优解为：<br>
<img src="/images/52nlp.cn/0688d4a65e4c1980384f99fb062fd6da.jpg" alt=""><br>
好，M步就这样， 现在来看E步，也是HMM比较核心的地方。刚才我们看到，E步需要求的是：<img src="/images/52nlp.cn/98b7d691808cb81f5d285f813cf737f9.jpg" alt=""><br>
由马尔科夫的性质，我们可以推出：<br>
<img src="/images/52nlp.cn/861783585412686f6dd662e1623f6fd2.jpg" alt=""></p>
<p><span style="font-family: 微软雅黑;font-size: 12pt">其中：<br>
</span></p>
<p><img src="/images/52nlp.cn/2d4f6a26ff07fbf58b31f1030a0a93f7.jpg" alt=""><span style="font-family: 微软雅黑;font-size: 12pt"><br>
接下来我们就建立<img src="/images/52nlp.cn/2f6b06a9c1f4c46ffc7927fc97bed61f.jpg" alt="">和<img src="/images/52nlp.cn/a6665b323030957ae42dd9831458c5a6.jpg" alt="">的递推公式<br>
<img src="/images/52nlp.cn/745f4de9d42f2d7a7614952d11d0abe3.jpg" alt=""><br>
其中：<br>
<img src="/images/52nlp.cn/87c6ad87d323a62678deb0dcde410129.jpg" alt=""><br>
这样我们从<img src="/images/52nlp.cn/48480bd6d6f051edcadeac0e151ee3fd.jpg" alt="">开始，可以递推出所有的<img src="/images/52nlp.cn/c9e0ebb93282120bdfe5c3621cbfabf7.jpg" alt="">，对于<img src="/images/52nlp.cn/72582463aa548d1997b0c374ff19b0ca.jpg" alt="">，也进行类似的推导：<br>
<img src="/images/52nlp.cn/f5cf1a95b03270463b50d95197da6576.jpg" alt=""><br>
</span></p>
<p><img src="/images/52nlp.cn/d45a4ae8080f65a83944c0916cfcc763.jpg" alt=""><span style="font-family: 微软雅黑;font-size: 12pt"><br>
从上式可以看到<img src="/images/52nlp.cn/928a4ff5354d175201f0199c8d1a8ad6.jpg" alt="">是一个逆推过程，所以我们需要初始值<img src="/images/52nlp.cn/c822f0a47b1d789a0ad6f9c91d85681a.jpg" alt="">，定义13.35并没有明确<img src="/images/52nlp.cn/c246542550e79656ad3e3126a2b0ae73.jpg" alt="">的定义：<br>
<img src="/images/52nlp.cn/38658037963f1aa9adcef132498ba581.jpg" alt=""><br>
因为z_N后没有观测数据，不过我们可以从<img src="/images/52nlp.cn/d57ab1a808c48c51dd22321d8dcfd55e.jpg" alt="">，得出:<br>
<img src="/images/52nlp.cn/58f44a00f48d59b728995fda22b9fcd6.jpg" alt=""><br>
这样<img src="/images/52nlp.cn/c75f6ae6ac230447236788b2309bd3f5.jpg" alt="">就等于1，现在我们可以方便的求出所有的<img src="/images/52nlp.cn/6a4a993e1090f69f4e3ceed436971b2a.jpg" alt="">和<img src="/images/52nlp.cn/4de686ea532dc33aa70d2783f9978db8.jpg" alt="">了，利用13.13也就可以求出所有的<img src="/images/52nlp.cn/f6a0e3e535c29cf0b07f88793d188a62.jpg" alt="">。类似的，我们可以求出<img src="/images/52nlp.cn/8b9c67e0d48e355537f650bd7a80ec11.jpg" alt="">：<br>
<img src="/images/52nlp.cn/6cff3d49ac9484aeb5eeef2bd4be2e67.jpg" alt=""><br>
这样在M步里求解所需要的分布就都求出来了，也就可以用EM来学习HMM的参数了，这里式子比较多，大家自己推一下会比较好理解。<br>
</span></p>
<p><span style="font-family: 微软雅黑;font-size: 12pt">第一个学习问题就这样了，接下来是预测问题，预测问题可以直接推导：<br>
<img src="/images/52nlp.cn/eca6ce061f900bfec03e84261f76aaa7.jpg" alt=""><br>
现在就剩最后一个解码问题，也就是argmax_Z{p(Z|X)}，刚才我们在E步已经求出了：<br>
<img src="/images/52nlp.cn/cf9adba69ca23e34bf5cac068fa17a22.jpg" alt=""><br>
但是现在的问题要复杂一点，因为我们要求概率最大的隐变量序列，用13.13可以求出单个隐变量，但是他们连在一起形成的整个序列可能概率很小，这个问题可以归结为一个动态规划：<br>
<img src="/images/52nlp.cn/06e01ca9d2702eeb138a498396f80666.jpg" alt=""><br>
</span></p>
<p><span style="font-family: 微软雅黑;font-size: 12pt">我们把HMM化成如上图的样子，最大化后验等价于最大化全概率，对于上图中的边，我们赋值为：<br>
log（<img src="/images/52nlp.cn/de3fdc919b228a823017f3f142235fe1.jpg" alt="">）<br>
初始节点赋值为：<br>
log(<img src="/images/52nlp.cn/e5271c8f4eab4d4457c02faad2afe4ef.jpg" alt="">*p(x_1|z_1)）<br>
其余节点赋值为：<br>
log(<img src="/images/52nlp.cn/2e47bc456f485948b0ad7a1ea2944c80.jpg" alt="">)<br>
这样任何一个序列Z，其全概率等于exp（Z对应路径上节点和边的值求和），这样，解码问题就转化为一个最长路径问题，用动态规划可以直接求解。大家看这里有没有问题，HMM的主要内容就是这些<img src="/images/52nlp.cn/c13ee5623f49d3491b6f040561f82f15.jpg" alt=""><br>
</span></p>
<p><span style="font-family: 微软雅黑;font-size: 12pt">接下来的Linear Dynamical Systems 其实和HMM大同小异，只是把离散分布换成了高斯，然后就是第二章公式的反复应用，都是细节问题，就不在这里讲了，大家看看有问题我们可以讨论。这一章还是式子主导的，略过了不少式子，大家推的时候有问题我们可以随时讨论。<br>
</span></p>
<p><span style="font-family: 微软雅黑;font-size: 12pt">天涯游(872352128) 21:09:21<br>
我对hmm 的理解，觉得这麻烦的是概率的理解的了，概率分解才是hmm的核心，当然了还有动态规划了。概率分解其实是实验事件的分解，如前向 和后向了，还有就是EM算法了。<br>
</span></p>
<p>注：PRML读书会系列文章由 <a href="http://weibo.com/dmalgorithms">@Nietzsche_复杂网络机器学习</a> 同学授权发布，转载请注明原作者和相关的主讲人，谢谢。</p>
<p>PRML读书会讲稿PDF版本以及更多资源下载地址：<a href="http://vdisk.weibo.com/u/1841149974">http://vdisk.weibo.com/u/1841149974</a></p>
<p>本文链接地址：<a href="prml%E8%AF%BB%E4%B9%A6%E4%BC%9A%E7%AC%AC%E5%8D%81%E4%B8%89%E7%AB%A0sequential-data">http://www.52nlp.cn/prml读书会第十三章sequential-data</a></p>
<p><span style="font-family: 微软雅黑;font-size: 12pt"><br>
</span></p>

											
{% endraw %}
