---
layout: post
title: 'PRML读书会第十一章  Sampling Methods'
time: 2015-01-31
site_name: 52nlp.cn
source_url: http://www.52nlp.cn/prml%e8%af%bb%e4%b9%a6%e4%bc%9a%e7%ac%ac%e5%8d%81%e4%b8%80%e7%ab%a0-sampling-methods
images:
  403848d79299f16681f756c183f294e8: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0440_PRMLSa1.jpg
  40bc5237d7d56eb35313e29f1918be21: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0440_PRMLSa2.jpg
  2befd35915d9d7a1851998c8f0d678f1: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0440_PRMLSa3.jpg
  aab3f5484f4a783a42609d85beaa65da: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0440_PRMLSa4.jpg
  b0d8daa13a9ec6dd954900ecbab087f0: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0440_PRMLSa5.jpg
  5bb6a4be5a345d2c28bce43d999b282a: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0440_PRMLSa6.jpg
  862e08e601f43c7d6dc5e98249b1d214: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0440_PRMLSa7.jpg
  e2d7356be52b9da625381a8771583b23: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0440_PRMLSa8.jpg
  1127c38c07bef5ee109bb58cb1afbc96: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0440_PRMLSa9.jpg
  19827aa245ca32df88df427f17ef2a8a: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0440_PRMLSa10.jpg
  5335f7eddc1bfc94459525f21a64cccd: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0440_PRMLSa11.jpg
  bebb478dcdf6559527166d7d2eeae9a7: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0440_PRMLSa12.jpg
  be6f1a0e3a7113b99b62a610cae45fc8: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0440_PRMLSa13.jpg
  2694fdfd3436300d514911d13a803268: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0440_PRMLSa14.jpg
  1d6a27c28ab04731de7c087ee837c6f4: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0440_PRMLSa15.jpg
  d78b12f5959dfd4faa56b688d0355c16: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0440_PRMLSa16.jpg
  13ef8b65acbb6c5782d1a38d05ba22c8: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0440_PRMLSa17.jpg
  3e73a3f73a4ae00c4525c56c919ea969: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0440_PRMLSa18.jpg
  c5c385f4dadcde1a5c514aac42d46626: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0440_PRMLSa19.jpg
  745780e669e082ab95e5743434069663: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0440_PRMLSa20.jpg
  46700a50239af108971dd994f4bc8ce7: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0440_PRMLSa21.jpg
  a542e621f3bf2e87bfcb62f8a649867e: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0440_PRMLSa22.jpg
  fbb5026f786fdd4a8f69ca775ae7cb31: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0440_PRMLSa23.jpg
  123d877f6137540dea4f50c8e35c4ef3: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0440_PRMLSa24.jpg
  1dbeae49b015e112fa68aaa8394dca06: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0440_PRMLSa25.jpg
  92f62a05c5a9d50f29284ea484573598: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0440_PRMLSa26.jpg
  e8a50f727e2d016e0ec9241fbdb6f15f: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0440_PRMLSa27.jpg
  1bd403dbc35da9c2fb9fc0eeedeb503f: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0440_PRMLSa28.jpg
  d61e7cec7d48d02a32bd1649a06bb723: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0440_PRMLSa29.jpg
  afb3bb9efe55fc209e8d6c2bd3bb67d7: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0440_PRMLSa30.jpg
  39d35252c7d5101688c653dc77a5bc6b: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0440_PRMLSa31.jpg
  fd85725b2c7551c38fae8b4d049590d8: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0440_PRMLSa32.jpg
  6fa7d4ba80f631b03b0ba44af688a1b4: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0440_PRMLSa33.jpg
  43f041743feebf9f9fb85fbda0f7f055: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0440_PRMLSa34.jpg
  5cbe9b3a26b7f54360235cc295972e53: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0440_PRMLSa35.jpg
  6866031857945261a263ec440bcda57b: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0440_PRMLSa36.jpg
  f2dffc8b35ae69f3ec91ff391feebe57: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0440_PRMLSa37.jpg
  b41a8dcffb6a465a9daddaf99f1e9915: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0440_PRMLSa38.jpg
  fea441012b2279b7a7add118c00680ce: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0440_PRMLSa39.jpg
  fa1a0f9f6a42445b34ff92d298222341: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0440_PRMLSa40.jpg
  e76e926fcdd5a41e89e1b5e5717eebba: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0440_PRMLSa41.jpg
  eb3c00a7a887dd30d35a1c89136c74b1: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0440_PRMLSa42.jpg
  f9126f7a0e11d7840df4fe185614473e: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0440_PRMLSa43.jpg
  6d2aa7a1adb367e291d45b5bc73f48ad: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0440_PRMLSa44.jpg
  f360c1132f5201b2bdf507123c2569ae: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0440_PRMLSa45.jpg
  2b92099fb5b39c580dead8c51756c99e: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0440_PRMLSa46.jpg
  814297d12db74fb385879441cbe84e94: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0440_PRMLSa47.jpg
  d137edac3fa62e5cc47b709b150f8977: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0440_PRMLSa48.jpg
  73826eb9c507562224a31264b60af43b: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0440_PRMLSa49.jpg
  7253cc1668ee4f62803c9cde7ce41132: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0440_PRMLSa50.jpg
  55a81f9a2ddd8ff7532cf8da0ef0970e: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0440_PRMLSa51.jpg
---
{% raw %}

						<p style="text-align: center"><span style="font-family: 微软雅黑;font-size: 15pt"><strong>PRML读书会第十一章 Sampling Methods<br>
</strong></span></p>
<p style="text-align: center"><span style="font-family: 微软雅黑;font-size: 15pt"><strong>主讲人 网络上的尼采<br>
</strong></span></p>
<p style="text-align: center"><span style="font-family: 微软雅黑;font-size: 12pt"><strong>（新浪微博: <a href="http://weibo.com/dmalgorithms">@Nietzsche_复杂网络机器学习</a>）<br>
</strong></span></p>
<p><span style="font-family: 微软雅黑;font-size: 12pt">网络上的尼采(813394698) 9:05:00<br>
今天的主要内容：Markov Chain Monte Carlo，Metropolis-Hastings，Gibbs Sampling，Slice Sampling，Hybrid Monte Carlo。<br>
</span></p>
<p><span style="font-family: 微软雅黑;font-size: 12pt">上一章讲到的平均场是统计物理学中常用的一种思想，将无法处理的复杂多体问题分解成可以处理的单体问题来近似，变分推断便是在平均场的假设约束下求泛函L(Q)极值的最优化问题，好处在于求解过程中可以推出精致的解析解。变分是从最优化的角度通过坐标上升法收敛到局部最优，这一章我们将通过计算从动力学角度见证Markov Chain Monte Carlo收敛到平稳分布。<br>
</span></p>
<p><span style="font-size: 12pt"><span style="font-family: 微软雅黑">先说sampling的原因，因为统计学中经常会遇到对复杂的分布做加和与积分，这往往是intractable的。MCMC方法出现后贝叶斯方法才得以发展，因为在那之前对不可观测变量（包括隐变量和参数）后验分布积分非常困难，对于这个问题上一章变分用的解决办法是通过最优化方法寻找一个和不可观测变量后验分布p(Z|X)近似的分布，这一章我们看下sampling的解决方法，举个简单的例子：比如我们遇到这种形式<img src="/images/52nlp.cn/403848d79299f16681f756c183f294e8.jpg" alt="">，z是个连续随机变量，p(z)是它的分布，我们求f(z)的期望。如果我们从p(z)中sampling一个数据集z<sup>(l)</sup>，然后再求个平均<img src="/images/52nlp.cn/40bc5237d7d56eb35313e29f1918be21.jpg" alt="">来近似f(z)的期望</span><span style="font-family: 宋体">，</span><span style="font-family: 微软雅黑">so,问题就解决了，关键是如何从p(z)中做无偏的sampling。<br>
为了说明sampling的作用，我们先举个EM的例子，最大似然计算中求分布的积分问题，我们在第九章提到了，完整数据的log似然函数是对隐变量Z的积分：<br>
</span></span><span id="more-7819"></span></p>
<p><img src="/images/52nlp.cn/2befd35915d9d7a1851998c8f0d678f1.jpg" alt=""><span style="font-family: 宋体;font-size: 12pt"><br>
</span></p>
<p><span style="font-family: 微软雅黑;font-size: 12pt">如果Z是比较复杂的分布，我们就需要对Z进行采样，从而得到：<br>
</span></p>
<p><img src="/images/52nlp.cn/aab3f5484f4a783a42609d85beaa65da.jpg" alt=""><span style="font-family: 宋体;font-size: 12pt"><br>
</span></p>
<p><span style="font-size: 12pt"><span style="font-family: 微软雅黑">具体就是从Z的后验分布<img src="/images/52nlp.cn/b0d8daa13a9ec6dd954900ecbab087f0.jpg" alt="">中进行采样。</span><span style="font-family: 宋体"><br>
</span></span></p>
<p><span style="font-size: 12pt"><span style="font-family: 微软雅黑">如果我们从贝叶斯的观点，把EM参数theta也当成一个分布的话，有下面一个IP算法:<br>
<img src="/images/52nlp.cn/5bb6a4be5a345d2c28bce43d999b282a.jpg" alt=""><br>
I步，我们无法直接对P(Z|X)取样，我们可以先对P(theta|X)取样<img src="/images/52nlp.cn/862e08e601f43c7d6dc5e98249b1d214.jpg" alt="">，然后再对Z的后验分布进行取样：<img src="/images/52nlp.cn/e2d7356be52b9da625381a8771583b23.jpg" alt=""></span><span style="font-family: 宋体"><br>
</span></span></p>
<p><span style="font-size: 12pt"><span style="font-family: 微软雅黑">P步，利用上一步对P(Z|X)的取样，来确定新的参数分布：<br>
<img src="/images/52nlp.cn/1127c38c07bef5ee109bb58cb1afbc96.jpg" alt=""><br>
然后按这个I步和P步的方式迭代。</span><span style="font-family: 宋体"><br>
</span></span></p>
<p><span style="font-family: 微软雅黑;font-size: 12pt">接下来我们讲sampling methods，为了节省时间，两个基本的rejection sampling和Importance sampling不讲了，这两种方法在高维下都会失效，我们直奔主题MCMC（Markov Chain Monte Carlo ）。蒙特卡洛，是个地中海海滨城市，气候宜人，欧洲富人们的聚集地，更重要的是它是世界三大赌城之一，用这个命名就知道这种方法是基于随机的，过会我们会讲到。马尔科夫大神就不用多说了，他当时用自己的名字命名马尔科夫链是预见到这个模型巨大作用。他还有一个师弟叫李雅普洛夫，控制论里面的李雅普洛夫函数说的就是这位，他们的老师叫契比雪夫，都是圣彼得堡学派的。俄国数学家对人类的贡献是无价的orz<br>
最早的MCMC方法是美国科学家在研制原子弹时算积分发明的。我们先介绍一个最基本的Metropolis方法，这种方法的接受率是：<br>
</span></p>
<p><img src="/images/52nlp.cn/19827aa245ca32df88df427f17ef2a8a.jpg" alt=""><span style="font-family: 宋体;font-size: 12pt"><br>
</span></p>
<p><span style="font-size: 12pt"><span style="font-family: 微软雅黑">但有个要求，就是proposal distribution满足<img src="/images/52nlp.cn/5335f7eddc1bfc94459525f21a64cccd.jpg" alt=""></span><span style="font-family: 宋体">。</span><span style="font-family: 微软雅黑">过程很简单，我们先找个比较容易采样的分布即proposal分布，然后从这个分布中取一个样本Z*，如果<img src="/images/52nlp.cn/bebb478dcdf6559527166d7d2eeae9a7.jpg" alt="">大于1直接接受，如果小于1就接着算出接受率，并且从（0，1）之间取一个随机数和这个接受率做比较来决定是否接受这个样本。过会会在Metropolis-Hastings algorithm方法中具体说。下图是一个简单的例子，对高斯分布做采样，绿线是表示接受的步骤，红线表示拒绝的：</span><span style="font-family: 宋体"><br>
</span></span></p>
<p><img src="/images/52nlp.cn/be6f1a0e3a7113b99b62a610cae45fc8.jpg" alt=""><span style="font-family: 宋体;font-size: 12pt"><br>
</span></p>
<p><span style="font-size: 12pt"><span style="font-family: 微软雅黑">讲Metropolis-Hastings方法前，我们先来回顾下马尔科夫链的性质，这个很重要。markov chains最基本的性质就是无后效性，就是这条链的下一个节点的状态由当前节点状态完全决定：<br>
<img src="/images/52nlp.cn/2694fdfd3436300d514911d13a803268.jpg" alt=""></span><span style="font-family: 宋体"><br>
</span></span></p>
<p><span style="font-size: 12pt"><span style="font-family: 微软雅黑">特定的齐次马尔科夫链可以收敛到平稳分布，也就是经过相当长的一段时间转移，收敛到的分布和初始值无关，转移核起着决定的作用。关于马尔科夫链的收敛，<strong>我们将介绍一个充分条件：detailed balance细致平稳条件。</strong><br>
先介绍两个公式，马尔科夫链节点状态的marginal distribution计算公式，由于无后效性我们可以得到<img src="/images/52nlp.cn/1d6a27c28ab04731de7c087ee837c6f4.jpg" alt=""></span><span style="font-family: 宋体"><br>
</span></span></p>
<p><span style="font-size: 12pt"><span style="font-family: 微软雅黑">上面公式的加和结合马儿科夫链的状态转移矩阵是比较容易理解。</span><span style="font-family: 宋体"><br>
</span></span></p>
<p><span style="font-family: 微软雅黑;font-size: 12pt">平稳分布的定义就是下面的形式：<br>
</span></p>
<p><img src="/images/52nlp.cn/d78b12f5959dfd4faa56b688d0355c16.jpg" alt=""><span style="font-family: 微软雅黑;font-size: 12pt"><br>
</span></p>
<p><span style="font-family: 微软雅黑;font-size: 12pt">其中<img src="/images/52nlp.cn/13ef8b65acbb6c5782d1a38d05ba22c8.jpg" alt="">是z’到z的转移概率，上面的公式不难理解，就是转移后分布不再发生变化。<br>
</span></p>
<p><span style="font-size: 12pt"><span style="font-family: 微软雅黑">下面我们给出细致平稳条件的公式：<img src="/images/52nlp.cn/3e73a3f73a4ae00c4525c56c919ea969.jpg" alt=""></span><span style="font-family: 宋体"><br>
</span></span></p>
<p><span style="font-size: 12pt"><span style="font-family: 微软雅黑">满足细致平稳条件就能收敛到平稳分布，下面是推导：<img src="/images/52nlp.cn/c5c385f4dadcde1a5c514aac42d46626.jpg" alt=""></span><span style="font-family: 宋体"><br>
</span></span></p>
<p><span style="font-family: 微软雅黑;font-size: 12pt">我们把上面细致平稳条件的公式11.40代入公式11.41最左边，从左朝右推导就是平稳分布的公式11.39。细致平稳条件的好处，就是我们能控制马尔科夫链收敛到我们指定的分布<img src="/images/52nlp.cn/745780e669e082ab95e5743434069663.jpg" alt="">。以后的Metropolis-Hastings方法及改进都是基于这个基础的。<br>
前面我们提到，Metropolis方法需要先选一个比较容易取样的proposal distribution，从这个分布里取样，然后通过接受率决定是否采用这个样本。一个简单的例子就是对于proposal distribution我们可以采用Gaussian centred on the current state，其实很好理解，就是上一步节点的值可以做下一步节点需要采样的proposal distribution即高斯分布的均值，这样下一步节点的状态由上一步完全决定，这就是一个马尔科夫链。马尔科夫链有了，我们怎么保证能收敛到目标分布呢？就是前面说的细致平稳条件，我们可以通过设置接受率的形式来满足这个条件。Metropolis-Hastings接受率的形式：<br>
</span></p>
<p><img src="/images/52nlp.cn/46700a50239af108971dd994f4bc8ce7.jpg" alt=""><span style="font-family: 宋体;font-size: 12pt"><br>
</span></p>
<p><img src="/images/52nlp.cn/a542e621f3bf2e87bfcb62f8a649867e.jpg" alt=""><span style="font-size: 12pt"><span style="font-family: 微软雅黑">来自于<img src="/images/52nlp.cn/fbb5026f786fdd4a8f69ca775ae7cb31.jpg" alt="">，分布q便是proposal distribution。<br>
范涛@推荐系统(289765648) 10:49:15<br>
Zp 是什么？</span><span style="font-family: 宋体"><br>
</span></span></p>
<p><span style="font-family: 微软雅黑;font-size: 12pt">网络上的尼采(813394698) 10:52:04<br>
Zp是分布中和z无关的部分。<br>
</span></p>
<p><span style="font-size: 12pt"><span style="font-family: 微软雅黑">为了使各位有个形象的理解，我描述一下过程，我们把<img src="/images/52nlp.cn/123d877f6137540dea4f50c8e35c4ef3.jpg" alt="">当做高斯分布的均值，方差是固定的。然后从这个分布取一个样本就是<img src="/images/52nlp.cn/1dbeae49b015e112fa68aaa8394dca06.jpg" alt="">，如果<img src="/images/52nlp.cn/92f62a05c5a9d50f29284ea484573598.jpg" alt="">大于1肯定接受，如果小于1，我们便从（0，1）之间取一个随机数，和这个接受率做比较，如果接受率大于这个随机数便接受，反之便拒绝。接受率这么设就能满足细致平稳条件的原因，看这个（11.45）公式：<br>
<img src="/images/52nlp.cn/e8a50f727e2d016e0ec9241fbdb6f15f.jpg" alt=""></span><span style="font-family: 宋体"><br>
</span></span></p>
<p><img src="/images/52nlp.cn/1bd403dbc35da9c2fb9fc0eeedeb503f.jpg" alt=""><span style="font-family: 宋体;font-size: 12pt"><br>
</span></p>
<p><span style="font-family: 微软雅黑;font-size: 12pt">我们把接受率公式11.44代入上面的公式的左边，会推出左右两边就是细致平稳条件的形式，红框部分便是细致平稳条件公式11.40的转移核，书上的公式明显错了，上面的这个是勘误过的。<br>
</span></p>
<p><span style="font-family: 微软雅黑;font-size: 12pt">刚才说了proposal distribution一般采用Gaussian centred on the current state，高斯分布的方差是固定的，其实方差就是步长，如何选择步长这是一个state of the art问题，步子太小扩散太慢，步子太大，拒绝率会很高，原地踏步。书中的一个例子，当用Gaussian centred on the current state作proposal distribution时，步长设为目标高斯分布的最小标准差最合适：<br>
</span></p>
<p><img src="/images/52nlp.cn/d61e7cec7d48d02a32bd1649a06bb723.jpg" alt=""><span style="font-family: 微软雅黑;font-size: 12pt"><br>
下面讲Gibbs Sampling，Gibbs Sampling其实是每次只对一个维度的变量进行采样，固定住其他维度的变量，然后迭代，可以看做是Metropolis-Hastings的特例，它的接受率一直是1.<br>
<img src="/images/52nlp.cn/afb3bb9efe55fc209e8d6c2bd3bb67d7.jpg" alt=""><br>
步骤是比较容易理解的，跟上一章的变分法的有相似之处。假设有三个变量的分布<img src="/images/52nlp.cn/39d35252c7d5101688c653dc77a5bc6b.jpg" alt="">，<br>
先固定住z2 z3对z1进行采样，<img src="/images/52nlp.cn/fd85725b2c7551c38fae8b4d049590d8.jpg" alt="">；<br>
然后固定住z1 z3对z2进行采样，<img src="/images/52nlp.cn/6fa7d4ba80f631b03b0ba44af688a1b4.jpg" alt="">；<br>
然后是Z3，<img src="/images/52nlp.cn/43f041743feebf9f9fb85fbda0f7f055.jpg" alt=""><br>
如此迭代。<br>
</span></p>
<p><span style="font-size: 12pt"><span style="font-family: 微软雅黑">根据Metropolis-Hastings，它的接受率恒为1。看下面的推导：<br>
<img src="/images/52nlp.cn/5cbe9b3a26b7f54360235cc295972e53.jpg" alt=""></span><span style="font-family: 宋体"><br>
</span></span></p>
<p><span style="font-size: 12pt"><span style="font-family: 微软雅黑">因为其他维度是固定不变的，所以<img src="/images/52nlp.cn/6866031857945261a263ec440bcda57b.jpg" alt="">，代入上式就都约去了，等于1.<br>
最后对于图模型采用gibbs sampling，条件概率<img src="/images/52nlp.cn/f2dffc8b35ae69f3ec91ff391feebe57.jpg" alt="">可以根据马尔科夫毯获得，下面一个是无向图，一个是有向图，蓝色的节点是和要采样的变量有关的其他变量：</span><span style="font-family: 宋体"><br>
</span></span></p>
<p><img src="/images/52nlp.cn/b41a8dcffb6a465a9daddaf99f1e9915.jpg" alt=""><span style="font-family: 宋体;font-size: 12pt"><br>
</span></p>
<p><span style="font-family: 微软雅黑;font-size: 12pt">关于更多的gibbs sampling的内容可以看MLAPP，里面有blocked gibbs和collapsed gibbs。<br>
</span></p>
<p> </p>
<p><span style="font-family: 微软雅黑;font-size: 12pt">刚才提到Metropolis-Hastings对步长敏感，针对这个问题，下面介绍两个增加辅助变量的方法，这些方法也是满足细致平稳条件的。先介绍slice sampling，这种方法增加了一个变量U，可以根据分布的特征自动调整步长： <img src="/images/52nlp.cn/fea441012b2279b7a7add118c00680ce.jpg" alt=""><br>
</span></p>
<p><span style="font-family: 微软雅黑;font-size: 12pt">步骤很简单：在<img src="/images/52nlp.cn/fa1a0f9f6a42445b34ff92d298222341.jpg" alt="">与<img src="/images/52nlp.cn/e76e926fcdd5a41e89e1b5e5717eebba.jpg" alt="">之间的这段距离随机取个值U，然后通过U画个横线，然后在包含<img src="/images/52nlp.cn/eb3c00a7a887dd30d35a1c89136c74b1.jpg" alt="">并且<img src="/images/52nlp.cn/f9126f7a0e11d7840df4fe185614473e.jpg" alt="">这段横线对z进行随机采样，然后按这种方式迭代。图(b)为了实际中便于操作，有时还需要多出那么一段，因为我们事先不知道目标分布的具体形式，所以包含<img src="/images/52nlp.cn/6d2aa7a1adb367e291d45b5bc73f48ad.jpg" alt="">并且<img src="/images/52nlp.cn/f360c1132f5201b2bdf507123c2569ae.jpg" alt="">这段横线没法确定，只能朝外延伸加单位长度进行试，最后会多出来一段，这一点书上并没有介绍详细。<br>
下面介绍The Hybrid Monte Carlo Algorithm （Hamiltonian MCMC）：哈密顿，神童，经典力学三巨头之一，这个算法引入了哈密顿动力系统的概念，计算接受率时考虑的是系统的总能量。Hybrid Monte Carlo 定义了势能和动能两种能量，它们的和便是系统总能量哈密顿量。先看势能，分布可以写成这种形式：<br>
</span></p>
<p><img src="/images/52nlp.cn/2b92099fb5b39c580dead8c51756c99e.jpg" alt=""><span style="font-family: 微软雅黑;font-size: 12pt"><br>
E(z)便是系统的势能。<br>
另外增加一个变量，状态变量变化的速率：<img src="/images/52nlp.cn/814297d12db74fb385879441cbe84e94.jpg" alt=""><br>
系统的动能便是：<br>
</span></p>
<p><img src="/images/52nlp.cn/d137edac3fa62e5cc47b709b150f8977.jpg" alt=""><span style="font-family: 微软雅黑;font-size: 12pt"><br>
总的能量便是：<img src="/images/52nlp.cn/73826eb9c507562224a31264b60af43b.jpg" alt=""><br>
</span></p>
<p><span style="font-family: 微软雅黑;font-size: 12pt">比如高斯分布的哈密顿量就可表示为：<br>
</span></p>
<p><img src="/images/52nlp.cn/7253cc1668ee4f62803c9cde7ce41132.jpg" alt=""><span style="font-family: 微软雅黑;font-size: 12pt"><br>
</span></p>
<p><span style="font-family: 微软雅黑;font-size: 12pt">下面这个公式便是Hybrid Monte Carlo 的接受率：<br>
</span></p>
<p><img src="/images/52nlp.cn/55a81f9a2ddd8ff7532cf8da0ef0970e.jpg" alt=""><span style="font-family: 微软雅黑;font-size: 12pt"><br>
可以证明 这种接受率是满足detailed balance条件的。<br>
ORC(267270520) 12:14:03<br>
推荐一本相关的书 Introducing Monte Carlo Methods with R (use R) ，PS：R做MCMC很方便。<br>
赞尼采讲的很精彩，学习了，嘿嘿<br>
</span></p>
<p><span style="font-family: 微软雅黑;font-size: 12pt">网络上的尼采(813394698) 12:36:37<br>
Markov Chain Monte Carlo In Practice(Gilks)这本书也挺不错。<br>
红烧鱼(403774317) 12:38:06<br>
这本读研的时候生读过，非常实用，随书附带code<br>
</span></p>
<p><span style="font-family: 微软雅黑;font-size: 12pt">网络上的尼采(813394698)<br>
</span></p>
<p><span style="font-family: 微软雅黑;font-size: 12pt">最后需要补充的是：判断MCMC的burn-in何时收敛是个问题，koller介绍了两种方法，即同一条链上设置不同的时间窗做比较，另一种同时跑多条链然后作比较。当然也有一条链跑到黑的。<br>
</span></p>
<p>注：PRML读书会系列文章由 <a href="http://weibo.com/dmalgorithms">@Nietzsche_复杂网络机器学习</a> 同学授权发布，转载请注明原作者和相关的主讲人，谢谢。</p>
<p>PRML读书会讲稿PDF版本以及更多资源下载地址：<a href="http://vdisk.weibo.com/u/1841149974">http://vdisk.weibo.com/u/1841149974</a></p>
<p>本文链接地址：<a href="http://www.52nlp.cn/prml%E8%AF%BB%E4%B9%A6%E4%BC%9A%E7%AC%AC%E5%8D%81%E4%B8%80%E7%AB%A0-sampling-methods">http://www.52nlp.cn/prml读书会第十一章-sampling-methods</a></p>
<p><span style="font-family: 微软雅黑;font-size: 9pt"><br>
</span></p>

											
{% endraw %}
