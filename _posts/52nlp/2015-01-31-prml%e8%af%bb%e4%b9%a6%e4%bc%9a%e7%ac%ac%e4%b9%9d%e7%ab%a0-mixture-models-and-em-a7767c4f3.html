---
layout: post
title: 'PRML读书会第九章  Mixture Models and EM'
time: 2015-01-31
site_name: 52nlp.cn
source_url: http://www.52nlp.cn/prml%e8%af%bb%e4%b9%a6%e4%bc%9a%e7%ac%ac%e4%b9%9d%e7%ab%a0-mixture-models-and-em
images:
  46837cdb2db9de12cf0e58aa9828e496: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0419_PRMLMix1.jpg
  ec21b28673bf918acfc0ffb1a19f7d6a: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0419_PRMLMix2.jpg
  7598cdf6a2910058717d1e31efb05bda: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0419_PRMLMix3.jpg
  38ad63e87623715f4c20ecd77af98815: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0419_PRMLMix4.jpg
  e678f035703211290bed1a74c9ba371a: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0419_PRMLMix5.jpg
  85e721be3166977e2f1aa987f1cba99b: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0419_PRMLMix6.jpg
  42e654495709a117c1093f0ac66a49e4: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0419_PRMLMix7.jpg
  3377e6ba6f505ea07bb1e3e945251f39: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0419_PRMLMix8.jpg
  f90fba04fd491181c63ee115401aaf4b: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0419_PRMLMix9.jpg
  f555fa502a4ad8c0ba1e6e0a05cda131: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0419_PRMLMix10.jpg
  904a80fea2276aa374479de00127ed8f: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0419_PRMLMix11.jpg
  b105469cae1f0a59218f1218b98c6917: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0419_PRMLMix12.jpg
  ca805a375bd0ed7af66d23a8ccc38f37: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0419_PRMLMix13.jpg
  3e952447b6aff2f048828e89e7640b31: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0419_PRMLMix14.jpg
  393125222a26a161318325c1342c9c23: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0419_PRMLMix15.jpg
  e868fc7713f28af916be0b413a18a9f8: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0419_PRMLMix16.jpg
  8fc48c3f196073406ece346bd8582008: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0419_PRMLMix17.jpg
  b44ebca0a92fda2e432578bcca6a66ff: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0419_PRMLMix18.jpg
  85a9b6d13d21ee510462edaf9d370390: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0419_PRMLMix19.jpg
  c4145dc79b970bc4da0511d536e839ad: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0419_PRMLMix20.jpg
  683a3882cdac58430bfc03a110f6e148: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0419_PRMLMix21.jpg
  029f14c8859e95dd4a6baae4510b42ff: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0419_PRMLMix22.jpg
  3198a4df5965b0bdc03d8b1d2b1b1271: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0419_PRMLMix23.jpg
  5d408efa7376e2982e2573bcc5b9f3d4: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0419_PRMLMix24.jpg
  7bb976ffbb06377117ecb49acfcdc7e7: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0419_PRMLMix25.jpg
  417c395286d53e86de7c5bd8543d7de5: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0419_PRMLMix26.jpg
  369c4c7aaa8e231d1879d3dc681018e1: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0419_PRMLMix27.jpg
  a6983d91f6455e55e54452a870159ab7: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0419_PRMLMix28.jpg
  46a87a121aedde9f18f311a15ca1b0fc: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0419_PRMLMix29.jpg
  f996995179951169639a0a15e5dcef4a: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0419_PRMLMix30.jpg
  9c7307b5bcc431605d934b5dc784949c: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0419_PRMLMix31.jpg
  5e93c35848ad64f7454b2507a7b78faf: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0419_PRMLMix32.jpg
  96fe6ca703ed64539952382065c4e591: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0419_PRMLMix33.jpg
  e361a37822c7dfbffc5e5c5a93cc6062: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0419_PRMLMix34.jpg
  ada06663b0f148ef71d3b8792a8a36a6: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0419_PRMLMix35.jpg
  2db3ba32db5c8cadb786ac0fddfabbd8: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0419_PRMLMix36.jpg
  0ac12718dee9547a9fe6c4e2f1286d14: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0419_PRMLMix37.jpg
  1414bf376650d7991084242f378aca24: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0419_PRMLMix38.jpg
  f244d5d0eaabe1ce163db96988479a59: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0419_PRMLMix39.jpg
  0ab8563bdfb9c7320e15ca59c948db9a: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0419_PRMLMix40.jpg
  2a6f7fa880f92e0af9997c915665ec2e: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0419_PRMLMix41.jpg
  6d84d3f5481ef52a5df3648ff85f18fc: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0419_PRMLMix42.jpg
  c30e96d7892982975ffacadeebc3afcf: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0419_PRMLMix43.jpg
  fa4f4f3b02293115dac180d475e553c1: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0419_PRMLMix44.jpg
  b45384983a6c3f39edeb181aa6c481ba: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0419_PRMLMix45.jpg
  d2a8b72a1a991c24e843619168652dcc: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0419_PRMLMix46.jpg
  9284d99061fd7b10adc0cdb1c73a5912: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0419_PRMLMix47.jpg
  8ab6be979e26dde23bb1f5571d315cb4: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0419_PRMLMix48.jpg
  e58cf86ef6e20f15298e720e16245974: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0419_PRMLMix49.jpg
  ed9fe9cd163a25c7015d7d2c576abf6c: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0419_PRMLMix50.jpg
  50c80aed6038d7de0dd64f67c310a8a6: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0419_PRMLMix51.jpg
  70edc6f0495ceac5a10da1dd7b7382b3: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0419_PRMLMix52.jpg
  067af07d7fb1cbd0dadcc0ce7165b7d6: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0419_PRMLMix53.jpg
  2083ed7c66c75b0a75f86ebced03a246: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0419_PRMLMix54.jpg
  489009f24edc0393ae3e99a5de227554: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0419_PRMLMix55.jpg
  ec93cb053b7fe2ccdd0e154e54c8f46f: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0419_PRMLMix56.jpg
  be83534220e9ff3f7423041e7e437345: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0419_PRMLMix57.jpg
  0772ad51bdbc2259e5de5d05d424c298: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0419_PRMLMix58.jpg
  0d6b36f529709da422275ec9c84ba62d: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0419_PRMLMix59.jpg
  768d61ba74934368420071b414838037: http://www.52nlp.cn/wp-content/uploads/2015/01/013115_0419_PRMLMix60.jpg
---
{% raw %}

						<p style="text-align: center"><span style="font-family: 微软雅黑;font-size: 15pt"><strong>PRML读书会第九章 Mixture Models and EM<br>
</strong></span></p>
<p style="text-align: center"><span style="font-family: 微软雅黑;font-size: 15pt"><strong>主讲人 网络上的尼采<br>
</strong></span></p>
<p style="text-align: center"><span style="font-family: 微软雅黑;font-size: 12pt"><strong>（新浪微博: <a href="http://weibo.com/dmalgorithms">@Nietzsche_复杂网络机器学习</a>）<br>
</strong></span></p>
<p><span style="font-size: 10pt"><span style="font-family: 微软雅黑">网络上的尼采(813394698) 9:10:56<br>
今天的主要内容有k-means、混合高斯模型、 EM算法。<br>
对于k-means大家都不会陌生，非常经典的一个聚类算法，已经50多年了，关于clustering推荐一篇不错的survey:</span><br>
<span style="font-family: 微软雅黑">Data clustering: 50 years beyond K-means。k-means表达的思想非常经典，就是对于复杂问题分解成两步不停的迭代进行逼近，并且每一步相对于前一步都是递减的。<br>
k-means有个目标函数 ：<br>
<img src="/images/52nlp.cn/46837cdb2db9de12cf0e58aa9828e496.jpg" alt=""></span><span style="font-family: 宋体"><br>
</span></span></p>
<p><span style="font-family: 微软雅黑;font-size: 10pt">假设有k个簇，<img src="/images/52nlp.cn/ec21b28673bf918acfc0ffb1a19f7d6a.jpg" alt="">是第k个簇的均值；每个数据点都有一个向量表示属于哪个簇，r<sub>nk</sub>是向量的元素，如果点x<sub>n</sub>属于第k个簇，则r<sub>nk</sub>是1，向量的其他元素是0。<br>
上面这个目标函数就是各个簇的点与簇均值的距离的总和，k-means要做的就是使这个目标函数最小。 这是个NP-hard问题，k-means只能收敛到局部最优。<br>
</span><span id="more-7668"></span></p>
<p><span style="font-size: 10pt"><span style="font-family: 微软雅黑">算法的步骤非常简单：<br>
先随机选k个中心点<br>
第一步也就是E步把离中心点近的数据点划分到这个簇里；<br>
第二步M步根据各个簇里的数据点重新确定均值，也就是中心点。<br>
然后就是迭代第一步和第二步，直到满足收敛条件为止。<br>
自强&lt;ccab4209211@qq.com&gt; 9:29:00<br>
收敛是怎么判断的呀？<br>
网络上的尼采(813394698) 9:30:16<br>
不再发生大的变化。大家思考下不难得出：无论E步还是M步,目标函数都比上一步是减少的。 下面是划分两个簇的过程 ：<br>
<img src="/images/52nlp.cn/7598cdf6a2910058717d1e31efb05bda.jpg" alt=""></span><span style="font-family: 宋体"><br>
</span></span></p>
<p><span style="font-family: 微软雅黑;font-size: 10pt">下面这个图说明聚类过程中目标函数单调递减，经过三轮迭代就收敛了，由于目标函数只减不增，并且有界，所以k-means是可以保证收敛的：<br>
</span></p>
<p><img src="/images/52nlp.cn/38ad63e87623715f4c20ecd77af98815.jpg" alt=""><span style="font-family: 宋体;font-size: 10pt"><br>
</span></p>
<p style="margin-left: 84pt"><span style="font-family: 微软雅黑;font-size: 10pt"> 书里还举例一个k-means对图像分割和压缩的例子：<br>
<img src="/images/52nlp.cn/e678f035703211290bed1a74c9ba371a.jpg" alt=""><br>
</span></p>
<p><span style="font-family: 微软雅黑;font-size: 10pt">图像分割后，每个簇由均值来表示，每个像素只存储它属于哪个簇就行了。压缩后图像的大小是k的函数 ：<br>
<img src="/images/52nlp.cn/85e721be3166977e2f1aa987f1cba99b.jpg" alt=""><br>
</span></p>
<p style="margin-left: 15pt"><span style="font-family: 微软雅黑;font-size: 10pt">现在讨论下k-means的性质和不足 ：<br>
</span></p>
<p style="margin-left: 15pt"><span style="font-family: 微软雅黑;font-size: 10pt">首先对初值敏感 ，由于只能收敛到局部最优，初值很大程度上决定它收敛到哪里；<br>
</span></p>
<p style="margin-left: 15pt"><span style="font-family: 微软雅黑;font-size: 10pt">从算法的过程可以看出，k-means对椭球形状的簇效果最好 ，不能发现任意形状的簇；<br>
</span></p>
<p><span style="font-family: 微软雅黑;font-size: 10pt">对孤立点干扰的鲁棒性差，孤立点是异质的，可以说是均值杀手，k-means又是围绕着均值展开的，试想下，原离簇的孤立点对簇的均值的拉动作用是非常大的。<br>
针对这些问题后来又有了基于密度的DBSCAN算法，最近Science上发了另一篇基于密度的方法：Clustering by fast search and find of density peaks。基于密度的方法无论对clustering还是outliers detection效果都不错，和k-means不同，这些算法已经没有了目标函数，但鲁棒性强，可以发现任意形状的簇。<br>
另外如何自动确定k-means的k是目前研究较多的一个问题。k-means就到这里，现在一块讨论下。<br>
</span></p>
<p><span style="font-family: 微软雅黑;font-size: 10pt">口水猫(465191936) 9:49:20<br>
如果对于这批数据想做k-mean 聚类，那么如果去换算距离？<br>
网络上的尼采(813394698) 9:49:53<br>
k-means一般基于欧式距离，关于距离度量是个专门的方向，点集有了度量才能有拓扑，有专门的度量学习这个方向。<br>
口水猫(465191936) 9:50:08<br>
嗯嗯 有没有一些参考意见了，k的选择可以参考coursera上的视频 选择sse下降最慢的那个拐点的k<br>
</span></p>
<p><span style="font-family: 微软雅黑;font-size: 10pt">网络上的尼采(813394698) 9:51:41<br>
</span></p>
<p><span style="font-size: 10pt"><span style="font-family: 微软雅黑">关于选哪个k是最优的比较主观，有从结果稳定性来考虑的，毕竟一个算法首先要保证的是多次运行后结果相差不大，关于这方面有一篇survey:</span><br>
<span style="font-family: 微软雅黑">Clustering stability an overview。另外还有其他自动选k的方法，DP、MDL什么的。MDL是最短描述长度，从压缩的角度来看k-means；DP是狄利克雷过程，一种贝叶斯无参方法，感兴趣可以看JORDAN小组的文章。<br>
</span></span></p>
<p> </p>
<p><span style="font-family: 微软雅黑;font-size: 10pt">我们下面讲混合高斯模型GMM，第二章我们说过，高斯分布有很多优点并且普遍存在，但是单峰函数，所以对于复杂的分布表达能力差，我们可以用多个高斯分布的线性组合来逼近这些复杂的分布。我们看下GMM的线性组合形式：<br>
</span></p>
<p><img src="/images/52nlp.cn/42e654495709a117c1093f0ac66a49e4.jpg" alt=""><span style="font-family: 宋体;font-size: 10pt"><br>
</span></p>
<p><span style="font-family: 微软雅黑;font-size: 10pt">对于每个数据点是哪个分布生成的，我们假设有个Z隐变量 ，和k-means类似，对于每个数据点都有一个向量z，如果是由第k个分布生成，元素z<sub>k</sub>=1,其他为0。<br>
z<sub>k</sub>=1的概率的先验就是高斯分布前的那个系数：<br>
</span></p>
<p><img src="/images/52nlp.cn/3377e6ba6f505ea07bb1e3e945251f39.jpg" alt=""><span style="font-size: 10pt"><br>
</span></p>
<p><span style="font-family: 微软雅黑;font-size: 10pt">下面是z<sub>k</sub>=1概率的后验，由贝叶斯公式推导的：<br>
</span></p>
<p><img src="/images/52nlp.cn/f90fba04fd491181c63ee115401aaf4b.jpg" alt=""><span style="font-family: 宋体;font-size: 10pt"><br>
</span></p>
<p><span style="font-family: 微软雅黑;font-size: 10pt">其实很好理解，如果没有<img src="/images/52nlp.cn/f555fa502a4ad8c0ba1e6e0a05cda131.jpg" alt="">限制，数据点由哪个分布得出的概率大z<sub>k</sub>=1的期望就大，但前面还有一个系数限制，所以期望形式是：<br>
</span></p>
<p><img src="/images/52nlp.cn/904a80fea2276aa374479de00127ed8f.jpg" alt=""><span style="font-size: 10pt"><span style="font-family: 微软雅黑"><br>
刚才有人问如何确定模型的参数，我们首先想到的就是log最大似然：<br>
<img src="/images/52nlp.cn/b105469cae1f0a59218f1218b98c6917.jpg" alt=""></span><span style="font-family: 宋体"><br>
</span></span></p>
<p><span style="font-size: 10pt"><span style="font-family: 微软雅黑">但是我们可以观察下这个目标函数，log里面有加和，求最优解是非常困难的，混合高斯和单个高斯的参数求法差别很大，如果里面有一个高斯分布坍缩成一个点，log似然函数会趋于无穷大。由于直接求解困难，这也是引入EM的原因。<br>
下面是GMM的图表示 ：<br>
<img src="/images/52nlp.cn/ca805a375bd0ed7af66d23a8ccc38f37.jpg" alt=""></span><span style="font-family: 宋体"><br>
</span></span></p>
<p><span style="font-size: 10pt"><span style="font-family: 微软雅黑">我们可以试想下，如果隐变量z<sub>n</sub>是可以观测的，也就是知道哪个数据点是由哪个分布生成的，那么我们求解就会很方便，可以利用高斯分布直接得到解析解。 但关键的是z<sub>n</sub>是隐变量，我们没法观测到。但是我们可以用它的期望来表示。 现在我们来看一下EM算法在GMM中的应用：<br>
<img src="/images/52nlp.cn/3e952447b6aff2f048828e89e7640b31.jpg" alt=""><br>
上面是EM对GMM的E步<br>
我们首先对模型的参数初始化<br>
E步就是我们利用这些参数得z<sub>nk</sub>的期望，这种形式我们前面已经提到了：<br>
<img src="/images/52nlp.cn/393125222a26a161318325c1342c9c23.jpg" alt=""></span><span style="font-family: 宋体"><br>
</span></span></p>
<p><span style="font-size: 10pt"><span style="font-family: 微软雅黑">现在我们有隐藏变量的期望了，由期望得新的模型参数也就是M步，高斯分布的好处就在这儿，可以推导出新参数的闭式解：<img src="/images/52nlp.cn/e868fc7713f28af916be0b413a18a9f8.jpg" alt=""></span><span style="font-family: 宋体"><br>
</span></span></p>
<p><span style="font-family: 微软雅黑;font-size: 10pt">然后不断迭代E步和M步直到满足收敛条件。<br>
</span></p>
<p><span style="font-family: 微软雅黑;font-size: 10pt">为什么要这么做，其实EM算法对我们前面提到的log最大似然目标函数：<img src="/images/52nlp.cn/8fc48c3f196073406ece346bd8582008.jpg" alt="">是单调递增的。<br>
karnon(447457116) 10:39:42<br>
用EM来解GMM其实是有问题的，解出来的解并不是最优的。。<br>
网络上的尼采(813394698) 10:40:14<br>
嗯，这个问题最后讲。<br>
</span></p>
<p><span style="font-family: 微软雅黑;font-size: 10pt">我们再来看EM更一般的形式：<br>
</span></p>
<p><img src="/images/52nlp.cn/b44ebca0a92fda2e432578bcca6a66ff.jpg" alt=""><span style="font-size: 10pt"><span style="font-family: 微软雅黑">是我们的目标函数，加入隐藏变量可以写成这种形式：<img src="/images/52nlp.cn/85a9b6d13d21ee510462edaf9d370390.jpg" alt=""></span><span style="font-family: 宋体"><br>
</span></span></p>
<p><span style="font-family: 微软雅黑;font-size: 10pt">先初始化模型的参数，由于隐变量无法观测到，我们用参数来得到它的后验<img src="/images/52nlp.cn/c4145dc79b970bc4da0511d536e839ad.jpg" alt=""><br>
</span></p>
<p><span style="font-family: 微软雅黑;font-size: 10pt">然后呢，我们通过隐藏变量的期望得到新的完整数据的最大似然函数：<br>
</span></p>
<p><img src="/images/52nlp.cn/683a3882cdac58430bfc03a110f6e148.jpg" alt=""><span style="font-family: 宋体;font-size: 10pt"><br>
</span></p>
<p><span style="font-size: 10pt"><span style="font-family: 微软雅黑">以上是E步</span><span style="font-family: 宋体">，</span><span style="font-family: 微软雅黑">M步是求这个似然函数的Q函数的最优解，也就是新的参数：<img src="/images/52nlp.cn/029f14c8859e95dd4a6baae4510b42ff.jpg" alt=""></span><span style="font-family: 宋体"><br>
</span></span></p>
<p><span style="font-size: 10pt"><span style="font-family: 微软雅黑">注意这个Q函数是包含隐变量的完整数据的似然函数，不是我们一开始的目标函数<img src="/images/52nlp.cn/3198a4df5965b0bdc03d8b1d2b1b1271.jpg" alt=""></span><span style="font-family: 宋体"><br>
</span></span></p>
<p><span style="font-family: 微软雅黑;font-size: 10pt">其实求完整数据的最大似然是在逼近我们目标函数的局部最优解，这个在后面讲。<br>
下面这个是一般化EM算法的步骤： <img src="/images/52nlp.cn/5d408efa7376e2982e2573bcc5b9f3d4.jpg" alt=""><br>
EM算法只所以用途广泛在于有潜在变量的场合都能用，并不局限于用在GMM上。现在我们回过头来看混合高斯模型的M步，这些得到的新参数就是Q函数的最优解：<br>
</span></p>
<p><img src="/images/52nlp.cn/7bb976ffbb06377117ecb49acfcdc7e7.jpg" alt=""><span style="font-family: 宋体;font-size: 10pt"><br>
</span></p>
<p><span style="font-size: 10pt"><span style="font-family: 微软雅黑">再思考下k-means，其实它是EM的特例，只不过是k-means对数据点的分配是硬性的，在E步每个数据点必须分配到一个簇，z里面只有一个1其他是0,而EM用的是z的期望：<img src="/images/52nlp.cn/417c395286d53e86de7c5bd8543d7de5.jpg" alt=""></span><span style="font-family: 宋体"><br>
</span></span></p>
<p><span style="font-size: 10pt"><span style="font-family: 微软雅黑">下面这个图说明k-means是EM算法特例，这与前面k-means聚类的过程的图对应：</span><span style="font-family: 宋体"><br>
</span></span></p>
<p><img src="/images/52nlp.cn/369c4c7aaa8e231d1879d3dc681018e1.jpg" alt=""><span style="font-family: 微软雅黑;font-size: 10pt"><br>
对于EM算法性质的证明最后讲，下面讲混合伯努利模型，高斯分布是针对连续的属性，伯努利是针对离散属性。混合形式：<br>
</span></p>
<p><img src="/images/52nlp.cn/a6983d91f6455e55e54452a870159ab7.jpg" alt=""><span style="font-family: 宋体;font-size: 10pt"><br>
</span></p>
<p><span style="font-family: 微软雅黑;font-size: 10pt"> 目标函数，log里面同样有加和：<br>
</span></p>
<p><img src="/images/52nlp.cn/46a87a121aedde9f18f311a15ca1b0fc.jpg" alt=""><span style="font-size: 10pt"><span style="font-family: 宋体">，</span><span style="font-family: 微软雅黑">。<br>
z<sub>nk</sub>=1的期望：<br>
<img src="/images/52nlp.cn/f996995179951169639a0a15e5dcef4a.jpg" alt=""></span><br>
</span></p>
<p><span style="font-family: 微软雅黑;font-size: 10pt">Q函数：<br>
</span></p>
<p><img src="/images/52nlp.cn/9c7307b5bcc431605d934b5dc784949c.jpg" alt=""><span style="font-family: 微软雅黑;font-size: 10pt"><br>
以上是E步，下面是M步的解，这两个：<br>
</span></p>
<p><img src="/images/52nlp.cn/5e93c35848ad64f7454b2507a7b78faf.jpg" alt=""><img src="/images/52nlp.cn/96fe6ca703ed64539952382065c4e591.jpg" alt=""><span style="font-family: 微软雅黑;font-size: 10pt"><br>
</span></p>
<p><span style="font-family: 微软雅黑;font-size: 10pt">其中：<br>
</span></p>
<p><img src="/images/52nlp.cn/e361a37822c7dfbffc5e5c5a93cc6062.jpg" alt=""><span style="font-family: 微软雅黑;font-size: 10pt"><br>
</span></p>
<p><span style="font-family: 微软雅黑;font-size: 10pt">书里举了一个手写字聚类的例子 ，先对像素二值化 ，然后聚成3个簇：<br>
<img src="/images/52nlp.cn/ada06663b0f148ef71d3b8792a8a36a6.jpg" alt=""><br>
这是3个簇的代表：<br>
</span></p>
<p><img src="/images/52nlp.cn/2db3ba32db5c8cadb786ac0fddfabbd8.jpg" alt=""><span style="font-family: 微软雅黑;font-size: 10pt"><br>
</span></p>
<p><span style="font-family: 微软雅黑;font-size: 10pt">k=1时簇的代表：<br>
<img src="/images/52nlp.cn/0ac12718dee9547a9fe6c4e2f1286d14.jpg" alt=""><br>
</span></p>
<p><span style="color: #333333;font-family: Arial;font-size: 10pt">=============================================================<br>
</span></p>
<p><span style="font-family: 微软雅黑;font-size: 10pt">最后说下EM算法为什么能收敛到似然函数的局部最优解:<br>
</span></p>
<p><span style="font-family: 微软雅黑;font-size: 10pt">我们的目标函数是分布<img src="/images/52nlp.cn/1414bf376650d7991084242f378aca24.jpg" alt="">的最大log似然<br>
</span></p>
<p><span style="font-size: 10pt"><span style="font-family: 微软雅黑">引入潜在变量，分布表示为：<img src="/images/52nlp.cn/f244d5d0eaabe1ce163db96988479a59.jpg" alt=""></span><span style="font-family: 宋体"><br>
</span></span></p>
<p><span style="font-family: 微软雅黑;font-size: 10pt">定义隐藏变量的分布为q(z)，目标函数可以表达为下面形式，这个地方是神来一笔：<br>
</span></p>
<p><img src="/images/52nlp.cn/0ab8563bdfb9c7320e15ca59c948db9a.jpg" alt=""><span style="font-family: 宋体;font-size: 10pt"><br>
</span></p>
<p><span style="font-size: 10pt"><span style="font-family: 微软雅黑">其中<img src="/images/52nlp.cn/2a6f7fa880f92e0af9997c915665ec2e.jpg" alt="">是<img src="/images/52nlp.cn/6d84d3f5481ef52a5df3648ff85f18fc.jpg" alt="">与q(z)的KL散度；<img src="/images/52nlp.cn/c30e96d7892982975ffacadeebc3afcf.jpg" alt="">是q(z)的泛函形式。<br>
我们原来讲过根据Jensen不等式来证明KL散度是非负的，这个性质在这里发挥了作用。由于<img src="/images/52nlp.cn/fa4f4f3b02293115dac180d475e553c1.jpg" alt="">是大于等于零的，所以<img src="/images/52nlp.cn/b45384983a6c3f39edeb181aa6c481ba.jpg" alt="">是目标函数<img src="/images/52nlp.cn/d2a8b72a1a991c24e843619168652dcc.jpg" alt="">的下界。<img src="/images/52nlp.cn/9284d99061fd7b10adc0cdb1c73a5912.jpg" alt="">与目标函数什么时候相等呢？其实就是<img src="/images/52nlp.cn/8ab6be979e26dde23bb1f5571d315cb4.jpg" alt="">等于0的时候，也就是q(z)与z的后验分布<img src="/images/52nlp.cn/e58cf86ef6e20f15298e720e16245974.jpg" alt="">相同时，这个时候就是E步z取它的期望时候。<img src="/images/52nlp.cn/ed9fe9cd163a25c7015d7d2c576abf6c.jpg" alt="">与目标函数相等是为了取一个比较紧的bound。<br>
M步就是最大化<img src="/images/52nlp.cn/50c80aed6038d7de0dd64f67c310a8a6.jpg" alt="">，随着<img src="/images/52nlp.cn/70edc6f0495ceac5a10da1dd7b7382b3.jpg" alt="">的增大，<img src="/images/52nlp.cn/067af07d7fb1cbd0dadcc0ce7165b7d6.jpg" alt="">开始大于0，也就是目标函数比<img src="/images/52nlp.cn/2083ed7c66c75b0a75f86ebced03a246.jpg" alt="">增大的幅度更大。这个过程是使目标函数一直单调递增的。下面是<img src="/images/52nlp.cn/489009f24edc0393ae3e99a5de227554.jpg" alt="">与Q函数的关系，这也是为什么M步新参数取完整数据的最大似然解的原因：<br>
<img src="/images/52nlp.cn/ec93cb053b7fe2ccdd0e154e54c8f46f.jpg" alt=""></span><span style="font-family: 宋体"><br>
</span></span></p>
<p style="margin-left: 15pt"><span style="font-family: 微软雅黑;font-size: 10pt">最后上一张非常形象的图，解释为什么EM能收敛到目标函数的局部最优：<br>
<img src="/images/52nlp.cn/be83534220e9ff3f7423041e7e437345.jpg" alt=""><br>
</span></p>
<p><span style="font-size: 10pt"><span style="font-family: 微软雅黑">红的曲线是目标函数；蓝的绿的曲线是两步迭代。<br>
咱们先看蓝的E步和M步：<br>
<img src="/images/52nlp.cn/0772ad51bdbc2259e5de5d05d424c298.jpg" alt=""></span><span style="font-family: 宋体"><br>
</span></span></p>
<p><span style="font-family: 微软雅黑;font-size: 10pt">E步时就是取z的期望的时候，这时目标函数与<img src="/images/52nlp.cn/0d6b36f529709da422275ec9c84ba62d.jpg" alt="">相同；<br>
M步就是最大化<img src="/images/52nlp.cn/768d61ba74934368420071b414838037.jpg" alt=""><br>
绿线是下一轮的迭代，EM过程中，目标函数一直是单调上升的，并且有界，所以EM能够保证收敛。但不一定能收敛到最优解，这与初始值有很大关系，试想一下，目标函数的曲线变动下，EM就有可能收敛到局部最优了。</span></p>
<p>注：PRML读书会系列文章由 <a href="http://weibo.com/dmalgorithms">@Nietzsche_复杂网络机器学习</a> 同学授权发布，转载请注明原作者和相关的主讲人，谢谢。</p>
<p>PRML读书会讲稿PDF版本以及更多资源下载地址：<a href="http://vdisk.weibo.com/u/1841149974">http://vdisk.weibo.com/u/1841149974</a></p>
<p>本文链接地址：<a href="http://www.52nlp.cn/prml%E8%AF%BB%E4%B9%A6%E4%BC%9A%E7%AC%AC%E4%B9%9D%E7%AB%A0-mixture-models-and-em">http://www.52nlp.cn/prml读书会第九章-mixture-models-and-em</a></p>

											
{% endraw %}
