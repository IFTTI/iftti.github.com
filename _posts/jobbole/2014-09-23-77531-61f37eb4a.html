---
layout: post
title: '深度学习之浅见'
time: 2014-09-23 00:00:00 +0800
site_name: jobbole.com
source_url: http://blog.jobbole.com/77531/
images:
  15a00e9d22eb3e970bfdc1173fec5567: http://jbcdn2.b0.upaiyun.com/2014/09/6b5f32a21b26dd7e8fd3d7f6799a0391.jpg
  d4e49519061741aeb825cded122ed4a6: http://jbcdn2.b0.upaiyun.com/2014/09/d82ee29e1c7b6471edda2852bd8f3d72.png
  d6c77e296a0509404c4170d7211b3a5c: http://jbcdn2.b0.upaiyun.com/2014/09/5449d0034c8f3a8cbb8c75cf31eb4817.png
  f09cea626cc7bf8c5c59382d2a142345: http://jbcdn2.b0.upaiyun.com/2014/09/a245748f66f27a9363bfdc50a88da269.png
  bc0e4933fb18d0243a64d4ee911589c8: http://jbcdn2.b0.upaiyun.com/2014/09/d37ff976dbd30ed8afb3b5c3a4c9fc5b.png
  ab2b4d4a741b2343aaaf64610bc4aa79: http://jbcdn2.b0.upaiyun.com/2014/09/ea1abe65d97b2d6646201c8640a0e60a.png
  f99acb2a279b6ccb7b9a68de26052ae3: http://jbcdn2.b0.upaiyun.com/2014/09/acb377d85dcdadebfb908c8c7fc99344.png
---
{% raw %}



<h2>1.深度学习</h2>
<p>深度学习，即Deep Learning，是一种学习算法（Learning algorithm）。学习算法这个很好理解，那么Deep指的是什么呢？这里的Deep是针对算法的结构而言的。</p>
<p>譬如，SVMs及Logistic Regression被称为浅层学习，是因为其隐藏节点只有一层。也就是说，这些算法只通过了一层“思考”就得到了答案，所以它们是浅显的。然而，深度学习的结构中有多个隐藏层，所以起了一个名字叫做Deep。博文<a href="http://www.cnblogs.com/tornadomeet/archive/2013/03/25/2980357.html">Deep learning：十六(deep networks)</a>中对这种结构的优缺点做了说明。</p>
<h2>2.波尔兹曼机</h2>
<p>深度学习是基于Deep Belief Nets提出的算法。Hinton的论文中，是从受限波尔兹曼机（RBMs）中引出的Deep Belief Nets。波尔兹曼机的结构图如下（来自百度图片，这个图实际上不对，隐层节点和可见层节点两两之间都有连线）。</p>
<p><a href="http://jbcdn2.b0.upaiyun.com/2014/09/6b5f32a21b26dd7e8fd3d7f6799a0391.jpg" rel="lightbox[77531]" title="深度学习之浅见"><img class="alignnone size-full wp-image-77540" alt="16095348-07e527b706494ee592db811dc3c0f39d (1)" src="/images/jobbole.com/15a00e9d22eb3e970bfdc1173fec5567.jpg"></a></p>
<p>受限波尔兹曼机去掉了同层之间节点的连线，结构图如下（来自<a href="http://blog.csdn.net/celerychen2009/article/details/8984316">受限波尔兹曼机</a>）。</p>
<p><a href="http://jbcdn2.b0.upaiyun.com/2014/09/d82ee29e1c7b6471edda2852bd8f3d72.png" rel="lightbox[77531]" title="深度学习之浅见"><img class="alignnone size-full wp-image-77533" alt="20130528144727586" src="/images/jobbole.com/d4e49519061741aeb825cded122ed4a6.jpg"></a></p>
<p> </p>
<p>波尔兹曼机的相关知识具体可以参考Simon Haykin的《神经网络原理》第三版。受限波尔兹曼机有很多好的博文，譬如<a href="http://blog.csdn.net/celerychen2009/article/details/8984316">受限波尔兹曼机</a>。波尔兹曼机的学习规则是使得学习之后的似然函数取值最大，然而作为无监督学习，似然函数从何而来？</p>
<p>Hinton原来是学物理的，《神经网络原理》中把波尔兹曼机分在了植根于统计力学的随机机器这一章中。波尔兹曼机中定义了一个系统能量，并基于此定义了各个状态的概率。这些都是基于统计热力学得出的。在这一基础上，可以写出似然函数，之后利用梯度方法对网络进行迭代。</p>
<p>受限波尔兹曼机的叠加得到了深度信度网络（deep belief nets），可参考<a href="http://www.cnblogs.com/xiaowanyer/p/3701944.html">深度学习概述：从感知机到深度网络</a>或Hinton的《A Fast Learning Algorithm for Deep Belief Nets》</p>
<p><a href="http://jbcdn2.b0.upaiyun.com/2014/09/5449d0034c8f3a8cbb8c75cf31eb4817.png" rel="lightbox[77531]" title="深度学习之浅见"><img class="alignnone size-full wp-image-77534" alt="221402481852529" src="/images/jobbole.com/d6c77e296a0509404c4170d7211b3a5c.jpg"></a></p>
<p> </p>
<p>深度信度网络的训练方法被称为是一种 fast, greedy algorithm。就如上图（来自<a href="http://blog.csdn.net/celerychen2009/article/details/8984316">受限波尔兹曼机</a>）所表示，首先，我们认为x和h1组成RBM，对其进行训练；之后，固定此处的w，训练h1和h2，并以此类推。</p>
<h2>3.稀疏自编码器</h2>
<p><a href="http://deeplearning.stanford.edu/wiki/index.php/UFLDL%E6%95%99%E7%A8%8B">UFLDL教程</a>中是利用稀疏自编码其来导出的深度信度网络，以及深度学习算法的。个人觉得这比玻尔兹曼机好理解多了。稀疏自编码器如下所示（来自UFLDL教程），它的理论建立在多层感知器的反向传播（back propagate）算法上。</p>
<p><a href="http://jbcdn2.b0.upaiyun.com/2014/09/a245748f66f27a9363bfdc50a88da269.png" rel="lightbox[77531]" title="深度学习之浅见"><img class="alignnone size-full wp-image-77535" alt="400px-Autoencoder636" src="/images/jobbole.com/f09cea626cc7bf8c5c59382d2a142345.jpg"></a></p>
<p> </p>
<p>和2中类似，深度网络可以表示为如下形式（来自<a href="http://blog.csdn.net/celerychen2009/article/details/8984316">受限波尔兹曼机</a>）。同样可以采用逐层训练的方法。</p>
<p><a href="http://jbcdn2.b0.upaiyun.com/2014/09/d37ff976dbd30ed8afb3b5c3a4c9fc5b.png" rel="lightbox[77531]" title="深度学习之浅见"><img class="alignnone size-full wp-image-77536" alt="052136291049785 (1)" src="/images/jobbole.com/bc0e4933fb18d0243a64d4ee911589c8.jpg"></a></p>
<p> </p>
<h2>4.我们所理解的，是我们看到的世界</h2>
<p>记得之前看过一本叫做《数学基础》的书，其中有一段印象深刻。“为何在集论可推出的无数种结构中，只有少数几种得到了重要的地位？为何有些概念更加有趣？形形色色的数学理论盛衰由人类的实践决定。”那么，我们是否可以认为，数学从一无所有开始，通过限定公理，建立起一套合适的数学理论来拟合我们看到的世界？而深度学习拟合的又是什么？为何深度学习能够在包括计算机视觉在内的众多领域取得成功？深度学习——机器学习——人工智能，拟合的是我们看到的世界。</p>
<hr>
<p><strong>稀疏自编码器的解释</strong></p>
<p><strong>   </strong>根据信息不增定理，在网络传播过程中熵减少。而稀疏编码器中的隐藏元个数小于输入维度。因此，隐藏元是一个自编码的过程，在这个过程中尽可能的保留输入的信息。在深度网络中，每一层的训练亦是如此。</p>
<p>研究表明，人脑的神经元连接也是多层的。由此发展出的神经网络（多层感知器）和由稀疏自编码器构成的深度网络具有相同的结构。一个不恰当的表示是，网络的训练过程是使得每一层保留的熵最大，这样整个网络的输出才有可能有尽可能大的熵（在限定的网络结构下）。那么，为何这一条规则可以在网络训练中取得成功？</p>
<p>因为熵是人定义出来的，并不是真实存在，却帮助人更好的去理解这个世界。由稀疏自编码器导出的深度学习网络利用这一条规则对人脑进行了成功的模拟。如果这一条规则（即人脑的对信息的处理不满足保留熵最大）不成立，那么就不符合人对世界的认知，在人类发展过程中，熵的定义就会被淘汰。所以这一条规则是成立的，这样训练出来的网络，和人的大脑有共同之处。</p>
<hr>
<p><strong>波尔兹曼机的解释</strong></p>
<p>对于波尔兹曼机及其背后的物理思想，自己还没有搞明白。然而，物理是一门对自然万物运行的机理进行解释的科学，统计热力学也不例外。人自身即置于这个世界之内，建立的理论大概也是殊途同归吧。对于这一点，由于自己学识所限，不敢妄加揣测。</p>
<hr>
<p> </p>
<h2>5.道可道，非常道</h2>
<p>看过《老子》之后后，大抵都会纠结于一个问题——什么是道？我个人倾向于把“道”理解为天地，或是天地之间的规则，而这个规则不是永恒不变的。“道”可以视作为一种机制，形形色色的生物是其运行的不同结果。那么，回到4中那句“形形色色的数学理论盛衰由人类的实践决定”，数学理论的盛衰真的是人类的实践决定的吗？只有在人意识到某一种理论的不足的时候，才会有新的理论的发展，有了发展和变化，才会有盛衰。人认识世界，世界改变人的认识，数学理论的盛衰由“道”决定。</p>
<p>回归正题。</p>
<hr>
<p>1.对于SVMs等浅层学习，什么是道？</p>
<p>2.深度学习的道？</p>
<p>3.道法自然、修道</p>
<hr>
<p>很多浅层学习算法可以看作是从被标记的数据之间寻找规律的过程，对于网络结构本身而言，它的“道”是一组组人为建立的被标记的数据。我们试图去建立一个属于它们的世界，让它们在我们建立的世界中学习。然而，这个过程是困难的，不谈浅层网络的结构对其表示的影响，和真实的世界相比，我们建立的用来训练浅层网络的世界太简单，对真实世界的表示有太大的局限性。</p>
<p><a href="http://jbcdn2.b0.upaiyun.com/2014/09/ea1abe65d97b2d6646201c8640a0e60a.png" rel="lightbox[77531]" title="深度学习之浅见"><img class="alignnone size-full wp-image-77537" alt="221403098572516" src="/images/jobbole.com/ab2b4d4a741b2343aaaf64610bc4aa79.jpg"></a></p>
<p> </p>
<p>深度学习的过程是不一样的。认识到建立一个世界的困难之后，试图利用世界本身去训练网络。然而这个过程是没有有反馈的（即无监督学习），我们把人本身作为桥梁，利用我们抽象出的规则，和合适的结构（多层感知器）去构造一个能够表达真实世界规律的人工网络。</p>
<p><a href="http://jbcdn2.b0.upaiyun.com/2014/09/acb377d85dcdadebfb908c8c7fc99344.png" rel="lightbox[77531]" title="深度学习之浅见"><img class="alignnone size-full wp-image-77538" alt="221403254823456" src="/images/jobbole.com/f99acb2a279b6ccb7b9a68de26052ae3.jpg"></a></p>
<p> </p>
<p>对于Deep belief Nets，我们做的还是太多了，太“有为”了。无论从结构上，还是其规则的限定上（我们把网络的训练问题过多的限制成优化问题了）。修道，一般指突破天地限制，大彻大悟的过程。如果有一天，算法能够突破人为的限定，和人所在的世界交互，那算是修道成功了吧！</p>
<h2>6.其他</h2>
<p>这段时间烦心事很多，博客本来都不想写了，后来想想，还是应该坚持下去。这篇博文主要是想把我现在的看法表达出来，行文有点惨不忍睹，观点也有很多错误，还请大家见谅。</p>


{% endraw %}


参考文档（IFTTI整理）

<ul>
<li><a href="http://www.toptal.com/machine-learning/an-introduction-to-deep-learning-from-perceptrons-to-deep-networks">An Introduction to Deep Learning: From Perceptrons to Deep Networks</a> by toptal
</ul>