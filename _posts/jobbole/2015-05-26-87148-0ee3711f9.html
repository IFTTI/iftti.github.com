---
layout: post
title: 'Netflix工程总监眼中的分类算法：深度学习优先级最低'
time: 2015-05-26 00:00:00 +0800
site_name: jobbole.com
source_url: http://blog.jobbole.com/87148/
images:
  8152c51793bfd7edcbb064a10c985d69: http://ww2.sinaimg.cn/mw690/aa213e02jw1eshn8jmc6wj20g50hsdin.jpg
---
{% raw %}


        <span style="display:block;margin-bottom:10px;"></span>
		<p>【编者按】针对Quora上的一个老问题：不同分类算法的优势是什么？Netflix公司工程总监Xavier Amatriain近日给出新的解答，他根据奥卡姆剃刀原理依次推荐了逻辑回归、SVM、决策树集成和深度学习，并谈了他的不同认识。他并不推荐深度学习为通用的方法，这也侧面呼应了我们之前讨论的问题：深度学习能否取代其他机器学习算法。</p>
<p>不同分类算法的优势是什么？例如有大量的训练数据集，上万的实例，超过10万的特征，我们选择哪种分类算法最好？Netflix公司工程总监Xavier Amatriain认为，应当根据奥卡姆剃刀原理（Occam’s Razor）来选择算法，建议先考虑逻辑回归。</p>
<p>选择一个合理的算法可以从很多方面来考察，包括：</p>
<ul>
<li>训练实例的数量？</li>
<li>特征空间的维度？</li>
<li>是否希望该问题线性可分？</li>
<li>特征是否是独立的？</li>
<li>是否预期特征能够线性扩展？</li>
<li>过度拟合是否会成为一个问题？</li>
<li>系统在速度/性能/内存使用等方面的要求如何？</li>
</ul>
<p><strong>逻辑回归</strong></p>
<p>作为一般的经验法则，我建议先考虑逻辑回归（LR，Logistic Regression）。逻辑回归是一个漂亮乖巧的分类算法，可以训练你希望的特征大致线性和问题线性可分。你可以很容易地做一些特征引擎把大部分的非线性特征转换为线性。逻辑回归对噪声也相当强劲，能避免过度拟合，甚至使用L2或L1正则化做特征选择。逻辑回归也可以用在大数据场景，因为它是相当有效的，并且可以分布使用，例如ADMM。 逻辑回归的最后一个优点是，输出可以被解释为概率。这是一个好的附加作用，例如，你可以使用它排名而不是分类。</p>
<p>即使在你不希望逻辑回归100%地工作，你也可以帮自己一个忙，在使用“票友”办法之前，运行一个简单的L2正则化逻辑回归作为基线。</p>
<p>好了，现在你已经设置逻辑回归基线，下一步你应该做的，我基本上会推荐两个可能的方向：支持向量机（SVM）或者决策树集成。如果我不知道你的具体问题，我肯定会选择后者，但我将开始描述为什么SVM可能是一个值得考虑的方法。</p>
<p><strong>支持向量机</strong></p>
<p>支持向量机使用一个与LR不同的损失函数（Hinge）。它们也有不同的解释（maximum-margin）。然而，在实践中，用线性核函数的SVM和逻辑回归是没有很大的不同的（如果你有兴趣，你可以观察Andrew Ng在他的Coursera机器学习课程如何从逻辑回归中驱动SVM）。用SVM代替逻辑回归的一个主要原因可能是因为你的问题线性不可分。在这种情况下，你将不得不使用有非线性内核的SVM（如RBF）。事实上，逻辑回归也可以伴随不同的内核使用，但出于实际原因你更可能选择SVM。另一个使用SVM的相关理由可能是高维空间。例如，SVM已经被报道在工作文本分类方面做得更出色。</p>
<p>不幸的是，SVM的主要缺点是，它们的训练低效到痛苦。所以，对于有大量训练样本的任何问题，我都不会推荐SVM。更进一步地说，我不会为大多数“工业规模”的应用程序推荐SVM。任何超出玩具/实验室的问题可能会使用其他的算法来更好地解决。</p>
<p><strong>决策树集成</strong></p>
<p>第三个算法家族：决策树集成（Tree Ensembles）。这基本上涵盖了两个不同的算法：随机森林（RF）和梯度提升决策树（GBDT）。它们之间的差异随后再谈，现在先把它们当做一个整体和逻辑回归比较。</p>
<p>决策树集成有超过LR的不同优势。一个主要优势是，它们并不指望线性特征，甚至是交互线性特性。在LR里我没有提到的是，它几乎不能处理分类（二进制）特性。而决策树集成因为仅仅是一堆决策树的结合，可以非常好地处理这个问题。另一主要优点是，因为它们构造了（使用bagging或boosting）的算法，能很好地处理高维空间以及大量的训练实例。</p>
<p>至于RF和GBDT之间的差别，可以简单理解为GBDT的性能通常会更好，但它们更难保证正确。更具体而言，GBDT有更多的超参数需要调整，并且也更容易出现过度拟合。RF几乎可以“开箱即用”，这是它们非常受欢迎的一个原因。</p>
<p><strong>深度学习</strong></p>
<p>最后但并非最不重要，没有深度学习的次要参考，这个答案将是不完整的。我绝对不会推荐这种方法作为通用的分类技术。但是，你可能会听说这些方法在某些情况下（如图像分类）表现如何。如果你已经通过了前面的步骤并且感觉你的解决方案还有优化的空间，你可能尝试使用深度学习方法。事实是，如果你使用一个开源工具（如Theano）实现，你会知道如何使这些方法在你的数据集中非常快地执行。</p>
<p><strong>总结</strong></p>
<p>综上所述，先用如逻辑回归一样简单的方法设定一个基准，如果你需要，再使问题变得更加复杂。这一点上，决策树集成可能正是要走的正确道路，特别是随机森林，它们很容易调整。如果你觉得还有改进的余地，尝试GBDT，或者更炫一些，选择深度学习。</p>
<p>你还可以看看Kaggle比赛。如果你搜索关键字“分类”，选择那些已经完成的，你能找到一些类似的东西，这样你可能会知道选择一个什么样的方法来赢得比赛。在这一点上，你可能会意识到，使用集成方法总容易把事情做好。当然集成的唯一问题，是需要保持所有独立的方法并行地工作。这可能是你的最后一步，花哨的一步。</p>
<p>编辑点评：Xavier Amatriain不推荐深度学习为通用算法的理由，并不能说是因为深度学习不好，而是因为深度学习会增加复杂性及成本，却无法保证在所有的场景表现出比逻辑回归、SVM及决策树集成更优的结果。事实上，Xavier Amatriain的Netflix团队早已开始研究人工神经网络和深度学习技术，希望借助AWS云服务和GPU加速的分布式神经网络，分析网民最爱看的电影电视剧，实现节目的个性化推荐。</p>
<p style="text-align: center"> <img class="alignnone" alt="" src="/images/jobbole.com/8152c51793bfd7edcbb064a10c985d69.jpg" width="581" height="640"></p>
<p style="text-align: center">Netflix推荐系统架构（图片来自Xavier Amatrain参与撰写的Netflix官方博客）</p>
<p>此后，Xavier Amatriain还分享了Netflix机器学习实践的十大经验教训，大致包括：</p>
<ul>
<li>更多的数据需要与更好的模型之匹配</li>
<li>你可能不需要所有的大数据</li>
<li>更复杂的模型未必意味着更好的结果，可能是你的样本集太简单</li>
<li>要充分考虑你的训练数据</li>
<li>学会处理偏差</li>
<li>UI是联系算法和最重要的用户之间唯一通道</li>
<li>正确的演进方式比数据和模型更重要</li>
<li>分布式算法重要，知道在哪个层级使用它更重要</li>
<li>选择合适的度量自动超参数优化</li>
<li>并非所有的事都能离线完成，近线处理也是一种选择</li>
</ul>

        
        
    <div class="post-adds">
        <span data-post-id="87148" class="btn-bluet href-style vote-post-up   register-user-only "><i class="fa  fa-thumbs-o-up"></i> <h10 id="87148votetotal"></h10> 赞</span>
        <span data-book-type="1" data-site-id="2" data-item-id="87148" data-item-type="1" class="btn-bluet href-style bookmark-btn  register-user-only "><i class="fa fa-bookmark-o  "></i>  收藏</span>

                <a href="#article-comment"><span class="btn-bluet href-style"><i class="fa fa-comments-o"></i>  评论</span></a>
        
            </div>


        <!-- BEGIN #author-bio -->


<!-- END #author-bio -->
	
{% endraw %}
