---
layout: post
title: 'LISA13 Metrics Workshop'
time: 2014-05-16 00:00:00 +0800
site_name: brendangregg.com
source_url: http://www.brendangregg.com/blog/2014-05-16/LISA13-metrics-workshop.html
images:
  7a92095afde233777b70b5ba1f73db14: /blog/images/2014/brendan-workshop-bw-600.png
---
{% raw %}

<div style="float:right;padding:0px 10px 10px 20px"><a href="/blog/images/2014/brendan-workshop-bw-600.png"><img border="0" src="/images/brendangregg.com/7a92095afde233777b70b5ba1f73db14.jpg" width="300"></a></div>

<p>At USENIX LISA`13 I helped run a <a href="https://www.usenix.org/conference/lisa13/workshops/metrics">Metrics Workshop</a>, along with Narayan Desai (Argonne National Laboratory), Kent Skaar (VMware, Inc.), Theo Schlossnagle (OmniTI), and Caskey Dickson (Google). This was an opportunity for many industry professionals to discuss problems with performance metrics and monitoring, and to propose and discuss solutions. It was a lot of fun, and was very useful to hear the different opinions and perspectives from those who attended.</p>

<p>We provided guidance for choosing more effective performance metrics, which involved helping people think more freely and creatively, instead of being bounded by the metrics that are currently or typically offered. I also covered key methodologies, including the USE Method, which provide a checklist of concise metrics that are designed to solve issues. I ended the day with five minute lightning talks on <a href="http://www.youtube.com/watch?v=HKmgdLcx8jQ">statistics and visualizations</a>.</p>

<p>There were about 30 participants, and <a href="http://www.beginningwithi.com">Deirdré Straughan</a> videoed the entire day long event, which includes the talks by the other moderators. The videos are on <a href="http://www.youtube.com/playlist?list=PL3oXECC9Rpm2BL0v1W7cgyj8eV9d44icc">youtube</a>:</p>

<p></p>
<center><iframe width="560" height="315" src="//www.youtube.com/embed/videoseries?list=PL3oXECC9Rpm2BL0v1W7cgyj8eV9d44icc" frameborder="0" allowfullscreen></iframe></center>

<p>As an exercise, we identified several targets of performance monitoring, formed groups to propose ideal metrics, then presented and discussed these metrics. I've listed a summary of the metrics below, and also submitted them to the monitoringsucks project on <a href="https://github.com/monitoringsucks/metrics-catalog/pull/1">github</a>.</p>

<h2>Network Infrastructure</h2>

<ul>
<li>Physical Infrastructure

<ul>
<li>bandwidth, utilization of individual links</li>
<li>CoS/QoS rate/drops</li>
<li>L2/L2 protocol health</li>
<li>churn</li>
<li>reachabality</li>
</ul>
</li>
<li>Per port:

<ul>
<li>packets/sec</li>
<li>packet size</li>
<li>buffer utilization</li>
<li>perf flow into:</li>
<li>app injection BW</li>
<li>app injectiov rate</li>
<li>app consumption rate</li>
<li>app consumption BW</li>
</ul>
</li>
<li>Component:

<ul>
<li>links</li>
<li>errors</li>
<li>latency</li>
<li>utilization</li>
</ul>
</li>
<li>Topology:

<ul>
<li>app to app latency</li>
<li>app to app low</li>
<li>symmetry</li>
</ul>
</li>
</ul>

<h2>Configuration</h2>

<ul>
<li>Apps should export flags, to check for consistency

<ul>
<li>a metadata to show the target configuration</li>
</ul>
</li>
<li>Versioning:

<ul>
<li>ldd, libraries linked against</li>
<li>time a config was applied</li>
</ul>
</li>
<li>Platform Type:

<ul>
<li>server H/W</li>
</ul>
</li>
<li>Cost of Configuration

<ul>
<li>cost of configuration upload/download</li>
<li>time to deployment: security changes (high priority), vs others</li>
<li>CPU and RAM usage during configuration</li>
</ul>
</li>
<li>People

<ul>
<li>deployment report</li>
</ul>
</li>
<li>Hardware

<ul>
<li>current hardware</li>
<li>max expected performance</li>
</ul>
</li>
<li>Process

<ul>
<li>compliance measurement of configuration: percent of systems</li>
</ul>
</li>
<li>Failure

<ul>
<li>failure of configuration deployment</li>
<li>rollbacks, rollforward: config metric didn't apply</li>
</ul>
</li>
<li>OS flags</li>
</ul>

<h2>Distributed system</h2>

<ul>
<li>Perceived latency: service time and queueing</li>
<li>Request rate</li>
<li>Error rate</li>
<li>Traffic origins</li>
<li>Histogram of latencies for each server, for comparisons</li>
<li>Visualizations:

<ul>
<li>heatmaps</li>
<li>for service</li>
<li>per server</li>
<li>per backend</li>
<li>system 'flame graph'</li>
<li>visualize traffic as graph, queue time, request flow</li>
</ul>
</li>
</ul>

<h2>Message Queueing</h2>

<ul>
<li>Distribution of message latency (ns)</li>
<li>Throughput</li>
<li>Total number of ns</li>
<li>Errors, drop, retransmits, discards</li>
<li>Message fanout distribution (gain: ratio of input to put)</li>
<li>For distribution message queues: see distributied systems</li>
<li>Queue lengths</li>
<li>Saturation: run out of space</li>
<li>Resource constraints on queueing systems</li>
<li>Last time of access</li>
</ul>

<h2>Web servers</h2>

<ul>
<li>Requests: referrer, origin, UA, resp code, count

<ul>
<li>origin</li>
<li>response code</li>
</ul>
</li>
<li>Req size: distribution</li>
<li>Response Size: resp code, distribution</li>
<li>Responce Count: resp code, counter</li>
<li>Time To First Bite: resp code, distribution</li>
<li>Time To Last Bite: resp code, distribution</li>
<li>Active Workers: guage</li>
<li>Worker Age: guage</li>
<li>Connections:  counter</li>
<li>Process Metrics from host</li>
</ul>

<h2>Application servers</h2>

<ul>
<li>Total requests served, rate</li>
<li>Latency:

<ul>
<li>time to serve a client</li>
<li>complete a client transaction</li>
<li>request queue time</li>
</ul>
</li>
<li>App error rate</li>
<li>Error counts on backend H/W</li>
<li>Bandwidth usage front and backend</li>
<li>System load on primary application server: CPU, memory, disk, swapping</li>
<li>Usage patterns:

<ul>
<li>which user, client time, session time, active vs idle time</li>
</ul>
</li>
</ul>

<h2>Databases</h2>

<ul>
<li>Queries/sec</li>
<li># of connections</li>
<li>connections/sec</li>
<li>avg time per query</li>
<li>cache hit rate</li>
<li>avg io latency</li>
<li>aggregate io</li>
<li>% of query time in io</li>
<li># of locks</li>
<li># of versions (for read consistency)</li>
<li>terminated connects</li>
<li>SQL statements</li>
<li>cache evictions</li>
<li>query errors by type</li>
<li>saturation: plan to execute

<ul>
<li>queueing on pool</li>
</ul>
</li>
<li>change in number of executed plans</li>
<li>latency of last checkpoint, and on-disk representation of wall log

<ul>
<li>(how much of DB to reply)</li>
</ul>
</li>
<li>checkpoint times</li>
</ul>

<h2>Resources/Devices</h2>

<ul>
<li>Utilization

<ul>
<li>per-device: eg, as a heat map for distribution over time</li>
</ul>
</li>
<li>Saturation

<ul>
<li>average queue length, or time waiting on queue</li>
</ul>
</li>
<li>Errors</li>
</ul>

<p>Thanks to all those who attended and helped out!</p>

{% endraw %}
