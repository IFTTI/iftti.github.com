---
layout: post
title: 'spark streaming 接收 kafka 数据示例'
time: 2015-03-14 00:00:00 +0800
site_name: chenlinux.com
source_url: http://chenlinux.com/2015/03/14/spark-streaming-kafka
---
{% raw %}

  
  <div style="background-color: #FFF;">
    <p>上个月曾经试过了用 spark streaming 读取 logstash 启动的 TCP Server 的数据。不过如果你有多台 logstash 的时候，这种方式就比较难办了 —— 即使你给 logstash 集群申请一个 VIP，也很难确定说转发完全符合。所以一般来说，更多的选择是采用 kafka 等队列方式由 spark streaming 去作为订阅者获取数据。</p>
<h2 id="section">环境部署</h2>
<p>这里只讲 kafka 单机的部署。只是示例嘛：</p>
<pre><code>cd kafka_2.10-0.8.2.0/bin/
./zookeeper-server-start.sh ../config/zookeeper.properties &amp;
./kafka-server-start.sh --daemon ../config/server.properties
</code></pre>
<h2 id="section-1">数据转发</h2>
<p>保持跟之前示例的连贯性，这里继续用 logstash 发送数据到 kafka。</p>
<p>首先创建一个 kafka 的 topic：</p>
<pre><code>./kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic logstash
</code></pre>
<p>然后到 logstash 里，修改配置为：</p>
<pre><code>input {
    file { path =&gt; "/var/log/*.log" }
}
filter {
    ruby {
        code =&gt; "event['lineno'] = 100 * rand(Math::E..Math::PI)"
    }
}
output {
    kafka {
        broker_list =&gt; "127.0.0.1:9092"
        topic_id =&gt; "logstash"
    }
}
</code></pre>
<h2 id="spark-streaming-">spark streaming 处理的代码：</h2>
<p>处理效果跟之前示例依然保持一致，就不重复贴冗余的函数了，只贴最开始的处理部分：</p>
<div class="highlight"><pre><code class="language-java" data-lang="java"><span class="kn">import</span> <span class="nn">org.apache.spark.SparkConf</span>
<span class="kn">import</span> <span class="nn">org.apache.spark.SparkContext</span>
<span class="kn">import</span> <span class="nn">org.apache.spark.SparkContext._</span>
<span class="kn">import</span> <span class="nn">org.apache.spark.streaming.</span><span class="o">{</span><span class="n">Seconds</span><span class="o">,</span> <span class="n">StreamingContext</span><span class="o">}</span>
<span class="kn">import</span> <span class="nn">org.apache.spark.streaming.StreamingContext._</span>
<span class="kn">import</span> <span class="nn">org.apache.spark.streaming.kafka.KafkaUtils</span>
<span class="kn">import</span> <span class="nn">org.json4s._</span>
<span class="kn">import</span> <span class="nn">org.json4s.jackson.JsonMethods._</span>
<span class="n">object</span> <span class="n">LogStash</span> <span class="o">{</span>
  <span class="n">implicit</span> <span class="n">val</span> <span class="n">formats</span> <span class="o">=</span> <span class="n">DefaultFormats</span>
  <span class="k">case</span> <span class="kd">class</span> <span class="nf">LogStashV1</span><span class="o">(</span><span class="nl">message:</span><span class="n">String</span><span class="o">,</span> <span class="nl">path:</span><span class="n">String</span><span class="o">,</span> <span class="nl">host:</span><span class="n">String</span><span class="o">,</span> <span class="nl">lineno:</span><span class="n">Double</span><span class="o">,</span> <span class="err">`</span><span class="nd">@timestamp</span><span class="err">`</span><span class="o">:</span><span class="n">String</span><span class="o">)</span>
  <span class="n">def</span> <span class="nf">main</span><span class="o">(</span><span class="nl">args:</span> <span class="n">Array</span><span class="o">[</span><span class="n">String</span><span class="o">])</span> <span class="o">{</span>
    <span class="n">val</span> <span class="nf">Array</span><span class="o">(</span><span class="n">zkQuorum</span><span class="o">,</span> <span class="n">group</span><span class="o">,</span> <span class="n">topics</span><span class="o">,</span> <span class="n">numThreads</span><span class="o">)</span> <span class="o">=</span> <span class="n">args</span>
    <span class="n">val</span> <span class="n">topicMap</span> <span class="o">=</span> <span class="n">topics</span><span class="o">.</span><span class="na">split</span><span class="o">(</span><span class="s">","</span><span class="o">).</span><span class="na">map</span><span class="o">((</span><span class="n">_</span><span class="o">,</span><span class="n">numThreads</span><span class="o">.</span><span class="na">toInt</span><span class="o">)).</span><span class="na">toMap</span>
    <span class="n">val</span> <span class="n">sparkConf</span> <span class="o">=</span> <span class="k">new</span> <span class="nf">SparkConf</span><span class="o">().</span><span class="na">setMaster</span><span class="o">(</span><span class="s">"local[2]"</span><span class="o">).</span><span class="na">setAppName</span><span class="o">(</span><span class="s">"LogStash"</span><span class="o">)</span>
    <span class="n">val</span> <span class="n">sc</span>  <span class="o">=</span> <span class="k">new</span> <span class="nf">SparkContext</span><span class="o">(</span><span class="n">sparkConf</span><span class="o">)</span>
    <span class="n">val</span> <span class="n">ssc</span> <span class="o">=</span> <span class="k">new</span> <span class="nf">StreamingContext</span><span class="o">(</span><span class="n">sc</span><span class="o">,</span> <span class="n">Seconds</span><span class="o">(</span><span class="mi">10</span><span class="o">))</span>
    <span class="n">val</span> <span class="n">lines</span> <span class="o">=</span> <span class="n">KafkaUtils</span><span class="o">.</span><span class="na">createStream</span><span class="o">(</span><span class="n">ssc</span><span class="o">,</span> <span class="n">zkQuorum</span><span class="o">,</span> <span class="n">group</span><span class="o">,</span> <span class="n">topicMap</span><span class="o">).</span><span class="na">map</span><span class="o">(</span><span class="n">_</span><span class="o">.</span><span class="na">_2</span><span class="o">)</span>
    <span class="n">lines</span><span class="o">.</span><span class="na">map</span><span class="o">(</span><span class="n">line</span> <span class="o">=&gt;</span> <span class="o">{</span>
      <span class="n">val</span> <span class="n">json</span> <span class="o">=</span> <span class="n">parse</span><span class="o">(</span><span class="n">line</span><span class="o">)</span>
      <span class="n">json</span><span class="o">.</span><span class="na">extract</span><span class="o">[</span><span class="n">LogStashV1</span><span class="o">]</span>
    <span class="o">}).</span><span class="na">print</span><span class="o">()</span>
    <span class="n">ssc</span><span class="o">.</span><span class="na">start</span><span class="o">()</span>
    <span class="n">ssc</span><span class="o">.</span><span class="na">awaitTermination</span><span class="o">()</span>
  <span class="o">}</span>
<span class="o">}</span></code></pre></div>
<p>这里面有一些跟网上常见资料不一样的地方。</p>
<p>第一个，<code>import org.apache.spark.streaming.kafka._</code> 并不会导出 <code>KafkaUtils</code>，必须明确写明才行。
第二个，之前示例里用了 scala 核心自带的 JSON 模块。但是这次我把 lineno 字段从整数改成浮点数后，发现 <code>JSON.parseFull()</code> 有问题。虽然我在 scala 的 repl 里测试没问题，但是写在 spark 里的时候，它并不像文档所说的”总是尝试解析成 Double 类型”，而是一直尝试用 <code>Integer.parseInteger()</code> 方法来解析。哪怕我明确定义 <code>JSON.globalNumberParser = {input:String =&gt; Float.parseFloat(input)}</code> 都不起作用。</p>
<p>所以，最后这里改用了 <a href="http://json4s.org">json4s 库</a>。据称这也是 scala 里性能和功能最好的 JSON 库。</p>
<p>json4s 库默认解析完后，不是标准的 Map、List 等对象，而是它自己的 JObject、JList、JString 等。想要转换成标准 scala 对象，需要调用 <code>.values</code> 才对。不过我这个示例里没有这么麻烦，而是直接采用 <code>.extract</code> 就变成了 cast class 对象了。非常简便。</p>
<p>另一个需要点出来的变动是：因为采用 <code>.extract</code>，所以 cast class 里的参数命名必须跟 JSON 里的 key 完全对应上。而我们都知道 logstash 里有几个特殊的字段，叫 <code>@timestamp</code> 和 <code>@version</code> 。这个 “@” 是不能直接裸字符的，所以要用反引号(<strong>`</strong>)包括起来。</p>
<h2 id="sbt-">sbt 打包</h2>
<p>sbt 打包也需要有所变动。spark streaming 的核心代码中，并不包含 kafka 的代码。还跟之前那样 <code>sbt package</code> 的话，就得另外指定 kafka 的 jar 地址才能运行了。更合适的办法，是打包一个完全包含的 jar 包。这就用到 <a href="https://github.com/sbt/sbt-assembly">sbt-assembly 扩展</a>。</p>
<p><em>刚刚收到的消息，spark 1.3 版发布 beta 了，spark streaming 会内置对 kafka 的底层直接支持。或许以后不用这么麻烦？</em></p>
<p>sbt-assembly 使用起来特别简单，尤其是当你使用的 sbt 版本比较新(大于 0.13.6) 的时候。</p>
<ol>
  <li>添加扩展</li>
</ol>
<p>在项目的 <code>project/</code> 目录下创建一个 <code>plugins.sbt</code> 文件，内容如下：</p>
<pre><code>addSbtPlugin("com.eed3si9n" % "sbt-assembly" % "0.13.0")
</code></pre>
<p>具体的版本选择，看官方 README 的 <a href="https://github.com/sbt/sbt-assembly#setup">Setup 部分</a>。</p>
<ol>
  <li>添加新增依赖模块</li>
</ol>
<p>现在可以去修改我们项目的 <code>build.sbt</code> 了：</p>
<div class="highlight"><pre><code class="language-scala" data-lang="scala"><span class="n">name</span> <span class="o">:=</span> <span class="s">"LogStash"</span>
<span class="n">version</span> <span class="o">:=</span> <span class="s">"1.0"</span>
<span class="n">scalaVersion</span> <span class="o">:=</span> <span class="s">"2.10.4"</span>
<span class="n">libraryDependencies</span> <span class="o">++=</span> <span class="nc">Seq</span><span class="o">(</span>
  <span class="s">"org.apache.spark"</span> <span class="o">%%</span> <span class="s">"spark-core"</span> <span class="o">%</span> <span class="s">"1.2.0"</span> <span class="o">%</span> <span class="s">"provided"</span><span class="o">,</span>
  <span class="s">"org.apache.spark"</span> <span class="o">%%</span> <span class="s">"spark-sql"</span> <span class="o">%</span> <span class="s">"1.2.0"</span> <span class="o">%</span> <span class="s">"provided"</span><span class="o">,</span>
  <span class="s">"org.apache.spark"</span> <span class="o">%%</span> <span class="s">"spark-streaming"</span> <span class="o">%</span> <span class="s">"1.2.0"</span> <span class="o">%</span> <span class="s">"provided"</span><span class="o">,</span>
  <span class="s">"org.apache.spark"</span> <span class="o">%%</span> <span class="s">"spark-streaming-kafka"</span> <span class="o">%</span> <span class="s">"1.2.0"</span><span class="o">,</span>
  <span class="s">"org.json4s"</span> <span class="o">%%</span> <span class="s">"json4s-native"</span> <span class="o">%</span> <span class="s">"3.2.10"</span><span class="o">,</span>
  <span class="s">"org.json4s"</span> <span class="o">%%</span> <span class="s">"json4s-jackson"</span> <span class="o">%</span> <span class="s">"3.2.10"</span>
<span class="o">)</span></code></pre></div>
<p>是的。新版本的 sbt-assembly 完全不需要单独修改 <code>build.sbt</code> 了。</p>
<p>需要注意，因为我们这次是需要把各种依赖全部打包到一起，这个可能会导致一些文件相互有冲突。比如我们用 spark-submit 提交任务，有关 spark 的核心文件，本身里面就已经有了的，那么就需要额外通过 <code>% "provided"</code> 指明这部分会另外提供，不需要打进去。这样运行的时候就不会有问题了。</p>
<ol>
  <li>打包</li>
</ol>
<p>采用 sbt-assembly 后的打包命令是：<code>sbt assembly</code>。注意输出的结果，会是直接读取 <code>build.sbt</code> 里的 <code>name</code> 变量，不做处理。，我们之前定义的叫 “LogStash Project”，<code>sbt package</code> 命令自动会转换成全小写且空格改成中横线的格式 <em>logstash-project_2.10-1.0.jar</em>。但是 <code>sbt assembly</code> 就会打包成 <em>LogStash Project-assembly-1.0.jar</em> 包。这个空格在走 spark-submit 提交的时候是有问题的。所以这里需要把 name 改成一个不会中断的字符串。。。</p>
    <hr>
    
    <hr>
  <!-- JiaThis Button BEGIN -->
<div class="jiathis_style">
<span class="jiathis_txt">分享到：</span>
<a class="jiathis_button_tsina">新浪微博</a>
<a class="jiathis_button_weixin">微信</a>
<a class="jiathis_button_renren">人人网</a>
<a class="jiathis_button_ydnote">有道云笔记</a>
<a class="jiathis_button_gmail">Gmail邮箱</a>
<a class="jiathis_button_twitter">Twitter</a>
<a class="jiathis_button_googleplus">Google+</a>
<a class="jiathis_button_hi">百度空间</a>
<a class="jiathis_button_fb">Facebook</a>
<a class="jiathis_button_douban">豆瓣</a>
<a href="http://www.jiathis.com/share?uid=1589850" class="jiathis jiathis_txt jiathis_separator jtico jtico_jiathis" target="_blank">更多</a>
<a class="jiathis_counter_style"></a>
</div>
<script type="text/javascript">
var jiathis_config={
	data_track_clickback:true,
	summary:"",
	ralateuid:{
		"tsina":"1035836154"
	},
	shortUrl:false,
	hideMore:false
}
</script>
<script type="text/javascript" src="http://v3.jiathis.com/code/jia.js?uid=1589850" charset="utf-8"></script>
<!-- JiaThis Button END -->
<!-- UY BEGIN -->


<!-- UY END -->
  </div>
{% endraw %}
