<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>IT技术干货</title>
    <description>[IT技术干货iftti.com] @KernelHacks</description>
    <link>http://iftti.com/</link>
    <atom:link href="http://iftti.com/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Thu, 16 Oct 2014 23:25:00 +0800</pubDate>
    <lastBuildDate>Thu, 16 Oct 2014 23:25:00 +0800</lastBuildDate>
    <generator>Jekyll v2.2.0</generator>
    
      <item>
        <title>『淘宝十年技术路』读书笔记</title>
        <description>

					
		
&lt;!-- div id=&quot;ad1&quot;&gt;
&lt;script type=&quot;text/javascript&quot;&gt;
google_ad_client = &quot;ca-pub-7056282119617872&quot;;
google_ad_slot = &quot;6645040531&quot;;
google_ad_width = 300;
google_ad_height = 250;
&lt;/script&gt;
&lt;script type=&quot;text/javascript&quot;
src=&quot;http://pagead2.googlesyndication.com/pagead/show_ads.js&quot;&gt;
&lt;/script&gt;
&lt;/div&gt;
&lt;br/ --&gt;

&lt;p&gt;最近有幸，在学校的图书馆借到了子柳先生的《&lt;a href=&quot;http://www.amazon.cn/gp/product/B00CK2H13K/ref=as_li_qf_sp_asin_il_tl?ie=UTF8&amp;amp;camp=536&amp;amp;creative=3200&amp;amp;creativeASIN=B00CK2H13K&amp;amp;linkCode=as2&amp;amp;tag=vastwork-23&quot; target=&quot;_blank&quot;&gt;淘宝技术这十年&lt;/a&gt;》，拜读一番，感慨万分。将书中内容加上自己的想法与诸君分享，毕竟未经人事看法粗浅，希望能得到前辈们的指点。&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://www.amazon.cn/gp/product/B00CK2H13K/ref=as_li_qf_sp_asin_il_tl?ie=UTF8&amp;amp;camp=536&amp;amp;creative=3200&amp;amp;creativeASIN=B00CK2H13K&amp;amp;linkCode=as2&amp;amp;tag=vastwork-23&quot; target=&quot;_blank&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/fd5f1ef9d64a75a6c650b52554d043fb.jpg&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;一、淘宝的核心技术（国内乃至国际的Top，这还是2011年的数据）&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;拥有全国最大的分布式Hadoop集群（云梯，2000左右节点，24000核CPU，48000GB内存，40PB存储容量）&lt;/li&gt;
&lt;li&gt;全国分布80+CDN节点，能够自动找寻最近的节点提供服务，支持流量超过800Gbps，足以拖垮一个城市的流量&lt;/li&gt;
&lt;li&gt;不逊于百度的搜索引擎，对数十亿商品进行搜索，全球最大的电商平台&lt;/li&gt;
&lt;li&gt;顶尖的负载均衡系统，顶尖的分布式系统，顶尖的互联网思想，功能多样运行极其稳定&lt;/li&gt;
&lt;li&gt;丰富的生态产业以及先进的数据挖掘技术&lt;/li&gt;
&lt;li&gt;……很多很多&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;二、淘宝网的诞生&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;马总在2003年4月7日秘密叫来阿里巴巴的十位员工，来到杭州一个隐秘的毛坯房，要求他们在一个月左右的时间内做出一个C2C网站。&lt;/p&gt;
&lt;p&gt;结果当然还是直接买的快，一个基于LAMP架构的网站，原名是PHPAuction，老美开发的一个拍卖网站。当然必须要做修改才能用。（作为一个曾经用老美开发的前端页面开发自己博客的同学，确实感觉用别人写的比较方便偷懒-_-，不过我确信虚竹、三丰、多隆等前辈是有足够实力开发自己网站的——还是马总催的紧）&lt;/p&gt;
&lt;p&gt;当时财大气粗的eBay正在中国耀武扬威，加上SARS肆虐，可能大家对网购产生了新的认识。而淘宝刻意保持低调，甚至连阿里的员工都不知道这是他们自己公司的产品。&lt;/p&gt;
&lt;p&gt;淘宝的员工积极回答着用户的问题，早起贪黑，锻炼身体的方法就是倒立。&lt;/p&gt;
&lt;p&gt;淘宝的功能也在不断的完善着，发布、管理、搜索、详情、购买等等，服务器也变成了三台。因为数据量大了，淘宝的搜索很慢（使用LIKE匹配…），多隆前辈把阿里巴巴的搜索引擎iSearch搬了过来。&lt;/p&gt;
&lt;p&gt;当时MySQL的默认存储引擎MyISAM会导致读写锁等待时间过长等等大量问题，所以意外还是很多的。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;2003年底，淘宝注册用户23万，PV 31万/day，半年成交额3371万。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;三、淘宝的更新&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;很显然MySQL无法撑得起如此大的访问量，数据库瓶颈出现了。幸好阿里的DBA队伍足够强大，他们使用Oracle替代了MySQL。&lt;/p&gt;
&lt;p&gt;Oracle那时就已经有了强大的并发性访问设计——连接池，从连接池取连接的耗费比单独建立连接少很多。但是PHP当时并没有官方提供支持语言连接池特性，于是多隆前辈用Google（不会是Baidu）搜到了一个开源的SQL Relay，于是数据库软件方面的瓶颈暂时解决了。&lt;/p&gt;
&lt;p&gt;但是硬件容量不够了，阿里买了NAS（后来因为延迟严重原因买了EMC的SAN低端存储），加上Oracle高性能RAC，硬件容量也暂时没问题了。&lt;/p&gt;
&lt;p&gt;开源的东西固然好，但是大胆使用也是一次尝试的过程，SQL Relay会频繁的导致死锁问题，导致工程师不得不定期进行重启服务，从书中的描述可以看出，淘宝的工程师们真的非常辛苦。&lt;/p&gt;
&lt;p&gt;淘宝网不会止步于仅仅为卖家和买家提供一个交易的网站而已，还需要建立一个完善的第三方体系，来保证卖家和买家之间的交易是安全的，于是支付宝诞生了。比较麻烦的是，当时虽有很多银行开放了网银接口，但是甚至不能保证付钱后就会扣款成功，还是需要工程师们辛苦的一板一眼去对账……&lt;/p&gt;
&lt;p&gt;淘宝为了便于用户的交流，开发了一个IM软件——旺旺，不仅给买卖双方使用，阿里内部也使用旺旺交流。&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;四、第一个里程碑&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;因为SQL Relay的问题实在过于严重，2004年于是淘宝终于做出了跨时代的决策——使用Java重写网站（鼓掌~~~）。&lt;/p&gt;
&lt;p&gt;没错，淘宝请了Sun的高级工程师来帮忙做Java架构。那么他们是如何做到修改编程语言而不改变网站使用呢——模块化替换，今天写好了A模块，另开一个新域名，将连接指向该模块，同时别的模块不变，等到全部模块完成的时候，原域名放弃。&lt;/p&gt;
&lt;p&gt;使用的框架：淘宝的架构师在Jakarta Turbine的基础上开发了自己的MVC框架——WebX。而Sun公司坚持使用EJB作为控制层（估计当时只有他们才能玩贯EJB），加上使用iBatis作为持久层，一个可扩展且高效的Java EE应用诞生了。BYW，支付宝也是Sun的工程师用同样的架构设计的。&lt;/p&gt;
&lt;p&gt;送走Sun的大牛们之后，阿里的数据存储又遇到了瓶颈，于是忍痛买了一台IBM小型机（我猜至少是百万级别的…….），也就有了IOE（IBM + Oracle + EMC）这样的传说。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;2004年底，淘宝注册用户400万，PV 4000万/day，全网成交额10个亿。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;五、再接再厉&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Oracle也有处理上限，当数量的级别是“亿”的时候，就不是一个Oracle服务器支撑的起的了。DBA们把数据分到了两个数据库中，通过ID的第一位决定查询哪一个数据。比如，‘0’至‘7’放在A数据库，‘8’至‘f’放在B数据库，通用信息放在C数据库。但是如何既查询’3′开头又查询’e&#39;开头的数据呢？一个数据库路由框架DBRoute由架构师行癫编写，统一处理合并问题而对上层透明。&lt;/p&gt;
&lt;p&gt;Spring诞生了，早闻Spring框架在Web应用不可或缺，而在淘宝网，Spring也达到了Rod Johnson设计它的目的——替代EJB。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;2005年底，淘宝注册用户1390万，PV 8931万/day，商品数目1663万个。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;说实话我真的好佩服，这么大的访问量都能如此坚挺，但是，考虑到未来的发展，这样的设施架构只是勉强可以应付现在的要求。于是，CDN技术派上用场了，一开始使用商用的ChinaCache，后来使用章文嵩博士搭建低耗能CDN网络，淘宝网的性能越来越好了。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;2006年底，淘宝注册用户3000万，PV 15000万/day，商品数目5000万，全网成交额169亿元。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;六、创造技术&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;为了考虑交易的公平性，淘宝增加了交易快照功能，将当前交易网页以图片的形式保存下来，淘宝的交易量如此之大，带来了一个问题——碎片图片过多，2010年，淘宝网的后端上保存着286亿张图片。&lt;/p&gt;
&lt;p&gt;淘宝在2007年之前，使用NetApp的商用存储系统，但是仍然不够应付迅速增长的趋势。同年Google公布了GFS的设计思想，参照它的思想，淘宝也开发了自己的文件系统——TFS。至于这个文件系统的具体原理书上给的并不详细（应该是我看不懂-_-），不过可以大概可以了解是专门为大量的图片设计的，从每个用户1张图片到TFS上线后5张再到1GB的图片空间，这些都得益于TFS集群的文件存储系统以及大量的图片服务器。淘宝使用实时生成缩率图，全局负载均衡以及一级和二级缓存来保证图片的访问优化与高效访问。&lt;/p&gt;
&lt;p&gt;淘宝的服务器软件使用Tengine，一个被优化过的nginx模块。&lt;/p&gt;
&lt;p&gt;淘宝也做过失败的产品，不是因为技术原因而是市场原因。首先是“团购”，失败在于人心叵测。再次是“我的淘宝”，使用了风靡全球的AJAX的技术，但是做的过于AJAX了，可能是太不容易上手了（马总亲口说的），还有“招财进宝”（被竞争对手认为是破坏了“免费”的承诺而大肆宣扬）。&lt;/p&gt;
&lt;p&gt;记录商品的访问量，使用传统的数据库I/O实在过于影响效率，所以淘宝使用了缓冲的技术，先是使用ESI（Edge Side Includes）,解决了片段缓冲问题。因为有些大店铺访问量过大，频繁的I/O实在得不偿失，于是多隆前辈写出了TBstore，可以缓存大量的数据，核心思想是使用Hash算法快速寻找。其核心是基于Berkeley DB，一种类内存数据库，导致的问题是内存数据量大了还是会刷到磁盘中，因此性能并不是那么的好。&lt;/p&gt;
&lt;p&gt;后来，淘宝分离出了UIC（User Information Center），供所有模块调用。多隆前辈再次为其编写出了TDBM，完全是基于内存的数据缓存（参考了memcached）。再然后，淘宝将TBstore和TDBM合并，写出了Tair，一个基于Key-Value的分布式缓存数据系统。然后升级了自己的iSearch系统。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;2007年底，淘宝注册用户5000万，PV 25000万/day，商品数目1个亿，全网成交额433亿元。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;七、更多的技术&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;一个电子商务平台不可缺少的细节——商品类目的处理。因为商品的类目实在过于庞大，因此如何根据类目划分商品成为了难题。机智的一灯前辈说，这些属性可以当做标签，直接“贴”在商品上（应该是这样的吧）。&lt;/p&gt;
&lt;p&gt;2008年，淘宝将支付宝单独分离出来。其中交易的底层业务叫交易中心TC（Trade Center），涉及订单之类的原子操作。交易的上层业务叫做交易管理TM（Trade Manager），不涉及对物流的操作。&lt;/p&gt;
&lt;p&gt;于是，应运而生的，第二个堪称里程碑的项目——系统拆分 诞生了。这个正是我们在阿里圆桌会议上HR所说一位元老级员工做的——“给一架高速飞行的飞机换发动机”这么惊险的重构任务。这些组件分割难度非常之大，以至于那张复杂的逻辑图我实在看不懂……总之，淘宝中间件诞生了。&lt;/p&gt;
&lt;p&gt;HSF（高性能服务框架）：核心，外号好舒服。请参见作者的博文http://www.blogjava.net/BlueDavy/archive/2008/01/24/177533.html&lt;/p&gt;
&lt;p&gt;Notify（消息中间件）：淘宝自主开发的消息队列产品。支撑了10亿+的消息通知。&lt;/p&gt;
&lt;p&gt;TDDL（分布式数据访问层）：优化了DBRoute，在JDBC和DB之间隔了一层，负责数据库的优化工作。&lt;/p&gt;
&lt;p&gt;Tbsession：因为Session保存在服务器中，但是用户可能会被动的频繁的切换服务器，淘宝的设计思路是将Session信息保存在Cookie中，最后使用Tair来保存。&lt;/p&gt;
&lt;p&gt;阿里的开放平台也相当有历史，有兴趣的可以参观参观&lt;a title=&quot;http://open.taobao.com/index.htm&quot; href=&quot;http://open.taobao.com/index.htm&quot; target=&quot;_blank&quot;&gt;http://open.taobao.com/index.htm&lt;/a&gt;&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;八、总结&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;当你处于业界中流时，你可以向老大学习，等当你成为业界老大之后，你就需要不断超越自己，用自己的力量来改变整个行业，乃至整个世界。无论是华为，还是阿里，当成为业内的Top时，责任反而更加重大。&lt;/p&gt;
&lt;p&gt;一直觉得自己想着随大流，但是却又心有不甘。如今有机会能进入全中国最好的互联网网站，一直为自己这些年的付出感到荣幸，同时不断勉励自己，你需要变得更强才能融入这个集体。&lt;/p&gt;
&lt;p&gt;任重而道远，纵望阿里淘宝这些年的发展之路，那些默默无闻却勇于探索钻研的人是最可爱的，遇到问题永远不服输，总会有办法去解决的。正如阿里圆桌会议HR所说的“在座的各位都是爱折腾的人”，我承认自己受之有愧，自己的身体一直不能保证毫无顾忌的拼斗，自己虽然每天坚持都去跑步，底子还是不行，想要成为一名武林中人，更漫长的路需要我坚持的走下去，意志力，我可以有。&lt;/p&gt;
&lt;p&gt;坚持学习，钻研学习，实践学习。希望自己能坚持这三点信条。&lt;/p&gt;
&lt;p&gt;相当佩服马总的思想理念和为人处事，也相当佩服那么多实力不凡而又忠心耿耿的部下，他们对得起他们的身价。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;子柳兄的《&lt;a href=&quot;http://www.amazon.cn/gp/product/B00CK2H13K/ref=as_li_qf_sp_asin_il_tl?ie=UTF8&amp;amp;camp=536&amp;amp;creative=3200&amp;amp;creativeASIN=B00CK2H13K&amp;amp;linkCode=as2&amp;amp;tag=vastwork-23&quot; target=&quot;_blank&quot;&gt;淘宝技术这十年&lt;/a&gt;》到此总结完毕，我相信淘宝的光辉路程的还有很长，我的学问之路，也必将一直走下去。&lt;/strong&gt;&lt;/p&gt;


&lt;!-- div id=&quot;ad1&quot;&gt;
&lt;/div --&gt;


	


	

</description>
        <pubDate>Wed, 15 Oct 2014 00:00:00 +0800</pubDate>
        <link>http://iftti.com/posts/2014/2014-10-15-78376-d3e7350c5.html</link>
        <guid isPermaLink="true">http://iftti.com/posts/2014/2014-10-15-78376-d3e7350c5.html</guid>
        
        
        <category>jobbole</category>
        
      </item>
    
      <item>
        <title>超酷算法：基数估计</title>
        <description>

					
		
&lt;!-- div id=&quot;ad1&quot;&gt;
&lt;script type=&quot;text/javascript&quot;&gt;
google_ad_client = &quot;ca-pub-7056282119617872&quot;;
google_ad_slot = &quot;6645040531&quot;;
google_ad_width = 300;
google_ad_height = 250;
&lt;/script&gt;
&lt;script type=&quot;text/javascript&quot;
src=&quot;http://pagead2.googlesyndication.com/pagead/show_ads.js&quot;&gt;
&lt;/script&gt;
&lt;/div&gt;
&lt;br/ --&gt;

&lt;p&gt;假设你有一个很大的数据集，非常非常大，以至于不能全部存入内存。这个数据集中有重复的数据，你想找出有多少重复的数据，但数据并没有排序，由于数据量太大所以排序是不切实际的。你如何来估计数据集中含有多少无重复的数据呢？这在许多应用中是很有用的，比如数据库中的计划查询：最好的查询计划不仅仅取决于总共有多少数据，它也取决于它含有多少无重复的数据。&lt;/p&gt;
&lt;p&gt;在你继续读下去之前，我会引导你思考很多，因为今天我们要讨论的算法虽然很简单，但极具创意，它不是这么容易就能想出来的。&lt;/p&gt;
&lt;h3&gt;一个简单的朴素基数估计器&lt;/h3&gt;
&lt;p&gt;让我们从一个简单的例子开始吧。假定某人以下列方式来生成数据:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;生成 n 个充分分散的随机数&lt;/li&gt;
&lt;li&gt;任意地从中选择一些数字，使其重复某次&lt;/li&gt;
&lt;li&gt;打乱这些数字&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;我们怎么估计结果数据集中有多少非重复的数字呢？了解到原来的数据集是随机数,且充分分散，一个非常简单的方法是：找出最小的数字。如果最大的可能的数值是 m，最小的值是 x，我们 可以估计大概有 m/x 个非重复的数字在数据集里面。举个例子，如果我们扫描一个数字在 0 到 1 之间的数据集，发现最小的数字是 0.01。我们有理由猜想可能数据集里大概有 100 个非重复的数字。如果我们找到一个更小的最小值的话，可能包含的数据个数可能就更多了。请注意不管每个数字重复了多少次都没关系，这是很自然的，因为重复多少次并不会影响?&lt;code&gt;min&lt;/code&gt;?的输出值.&lt;/p&gt;
&lt;p&gt;这个过程的优点是非常直观，但同时它也很不精确。不难举出一个反例：一个只包含少数几个非重复数字的数据集里面有一个很小的数。同样的一个含有许多非重复数字的数据集含有一个比我们想像中更大的最小值，用这种估计方法也会很不精确。最后，很少有数据充分分散充分随机的数据集。但是这个算法原型给了我们一些灵感使得我们有可能达到我们的目的，我们需要更精致一些的算法.&lt;/p&gt;
&lt;h3&gt;基于概率的计数&lt;/h3&gt;
&lt;p&gt;第一处改进来来自 Flajolet 和 Martin 的论文&lt;a href=&quot;http://www.cse.unsw.edu.au/~cs9314/07s1/lectures/Lin_CS9314_References/fm85.pdf&quot;&gt; Probabilistic Counting Algorithms for Data Base Applications。&lt;/a&gt; 进一步的改进来自 Durand-Flajolet 的论文 &lt;a href=&quot;http://www.ic.unicamp.br/~celio/peer2peer/math/bitmap-algorithms/durand03loglog.pdf&quot;&gt;LogLog counting of large cardinalities&lt;/a&gt; 和 Flajolet et al 的论文 HyperLogLog：&lt;a href=&quot;http://algo.inria.fr/flajolet/Publications/FlFuGaMe07.pdf&quot;&gt;The analysis of a near-optimal cardinality estimation algorithm。&lt;/a&gt;从一篇论文到另一篇论文来观察想法的产生和改进很有趣，但我的方法稍有不同，我会演示如何从头开始构建并改善一个解决方法，省略了一些原始论文中的算法。有兴趣的读者可以读一下那三篇论文，论文里面包含了大量的数学知识，我这里不会详细探讨.&lt;/p&gt;
&lt;p&gt;首先，Flajolet 和 Martin 发现对于任意数据集，我们总可以给出一个好的哈希函数，使得哈希后的数据集可以是我们需要的任意一种排列。甚至充分分散的(伪)随机数也是如此。通过这个简单的灵感，我们可以把我们之前产生的数据集转化为我们想要的数据集，但是这远远还不够.&lt;/p&gt;
&lt;p&gt;接下来，他们发现存在更好的估计非重复数个数的方法。部分方法比记录最小的哈希值表现得更好。Flajolet 和 Martin 用的估计方法是计算哈希后的值的首部的 0 字的个数。显然在一个随机的数据集中，平均每 2^k 个元素就出现一个长度为 k 的全为 0 的比特序列。我们要做的就是找出这些序列并记录最长的来估计非重复元素的个数。然而这仍然不是一个很棒的估计器。它最多只能给我们一个 2 的幂的数量的估计。而且不像基于最小值的估计方法，这个方法的方差很大。但在另一个方面，我们的估计需要的空间非常小：为了记录最长 32 比特的前导 0 比特序列，我们只需要一个 5 比特的数字就可以了.&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;color: #888888;&quot;&gt;附注：Flajolet-Martin 原先的论文在这里继续讨论了一种基于 bitmap 的过程来获得一个更精确的估计。我不会讨论这个细节因为它马上就会在随后的方法中得到改进。更多细节对于有兴趣的读者可以阅读原论文。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;现在我们得到了一个确实比较糟糕的比特式估计方法。我们能做出一些什么改进呢？一个直接的想法是使用多个独立的哈希函数。如果每个哈希函数?输出它自己的随机数据集，我们可以记录最长的前导 0 比特序列。然后在最后我们就可以对其求一个平均值以得到一个更精确的估计。&lt;/p&gt;
&lt;p&gt;从实验统计上来看这给了我们一个相当好的结果，但哈希的代价的是很高的。一个更好的方式是一个叫做随机平均的方法。相比使用多个哈希函数，我们仅仅使用一个哈希函数。但是把它的输出进行分割然后使用它的一部分作为桶序号来放到许多桶中一个桶里去。假设我们需要 1024 个值，我们可以使用哈希函数的前 10 个比特值作为桶的序号，然后使用剩下的哈希值来计算前导 0 比特序列。这个方法并不会损失精确度，但是节省了大量的哈希计算.&lt;/p&gt;
&lt;p&gt;把我们目前学到的应用一下，这里有一个简单的实现。这和 Durand-Flajolet 的论文中的算法是等价的，为了实现方便和清晰所以我计算的是尾部的 0 比特序列。结果是完全等价的。&lt;/p&gt;
&lt;pre class=&quot;brush: python; gutter: true&quot;&gt;def trailing_zeroes(num):
  &quot;&quot;&quot;Counts the number of trailing 0 bits in num.&quot;&quot;&quot;
  if num == 0:
    return 32 # Assumes 32 bit integer inputs!
  p = 0
  while (num &amp;gt;&amp;gt; p) &amp;amp; 1 == 0:
    p += 1
  return p

def estimate_cardinality(values，k):
  &quot;&quot;&quot;Estimates the number of unique elements in the input set values.

  Arguments:
    values：An iterator of hashable elements to estimate the cardinality of.
    k：The number of bits of hash to use as a bucket number; there will be 2**k buckets.
  &quot;&quot;&quot;
  num_buckets = 2 ** k
  max_zeroes = [0] * num_buckets
  for value in values:
    h = hash(value)
    bucket = h &amp;amp; (num_buckets - 1) # Mask out the k least significant bits as bucket ID
    bucket_hash = h &amp;gt;&amp;gt; k
    max_zeroes[bucket] = max(max_zeroes[bucket]，trailing_zeroes(bucket_hash))
  return 2 ** (float(sum(max_zeroes)) / num_buckets) * num_buckets * 0.79402&lt;/pre&gt;
&lt;p&gt;这很漂亮就像我们描述的一样：我们保持一个计算前导(或尾部)0个数的数组，然后在最后对个数求平均值，如果我们的平均值是 x，我们的估计就是 2^x 乘以桶的个数。前面没有说到 的是这个魔术数 0.79402。数据统计表明我们的程序存在一个可预测的偏差，它会给出一个比实际更大的估计值。这个在 Durand-Flajolet 的论文中导出的魔术常数是用来修正这个偏差的。实际上这个数字随着使用的桶的个数(最大2^64)而发生变化，但是对于更多数目的桶数，它会收敛到我们上面用到的算法的估计数字。大量更多的信息请看完整的论文，包括那个魔术数是怎么导出的。&lt;/p&gt;
&lt;p&gt;这个程序给了我们一个非常好的估计，对于 m 个桶来说，平均错误率大概在 1.3/sqrt(m) 左右。所以1024个桶时()，我们大概会有 4% 的期望错误率。为了估计每篇最多 2^27 个数据的数据集每个桶仅需要 5 比特就够了。少于 1 kb 内存，这真的很赞(1024 * 5 = 5120，即 640 字节)!&lt;/p&gt;
&lt;p&gt;让我们在一些随机的数据上测试一下它:&lt;/p&gt;
&lt;pre class=&quot;brush: python; gutter: true&quot;&gt;&amp;gt;&amp;gt;&amp;gt; [100000/estimate_cardinality([random.random() for i in range(100000)]，10) for j in range(10)]
[0.9825616152548807，0.9905752876839672，0.979241749110407，1.050662616357679，0.937090578752079，0.9878968276629505，0.9812323203117748，1.0456960262467019，0.9415413413873975，0.9608567203911741]&lt;/pre&gt;
&lt;p&gt;结果不坏，一些估计超过 4% 的预期偏差，但总而言之结果都很好。如果你自己再尝试一遍这个实验，请注意：Python 内建的 hash() 函数将整数哈希为它们本身。导致运行像 estimate_cardinality(range(10000)，10) 这样的会给出偏差很大的结果，因为此时的 hash() 不是一个好的哈希函数。当然使用上述例子中的随机数是没有问题的.&lt;/p&gt;
&lt;h3&gt;改进准确度：SuperLogLog 和 HyperLogLog&lt;/h3&gt;
&lt;p&gt;虽然我们已经得到了一个非常好的估计，但它有可能做到更好。Durand 和 Flajolet 发现极端数值会很大地影响估计结果的准确度。通过在求平均前舍弃一些最大值，准确度可以得到提高。特别地，舍弃前 30% 大的桶，仅仅计算 70% 的桶的平均值，精确度可以用 1.30/sqrt(m) 提高到 1.05/sqrt(m)! 这意味着在我们之前的例子中，用 640 字节的状态，平均错误率从 4% 变成了大约 3.2%。但并没增加空间的使用.&lt;/p&gt;
&lt;p&gt;最后，Flajolet et al 的论文的贡献就是使用了一个不同类型的平均数。使用调和平均数而不是几何平均数。通过这么做，我们可以把错误率降到 1.04/sqrt(m)，同样不增加需要的空间。当然完整的算法要更复杂一点，因为它必须修正小的和大的基数误差。有兴趣的读者应该，可能你已经猜到了，就是去阅读完整的论文.&lt;/p&gt;
&lt;h3&gt;并行化&lt;/h3&gt;
&lt;p&gt;这些方案所共有的整齐性使得它们很容易就能并行化。多台机器可以独立地运行同样的哈希函数同样数目的桶。我们在最后只需要把结果结合起来，取每个算法实例中每个桶最大的值就可以了。这不仅很好实现，因为我们最多只需要传输不到 1kb 的数据就可以了，而且和在单台机器上运行的结果是完全一模一样的.&lt;/p&gt;
&lt;h3&gt;总结&lt;/h3&gt;
&lt;p&gt;就像我们刚刚讨论过的基数排序算法，使得有可能得到一个非重复数字个数的很好的估计。通常只用不到 1kb 空间。我们可以不依赖数据的种类而使用它，并且可以分布式地在多台机器上工作，机器间的协调和数据的传输达到最小。结果估计数可以用来做许多事情，比如流量监控(多少个独立IP访问过？)和数据库查询优化(我们应该排序然后归并呢还是构造一个哈希表呢？)。&lt;/p&gt;


&lt;!-- div id=&quot;ad1&quot;&gt;
&lt;/div --&gt;


	


	

</description>
        <pubDate>Wed, 15 Oct 2014 00:00:00 +0800</pubDate>
        <link>http://iftti.com/posts/2014/2014-10-15-78255-9fab71e7d.html</link>
        <guid isPermaLink="true">http://iftti.com/posts/2014/2014-10-15-78255-9fab71e7d.html</guid>
        
        
        <category>jobbole</category>
        
      </item>
    
      <item>
        <title>SACC-2014-小米-部署系统的前世今生</title>
        <description>

                &lt;p&gt;2014中国系统架构师大会上，小米系统运维部，分享的《部署系统的前世今生》相关ppt。&lt;/p&gt;
&lt;p&gt;PDF下载：&lt;/p&gt;
&lt;p&gt;&lt;span id=&quot;more-1633&quot;&gt;&lt;/span&gt;&lt;/p&gt;
&lt;a href=&quot;http://noops.me/wp-content/plugins/pdfjs-viewer-shortcode/web/viewer.php?file=http://noops.me/wp-content/uploads/2014/10/SACC2014-xiaomi-fangshaosen.pdf&amp;amp;download=true&amp;amp;print=false&amp;amp;openfile=false&quot;&gt;View Fullscreen&lt;/a&gt;&lt;br&gt;&lt;iframe width=&quot;100%;&quot; height=&quot;400px;&quot; src=&quot;http://noops.me/wp-content/plugins/pdfjs-viewer-shortcode/web/viewer.php?file=http://noops.me/wp-content/uploads/2014/10/SACC2014-xiaomi-fangshaosen.pdf&amp;amp;download=true&amp;amp;print=false&amp;amp;openfile=false&quot;&gt;&lt;/iframe&gt; 
            

</description>
        <pubDate>Sat, 11 Oct 2014 00:00:00 +0800</pubDate>
        <link>http://iftti.com/posts/2014/2014-10-11--p=1633-a5a872824.html</link>
        <guid isPermaLink="true">http://iftti.com/posts/2014/2014-10-11--p=1633-a5a872824.html</guid>
        
        
        <category>noops</category>
        
      </item>
    
      <item>
        <title>从源代码运行 Kibana 4</title>
        <description>

  
  &lt;div style=&quot;background-color: #FFF;&quot;&gt;
    &lt;p&gt;Kibana 4 发布了，出人意料的是提供的居然是一个 jar 包的运行方式。好在有源码可看，根据源码可以分析得知，v4 版其实是一个 angularjs 写的 kibana 配上一个 sinatra 写的 proxyserver。这么一来，我们也就知道怎么来从源代码运行 Kibana 4，而不是用 Java 启动了。&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;c&quot;&gt;# 安装 nodejs 和 npm 命令，仅用于下载依赖包，实际运行不需要&lt;/span&gt;
port install nodejs npm
&lt;span class=&quot;c&quot;&gt;# 下载 kibana 4 源码&lt;/span&gt;
git clone https://github.com/elasticsearch/kibana.git kibana4
&lt;span class=&quot;nb&quot;&gt;cd &lt;/span&gt;kibana4/
&lt;span class=&quot;c&quot;&gt;# 安装 bower 工具&lt;/span&gt;
npm install -g bower
&lt;span class=&quot;c&quot;&gt;# 读取目录中的 bower.json，&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# 依此下载所有 js 依赖库到其中定义的路径&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# src/kibana/bower_components 下&lt;/span&gt;
bower install
&lt;span class=&quot;nb&quot;&gt;cd &lt;/span&gt;src/server
&lt;span class=&quot;c&quot;&gt;# 安装 bundler 工具&lt;/span&gt;
gem install bundler
&lt;span class=&quot;c&quot;&gt;# 读取目录中的 Gemfile，&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# 依此安装所有的 RubyGem 依赖库&lt;/span&gt;
bundle install
&lt;span class=&quot;c&quot;&gt;# 安装 lessc 工具&lt;/span&gt;
npm install -g less
&lt;span class=&quot;c&quot;&gt;# kibana 4 源码中在导入 lesshat 的时候都没写具体路径，所以要切换到对应目录下执行&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;cd&lt;/span&gt; ../src/kibana/bower_components/lesshat/build
&lt;span class=&quot;c&quot;&gt;# 编译 kibana 内的 *.less 文件为 *.css 文件&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; i in &lt;span class=&quot;sb&quot;&gt;`&lt;/span&gt;find ../../.. -name &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;a-z&lt;span class=&quot;o&quot;&gt;]&lt;/span&gt;*.less&lt;span class=&quot;p&quot;&gt;|&lt;/span&gt;grep -v bower_components&lt;span class=&quot;sb&quot;&gt;`&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;do&lt;/span&gt;
    ../../../../../node_modules/.bin/lessc &lt;span class=&quot;nv&quot;&gt;$i&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;/.less/.css/&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;done&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# 进入代理服务器目录&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;cd&lt;/span&gt; ../../../../server/
&lt;span class=&quot;c&quot;&gt;# 启动 sinatra 服务器&lt;/span&gt;
./bin/initialize&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;这样就可以通过 “localhost:5601” 访问了。&lt;/p&gt;
&lt;p&gt;此外，Elasticsearch 集群的地址，在 &lt;code&gt;src/server/config/kibana.yml&lt;/code&gt; 中配置。注意里面的 &lt;code&gt;kibana-int&lt;/code&gt; 建议大家使用的时候改个名儿，不然万一跟你原先 kibana3 的混合在一起了就不好了。&lt;/p&gt;
&lt;p&gt;最后，如果你的集群版本低于 1.4.0.BETA1，也不要着急，其实目前代码并没有用上什么这个版本的特性，所以可以通过修改 &lt;code&gt;src/kibana/index.js&lt;/code&gt; 改变这个版本检测：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;--- a/src/kibana/index.js
+++ b/src/kibana/index.js
@@ -33,7 +33,7 @@ define(function (require) {
     // Use this for cache busting partials
     .constant(&#39;cacheBust&#39;, window.KIBANA_COMMIT_SHA)
     // The minimum Elasticsearch version required to run Kibana
-    .constant(&#39;minimumElasticsearchVersion&#39;, &#39;1.4.0.Beta1&#39;)
+    .constant(&#39;minimumElasticsearchVersion&#39;, &#39;1.1.0&#39;)
     // When we need to identify the current session of the app, ef shard preference
     .constant(&#39;sessionId&#39;, Date.now())
     // attach the route manager&#39;s known routes
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Kibana 4 的界面，改成了 Query -&amp;gt; Visual -&amp;gt; Dashboard 三个解耦层次。而且不再是固定的提供某种某种 panel，改成自己选择、拼接甚至书写 Aggr 聚合函数的方式来灵活的生成图表。可以说，对使用者的 ES 知识，要求更高了。&lt;/p&gt;
&lt;p&gt;后续如何发展，大家一起关注吧。&lt;/p&gt;
    &lt;hr&gt;
    
    &lt;hr&gt;
  &lt;!-- UY BEGIN --&gt;


&lt;!-- UY END --&gt;
  &lt;/div&gt;

</description>
        <pubDate>Fri, 10 Oct 2014 00:00:00 +0800</pubDate>
        <link>http://iftti.com/posts/2014/2014-10-10-run-kibana4-without-jar-bffff7f65.html</link>
        <guid isPermaLink="true">http://iftti.com/posts/2014/2014-10-10-run-kibana4-without-jar-bffff7f65.html</guid>
        
        
        <category>chenlinux</category>
        
      </item>
    
      <item>
        <title>【翻译】Elasticsearch 1.4.0 beta 1 发版日志</title>
        <description>

  
  &lt;div style=&quot;background-color: #FFF;&quot;&gt;
    &lt;p&gt;原文见：&lt;a href=&quot;http://www.elasticsearch.org/blog/elasticsearch-1-4-0-beta-released/&quot;&gt;http://www.elasticsearch.org/blog/elasticsearch-1-4-0-beta-released/&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;今天，我们很高兴公告基于 &lt;strong&gt;Lucene 4.10.1&lt;/strong&gt; 的 &lt;strong&gt;Elasticsearch 1.4.0.Beta1&lt;/strong&gt; 发布。你可以从这里下载并阅读完整的变更列表：&lt;a href=&quot;http://www.elasticsearch.org/downloads/1-4-0-Beta1&quot;&gt;Elasticsearch 1.4.0.Beta1&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;1.4.0 版的主题就是&lt;strong&gt;弹性&lt;/strong&gt;：让 Elasticsearch 比过去更稳定更可靠。当所有东西都按照它应该的样子运行的时候，就很容易变得可靠了。但是不在意料中的事情发生时，复杂的部分就来了：节点内存溢出，它们的性能被慢垃圾回收或者超重的 I/O 拖累，网络连接失败，或者数据传输不规律。&lt;/p&gt;
&lt;p&gt;这次 beta 版主要在三方面力图改善弹性：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;通过减少&lt;a href=&quot;#section&quot;&gt;内存使用&lt;/a&gt;提供更好的节点稳定性。&lt;/li&gt;
  &lt;li&gt;通过改进发现算法提供更好的&lt;a href=&quot;#section-1&quot;&gt;集群稳定性&lt;/a&gt;。&lt;/li&gt;
  &lt;li&gt;通过&lt;a href=&quot;#checksums&quot;&gt;checksums&lt;/a&gt;提供更好的数据损坏检测。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;分布式系统是复杂的。我们已经有一个广泛的测试套件，可以创建随机场景，模拟我们自己都没想过的条件。但是依然会有无限多在此范围之外的情况。1.4.0.Beta1 里已经包含了我们目前能做到的各种优化努力。真心期望大家在实际运用中测试这些变更，然后&lt;a href=&quot;https://github.com/elasticsearch/elasticsearch/issues&quot;&gt;告诉我们你碰到的问题&lt;/a&gt;。&lt;/p&gt;
&lt;h2 id=&quot;section&quot;&gt;内存管理&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;内存压力&lt;/li&gt;
  &lt;li&gt;swap (参见 &lt;a href=&quot;http://www.elasticsearch.org/guide/en/elasticsearch/reference/1.4/setup-configuration.html#setup-configuration-memory&quot;&gt;memory settings&lt;/a&gt;)&lt;/li&gt;
  &lt;li&gt;太大的 heaps&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;这次发版包括了一系列变更来提升内存管理，并由此提升节点稳定性：&lt;/p&gt;
&lt;h3 id=&quot;doc-values&quot;&gt;doc values&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;fielddata&lt;/em&gt; 是最主要的内存大户。为了让聚合、排序以及脚本访问字段值时更快速，我们会加载字段值到内存，并保留在内存中。内存的堆空间非常宝贵，所以内存里的数据需要使用复杂的压缩算法和微优化来完成每次计算。正常情况下这样会工作的很好，直到你的数据大小超过了堆空间大小。这个问题看起来可以通过添加更多节点的方式解决。不过通常来说，堆空间问题总是会在 CPU 和 I/O 之前先到达瓶颈。&lt;/p&gt;
&lt;p&gt;现有版本已经添加了 doc values 支持。本质上，doc values 提供了和内存中 fielddata 一样的功能，不过他们在写入索引的时候就直接落到了磁盘上。而好处就是：他们&lt;strong&gt;消耗很少的堆空间&lt;/strong&gt;。Doc values 在读取的时候也不是从内存，而是从磁盘上读取。虽然访问磁盘很慢，但是 doc values 可以利用内核的文件系统缓存。文件系统缓存可不像 JVM 的堆，不会有 32GB 的限制。所以把 fielddata 从堆转移到文件系统缓存里，你只用消耗更小的堆空间，也意味着更快的垃圾回收，以及&lt;strong&gt;更稳定的节点&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;在本次发版之前，doc values 明显慢于在内存里的 fielddata 。而这次我们显著提升了性能，几乎达到了和在内存里一样快的效果。&lt;/p&gt;
&lt;p&gt;用doc values 替换内存 fielddata，你只需要向下面这样构建新字段就行：&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-json&quot; data-lang=&quot;json&quot;&gt;&lt;span class=&quot;err&quot;&gt;PUT&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;/my_index&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;nt&quot;&gt;&quot;mappings&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;&quot;my_type&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
      &lt;span class=&quot;nt&quot;&gt;&quot;properties&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;nt&quot;&gt;&quot;timestamp&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
          &lt;span class=&quot;nt&quot;&gt;&quot;type&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;       &lt;span class=&quot;s2&quot;&gt;&quot;date&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
          &lt;span class=&quot;nt&quot;&gt;&quot;doc_values&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;kc&quot;&gt;true&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
      &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;有了这个映射表，要用这个字段数据都会自动从磁盘加载 doc values 而不是进到内存里。&lt;strong&gt;注意&lt;/strong&gt;：目前 doc values 还不能在经过分词器的 &lt;code&gt;string&lt;/code&gt; 字段上使用。&lt;/p&gt;
&lt;h3 id=&quot;request-circuit-breaker&quot;&gt;request circuit breaker&lt;/h3&gt;
&lt;p&gt;fielddata 断路器之前已经被加入，用作限制 fielddata 可用的最大内存，这是导致 OOM 的最大恶因。而限制，我们把这个机制扩展到&lt;a href=&quot;http://www.elasticsearch.org/guide/en/elasticsearch/reference/1.4/index-modules-fielddata.html#request-circuit-breaker&quot;&gt;请求界别&lt;/a&gt;，用来限制每次请求可用的最大内存。&lt;/p&gt;
&lt;h3 id=&quot;bloom-filters&quot;&gt;bloom filters&lt;/h3&gt;
&lt;p&gt;&lt;a href=&quot;http://en.wikipedia.org/wiki/Bloom_filter&quot;&gt;Bloom filters&lt;/a&gt; 在写入索引是提供了重要的性能优化 – 用以检查是否有已存在的文档 id ，在通过 id 访问文档时，用来探测哪个 segment 包含这个文档。不过当然的，这也有代价，就是内存消耗。目前的改进是移除了对 bloom filters 的依赖。目前 Elasticsearch 只在写入索引(仅是真实用例上的经验，没有我们的测试用例证明)的时候构建它，但&lt;a href=&quot;http://www.elasticsearch.org/guide/en/elasticsearch/reference/1.4/indices-update-settings.html#codec-bloom-load&quot;&gt;默认&lt;/a&gt;不再加载进内存。如果一切顺利的话，未来的版本里我们会彻底移除它。&lt;/p&gt;
&lt;h2 id=&quot;section-1&quot;&gt;集群稳定性&lt;/h2&gt;
&lt;p&gt;提高集群稳定性最大的工作就是提高节点稳定性。如果节点稳定且响应及时，就极大的减少了集群不稳定的可能。换句话说，我们活在一个不完美的世界 – 事情总是往意料之外发展，而集群就需要能无损的从这些情况中恢复回来。&lt;/p&gt;
&lt;p&gt;我们在 improve_zen 分支上花了几个月的时间来提高 Elasticsearch 从失败中恢复的能力。首先，我们添加测试用例来复原复杂的网络故障。然后为每个测试用例添加补丁。肯定还有很多需要做的，不过目前来说，用户们已经碰到过的绝大多数问题我们已经解决了，包括&lt;a href=&quot;https://github.com/elasticsearch/elasticsearch/issues/2488&quot;&gt;issue #2488&lt;/a&gt; – “minimum_master_nodes 在交叉脑裂时不起作用”。&lt;/p&gt;
&lt;p&gt;我们非常认真的对待集群的弹性问题。希望你能明白 Elasticsearch 能为你做什么，也能明白它的弱点在哪。考虑到这点，我们创建了&lt;a href=&quot;http://www.elasticsearch.org/guide/en/elasticsearch/resiliency/current/index.html&quot;&gt;弹性状态文档&lt;/a&gt;。这个文档记录了我们以及我们的用户碰到过各种弹性方面的问题，有些可能已经修复，有些可能还没有。请认真阅读这篇文档，采取适当的措施来保护你的数据。&lt;/p&gt;
&lt;h2 id=&quot;section-2&quot;&gt;数据损坏探测&lt;/h2&gt;
&lt;p&gt;从网络恢复过来的分片的 checksum 帮助我们发现过一个压缩库的 bug，这是 1.3.2 版本的时候发生的事情。从那天起，我们给 Elasticsearch 添加了越来越多的 checksum 认证。&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;在合并时，segment 中的所有文件都有自己的 checksum 验证(&lt;a href=&quot;https://github.com/elasticsearch/elasticsearch/issues/7360&quot;&gt;#7360&lt;/a&gt;).&lt;/li&gt;
  &lt;li&gt;重新开所有索引的时候，segment 里的小文件完整的验证，大文件则做轻量级的分段验证(&lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-5842&quot;&gt;LUCENE-5842&lt;/a&gt;).&lt;/li&gt;
  &lt;li&gt;从 transaction 日志重放事件的时候，每个事件都有自己的 checksum 验证(&lt;a href=&quot;https://github.com/elasticsearch/elasticsearch/issues/6554&quot;&gt;#6554&lt;/a&gt;).&lt;/li&gt;
  &lt;li&gt;During shard recovery, or when restoring from a snapshot, Elasticsearch needs to compare a local file with a remote copy to ensure that they are identical. Using just the file length and checksum proved to be insufficient. Instead, we now check the identity of all the files in the segment (&lt;a href=&quot;https://github.com/elasticsearch/elasticsearch/issues/7159&quot;&gt;#7159&lt;/a&gt;).&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;section-3&quot;&gt;其他亮点&lt;/h2&gt;
&lt;p&gt;你可以在 &lt;a href=&quot;http://www.elasticsearch.org/downloads/1-4-0-Beta1&quot;&gt;Elasticsearch 1.4.0.Beta1 changelog&lt;/a&gt; 里读到这个版本的所有特性，功能和修复。不过还是有些小改动值得单独提一下的：&lt;/p&gt;
&lt;h3 id=&quot;groovy--mvel&quot;&gt;groovy 代替了 mvel&lt;/h3&gt;
&lt;p&gt;Groovy 现在成为了新的默认脚本语言。之前的 MVEL 太老了，而且它不能运行在沙箱里也带来了安全隐患。Groovy 是沙箱化的(这意味着可以放心的开启)(译者注：还记得1.2版本时候的所谓安全漏洞吧)，而且 Groovy 有个很好的管理团队，运行速度也&lt;strong&gt;很快&lt;/strong&gt;！更多信息见&lt;a href=&quot;http://www.elasticsearch.org/blog/scripting/&quot;&gt;博客关于脚本的内容&lt;/a&gt;。&lt;/p&gt;
&lt;h3 id=&quot;cors&quot;&gt;默认关闭 cors&lt;/h3&gt;
&lt;p&gt;默认配置下的 Elasticsearch 很容易遭受跨站攻击。所以我们默认关闭掉 &lt;a href=&quot;http://en.wikipedia.org/wiki/Cross-origin_resource_sharing&quot;&gt;CORS&lt;/a&gt;。Elasticsearch 里的 site 插件会照常工作，但是外部站点不再被允许访问远程集群，除非你再次打开 CORS。我们还添加了更多的&lt;a href=&quot;http://www.elasticsearch.org/guide/en/elasticsearch/reference/1.4/modules-http.html#_settings_2&quot;&gt;CORS 配置项&lt;/a&gt;让你可以控制哪些站点可以被允许访问。更多信息请看我们的&lt;a href=&quot;http://www.elasticsearch.org/community/security&quot;&gt;安全页&lt;/a&gt;。&lt;/p&gt;
&lt;h3 id=&quot;query-cache&quot;&gt;请求缓存(query cache)&lt;/h3&gt;
&lt;p&gt;一个新的实验性&lt;a href=&quot;http://www.elasticsearch.org/guide/en/elasticsearch/reference/1.4/index-modules-shard-query-cache.html&quot;&gt;分片层次的请求缓存&lt;/a&gt;可以让在静态索引上的聚合请求瞬间返回响应。想想你有一个仪表板展示你的网站每天的 PV 数。这个书在过去的索引上不可能再变化了，但是聚合请求在每次页面刷新的时候都需要重新计算。有了新的请求缓存，聚合结果就可以直接从缓存中返回，除非分片中的数据发生了变化。你不用担心会从缓存中得到过期的结果 – 它永远都会跟没缓存一样。&lt;/p&gt;
&lt;h3 id=&quot;section-4&quot;&gt;新的聚合函数&lt;/h3&gt;
&lt;p&gt;我们添加了三个新的聚合函数：&lt;/p&gt;
&lt;p&gt;&lt;code&gt;filters&lt;/code&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;这是 `filter` 聚合的扩展。允许你定义多个桶(bucket)，每个桶里有不同的过滤器。
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;children&lt;/code&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;相当于 `nested` 的父子聚合，`children` 可以针对属于某个父文档的子文档做聚合。
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;scripted_metric&lt;/code&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;给你完全掌控数据数值运算的能力。提供了在初始化、文档收集、分片层次合并，以及全局归并阶段的钩子。
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&quot;index-&quot;&gt;获取 /index 的接口&lt;/h3&gt;
&lt;p&gt;之前，你可以分别为一个索引获取他的别名，映射表，配置等等。而&lt;a href=&quot;http://www.elasticsearch.org/guide/en/elasticsearch/reference/1.4/indices-get-index.html&quot;&gt;&lt;code&gt;get-index&lt;/code&gt; 接口&lt;/a&gt; 现在让你可以一次获取一个或者多个索引的全部信息。这在你需要创建一个跟已有索引很类似或者几乎一样的新索引的时候，相当有用。&lt;/p&gt;
&lt;h3 id=&quot;section-5&quot;&gt;索引写入和更新&lt;/h3&gt;
&lt;p&gt;在文档写入和更新方面也有一些改进：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;我们现在用 &lt;a href=&quot;http://boundary.com/blog/2012/01/12/flake-a-decentralized-k-ordered-unique-id-generator-in-erlang&quot;&gt;Flake IDs&lt;/a&gt; 自动生成文档的 ID。在查找主键的时候，能提供更好的性能。&lt;/li&gt;
  &lt;li&gt;如果设置 &lt;code&gt;detect_noop&lt;/code&gt; 为 &lt;code&gt;true&lt;/code&gt;，一个不做任何实际变动的更新操作现在消耗更小了。打开这个参数，就只有变更了 &lt;code&gt;_source&lt;/code&gt; 字段内容的更新请求才能写入新版本文档。&lt;/li&gt;
  &lt;li&gt;更新操作可以完全由脚本控制。之前，脚本只能在字段已经存在的时候运行，否则会插入一个 &lt;code&gt;upsert&lt;/code&gt; 文档。现在 &lt;code&gt;scripted_upsert&lt;/code&gt; 参数允许你在脚本中直接处理文档创建工作。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;function-score&quot;&gt;function score&lt;/h3&gt;
&lt;p&gt;非常有用的 &lt;a href=&quot;http://www.elasticsearch.org/guide/en/elasticsearch/reference/1.4/query-dsl-function-score-query.html&quot;&gt;&lt;code&gt;function_score&lt;/code&gt; 请求&lt;/a&gt;现在支持权重参数，用来优化每个指定函数的相关性影响。这样你可以把更多权重给新近的而不是热点的，给价格而不是位置。此外，&lt;code&gt;random_score&lt;/code&gt;函数不再被 segment 合并影响，增强了排序一致性。&lt;/p&gt;
&lt;h2 id=&quot;section-6&quot;&gt;试一试&lt;/h2&gt;
&lt;p&gt;请&lt;a href=&quot;http://www.elasticsearch.org/downloads/1-4-0-Beta1&quot;&gt;下载 Elasticsearch 1.4.0.Beta1&lt;/a&gt;，尝试一下，然后在 Twitter 上&lt;a href=&quot;https://twitter.com/elasticsearch&quot;&gt;@elasticsearch&lt;/a&gt;) 说出你的想法。你也可以在 &lt;a href=&quot;https://github.com/elasticsearch/elasticsearch/issues&quot;&gt;GitHub issues 页&lt;/a&gt;上报告问题。&lt;/p&gt;
    &lt;hr&gt;
    
    &lt;hr&gt;
  &lt;!-- UY BEGIN --&gt;


&lt;!-- UY END --&gt;
  &lt;/div&gt;

</description>
        <pubDate>Fri, 10 Oct 2014 00:00:00 +0800</pubDate>
        <link>http://iftti.com/posts/2014/2014-10-10-elasticsearch-1-4-beta-1-released-b51ecbab3.html</link>
        <guid isPermaLink="true">http://iftti.com/posts/2014/2014-10-10-elasticsearch-1-4-beta-1-released-b51ecbab3.html</guid>
        
        
        <category>chenlinux</category>
        
      </item>
    
      <item>
        <title>互联网全站HTTPS的时代已经到来</title>
        <description>

					
		
&lt;!-- div id=&quot;ad1&quot;&gt;
&lt;script type=&quot;text/javascript&quot;&gt;
google_ad_client = &quot;ca-pub-7056282119617872&quot;;
google_ad_slot = &quot;6645040531&quot;;
google_ad_width = 300;
google_ad_height = 250;
&lt;/script&gt;
&lt;script type=&quot;text/javascript&quot;
src=&quot;http://pagead2.googlesyndication.com/pagead/show_ads.js&quot;&gt;
&lt;/script&gt;
&lt;/div&gt;
&lt;br/ --&gt;

&lt;h1&gt;前言&lt;/h1&gt;
&lt;p&gt;我目前正在从事HTTPS方面的性能优化工作。在HTTPS项目的开展过程中明显感觉到目前国内互联网对HTTPS并不是很重视，其实也就是对用户隐私和网络安全不重视。本文从保护用户隐私的角度出发，简单描述现在存在的用户隐私泄露和流量劫持现象，然后进一步说明为什么HTTPS能够保护用户安全以及HTTPS使用过程中需要注意的地方。&lt;/p&gt;
&lt;p&gt;国外很多网站包括Google、Facebook、Twitter都支持了全站HTTPS，而国内目前还没有一家大型网站全站支持HTTPS（PC端的微信全部使用了HTTPS，但是PC端用户应该不多）。甚至一些大型网站明显存在很多HTTPS使用不规范或者过时的地方。比如支付宝使用的是tls1.0和RC4,而京东(quickpay.jd.com)竟然还使用着SSL3.0这个早就不安全并且性能低下的协议，其他很多网站的HTTPS登陆页面也存在着不安全的HTTP链接，这个也为黑客提供了可乘之机。&lt;/p&gt;
&lt;p&gt;由于篇幅关系，文中几乎没有详细描述任何细节，后面有时间我再一一整理成博客发表出来。同时由于水平有限，本文肯定存在很多错误，希望大家不吝赐教。本文的大部分内容都能从互联网上搜索到，有些地方我也标明了引用，可以直接跳转过去。但是全文都是在我自己理解的基础上结合开发部署过程中的一些经验和测试数据一个字一个字敲出来的，最后决定将它们分享出来的原因是希望能和大家多多交流，抛砖引玉，共同推进中国互联网的HTTPS发展。&lt;/p&gt;
&lt;p&gt;本文不会科普介绍HTTPS、TLS及PKI，如果遇到一些基本概念文中只是提及而没有描述，请大家自行百度和google。本文重点是想告诉大家HTTPS没有想像中难用可怕，只是没有经过优化。&lt;/p&gt;
&lt;p&gt;中国互联网全站使用HTTPS的时代已经到来。&lt;/p&gt;
&lt;h1&gt;
&lt;a name=&quot;t1&quot;&gt;&lt;/a&gt;1，用户隐私泄露的风险很大&lt;/h1&gt;
&lt;p&gt;人们的生活现在已经越来越离不开互联网，不管是社交、购物还是搜索，互联网都能带给人们很多的便捷。与此同时，用户“裸露”在互联网的信息也越来越多，另一个问题也日益严重，那就是隐私和安全。&lt;/p&gt;
&lt;p&gt;几乎所有的互联网公司都存在用户隐私泄露和流量劫持的风险。BAT树大招风，这方面的问题尤其严重。比如用户在百度搜索一个关键词，“人流”，很快就会有医院打电话过来推销人流手术广告，不知情的用户还以为是百度出卖了他的手机号和搜索信息。同样地，用户在淘宝搜索的关键词也很容易被第三方截获并私下通过电话或者其他广告形式骚扰用户。而QQ和微信呢，显然用户不希望自己的聊天内容被其他人轻易知道。为什么BAT不可能出卖用户隐私信息给第三方呢？因为保护用户隐私是任何一个想要长期发展的互联网公司的安身立命之本，如果用户发现使用一个公司的产品存在严重的隐私泄露问题，显然不会再信任该公司的产品，最终该公司也会因为用户大量流失而陷入危机。所以任何一家大型互联网公司都不可能因为短期利益而出卖甚至忽视用户隐私。&lt;/p&gt;
&lt;p&gt;那既然互联网公司都知道用户隐私的重要性，是不是用户隐私就得到了很好的保护呢？现实却并不尽如人意。由于目前的WEB应用和网站绝大部分是基于HTTP协议，国内没有任何一家大型互联网公司采用全站HTTPS方案来保护用户隐私（排除支付和登陆相关的网站或者页面以及PC端的微信）。因为HTTP协议简单方便，易于部署，并且设计之初也没有考虑安全性，所有内容都是明文传输，也就为现在的安全问题埋下了隐患。用户在基于HTTP协议的WEB应用上的传输内容都可以被中间者轻易查看和修改。&lt;/p&gt;
&lt;p&gt;比如你在百度搜索了一个关键词“https“，中间者通过tcpdump或者wireshark等工具就很容易知道发送请求的全部内容。wireshark的截图如下：&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/904e130489fdd04df18787cdc28febb8.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;这里所谓的中间者是指网络传输内容需要经过的网络节点，既有硬件也有软件，比如中间代理服务器、路由器、小区WIFI热点、公司统一网关出口等。这里面最容易拿到用户内容的就是各种通信服务运营商和二级网络带宽提供商。而最有可能被第三方黑客动手脚的就是离用户相对较近的节点。&lt;/p&gt;
&lt;p&gt;中间者为什么要查看或者修改用户真实请求内容呢？很简单，为了利益。常见的几种危害比较大的中间内容劫持形式如下：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;获取无线用户的手机号和搜索内容并私下通过电话广告骚扰用户。为什么能够获取用户手机号？呵呵，因为跟运营商有合作。&lt;/li&gt;
&lt;li&gt;获取用户帐号cookie，盗取帐号有用信息。&lt;/li&gt;
&lt;li&gt;在用户目的网站返回的内容里添加第三方内容，比如广告、钓鱼链接、植入木马等。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;总结来讲，由于HTTP明文传输，同时中间内容劫持的利益巨大，所以用户隐私泄露的风险非常高。&lt;/p&gt;
&lt;h1&gt;
&lt;a name=&quot;t2&quot;&gt;&lt;/a&gt;2，HTTPS能有效保护用户隐私&lt;/h1&gt;
&lt;p&gt;HTTPS就等于HTTP加上TLS（SSL)，HTTPS协议的目标主要有三个：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;数据保密性。保证内容在传输过程中不会被第三方查看到。就像快递员传递包裹时都进行了封装，别人无法知道里面装了什么东西。&lt;/li&gt;
&lt;li&gt;数据完整性。及时发现被第三方篡改的传输内容。就像快递员虽然不知道包裹里装了什么东西，但他有可能中途掉包，数据完整性就是指如果被掉包，我们能轻松发现并拒收。&lt;/li&gt;
&lt;li&gt;身份校验。保证数据到达用户期望的目的地。就像我们邮寄包裹时，虽然是一个封装好的未掉包的包裹，但必须确定这个包裹不会送错地方。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;通俗地描述上述三个目标就是封装加密，防篡改掉包，防止身份冒充，那TLS是如何做到上述三点的呢？我分别简述一下。&lt;/p&gt;
&lt;h2&gt;
&lt;a name=&quot;t3&quot;&gt;&lt;/a&gt;2.1 数据保密性&lt;/h2&gt;
&lt;h3&gt;
&lt;a name=&quot;t4&quot;&gt;&lt;/a&gt;2.1.1 非对称加密及密钥交换&lt;/h3&gt;
&lt;p&gt;数据的保密性主要是通过加密完成的。加密算法一般分为两种，一种是非对称加密（也叫公钥加密），另外一种是对称加密（也叫密钥加密）。所谓非对称加密就是指加密和解密使用的密钥不一样，如下图：&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/b6162e52fe83d9bc6f9abc5a25314ae6.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;HTTPS使用非对称加解密主要有两个作用，一个是密钥协商，另外可以用来做数字签名。所谓密钥协商简单说就是根据双方各自的信息计算得出双方传输内容时对称加解密需要使用的密钥。&lt;/p&gt;
&lt;p&gt;公钥加密过程一般都是服务器掌握私钥，客户端掌握公钥，私钥用来解密，公钥用来加密。公钥可以发放给任何人知道，但是私钥只有服务器掌握，所以公钥加解密非常安全。当然这个安全性必须建立在公钥长度足够大的基础上，目前公钥最低安全长度也需要达到2048位。大的CA也不再支持2048位以下的企业级证书申请。因为1024位及以下的公钥长度已经不再安全，可以被高性能计算机比如量子计算机强行破解。计算性能基本会随着公钥的长度而呈2的指数级下降。&lt;/p&gt;
&lt;p&gt;既然如此为什么还需要对称加密？为什么不一直使用非对称加密算法来完成全部的加解密过程？主要是两点：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;非对称加解密对性能的消耗非常大，一次完全TLS握手，密钥交换时的非对称解密计算量占整个握手过程的95%。而对称加密的计算量只相当于非对称加密的0.1%，如果应用层数据也使用非对称加解密，性能开销太大，无法承受。&lt;/li&gt;
&lt;li&gt;非对称加密算法对加密内容的长度有限制，不能超过公钥长度。比如现在常用的公钥长度是2048位，意味着待加密内容不能超过256个字节。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;目前常用的非对称加密算法是RSA，想强调一点的就是RSA是整个PKI体系及加解密领域里最重要的算法。如果想深入理解HTTPS的各个方面，RSA是必需要掌握的知识点。它的原理主要依赖于三点：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;乘法的不可逆特性。即我们很容易由两个乘数求出它们的积，但是给定一个乘积，很难求出它是由哪两个乘数因子相乘得出的。&lt;/li&gt;
&lt;li&gt;欧拉函数。欧拉函数&lt;img alt=&quot;\varphi(n)&quot; src=&quot;/images/jobbole.com/f0eca0594cd81ea9af596f13ed39dbde.jpg&quot;&gt;是小于或等于n的正整数中与n&lt;a title=&quot;互质&quot; href=&quot;http://zh.wikipedia.org/wiki/%E4%BA%92%E8%B3%AA&quot; target=&quot;_blank&quot;&gt;互质&lt;/a&gt;的数的数目&lt;/li&gt;
&lt;li&gt;费马小定理。假如a是一个整数，p是一个质数，那么&lt;img alt=&quot;a^p - a &quot; src=&quot;/images/jobbole.com/da3636ba68ef373813464ce5ae621d55.jpg&quot;&gt;是p的倍数。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;这篇中文博客对RSA的原理解释得比较清楚易懂：&lt;a href=&quot;http://www.ruanyifeng.com/blog/2013/06/rsa_algorithm_part_one.html&quot; target=&quot;_blank&quot;&gt;RSA算法原理&lt;/a&gt;。&lt;br&gt;
RSA算法是第一个也是目前唯一一个既能用于密钥交换又能用于数字签名的算法。另外一个非常重要的密钥协商算法是diffie-hellman(DH).DH不需要预先知道通信双方的信息就能完成密钥的协商，它使用一个素数P的整数乘法群以及原根G，理论依据就是离散对数。&lt;/p&gt;
&lt;p&gt;openssl目前只支持如下密钥交换算法：RSA，DH，ECDH, DHE，ECDHE。各个算法的性能和对速度的影响可以参考后面章节，由于篇幅有限，具体实现不再做详细介绍。&lt;/p&gt;
&lt;h3&gt;
&lt;a name=&quot;t5&quot;&gt;&lt;/a&gt;2.1.2 对称加密&lt;/h3&gt;
&lt;p&gt;对称加密就是加密和解密都使用的是同一个密钥。如下图：&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/0368b27387610a8f010cc7b0dfc42ab9.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;采用非对称密码算法的密钥协商过程结束之后就已经得出了本次会话需要使用的对称密钥。对称加密又分为两种模式：流式加密和分组加密。流式加密现在常用的就是RC4，不过&lt;a href=&quot;http://en.wikipedia.org/wiki/Transport_Layer_Security#RC4_attacks&quot; target=&quot;_blank&quot;&gt;RC4已经不再安全&lt;/a&gt;，微软也&lt;a href=&quot;http://blogs.technet.com/b/srd/archive/2013/11/12/security-advisory-2868725-recommendation-to-disable-rc4.aspx&quot; target=&quot;_blank&quot;&gt;建议网站尽量不要使用RC4流式加密&lt;/a&gt;。支付宝可能没有意识到这一点，也可能是由于其他原因，他们仍然在使用RC4算法和TLS1.0协议。&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/8b0846a1fc9e22f49c74063e1a042a64.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;一种新的替代RC4的流式加密算法叫ChaCha20，它是google推出的速度更快，更安全的加密算法。目前已经被android和chrome采用，也编译进了google的开源openssl分支—boring ssl，并且&lt;a href=&quot;http://forum.nginx.org/read.php?27,252308,252308#msg-252308&quot; target=&quot;_blank&quot;&gt;nginx 1.7.4也支持编译boringssl&lt;/a&gt;。我目前还没有比较这种算法的性能，但部分资料显示这个算法对性能的消耗比较小，特别是移动端提升比较明显。&lt;/p&gt;
&lt;p&gt;分组加密以前常用的模式是AES-CBC，但是CBC已经被证明容易遭受&lt;a href=&quot;http://resources.infosecinstitute.com/beast-vs-crime-attack/&quot; target=&quot;_blank&quot;&gt;BEAST&lt;/a&gt;和&lt;a href=&quot;http://www.isg.rhul.ac.uk/tls/TLStiming.pdf&quot; target=&quot;_blank&quot;&gt;LUCKY13攻击&lt;/a&gt;。目前建议使用的分组加密模式是AES-GCM，不过它的缺点是计算量大，性能和电量消耗都比较高，不适用于移动电话和平板电脑。尽管如此，它仍然是我们的优先选择。&lt;/p&gt;
&lt;h2&gt;
&lt;a name=&quot;t6&quot;&gt;&lt;/a&gt;2.2 数据完整性&lt;/h2&gt;
&lt;p&gt;这部分内容相对比较简单，openssl现在使用的完整性校验算法有两种：MD5或者SHA。由于MD5在实际应用中存在冲突的可能性比较大，所以尽量别采用MD5来验证内容一致性。SHA也不能使用SHA0和SHA１，中国山东大学的王小云教授在2005年就牛逼地宣布&lt;a href=&quot;http://en.wikipedia.org/wiki/Wang_Xiaoyun&quot; target=&quot;_blank&quot;&gt;破解了SHA-1完整版算法&lt;/a&gt;。建议使用SHA２算法，即输出的摘要长度超过224位。&lt;/p&gt;
&lt;h2&gt;2.3 身份验证和授权&lt;/h2&gt;
&lt;p&gt;这里主要介绍的就是PKI和数字证书。数字证书有两个作用：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;身份验证。确保客户端访问的网站是经过CA验证的可信任的网站。&lt;/li&gt;
&lt;li&gt;分发公钥。每个数字证书都包含了注册者生成的公钥。在SSL握手时会通过certificate消息传输给客户端。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;这里简单介绍一下数字证书是如何验证网站身份的。&lt;/p&gt;
&lt;p&gt;证书申请者首先会生成一对密钥，包含公钥和密钥，然后把公钥及域名还有CU等资料制作成CSR格式的请求发送给RA，RA验证完这些内容之后（RA会请独立的第三方机构和律师团队确认申请者的身份）再将CSR发送给CA，CA然后制作X.509格式的证书。&lt;/p&gt;
&lt;p&gt;那好，申请者拿到CA的证书并部署在网站服务器端，那浏览器访问时接收到证书后，如何确认这个证书就是CA签发的呢？怎样避免第三方伪造这个证书？&lt;/p&gt;
&lt;p&gt;答案就是数字签名（digital signature）。数字签名可以认为是一个证书的防伪标签，目前使用最广泛的SHA-RSA数字签名的制作和验证过程如下：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;数字签名的签发。首先是使用哈希函数对证书数据哈希，生成消息摘要，然后使用CA自己的私钥对证书内容和消息摘要进行加密。&lt;/li&gt;
&lt;li&gt;数字签名的校验。使用CA的公钥解密签名，然后使用相同的签名函数对证书内容进行签名并和服务端的数字签名里的签名内容进行比较，如果相同就认为校验成功。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;图形表示如下：&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/df6566e0053cf8c745dc2657ab3b3b5f.jpg&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/a51e8284cc27cb03f0d17a779ad7a5c7.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;这里有几点需要说明：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;数字签名签发和校验使用的密钥对是CA自己的公私密钥，跟证书申请者提交的公钥没有关系。&lt;/li&gt;
&lt;li&gt;数字签名的签发过程跟公钥加密的过程刚好相反，即是用私钥加密，公钥解密。&lt;/li&gt;
&lt;li&gt;现在大的CA都会有证书链，证书链的好处一是安全，保持根CA的私钥离线使用。第二个好处是方便部署和撤销，即如何证书出现问题，只需要撤销相应级别的证书，根证书依然安全。&lt;/li&gt;
&lt;li&gt;根CA证书都是自签名，即用自己的公钥和私钥完成了签名的制作和验证。而证书链上的证书签名都是使用上一级证书的密钥对完成签名和验证的。&lt;/li&gt;
&lt;li&gt;怎样获取根CA和多级CA的密钥对？它们是否可信？当然可信，因为这些厂商跟浏览器和操作系统都有合作，它们的公钥都默认装到了浏览器或者操作系统环境里。比如&lt;a href=&quot;http://mxr.mozilla.org/mozilla-central/source/security/nss/lib/ckfw/builtins/certdata.txt&quot; target=&quot;_blank&quot;&gt;firefox就自己维护了一个可信任的CA列表&lt;/a&gt;，而&lt;a href=&quot;http://support2.microsoft.com/kb/931125&quot; target=&quot;_blank&quot;&gt;chrome和IE使用的是操作系统的CA列表&lt;/a&gt;。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;数字证书的费用其实也不高，对于中小网站可以使用&lt;a href=&quot;http://www.startssl.com/&quot; target=&quot;_blank&quot;&gt;便宜甚至免费的数字证书服务&lt;/a&gt;（可能存在安全隐患），像著名的verisign公司的证书一般也就几千到几万块一年不等。当然如果公司对证书的需求比较大，定制性要求高，可以建立自己的CA站点，比如google，能够随意签发google相关证书。&lt;/p&gt;
&lt;h1&gt;
&lt;a name=&quot;t8&quot;&gt;&lt;/a&gt;3，HTTPS对速度和性能的影响&lt;/h1&gt;
&lt;p&gt;既然HTTPS非常安全，数字证书费用也不高，那为什么互联网公司不全部使用HTTPS呢？原因主要有两点：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;HTTPS对速度的影响非常明显。每个HTTPS连接一般会增加1-3个RTT，加上加解密对性能的消耗，延时还有可能再增加几十毫秒。&lt;/li&gt;
&lt;li&gt;HTTPS对CPU计算能力的消耗很严重，完全握手时，web server的处理能力会降低至HTTP的10%甚至以下。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;下面简单分析一下这两点。&lt;/p&gt;
&lt;h2&gt;
&lt;a name=&quot;t9&quot;&gt;&lt;/a&gt;3.1 HTTPS对访问速度的影响&lt;/h2&gt;
&lt;h1&gt;
&lt;a name=&quot;t10&quot;&gt;&lt;/a&gt;我用一张图来表示一个用户访问使用HTTPS网站可能增加的延时：&lt;/h1&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/6701e4f736b813d230f9e7d133aa69aa.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;HTTPS增加的延时主要体现在三个阶段，包含了上图所示的2和3阶段。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;302跳转。为什么需要302？因为用户懒。我想绝大部分网民平时访问百度时都是输入www.baidu.com或者baidu.com吧？很少有输入http://www.baidu.com访问百度搜索的吧？至于直接输入https://www.baidu.com来访问百度的HTTPS服务的就更加少了。所以为了强制用户使用HTTPS服务，只有将用户发起的HTTP请求www.baidu.com302成https://www.baidu.com。这无疑是增加一个RTT的跳转延时。&lt;/li&gt;
&lt;li&gt;上图第三阶段的SSL完全握手对延时的影响就更加明显了，这个影响不仅体现在网络传输的RTT上，还包含了数字签名的校验，由于客户端特别是移动端的计算性能弱，增加几十毫秒的计算延时是很常见的。&lt;/li&gt;
&lt;li&gt;还有一个延时没有画出来，就是证书的状态检查，现在稍微新一点的浏览器都使用ocsp来检查证书的撤销状态，在拿到服务器的证书内容之后会访问ocsp站点获取证书的状态，检查证书是否撤销。如果这个ocsp站点在国外或者ocsp服务器出现故障，显然会影响这个正常用户的访问速度。不过还好ocsp的检查周期一般都是7天一次，所以这个对速度的影响还不是很频繁。 另外chrome默认是关闭了ocsp及crl功能，最新版的firefox开启了这个功能，如果ocsp返回不正确，用户无法打开访问网站。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;实际测试发现，在没有任何优化的情况下，HTTPS会增加200ms以上的延时&lt;/strong&gt;&lt;strong&gt;。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;那是不是对于这些延时我们就无法优化了呢？显然不是，部分优化方式参考如下：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;服务器端配置HSTS，减少302跳转，其实HSTS的最大作用是防止302 HTTP劫持。HSTS的缺点是浏览器支持率不高，另外配置HSTS后HTTPS很难实时降级成HTTP。&lt;/li&gt;
&lt;li&gt;设置ssl session 的共享内存cache. 以nginx为例，它目前只支持session cache的单机多进程共享。配置如下：
&lt;pre class=&quot;brush: actionscript3; gutter: true&quot;&gt;ssl_session_cache    shared:SSL:10m;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;配置相同的session ticket key，部署在多个服务器上，这样多个不同的服务器也能产生相同的 session ticket。session ticket的缺点是支持率不广，大概只有40%。而session id是client hello的标准内容，从SSL2.0开始就被全部客户支持。
&lt;pre class=&quot;brush: actionscript3; gutter: true&quot;&gt;ssl_session_tickets    on;
ssl_session_ticket_key ticket_keys;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;设置ocsp stapling file，这样ocsp的请求就不会发往ca提供的ocsp站点，而是发往网站的webserver。ocsp的配置和生成命令如下：
&lt;pre class=&quot;brush: actionscript3; gutter: true&quot;&gt;       ssl_stapling on;
       ssl_stapling_file domain.staple;
上面是nginx配置，如下是ocsp_stapling_file的生成命令：
openssl s_client -showcerts -connect yourdomain:443 &amp;lt; /dev/null | awk -v c=-1 &#39;/-----BEGIN CERTIFICATE-----/{inc=1;c++} inc {print &amp;gt; (&quot;level&quot; c &quot;.crt&quot;)} /---END CERTIFICATE-----/{inc=0}&#39; 

for i in level?.crt; 
do 
       openssl x509 -noout -serial -subject -issuer -in &quot;$i&quot;; 
echo; 
done 

openssl ocsp -text -no_nonce -issuer level1.crt -CAfile CAbundle.crt -cert level0.crt -VAfile level1.crt -url $ocsp_url -respout domain.staple ,其中$ocsp_url等于ocsp站点的URL，可以通过如下命令求出：for i in level?.crt; do echo &quot;$i:&quot;; openssl x509 -noout -text -in &quot;$i&quot; | grep OCSP; done，如果是证书链，一般是最底层的值。&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;优先使用ecdhe密钥交换算法，因为它支持PFS(&lt;a href=&quot;https://www.eff.org/deeplinks/2014/04/why-web-needs-perfect-forward-secrecy&quot; target=&quot;_blank&quot;&gt;perfect forward secrecy)&lt;/a&gt;，能够实现&lt;a href=&quot;http://tools.ietf.org/html/draft-bmoeller-tls-falsestart-00&quot; target=&quot;_blank&quot;&gt;false start&lt;/a&gt;。&lt;/li&gt;
&lt;li&gt;设置tls record size，最好是能动态调整record size，即连接刚建立时record size设置成msg，连接稳定之后可以将record size动态增加。&lt;/li&gt;
&lt;li&gt;如果有条件的话可以启用tcp fast open。虽然现在没有什么客户端支持。&lt;/li&gt;
&lt;li&gt;启用SPDY。SPDY是强制使用HTTPS的，协议比较复杂，需要单独的文章来分析。可以肯定的一点是使用SPDY的请求不仅明显提升了HTTPS速度，甚至比HTTP还要快。在无线WIFI环境下，SPDY比HTTP要快50ms左右，3G环境下比HTTP要快250ms。&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;
&lt;a name=&quot;t11&quot;&gt;&lt;/a&gt;3.2 HTTPS 对性能的影响&lt;/h2&gt;
&lt;h3&gt;
&lt;a name=&quot;t12&quot;&gt;&lt;/a&gt;HTTPS为什么会严重降低性能？主要是握手阶段时的大数运算。其中最消耗性能的又是密钥交换时的私钥解密阶段（函数是rsa_private_decryption）。这个阶段的性能消耗占整个SSL握手性能消耗的95%。&lt;/h3&gt;
&lt;p&gt;前面提及了openssl密钥交换使用的算法只有四种：rsa, dhe, ecdhe，dh。dh由于安全问题目前使用得非常少，所以这里可以比较下前面三种密钥交换算法的性能，具体的数据如下：&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/708fb5b3329daad8ed34161f0cc5fbf2.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;上图数据是指完成1000次握手需要的时间，显然时间数值越大表示性能越低。图片和测试方法都可以参考原文，地址如下：http://vincent.bernat.im/en/blog/2011-ssl-perfect-forward-secrecy.html。&lt;br&gt;
密钥交换步骤是SSL完全握手过程中无法绕过的一个阶段。我们只能采取如下措施：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;通过session cache和session ticket提升session reuse率，减少完全握手(full handshake)次数，提升简化握手(abbreviated handshake)率。&lt;/li&gt;
&lt;li&gt;出于前向加密和false start的考虑，我们优先配置ecdhe用于密钥交换，但是性能不足的情况下可以将rsa配置成密钥交换算法，提升性能。&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;
&lt;a name=&quot;t13&quot;&gt;&lt;/a&gt;openssl 自带的工具可以计算出对称加密、数字签名及HASH函数的各个性能，所以详细数据我就不再列举，读者可以自行测试 。&lt;/h3&gt;
&lt;p&gt;结论就是对称加密RC4的性能最快，但是RC4本身不安全，所以还是正常情况下还是采用AES。HASH函数MD5和SHA1差不多。数字签名是ecdsa算法最快，但是支持率不高。&lt;br&gt;
事实上由于密钥交换在整个握手过程中消耗性能占了95%，而对称加解密的性能消耗不到0.1%，所以server端对称加密的优化收益不大。相反，由于客户端特别是移动端的CPU计算能力本来就比较弱，所以对称加密和数字签名的优化主要是针对移动客户端。&lt;br&gt;
&lt;a href=&quot;https://www.imperialviolet.org/2013/10/07/chacha20.html&quot; target=&quot;_blank&quot;&gt;poly1350&lt;/a&gt;是google推出的号称优于aes-gcm的对称加密算法，适用于移动端，可以试用一下。&lt;/p&gt;
&lt;p&gt;最后经过测试，综合安全和性能的最优cipher suite配置是: ?&lt;strong&gt;ECDHE-RSA-AES128-GCM-SHA256.&lt;/strong&gt;&lt;br&gt;
如果性能出现大幅度下降，可以修改配置，提升性能但是弱化了安全性，配置是：rc4-md5，根据openssl的规则，密钥交换和数字签名默认都是使用rsa。&lt;/p&gt;
&lt;h1&gt;
&lt;a name=&quot;t14&quot;&gt;&lt;/a&gt;4，HTTPS的支持率分析&lt;/h1&gt;
&lt;h1&gt;
&lt;a name=&quot;t15&quot;&gt;&lt;/a&gt;分析了百度服务器端一百万的无线访问日志（主要为手机和平板电脑的浏览器），得出协议和握手时间的关系如下：&lt;/h1&gt;
&lt;table width=&quot;200&quot; border=&quot;1&quot; cellspacing=&quot;1&quot; cellpadding=&quot;1&quot;&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;tls协议版本&lt;/td&gt;
&lt;td&gt;客户端使用率&lt;/td&gt;
&lt;td&gt;握手时间 ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;tls 1.2&lt;/td&gt;
&lt;td&gt;24.8%&lt;/td&gt;
&lt;td&gt;299.496&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;tls 1.1&lt;/td&gt;
&lt;td&gt;0.9%&lt;/td&gt;
&lt;td&gt;279.383&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;tls 1.0&lt;/td&gt;
&lt;td&gt;74%&lt;/td&gt;
&lt;td&gt;307.077&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ssl 3.0&lt;/td&gt;
&lt;td&gt;0.3%&lt;/td&gt;
&lt;td&gt;484.564&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h1&gt;
&lt;a name=&quot;t16&quot;&gt;&lt;/a&gt;从上表可以发现，ssl3.0速度最慢，不过支持率非常低。tls 1.0支持率最广泛。&lt;/h1&gt;
&lt;p&gt;加密套件和握手时间的关系如下：&lt;/p&gt;
&lt;table width=&quot;400&quot; border=&quot;1&quot; cellspacing=&quot;1&quot; cellpadding=&quot;1&quot;&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;加密套件&lt;/td&gt;
&lt;td&gt;客户端使用率&lt;/td&gt;
&lt;td&gt;握手时间&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ECDHE-RSA-AES128-SHA&lt;/td&gt;
&lt;td&gt;58.5%&lt;/td&gt;
&lt;td&gt;294.36&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ECDHE-RSA-AES128-SHA256&lt;/td&gt;
&lt;td&gt;21.1%&lt;/td&gt;
&lt;td&gt;303.065&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;DHE-RSA-AES128-SHA&lt;/td&gt;
&lt;td&gt;16.7%&lt;/td&gt;
&lt;td&gt;351.063&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ECDHE-RSA-AES128-GCM-SHA256&lt;/td&gt;
&lt;td&gt;3.7%&lt;/td&gt;
&lt;td&gt;274.83&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;显然DHE对速度的影响比较大，ECDHE的性能确实要好出很多，而AES128-GCM对速度也有一点提升。&lt;/p&gt;
&lt;p&gt;通过tcpdump分析client hello请求，发现有56.53%的请求发送了session id。也就意味着这些请求都能通过session cache得到复用。其他的一些扩展属性支持率如下：&lt;/p&gt;
&lt;table width=&quot;200&quot; border=&quot;1&quot; cellspacing=&quot;1&quot; cellpadding=&quot;1&quot;&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;tls扩展名&lt;/td&gt;
&lt;td&gt;支持率&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;server_name&lt;/td&gt;
&lt;td&gt;76.99%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;session_tickets&lt;/td&gt;
&lt;td&gt;38.6%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;next_protocol_negotiation&lt;/td&gt;
&lt;td&gt;40.54%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;elliptic_curves&lt;/td&gt;
&lt;td&gt;90.6%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ec_point_formats&lt;/td&gt;
&lt;td&gt;90.6%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;这几个扩展都非常有意义，解释如下：&lt;/p&gt;
&lt;p&gt;server_name,，即 sni （server name indicator)，有77%的请求会在client hello里面携带想要访问的域名，允许服务端使用一个IP支持多个域名。&lt;/p&gt;
&lt;p&gt;next_protocol_negotiation，即NPN，意味着有40.54%的客户端支持spdy.&lt;/p&gt;
&lt;p&gt;session_tickets只有38.6%的支持率，比较低。这也是我们为什么会修改nginx主干代码实现session cache多机共享机制的原因。&lt;/p&gt;
&lt;p&gt;elliptic_curves即是之前介绍的ECC（椭圆曲线系列算法），能够使用更小KEY长度实现DH同样级别的安全，极大提升运算性能。&lt;/p&gt;
&lt;h1&gt;
&lt;a name=&quot;t17&quot;&gt;&lt;/a&gt;5，结论&lt;/h1&gt;
&lt;p&gt;现在互联网上HTTPS的中文资料相对较少，同时由于HTTPS涉及到大量协议、密码学及PKI体系的知识，学习门槛相对较高。另外在具体的实践过程中还有很多坑和待持续改进的地方。希望本文对大家有一些帮助，同时由于我本人在很多地方掌握得也比较粗浅，一知半解，希望大家能多提意见，共同进步。&lt;/p&gt;
&lt;p&gt;最后，为了防止流量劫持，保护用户隐私，大家都使用HTTPS吧，全网站支持。事实上，HTTPS并没有那么难用和可怕，只是你没有好好优化。&lt;/p&gt;


&lt;!-- div id=&quot;ad1&quot;&gt;
&lt;/div --&gt;


	


	

</description>
        <pubDate>Thu, 09 Oct 2014 00:00:00 +0800</pubDate>
        <link>http://iftti.com/posts/2014/2014-10-09-78042-47b5a041c.html</link>
        <guid isPermaLink="true">http://iftti.com/posts/2014/2014-10-09-78042-47b5a041c.html</guid>
        
        
        <category>jobbole</category>
        
      </item>
    
      <item>
        <title>天气又转热了，我给她做了个智能小风扇</title>
        <description>

					
		
&lt;!-- div id=&quot;ad1&quot;&gt;
&lt;script type=&quot;text/javascript&quot;&gt;
google_ad_client = &quot;ca-pub-7056282119617872&quot;;
google_ad_slot = &quot;6645040531&quot;;
google_ad_width = 300;
google_ad_height = 250;
&lt;/script&gt;
&lt;script type=&quot;text/javascript&quot;
src=&quot;http://pagead2.googlesyndication.com/pagead/show_ads.js&quot;&gt;
&lt;/script&gt;
&lt;/div&gt;
&lt;br/ --&gt;

&lt;pre class=&quot;brush: text; gutter: true&quot;&gt;黄花深巷，
红叶低窗，
凄凉一片秋声。
豆雨声来，
中间夹带风声。
疏疏二十五点，
丽谯门、不锁更声。
故人远，
问谁摇玉佩，
檐底铃声。
彩角声吹月堕，
渐连营马动，
四起笳声。
闪烁邻灯，
灯前尚有砧声。
知他诉愁到晓，
碎哝哝、多少蛩声。
诉未了，把一半、分与雁声。&lt;/pre&gt;
&lt;p&gt;一首短诗《声声慢 秋声》献与大家。蒋捷的这首短诗共96字，其中有十个“声”字；其通过风声、雨声、更声、铃声、笳声、砧声、蛩声、雁声来形容秋天的到来，颇有节奏感。&lt;/p&gt;
&lt;p&gt;一直颇喜欢古典诗词，怎奈天赋不佳，至今连完整的四言诗都木写出来过……哎……不提伤心往事了……五音不全不能阻止我喜爱音乐，不能作诗亦不能阻止我爱诗哒！！记得小时候在乡间，对四季的变化非常敏锐，树枝张新芽了，知春来了；知了叫了，知夏来了，树叶黄了，知秋来了；雪花飘了，知深冬了。如今，生活在钢筋水泥之间，对季节的感觉淡漠了，似乎是麻木了，唯一触发人知觉的只有温度，冷了暖气，热了风扇/空调；对于季节的感受全然变了被遗弃的孩子，尽管它在冬天里猛力的哭号，也没有多少人能听见……向往古人感受季节的敏锐……&lt;/p&gt;
&lt;p&gt;……&lt;/p&gt;
&lt;p&gt;我这是说哪去了呢……哎……情不自禁啊……大家谅解……&lt;/p&gt;
&lt;p&gt;话说，长沙天气的热是出了名的，四大火炉之一啊！只是今年天气有点异常，只有在7月热了一小段时间，后面一直是清凉气爽的天！！！在这样的天气下，舒服地过了几个月后（咱家席子都换成被子了），现在又突然变热，大家都有点不适应，GF表现得尤为突出。坐一会就焦躁不安地站起来，说热死了，热死了，显得燥热难耐的样子（她本身散热面积就挺大，不知为啥比我还热……）。好吧，那就开风扇吧，可没吹多久，又说这风吹着不舒服，风大了点还冷。额……，这可是开得最小档啊！为了让GF乖乖的做在我旁边陪我写代码（当然她可以用手机玩游戏），我试着用技术来解决这个问题……&lt;/p&gt;
&lt;p&gt;学技术，去哪里，当然是山东蓝翔啊，哈哈……开个玩笑，最近关于蓝翔的段子还真不少，咱就不跟这个风了。 咱可是程序员，用一现有技术拼凑（高大上的说法是集成）就能解决问题好不。下面就说说我要做的智能风扇的功能吧：&lt;/p&gt;
&lt;p&gt;1、  风速无级调节&lt;/p&gt;
&lt;p&gt;2、  手机控制风扇开关及风速&lt;/p&gt;
&lt;p&gt;3、  语音控制风扇开关及风速（能够根据语义进行智能控制）&lt;/p&gt;
&lt;p&gt;4、  通过检测皮肤的温湿度控制风扇开关及风速（手环啊）&lt;/p&gt;
&lt;p&gt;要做的功能确定了，那就设计方案和准备材料吧。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;硬件&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;1、风扇&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/d673736d0c3e882bf1a32c04b438ab95.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;7寸大号丝雨usb电风扇，用着好用就帮人家推荐下（塑料的，轻，便宜），这是我做人的原则&lt;/p&gt;
&lt;p&gt;2、主控板&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/243a5813c1205aa9d364cde7c78d7c69.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;拿着师弟做的带WiFi功能的Arduino板子，不给钱的，光线暗拍着不好看，其实做的还蛮精致的。&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/293d9805a60e604cbd491b0059ce0594.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;3、  控制接口电路&lt;/p&gt;
&lt;p&gt;通过主控板pwm输出控制电源给风扇供电，用万用板焊接好后如下&lt;br&gt;
&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/4360f0f54dfbaa77a52cb8e4ada951a7.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;3、  手机&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/cdb4e5769b7ef86b8514fc22602a1c31.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;如果配送这个手机的话，风扇应该会被扔掉，所以坚决不用这种档次的手机。&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;3、  手环&lt;/p&gt;
&lt;p&gt;听说又要变天了，为了在天气变凉前把风扇做出来，这个还是在第二版再做吧。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;软件&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;1、  Arduino IDE编程&lt;/p&gt;
&lt;p&gt;是的，控制端的程序我就是用arduino写的，你们就尽情的鄙视我吧。&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/c4f946daa5cce96a2a10cb9849e20273.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;而且关于pwm调节只要这么一条语句就实现了哦。&lt;/p&gt;
&lt;pre class=&quot;brush: actionscript3; gutter: true&quot;&gt;analogWrite( 7 , hp*25 );&lt;/pre&gt;
&lt;div&gt;&lt;/div&gt;
&lt;p&gt;2、  Alljoyn通信&lt;/p&gt;
&lt;p&gt;用Alljoyn完全是为了装逼，可以对别人吹牛说，我的风扇里用了最先进的物联网技术，其实用tcp通信就能实现。&lt;/p&gt;
&lt;p&gt;主控板端Alljoyn接口代码：&lt;/p&gt;
&lt;div&gt;&lt;/div&gt;
&lt;pre class=&quot;brush: actionscript3; gutter: true&quot;&gt;static const char* const testInterface[] = {
    &quot;org.alljoyn.sample.ledcontroller&quot;,
    &quot;?Flash msec&amp;lt;u&quot;,
    &quot;?On&quot;,
    &quot;?Off&quot;,
    NULL
};&lt;/pre&gt;
&lt;p&gt;3、  科大飞讯语音引擎&lt;/p&gt;
&lt;p&gt;大家用着都说好，我试了下，果然不错！科大飞讯，中国人都用它。&lt;/p&gt;
&lt;p&gt;语音识别关键部分代码：&lt;/p&gt;
&lt;div&gt;
&lt;div id=&quot;highlighter_280789&quot;&gt;
&lt;pre class=&quot;brush: actionscript3; gutter: true&quot;&gt;private InitListener mInitListener = new InitListener() {

        @Override
        public void onInit(int code) {
            Log.d(TAG, &quot;SpeechRecognizer init() code = &quot; + code);
            if (code == ErrorCode.SUCCESS) {
                findViewById(R.id.isr_recognize).setEnabled(true);
            }
        }
    };

    /**
     * 构建语法监听器。
     */
    private GrammarListener grammarListener = new GrammarListener() {
        @Override
        public void onBuildFinish(String grammarId, SpeechError error) {
            if(error == null){
                String grammarID = new String(grammarId);
                Editor editor = mSharedPreferences.edit();
                if(!TextUtils.isEmpty(grammarId))
                    editor.putString(KEY_GRAMMAR_ABNF_ID, grammarID);
                editor.commit();
                showTip(&quot;语法构建成功：&quot; + grammarId);
            }else{
                showTip(&quot;语法构建失败,错误码：&quot; + error.getErrorCode());
            }           
        }
    };

    /**
     * 识别监听器。
     */
    private RecognizerListener mRecognizerListener = new RecognizerListener() {

        @Override
        public void onVolumeChanged(int volume) {
            showTip(&quot;当前正在说话，音量大小：&quot; + volume);
        }

        @Override
        public void onResult(final RecognizerResult result, boolean isLast) {
            runOnUiThread(new Runnable() {
                @Override
                public void run() {
                    if (null != result) {
                        Log.d(TAG, &quot;recognizer result：&quot; + result.getResultString());
                        String text = &quot;&quot;;
                        if(mEngineType.equals(SpeechConstant.TYPE_CLOUD))
                        {
                            //解析云端结果
                            text = JsonParser.parseGrammarResult(result.getResultString());
                        }else {
                            //解析本地结果
                            text = XmlParser.parseNluResult(result.getResultString());
                        }
                        // 显示
                        //((EditText)findViewById(R.id.isr_text)).setText(text);   

                        //分析识别结果
                        Boolean analyzeRelult = analyzeWords(text);

                        //发送控制指令
                        if(analyzeRelult)
                        {
                            controlFan(mWindLevel);
                        }

                    } else {
                        Log.d(TAG, &quot;recognizer result : null&quot;);
                    }   
                    SystemClock.sleep(100);
                    //下一次语音识别
                    Button recognizeButton = (Button)findViewById(R.id.isr_recognize);

                    recognizeButton.performClick();
                }
            });

        }

        @Override
        public void onEndOfSpeech() {
            showTip(&quot;结束说话&quot;);

        }

        @Override
        public void onBeginOfSpeech() {
            showTip(&quot;开始说话&quot;);
        }

        @Override
        public void onError(SpeechError error) {
            showTip(&quot;onError Code：&quot; + error.getErrorCode());
            SystemClock.sleep(100);
            //下一次语音识别
            Button recognizeButton = (Button)findViewById(R.id.isr_recognize);

            recognizeButton.performClick();
        }

        @Override
        public void onEvent(int eventType, int arg1, int agr2, String msg) {
        }
    };&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;4、  Android&lt;/p&gt;
&lt;p&gt;做出来手机软件界面是这样的&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/dc5203fe4034fcc536b8f0d9ebfdf6e5.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结果&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;我最初想做出来的智能风扇应该是这样的：&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/dc404baea36c60455963b4108bb893cd.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;但事实上做出来却是这样的：&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/8932efb2dd9d88e44fd3d72ce3192922.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;至于具体功能怎么样，我晚点上视频。至于GF用着什么反应，这个……&lt;/p&gt;


&lt;!-- div id=&quot;ad1&quot;&gt;
&lt;/div --&gt;


	


	

</description>
        <pubDate>Wed, 08 Oct 2014 00:00:00 +0800</pubDate>
        <link>http://iftti.com/posts/2014/2014-10-08-77963-96ca9f478.html</link>
        <guid isPermaLink="true">http://iftti.com/posts/2014/2014-10-08-77963-96ca9f478.html</guid>
        
        
        <category>jobbole</category>
        
      </item>
    
      <item>
        <title>优酷真实视频地址解析</title>
        <description>

					
		
&lt;!-- div id=&quot;ad1&quot;&gt;
&lt;script type=&quot;text/javascript&quot;&gt;
google_ad_client = &quot;ca-pub-7056282119617872&quot;;
google_ad_slot = &quot;6645040531&quot;;
google_ad_width = 300;
google_ad_height = 250;
&lt;/script&gt;
&lt;script type=&quot;text/javascript&quot;
src=&quot;http://pagead2.googlesyndication.com/pagead/show_ads.js&quot;&gt;
&lt;/script&gt;
&lt;/div&gt;
&lt;br/ --&gt;

&lt;p&gt;序：优酷之前更新了次算法(很久之前了，呵呵。。。)，故此很多博客的解析算法已经无法使用。很多大牛也已经更新了新的解析方法。我也在此写篇解析过程的文章。本文使用语言为C#。&lt;/p&gt;
&lt;p&gt;由于优酷视频地址时间限制，在你访问本篇文章时，下面所属链接有可能已经失效，望见谅。&lt;/p&gt;
&lt;p&gt;例：http://v.youku.com/v_show/id_&lt;span style=&quot;color: #ff0000;&quot;&gt;&lt;strong&gt;XNzk2NTI0MzMy&lt;/strong&gt;&lt;/span&gt;.html&lt;/p&gt;
&lt;h1&gt;1:获取视频vid&lt;/h1&gt;
&lt;p&gt;在视频url中标红部分。一个&lt;a href=&quot;http://www.cnblogs.com/zhaojunjie/p/3344909.html&quot; target=&quot;_blank&quot;&gt;正则表达式&lt;/a&gt;即可获取。&lt;/p&gt;
&lt;pre class=&quot;brush: csharp; gutter: true&quot;&gt;string getVid(string url)
{
    string strRegex = &quot;(?&amp;lt;=id_)(\\w+)&quot;;
    Regex reg = new Regex(strRegex);
    Match match = reg.Match(url);
    return match.ToString();
}&lt;/pre&gt;
&lt;h1&gt;&lt;/h1&gt;
&lt;h1&gt;2:获取视频元信息&lt;/h1&gt;
&lt;p&gt;&lt;a href=&quot;http://v.youku.com/player/getPlayList/VideoIDS/XNzk2NTI0MzMy/Pf/4/ctype/12/ev/1&quot; target=&quot;_blank&quot;&gt;http://v.youku.com/player/getPlayList/VideoIDS/&lt;/a&gt;&lt;a href=&quot;http://v.youku.com/player/getPlayList/VideoIDS/XNzk2NTI0MzMy/Pf/4/ctype/12/ev/1&quot; target=&quot;_blank&quot;&gt;&lt;strong&gt;XNzk2NTI0MzMy&lt;/strong&gt;/Pf/4/ctype/12/ev/1&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;将前述vid嵌入到上面url中访问即可得到视频信息文件。由于视频信息过长不在此贴出全部内容。下面是部分重要内容的展示。(获取文件为json文件，可直接解析)&lt;/p&gt;
&lt;pre class=&quot;brush: csharp; gutter: true&quot;&gt;{ &quot;data&quot;: [ {
            &quot;ip&quot;: 1991941296,
            &quot;ep&quot;: &quot;MwXRTAsbJLnb0PbJ8uJxAdSivUU11wnKXxc=&quot;,
            &quot;segs&quot;: {
                &quot;hd2&quot;: [
                    {
                        &quot;no&quot;: &quot;0&quot;,
                        &quot;size&quot;: &quot;34602810&quot;,
                        &quot;seconds&quot;: 205,
                        &quot;k&quot;: &quot;248fe14b4c1b37302411f67a&quot;,
                        &quot;k2&quot;: &quot;1c8e113cecad924c5&quot;
                    },
                    {
                        &quot;no&quot;: &quot;1&quot;,
                    },] }, } ],}&lt;/pre&gt;
&lt;p&gt;上面显示的内容后面都会使用到。其中&lt;strong&gt;segs包含hd3,hd2,flv,mp4,3gp&lt;/strong&gt;等各种格式，并且每种格式下均分为若干段。本次选用清晰度较高的hd2(视频格式为flv)&lt;/p&gt;
&lt;h1&gt;&lt;strong&gt;3:拼接m3u8地址&lt;/strong&gt;&lt;/h1&gt;
&lt;pre class=&quot;brush: text; gutter: true&quot;&gt;http://pl.youku.com/playlist/m3u8?ctype=12&amp;amp;ep={0}&amp;amp;ev=1&amp;amp;keyframe=1&amp;amp;oip={1}&amp;amp;sid={2}&amp;amp;token={3}&amp;amp;type={4}&amp;amp;vid={5}&lt;/pre&gt;
&lt;p&gt;以上共有6个参数，其中vid和oip已经得到，分别之前的vid和json文件中的ip字段，即(&lt;strong&gt;XNzk2NTI0MzMy&lt;/strong&gt;和&lt;strong&gt;1991941296&lt;/strong&gt;)，但是ep,sid,token需要重新计算(json文件中的ep值不能直接使用)。type比较简单，后面会说。&lt;/p&gt;
&lt;h2&gt;3.1计算ep,sid,token&lt;/h2&gt;
&lt;p&gt;计算方法单纯的为数学计算，下面给出计算的函数。三个参数可一次性计算得到。其中涉及到Base64编码解码知识，&lt;a href=&quot;http://www.cnblogs.com/zhaojunjie/p/3946427.html&quot; target=&quot;_blank&quot;&gt;点击查看&lt;/a&gt;。&lt;/p&gt;
&lt;pre class=&quot;brush: csharp; gutter: true&quot;&gt;private static string myEncoder(string a, byte[] c, bool isToBase64)
        {
            string result = &quot;&quot;;
            List&amp;lt;Byte&amp;gt; bytesR = new List&amp;lt;byte&amp;gt;();
            int f = 0, h = 0, q = 0;
            int[] b = new int[256];
            for (int i = 0; i &amp;lt; 256; i++)
                    b[i] = i;
            while (h &amp;lt; 256)
            {
                f = (f + b[h] + a[h % a.Length]) % 256;
                int temp = b[h];
                b[h] = b[f];
                b[f] = temp;
                h++;
            }
            f = 0; h = 0; q = 0;
            while (q &amp;lt; c.Length)
            {
                h = (h + 1) % 256;
                f = (f + b[h]) % 256;
                int temp = b[h];
                b[h] = b[f];
                b[f] = temp;
                byte[] bytes = new byte[] { (byte)(c[q] ^ b[(b[h] + b[f]) % 256]) };
                bytesR.Add(bytes[0]);
                result += System.Text.ASCIIEncoding.ASCII.GetString(bytes);
                q++;
            }
            if (isToBase64)
            {
                Byte[] byteR = bytesR.ToArray();
                result = Convert.ToBase64String(byteR);
            }
            return result;
        }
        public static void getEp(string vid, string ep, ref string pNew, ref string token, ref string sid)
        {
            string template1 = &quot;becaf9be&quot;;
            string template2 = &quot;bf7e5f01&quot;;
            byte[] bytes = Convert.FromBase64String(ep);
            ep = ystem.Text.ASCIIEncoding.ASCII.GetString(bytes);
            string temp = myEncoder(template1, bytes, false);
            string[] part = temp.Split(&#39;_&#39;);
            sid = part[0];
            token = part[1];
            string whole = string.Format(&quot;{0}_{1}_{2}&quot;, sid, vid, token);
            byte[] newbytes = System.Text.ASCIIEncoding.ASCII.GetBytes(whole);
            epNew = myEncoder(template2, newbytes, true);
        }&lt;/pre&gt;
&lt;p&gt;计算得到ep,token,sid分别为diaVGE+IVMwB5CXXjz8bNHi0cCEHXJZ0vESH/7YbAMZuNaHQnT/Wzw==, 4178, 441265221168712cdf4f8。注意，此时ep并不能直接拼接到url中，需要对此做一下url编码ToUrlEncode(ep)。最终ep为diaVGE%2bIVMwB5CXXjz8bNHi0cCEHXJZ0vESH%2f7YbAMZuNaHQnT%2fWzw%3d%3d&lt;/p&gt;
&lt;h2 align=&quot;left&quot;&gt;3.2计算type&lt;/h2&gt;
&lt;p align=&quot;left&quot;&gt;Type值和选择的segs有密切关系。如本文选择的hd2，type即为flv，下面是segs,type和清晰度的对照。&lt;/p&gt;
&lt;pre class=&quot;brush: csharp; gutter: true&quot;&gt;“segs”,”type”,”清晰度”
&quot;hd3&quot;, &quot;flv&quot;, &quot;1080P&quot;
&quot;hd2&quot;, &quot;flv&quot;, &quot;超清&quot;
&quot;mp4&quot;, &quot;mp4&quot;, &quot;高清&quot;
&quot;flvhd&quot;, &quot;flv&quot;, &quot;高清&quot;
&quot;flv&quot;, &quot;flv&quot;, &quot;标清&quot;
&quot;3gphd&quot;, &quot;3gp&quot;, &quot;高清&quot;&lt;/pre&gt;
&lt;h2 align=&quot;left&quot;&gt;3.3拼接地址&lt;/h2&gt;
&lt;p align=&quot;left&quot;&gt;　　最后的m3u8地址为&lt;/p&gt;
&lt;p align=&quot;left&quot;&gt;&lt;a href=&quot;http://pl.youku.com/playlist/m3u8?ctype=12&amp;amp;ep=diaVGE%2bIVMwB5CXXjz8bNHi0cCEHXJZ0vESH%2f7YbAMZuNaHQnT%2fWzw%3d%3d&amp;amp;ev=1&amp;amp;keyframe=1&amp;amp;oip=1991941296&amp;amp;sid=441265221168712cdf4f8&amp;amp;token=4178&amp;amp;type=flv&amp;amp;vid=XNzk2NTI0MzMy&quot; target=&quot;_blank&quot;&gt;http://pl.youku.com/playlist/m3u8?ctype=12&amp;amp;ep=diaVGE%2bIVMwB5CXXjz8bNHi0cCEHXJZ0vESH%2f7YbAMZuNaHQnT%2fWzw%3d%3d&amp;amp;ev=1&amp;amp;keyframe=1&amp;amp;oip=1991941296&amp;amp;sid=441265221168712cdf4f8&amp;amp;token=4178&amp;amp;type=flv&amp;amp;vid=XNzk2NTI0MzMy&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;4:获取视频地址&lt;/h1&gt;
&lt;p&gt;将上述m3u8文件下载后，其中内容即为真实地址，不过还需要稍微处理一下。部分内容如下：&lt;/p&gt;
&lt;pre class=&quot;brush: text; gutter: true&quot;&gt;#EXTM3U
#EXT-X-TARGETDURATION:12
#EXT-X-VERSION:3
#EXTINF:6,

http://59.108.137.14/696CD107FE4D821FFBF173EB3/03000208005430B01849631468DEFEC61C5678-3A78-37BA-1971-21A0D4EEA0E7.flv?ts_start=0&amp;amp;ts_end=5.9&amp;amp;ts_seg_no=0&amp;amp;ts_keyframe=1

#EXTINF:5.533,

http://59.108.137.14/696CD107FE4D821FFBF173EB3/03000208005430B01849631468DEFEC61C5678-3A78-37BA-1971-21A0D4EEA0E7.flv?ts_start=5.9&amp;amp;ts_end=11.433&amp;amp;ts_seg_no=1&amp;amp;ts_keyframe=1

#EXTINF:5.467,

http://59.108.137.14/696CD107FE4D821FFBF173EB3/03000208005430B01849631468DEFEC61C5678-3A78-37BA-1971-21A0D4EEA0E7.flv?ts_start=11.433&amp;amp;ts_end=16.9&amp;amp;ts_seg_no=2&amp;amp;ts_keyframe=1

#EXTINF:9.267,&lt;/pre&gt;
&lt;p&gt;其中每条url只包含6s左右视频，但是可将url中参数部分去掉即可得到实际的长度。但是每条去掉后需合并一下相同的url，如上述列表可得到url片段&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://59.108.137.14/696CD107FE4D821FFBF173EB3/03000208005430B01849631468DEFEC61C5678-3A78-37BA-1971-21A0D4EEA0E7.flv&quot; target=&quot;_blank&quot;&gt;http://59.108.137.14/696CD107FE4D821FFBF173EB3/03000208005430B01849631468DEFEC61C5678-3A78-37BA-1971-21A0D4EEA0E7.flv&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;将m3u8中所有的url片段全部下载即可大功告成。&lt;/p&gt;


&lt;!-- div id=&quot;ad1&quot;&gt;
&lt;/div --&gt;


	


	

</description>
        <pubDate>Wed, 08 Oct 2014 00:00:00 +0800</pubDate>
        <link>http://iftti.com/posts/2014/2014-10-08-77936-d38b502cb.html</link>
        <guid isPermaLink="true">http://iftti.com/posts/2014/2014-10-08-77936-d38b502cb.html</guid>
        
        
        <category>jobbole</category>
        
      </item>
    
      <item>
        <title>谈谈网站测试中的AB测试方法</title>
        <description>

					
		
&lt;!-- div id=&quot;ad1&quot;&gt;
&lt;script type=&quot;text/javascript&quot;&gt;
google_ad_client = &quot;ca-pub-7056282119617872&quot;;
google_ad_slot = &quot;6645040531&quot;;
google_ad_width = 300;
google_ad_height = 250;
&lt;/script&gt;
&lt;script type=&quot;text/javascript&quot;
src=&quot;http://pagead2.googlesyndication.com/pagead/show_ads.js&quot;&gt;
&lt;/script&gt;
&lt;/div&gt;
&lt;br/ --&gt;

&lt;p&gt;&lt;strong&gt;什么是A/B测试?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;A / B测试，即你设计的页面有两个版本(A和B)，A为现行的设计， B是新的设计。比较这两个版本之间你所关心的数据(转化率，业绩，跳出率等) ，最后选择效果最好的版本。&lt;/p&gt;
&lt;p&gt;A / B测试不是一个时髦名词。现在很多有经验的营销和设计工作者用它来获得访客行为信息来提高转换率。这是一种很有效的方式，并且由于各种分析工具的发展，测试成本也越来越低，因此很多电商网站都会采用。&lt;/p&gt;
&lt;p&gt;但是大部分人对于A/B测试只有一个基本的认知，如何将它的效应发挥到最大?本文提供19个建议。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;1、减少页面摩擦&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;页面摩擦就是用户在浏览网页的过程中遇到了一些阻碍，会降低转换率。通常造成页面摩擦的原因有三：&lt;/p&gt;
&lt;p&gt;信息栏——要求用户填写信息&lt;/p&gt;
&lt;p&gt;步骤指引——网站地图太复杂&lt;/p&gt;
&lt;p&gt;长页面——太长的页面会磨掉用户的耐心。&lt;/p&gt;
&lt;p&gt;最好的状态是一种“不在场”的状态，就像人的身体一样，没有病痛的时候你不会记得身体的存在。用户用得行云流水，所有的步骤都顺理成章，这才是最好的体验。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;2、信息输入焦虑&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;有的用户不愿意输入太多信息，因为不确定输了那么多信息以后会不会得到应有的回馈(有的人填了一大推信息之后得到一封广告邮件之类的东西，会产生一种被坑的感觉)。越多的信息需要填写，用户流失率就会越高。&lt;/p&gt;
&lt;p&gt;但如果用户很明确知道他们的努力可能会换来什么回馈，他们就很乐意按照网页的指引一步一步往下走，也愿意填那些表格。&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/cf8d2b77036a9b698081b3a3e897d870.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;3、明晰每一个页面的目的&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;有时候“目标清晰”比什么都重要，回答下面3个问题，你可以省略很多不必要的步骤|：&lt;/p&gt;
&lt;p&gt;这个页面是什么?让用户清晰地知道他到了哪一个步骤;&lt;/p&gt;
&lt;p&gt;我可以再这里干什么?让用户一眼看明白这一个页面是为了展示什么;&lt;/p&gt;
&lt;p&gt;为什么我要在这个页面?要把核心优势直观展示出来。用户不需要去思考在这一页可以干什么，自然也不需要思考为什么要在这一页停留。&lt;/p&gt;
&lt;p&gt;用户都很懒，一旦他弄不明白他在哪个网页上可以做什么，他可能马上就关掉那个网页。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;4、倾听用户需求&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;一个B2C的网站，最好是把B和C都找来，听听他们各自的需求，请他们互相提要求。请用户试用网站，并观察他们的使用习惯，这总是有百利而无一害的。&lt;/p&gt;
&lt;p&gt;最后，单独留下C，请他们说说更深层的意见，以及他们是如何与网站交互，哪些功能很好，哪些多余等等。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;5、定价&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;对于电商网站来说，定价是一件至关重要的事。消费者除了关心数字，还关心价值，除了数字，还可以在文案、图片上面做工作。一个完美的定价不是一味只考虑便宜，而是要让消费者觉得他占到了便宜。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;6、尝试提价&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;不要想当然地认为价钱便宜就一定会提升销量，反之，价格高也不等于销量少。有的消费者看到价钱便宜的商品会懒得点开看，因为觉得“便宜没好货”，实际上那个商品质量还不错——所以定价要秉着一分货一分钱的原则。&lt;/p&gt;
&lt;p&gt;此外，还可以尝试小额的加价。比如一次加2%，看看销售量如何，在消费者承受范围之内再加个2%，小额的加价不会让用户觉得你在漫天要价。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;7、测试社交因素&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;很多产品旁边都有一键分享至社交网站的功能。但是，电商们有真正调查过这些功能会提升还是抑制销量吗?&lt;/p&gt;
&lt;p&gt;我看过一个很有意思的调研报告：说是一个祛痘产品的页面因为有了分享功能而减少了25%的销量。毕竟，有的敏感的商品消费者是不愿意和别人分享的(设想一下如果人家买的是杜蕾斯或是什么，你也要他分享到Facebook上吗)。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;8、把广告位放低一点&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;常识可能告诉你广告位越高越显眼就会给目标页面带来更多流量——但是A\B测试通常就是要测那些自以为是常识的东西。&lt;/p&gt;
&lt;p&gt;你花了一定的成本获得了一个位置很好的广告位，这个广告为你提升了50%的销量，但实际上这些收益还抵不上你为广告花费的成本。稍微算一算你就知道投入产出比了。这个报告告诉我们：即便你认为是常识的东西，也不妨去做一个A\B测试。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;9、测试每一个“黄金准则”&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;上一条告诉你要测试常识性的东西，这一条还要补充一点：测试看起来是黄金准则的准则。黄金准则之所以黄金，也是因为经过了无数次的测试(那么也不在乎再多倍测试一次)，比如标题党会让客户对你的信誉产生质疑，这就是一条黄金准则。&lt;/p&gt;
&lt;p&gt;但是，非常时段可以用一些非常方法，如果销售结果总是不如预期，那么你也可以去测一下是不是某条黄金准则出了问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;10、利用一些工具&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;如果你需要找到数据变动的原因又不想花太多时间，可以用一些第三方工具，比如Silverback，可以帮你记录用户在网页上的操作并给出有效的数据。&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/62c066af0b7eab57d48d444ecdf592bd.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;11、时刻记得支撑起转化率的“三只脚”&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;相关性：你的登陆页是否满足用户的预期?你能保持这种风格的连贯性?&lt;/p&gt;
&lt;p&gt;价值：你能符合用户的价值期待吗?你能给他们想要的东西?&lt;/p&gt;
&lt;p&gt;应激性：用户知道自己来这个网站要干什么吗?用户知道要怎么操作吗?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;12、试试不同的用词&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;微小的网页调整会改变转化率，微小的用词上的改变当然也可以引起不同的结果。比如，“Join Now”和“Buy Now”哪一个更能刺激用户的购买欲?测试一下。同理，整个网页上的文案风格的转变也能造成不同的效果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;13、一个页面只展示一个信息&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;转化率最高的页面都有一个共同特点：一个页面集中展示一个信息，不要让你的用户感到迷茫，让他们看一眼就知道想干什么可以干什么。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;14、测试哪个属性是最吸引的&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;一个商品有无数个属性，价格、颜色、材质、产地，等等，那一种属性对用户构成最致命的吸引?一个一个地尝试。再一次重申，不要想当然的替消费者决定他们在标题里最想看到的是哪一个，你要测试才知道。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;15、连小得变态的细节都不放过&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;2007年AJ Kohn测试了两个域名www.YourDomain.com和www.yourdomain.com，仅仅是首字母大小写的问题，结果令人大吃一惊：大写的那个点击率比小写的高出53%!这个事件说明有时候你看不上眼的小细节也能造成很不同的后果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;16、完美?No!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;有的人想要做出“完美”的登录页面，可是我想告诉你，没有完美的页面，A\B测试的精髓就是让每一次测试的结果都比上次更好。&lt;/p&gt;
&lt;p&gt;那句广告语是怎么说的? 没有最好，只有更好。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;17、寻求成本更低的测试方式&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;A\B测试不是要让你用最新的技术、最新的软件或者算法，大部分时候一个纸上的原型或者线框里5秒钟的测试都能帮你找到方向。好好利用那些简单、低廉的测试方式。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;18、等到测试完成&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;上文里无数次地强调不要想当然，在测试没有结束之前，所有的数据都可能是片面的，不要想着用部分的结果去替代全部。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;19、永远不停地测试&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;A\B测试的精髓就在于：永远不要满足于目前的结果，总有更好的解决方案。一次的A\B测试也许能提升50%甚至更好的转换率，但这并不意味着到顶了。生命不息，测试不止。&lt;/p&gt;


&lt;!-- div id=&quot;ad1&quot;&gt;
&lt;/div --&gt;


	


	

</description>
        <pubDate>Wed, 08 Oct 2014 00:00:00 +0800</pubDate>
        <link>http://iftti.com/posts/2014/2014-10-08-77933-f1476a44d.html</link>
        <guid isPermaLink="true">http://iftti.com/posts/2014/2014-10-08-77933-f1476a44d.html</guid>
        
        
        <category>jobbole</category>
        
      </item>
    
      <item>
        <title>互联网协议入门（二）</title>
        <description>

					
		
&lt;!-- div id=&quot;ad1&quot;&gt;
&lt;script type=&quot;text/javascript&quot;&gt;
google_ad_client = &quot;ca-pub-7056282119617872&quot;;
google_ad_slot = &quot;6645040531&quot;;
google_ad_width = 300;
google_ad_height = 250;
&lt;/script&gt;
&lt;script type=&quot;text/javascript&quot;
src=&quot;http://pagead2.googlesyndication.com/pagead/show_ads.js&quot;&gt;
&lt;/script&gt;
&lt;/div&gt;
&lt;br/ --&gt;

&lt;p&gt;&lt;a href=&quot;http://blog.jobbole.com/77851/&quot; target=&quot;_blank&quot;&gt;上一篇文章&lt;/a&gt;分析了互联网的总体构思，从下至上，每一层协议的设计思想。&lt;/p&gt;
&lt;p&gt;这是从设计者的角度看问题，今天我想切换到用户的角度，看看用户是如何从上至下，与这些协议互动的。&lt;/p&gt;
&lt;p&gt;（接上文）&lt;/p&gt;
&lt;p&gt;七、一个小结&lt;/p&gt;
&lt;p&gt;先对前面的内容，做一个小结。&lt;/p&gt;
&lt;p&gt;我们已经知道，网络通信就是交换数据包。电脑A向电脑B发送一个数据包，后者收到了，回复一个数据包，从而实现两台电脑之间的通信。数据包的结构，基本上是下面这样：&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/61f8606c50afb3c2812f2da0f237bd52.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;发送这个包，需要知道两个地址：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;* 对方的MAC地址&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;有了这两个地址，数据包才能准确送到接收者手中。但是，前面说过，MAC地址有局限性，如果两台电脑不在同一个子网络，就无法知道对方的MAC地址，必须通过网关（gateway）转发。&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/aba2e22e73bc10caa8522a29541f643e.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;上图中，1号电脑要向4号电脑发送一个数据包。它先判断4号电脑是否在同一个子网络，结果发现不是（后文介绍判断方法），于是就把这个数据包发到网关A。网关A通过路由协议，发现4号电脑位于子网络B，又把数据包发给网关B，网关B再转发到4号电脑。&lt;/p&gt;
&lt;p&gt;1号电脑把数据包发到网关A，必须知道网关A的MAC地址。所以，数据包的目标地址，实际上分成两种情况：&lt;/p&gt;
&lt;table width=&quot;70%&quot; border=&quot;1&quot;&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;场景&lt;/td&gt;
&lt;td&gt;数据包地址&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;同一个子网络&lt;/td&gt;
&lt;td&gt;对方的MAC地址，对方的IP地址&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;非同一个子网络&lt;/td&gt;
&lt;td&gt;网关的MAC地址，对方的IP地址&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;发送数据包之前，电脑必须判断对方是否在同一个子网络，然后选择相应的MAC地址。接下来，我们就来看，实际使用中，这个过程是怎么完成的。&lt;/p&gt;
&lt;p&gt;八、用户的上网设置&lt;/p&gt;
&lt;p&gt;8.1 静态IP地址&lt;/p&gt;
&lt;p&gt;你买了一台新电脑，插上网线，开机，这时电脑能够上网吗？&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/cf38358c8173555f49cf31572dd25294.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;通常你必须做一些设置。有时，管理员（或者ISP）会告诉你下面四个参数，你把它们填入操作系统，计算机就能连上网了：&lt;/p&gt;
&lt;blockquote&gt;&lt;/blockquote&gt;
&lt;p&gt;下图是Windows系统的设置窗口。&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/c7b95b675366be6c80e1b92454b6a7f6.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;这四个参数缺一不可，后文会解释为什么需要知道它们才能上网。由于它们是给定的，计算机每次开机，都会分到同样的IP地址，所以这种情况被称作”静态IP地址上网”。&lt;/p&gt;
&lt;p&gt;但是，这样的设置很专业，普通用户望而生畏，而且如果一台电脑的IP地址保持不变，其他电脑就不能使用这个地址，不够灵活。出于这两个原因，大多数用户使用”动态IP地址上网”。&lt;/p&gt;
&lt;p&gt;8.2 动态IP地址&lt;/p&gt;
&lt;p&gt;所谓”动态IP地址”，指计算机开机后，会自动分配到一个IP地址，不用人为设定。它使用的协议叫做&lt;a href=&quot;http://zh.wikipedia.org/zh/DHCP&quot; target=&quot;_blank&quot;&gt;DHCP协议&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;这个协议规定，每一个子网络中，有一台计算机负责管理本网络的所有IP地址，它叫做”DHCP服务器”。新的计算机加入网络，必须向”DHCP服务器”发送一个”DHCP请求”数据包，申请IP地址和相关的网络参数。&lt;/p&gt;
&lt;p&gt;前面说过，如果两台计算机在同一个子网络，必须知道对方的MAC地址和IP地址，才能发送数据包。但是，新加入的计算机不知道这两个地址，怎么发送数据包呢？&lt;/p&gt;
&lt;p&gt;DHCP协议做了一些巧妙的规定。&lt;/p&gt;
&lt;p&gt;8.3 DHCP协议&lt;/p&gt;
&lt;p&gt;首先，它是一种应用层协议，建立在UDP协议之上，所以整个数据包是这样的：&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/f1771fd8016420a021b30298fc9530f1.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;（1）最前面的”以太网标头”，设置发出方（本机）的MAC地址和接收方（DHCP服务器）的MAC地址。前者就是本机网卡的MAC地址，后者这时不知道，就填入一个广播地址：FF-FF-FF-FF-FF-FF。&lt;/p&gt;
&lt;p&gt;（2）后面的”IP标头”，设置发出方的IP地址和接收方的IP地址。这时，对于这两者，本机都不知道。于是，发出方的IP地址就设为0.0.0.0，接收方的IP地址设为255.255.255.255。&lt;/p&gt;
&lt;p&gt;（3）最后的”UDP标头”，设置发出方的端口和接收方的端口。这一部分是DHCP协议规定好的，发出方是68端口，接收方是67端口。&lt;/p&gt;
&lt;p&gt;这个数据包构造完成后，就可以发出了。以太网是广播发送，同一个子网络的每台计算机都收到了这个包。因为接收方的MAC地址是FF-FF-FF-FF-FF-FF，看不出是发给谁的，所以每台收到这个包的计算机，还必须分析这个包的IP地址，才能确定是不是发给自己的。当看到发出方IP地址是0.0.0.0，接收方是255.255.255.255，于是DHCP服务器知道”这个包是发给我的”，而其他计算机就可以丢弃这个包。&lt;/p&gt;
&lt;p&gt;接下来，DHCP服务器读出这个包的数据内容，分配好IP地址，发送回去一个”DHCP响应”数据包。这个响应包的结构也是类似的，以太网标头的MAC地址是双方的网卡地址，IP标头的IP地址是DHCP服务器的IP地址（发出方）和255.255.255.255（接收方），UDP标头的端口是67（发出方）和68（接收方），分配给请求端的IP地址和本网络的具体参数则包含在Data部分。&lt;/p&gt;
&lt;p&gt;新加入的计算机收到这个响应包，于是就知道了自己的IP地址、子网掩码、网关地址、DNS服务器等等参数。&lt;/p&gt;
&lt;p&gt;8.4 上网设置：小结&lt;/p&gt;
&lt;p&gt;这个部分，需要记住的就是一点：不管是”静态IP地址”还是”动态IP地址”，电脑上网的首要步骤，是确定四个参数。这四个值很重要，值得重复一遍：&lt;/p&gt;
&lt;blockquote&gt;&lt;/blockquote&gt;
&lt;p&gt;有了这几个数值，电脑就可以上网”冲浪”了。接下来，我们来看一个实例，当用户访问网页的时候，互联网协议是怎么运作的。&lt;/p&gt;
&lt;p&gt;九、一个实例：访问网页&lt;/p&gt;
&lt;p&gt;9.1 本机参数&lt;/p&gt;
&lt;p&gt;我们假定，经过上一节的步骤，用户设置好了自己的网络参数：&lt;/p&gt;
&lt;blockquote&gt;&lt;/blockquote&gt;
&lt;p&gt;然后他打开浏览器，想要访问Google，在地址栏输入了网址：www.google.com。&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/adc208dc641d8cec9c07cdb310ea403b.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;这意味着，浏览器要向Google发送一个网页请求的数据包。&lt;/p&gt;
&lt;p&gt;9.2 DNS协议&lt;/p&gt;
&lt;p&gt;我们知道，发送数据包，必须要知道对方的IP地址。但是，现在，我们只知道网址www.google.com，不知道它的IP地址。&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://en.wikipedia.org/wiki/Domain_Name_System&quot; target=&quot;_blank&quot;&gt;DNS协议&lt;/a&gt;可以帮助我们，将这个网址转换成IP地址。已知DNS服务器为8.8.8.8，于是我们向这个地址发送一个DNS数据包（53端口）。&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/7e8da9f2fa800025425a8257effcde6a.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;然后，DNS服务器做出响应，告诉我们Google的IP地址是172.194.72.105。于是，我们知道了对方的IP地址。&lt;/p&gt;
&lt;p&gt;9.3 子网掩码&lt;/p&gt;
&lt;p&gt;接下来，我们要判断，这个IP地址是不是在同一个子网络，这就要用到子网掩码。&lt;/p&gt;
&lt;p&gt;已知子网掩码是255.255.255.0，本机用它对自己的IP地址192.168.1.100，做一个二进制的AND运算（两个数位都为1，结果为1，否则为0），计算结果为192.168.1.0；然后对Google的IP地址172.194.72.105也做一个AND运算，计算结果为172.194.72.0。这两个结果不相等，所以结论是，Google与本机不在同一个子网络。&lt;/p&gt;
&lt;p&gt;因此，我们要向Google发送数据包，必须通过网关192.168.1.1转发，也就是说，接收方的MAC地址将是网关的MAC地址。&lt;/p&gt;
&lt;p&gt;9.4 应用层协议&lt;/p&gt;
&lt;p&gt;浏览网页用的是HTTP协议，它的整个数据包构造是这样的：&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/29a70b6266426bf6d7aa963b09662a44.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;HTTP部分的内容，类似于下面这样：&lt;/p&gt;
&lt;blockquote&gt;&lt;/blockquote&gt;
&lt;p&gt;我们假定这个部分的长度为4960字节，它会被嵌在TCP数据包之中。&lt;/p&gt;
&lt;p&gt;9.5 TCP协议&lt;/p&gt;
&lt;p&gt;TCP数据包需要设置端口，接收方（Google）的HTTP端口默认是80，发送方（本机）的端口是一个随机生成的1024-65535之间的整数，假定为51775。&lt;/p&gt;
&lt;p&gt;TCP数据包的标头长度为20字节，加上嵌入HTTP的数据包，总长度变为4980字节。&lt;/p&gt;
&lt;p&gt;9.6 IP协议&lt;/p&gt;
&lt;p&gt;然后，TCP数据包再嵌入IP数据包。IP数据包需要设置双方的IP地址，这是已知的，发送方是192.168.1.100（本机），接收方是172.194.72.105（Google）。&lt;/p&gt;
&lt;p&gt;IP数据包的标头长度为20字节，加上嵌入的TCP数据包，总长度变为5000字节。&lt;/p&gt;
&lt;p&gt;9.7 以太网协议&lt;/p&gt;
&lt;p&gt;最后，IP数据包嵌入以太网数据包。以太网数据包需要设置双方的MAC地址，发送方为本机的网卡MAC地址，接收方为网关192.168.1.1的MAC地址（通过ARP协议得到）。&lt;/p&gt;
&lt;p&gt;以太网数据包的数据部分，最大长度为1500字节，而现在的IP数据包长度为5000字节。因此，IP数据包必须分割成四个包。因为每个包都有自己的IP标头（20字节），所以四个包的IP数据包的长度分别为1500、1500、1500、560。&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/73ccfcecc36c2084ef611b640be63a45.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;9.8 服务器端响应&lt;/p&gt;
&lt;p&gt;经过多个网关的转发，Google的服务器172.194.72.105，收到了这四个以太网数据包。&lt;/p&gt;
&lt;p&gt;根据IP标头的序号，Google将四个包拼起来，取出完整的TCP数据包，然后读出里面的”HTTP请求”，接着做出”HTTP响应”，再用TCP协议发回来。&lt;/p&gt;
&lt;p&gt;本机收到HTTP响应以后，就可以将网页显示出来，完成一次网络通信。&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/99451c93ac3a51be2b4802d89cac57c8.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;这个例子就到此为止，虽然经过了简化，但它大致上反映了互联网协议的整个通信过程。&lt;/p&gt;


&lt;!-- div id=&quot;ad1&quot;&gt;
&lt;/div --&gt;


	


	

</description>
        <pubDate>Wed, 08 Oct 2014 00:00:00 +0800</pubDate>
        <link>http://iftti.com/posts/2014/2014-10-08-77931-ee859b61b.html</link>
        <guid isPermaLink="true">http://iftti.com/posts/2014/2014-10-08-77931-ee859b61b.html</guid>
        
        
        <category>jobbole</category>
        
      </item>
    
      <item>
        <title>互联网协议入门（一）</title>
        <description>

					
		
&lt;!-- div id=&quot;ad1&quot;&gt;
&lt;script type=&quot;text/javascript&quot;&gt;
google_ad_client = &quot;ca-pub-7056282119617872&quot;;
google_ad_slot = &quot;6645040531&quot;;
google_ad_width = 300;
google_ad_height = 250;
&lt;/script&gt;
&lt;script type=&quot;text/javascript&quot;
src=&quot;http://pagead2.googlesyndication.com/pagead/show_ads.js&quot;&gt;
&lt;/script&gt;
&lt;/div&gt;
&lt;br/ --&gt;

&lt;p&gt;我们每天使用互联网，你是否想过，它是如何实现的？&lt;/p&gt;
&lt;p&gt;全世界几十亿台电脑，连接在一起，两两通信。上海的某一块网卡送出信号，洛杉矶的另一块网卡居然就收到了，两者实际上根本不知道对方的物理位置，你不觉得这是很神奇的事情吗？&lt;/p&gt;
&lt;p&gt;互联网的核心是一系列协议，总称为”互联网协议”（Internet Protocol Suite）。它们对电脑如何连接和组网，做出了详尽的规定。理解了这些协议，就理解了互联网的原理。&lt;/p&gt;
&lt;p&gt;下面就是我的学习笔记。因为这些协议实在太复杂、太庞大，我想整理一个简洁的框架，帮助自己从总体上把握它们。为了保证简单易懂，我做了大量的简化，有些地方并不全面和精确，但是应该能够说清楚互联网的原理。&lt;/p&gt;
&lt;p&gt;一、概述&lt;/p&gt;
&lt;p&gt;1.1 五层模型&lt;/p&gt;
&lt;p&gt;互联网的实现，分成好几层。每一层都有自己的功能，就像建筑物一样，每一层都靠下一层支持。&lt;/p&gt;
&lt;p&gt;用户接触到的，只是最上面的一层，根本没有感觉到下面的层。要理解互联网，必须从最下层开始，自下而上理解每一层的功能。&lt;/p&gt;
&lt;p&gt;如何分层有不同的模型，有的模型分七层，有的分四层。我觉得，把互联网分成五层，比较容易解释。&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/9958d9fec0030c01cc5a9a3c74d73779.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;如上图所示，最底下的一层叫做”实体层”（Physical Layer），最上面的一层叫做”应用层”（Application Layer），中间的三层（自下而上）分别是”链接层”（Link Layer）、”网络层”（Network Layer）和”传输层”（Transport Layer）。越下面的层，越靠近硬件；越上面的层，越靠近用户。&lt;/p&gt;
&lt;p&gt;它们叫什么名字，其实并不重要。只需要知道，互联网分成若干层就可以了。&lt;/p&gt;
&lt;p&gt;1.2 层与协议&lt;/p&gt;
&lt;p&gt;每一层都是为了完成一种功能。为了实现这些功能，就需要大家都遵守共同的规则。&lt;/p&gt;
&lt;p&gt;大家都遵守的规则，就叫做”协议”（protocol）。&lt;/p&gt;
&lt;p&gt;互联网的每一层，都定义了很多协议。这些协议的总称，就叫做”互联网协议”（Internet Protocol Suite）。它们是互联网的核心，下面介绍每一层的功能，主要就是介绍每一层的主要协议。&lt;/p&gt;
&lt;p&gt;二、实体层&lt;/p&gt;
&lt;p&gt;我们从最底下的一层开始。&lt;/p&gt;
&lt;p&gt;电脑要组网，第一件事要干什么？当然是先把电脑连起来，可以用光缆、电缆、双绞线、无线电波等方式。&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/db76b0fc26c9ce194d5882fed39bc426.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;这就叫做”实体层”，它就是把电脑连接起来的物理手段。它主要规定了网络的一些电气特性，作用是负责传送0和1的电信号。&lt;/p&gt;
&lt;p&gt;三、链接层&lt;/p&gt;
&lt;p&gt;3.1 定义&lt;/p&gt;
&lt;p&gt;单纯的0和1没有任何意义，必须规定解读方式：多少个电信号算一组？每个信号位有何意义？&lt;/p&gt;
&lt;p&gt;这就是”链接层”的功能，它在”实体层”的上方，确定了0和1的分组方式。&lt;/p&gt;
&lt;p&gt;3.2 以太网协议&lt;/p&gt;
&lt;p&gt;早期的时候，每家公司都有自己的电信号分组方式。逐渐地，一种叫做&lt;a href=&quot;http://zh.wikipedia.org/wiki/%E4%BB%A5%E5%A4%AA%E7%BD%91&quot; target=&quot;_blank&quot;&gt;“以太网”&lt;/a&gt;（Ethernet）的协议，占据了主导地位。&lt;/p&gt;
&lt;p&gt;以太网规定，一组电信号构成一个数据包，叫做”帧”（Frame）。每一帧分成两个部分：标头（Head）和数据（Data）。&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/4c1c395fca52fec968d1bcb51b84df6b.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;“标头”包含数据包的一些说明项，比如发送者、接受者、数据类型等等；”数据”则是数据包的具体内容。&lt;/p&gt;
&lt;p&gt;“标头”的长度，固定为18字节。”数据”的长度，最短为46字节，最长为1500字节。因此，整个”帧”最短为64字节，最长为1518字节。如果数据很长，就必须分割成多个帧进行发送。&lt;/p&gt;
&lt;p&gt;3.3 MAC地址&lt;/p&gt;
&lt;p&gt;上面提到，以太网数据包的”标头”，包含了发送者和接受者的信息。那么，发送者和接受者是如何标识呢？&lt;/p&gt;
&lt;p&gt;以太网规定，连入网络的所有设备，都必须具有”网卡”接口。数据包必须是从一块网卡，传送到另一块网卡。网卡的地址，就是数据包的发送地址和接收地址，这叫做MAC地址。&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/76968e443b9d437e300cef4b761f568a.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;每块网卡出厂的时候，都有一个全世界独一无二的MAC地址，长度是48个二进制位，通常用12个十六进制数表示。&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/d70d8ab3fed257d982cea31fbf8d3920.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;前6个十六进制数是厂商编号，后6个是该厂商的网卡流水号。有了MAC地址，就可以定位网卡和数据包的路径了。&lt;/p&gt;
&lt;p&gt;3.4 广播&lt;/p&gt;
&lt;p&gt;定义地址只是第一步，后面还有更多的步骤。&lt;/p&gt;
&lt;p&gt;首先，一块网卡怎么会知道另一块网卡的MAC地址？&lt;/p&gt;
&lt;p&gt;回答是有一种ARP协议，可以解决这个问题。这个留到后面介绍，这里只需要知道，以太网数据包必须知道接收方的MAC地址，然后才能发送。&lt;/p&gt;
&lt;p&gt;其次，就算有了MAC地址，系统怎样才能把数据包准确送到接收方？&lt;/p&gt;
&lt;p&gt;回答是以太网采用了一种很”原始”的方式，它不是把数据包准确送到接收方，而是向本网络内所有计算机发送，让每台计算机自己判断，是否为接收方。&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/5d6cee57000a2ba8619f72dbb6cd2002.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;上图中，1号计算机向2号计算机发送一个数据包，同一个子网络的3号、4号、5号计算机都会收到这个包。它们读取这个包的”标头”，找到接收方的MAC地址，然后与自身的MAC地址相比较，如果两者相同，就接受这个包，做进一步处理，否则就丢弃这个包。这种发送方式就叫做”广播”（broadcasting）。&lt;/p&gt;
&lt;p&gt;有了数据包的定义、网卡的MAC地址、广播的发送方式，”链接层”就可以在多台计算机之间传送数据了。&lt;/p&gt;
&lt;p&gt;四、网络层&lt;/p&gt;
&lt;p&gt;4.1 网络层的由来&lt;/p&gt;
&lt;p&gt;以太网协议，依靠MAC地址发送数据。理论上，单单依靠MAC地址，上海的网卡就可以找到洛杉矶的网卡了，技术上是可以实现的。&lt;/p&gt;
&lt;p&gt;但是，这样做有一个重大的缺点。以太网采用广播方式发送数据包，所有成员人手一”包”，不仅效率低，而且局限在发送者所在的子网络。也就是说，如果两台计算机不在同一个子网络，广播是传不过去的。这种设计是合理的，否则互联网上每一台计算机都会收到所有包，那会引起灾难。&lt;/p&gt;
&lt;p&gt;互联网是无数子网络共同组成的一个巨型网络，很像想象上海和洛杉矶的电脑会在同一个子网络，这几乎是不可能的。&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/8df8ad3968765af9aa2d6011d73e61f2.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;因此，必须找到一种方法，能够区分哪些MAC地址属于同一个子网络，哪些不是。如果是同一个子网络，就采用广播方式发送，否则就采用”路由”方式发送。（”路由”的意思，就是指如何向不同的子网络分发数据包，这是一个很大的主题，本文不涉及。）遗憾的是，MAC地址本身无法做到这一点。它只与厂商有关，与所处网络无关。&lt;/p&gt;
&lt;p&gt;这就导致了”网络层”的诞生。它的作用是引进一套新的地址，使得我们能够区分不同的计算机是否属于同一个子网络。这套地址就叫做”网络地址”，简称”网址”。&lt;/p&gt;
&lt;p&gt;于是，”网络层”出现以后，每台计算机有了两种地址，一种是MAC地址，另一种是网络地址。两种地址之间没有任何联系，MAC地址是绑定在网卡上的，网络地址则是管理员分配的，它们只是随机组合在一起。&lt;/p&gt;
&lt;p&gt;网络地址帮助我们确定计算机所在的子网络，MAC地址则将数据包送到该子网络中的目标网卡。因此，从逻辑上可以推断，必定是先处理网络地址，然后再处理MAC地址。&lt;/p&gt;
&lt;p&gt;4.2 IP协议&lt;/p&gt;
&lt;p&gt;规定网络地址的协议，叫做IP协议。它所定义的地址，就被称为IP地址。&lt;/p&gt;
&lt;p&gt;目前，广泛采用的是IP协议第四版，简称IPv4。这个版本规定，网络地址由32个二进制位组成。&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/2b37e2ec9913b84d1ffd9b33384f0087.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;习惯上，我们用分成四段的十进制数表示IP地址，从0.0.0.0一直到255.255.255.255。&lt;/p&gt;
&lt;p&gt;互联网上的每一台计算机，都会分配到一个IP地址。这个地址分成两个部分，前一部分代表网络，后一部分代表主机。比如，IP地址172.16.254.1，这是一个32位的地址，假定它的网络部分是前24位（172.16.254），那么主机部分就是后8位（最后的那个1）。处于同一个子网络的电脑，它们IP地址的网络部分必定是相同的，也就是说172.16.254.2应该与172.16.254.1处在同一个子网络。&lt;/p&gt;
&lt;p&gt;但是，问题在于单单从IP地址，我们无法判断网络部分。还是以172.16.254.1为例，它的网络部分，到底是前24位，还是前16位，甚至前28位，从IP地址上是看不出来的。&lt;/p&gt;
&lt;p&gt;那么，怎样才能从IP地址，判断两台计算机是否属于同一个子网络呢？这就要用到另一个参数”子网掩码”（subnet mask）。&lt;/p&gt;
&lt;p&gt;所谓”子网掩码”，就是表示子网络特征的一个参数。它在形式上等同于IP地址，也是一个32位二进制数字，它的网络部分全部为1，主机部分全部为0。比如，IP地址172.16.254.1，如果已知网络部分是前24位，主机部分是后8位，那么子网络掩码就是11111111.11111111.11111111.00000000，写成十进制就是255.255.255.0。&lt;/p&gt;
&lt;p&gt;知道”子网掩码”，我们就能判断，任意两个IP地址是否处在同一个子网络。方法是将两个IP地址与子网掩码分别进行AND运算（两个数位都为1，运算结果为1，否则为0），然后比较结果是否相同，如果是的话，就表明它们在同一个子网络中，否则就不是。&lt;/p&gt;
&lt;p&gt;比如，已知IP地址172.16.254.1和172.16.254.233的子网掩码都是255.255.255.0，请问它们是否在同一个子网络？两者与子网掩码分别进行AND运算，结果都是172.16.254.0，因此它们在同一个子网络。&lt;/p&gt;
&lt;p&gt;总结一下，IP协议的作用主要有两个，一个是为每一台计算机分配IP地址，另一个是确定哪些地址在同一个子网络。&lt;/p&gt;
&lt;p&gt;4.3 IP数据包&lt;/p&gt;
&lt;p&gt;根据IP协议发送的数据，就叫做IP数据包。不难想象，其中必定包括IP地址信息。&lt;/p&gt;
&lt;p&gt;但是前面说过，以太网数据包只包含MAC地址，并没有IP地址的栏位。那么是否需要修改数据定义，再添加一个栏位呢？&lt;/p&gt;
&lt;p&gt;回答是不需要，我们可以把IP数据包直接放进以太网数据包的”数据”部分，因此完全不用修改以太网的规格。这就是互联网分层结构的好处：上层的变动完全不涉及下层的结构。&lt;/p&gt;
&lt;p&gt;具体来说，IP数据包也分为”标头”和”数据”两个部分。&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/8b44d0d0135d42b5995d10cea39927e1.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;“标头”部分主要包括版本、长度、IP地址等信息，”数据”部分则是IP数据包的具体内容。它放进以太网数据包后，以太网数据包就变成了下面这样。&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/d4be7e62ee72f2584d711d6426174dd9.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;IP数据包的”标头”部分的长度为20到60字节，整个数据包的总长度最大为65,535字节。因此，理论上，一个IP数据包的”数据”部分，最长为65,515字节。前面说过，以太网数据包的”数据”部分，最长只有1500字节。因此，如果IP数据包超过了1500字节，它就需要分割成几个以太网数据包，分开发送了。&lt;/p&gt;
&lt;p&gt;4.4 ARP协议&lt;/p&gt;
&lt;p&gt;关于”网络层”，还有最后一点需要说明。&lt;/p&gt;
&lt;p&gt;因为IP数据包是放在以太网数据包里发送的，所以我们必须同时知道两个地址，一个是对方的MAC地址，另一个是对方的IP地址。通常情况下，对方的IP地址是已知的（后文会解释），但是我们不知道它的MAC地址。&lt;/p&gt;
&lt;p&gt;所以，我们需要一种机制，能够从IP地址得到MAC地址。&lt;/p&gt;
&lt;p&gt;这里又可以分成两种情况。第一种情况，如果两台主机不在同一个子网络，那么事实上没有办法得到对方的MAC地址，只能把数据包传送到两个子网络连接处的”网关”（gateway），让网关去处理。&lt;/p&gt;
&lt;p&gt;第二种情况，如果两台主机在同一个子网络，那么我们可以用ARP协议，得到对方的MAC地址。ARP协议也是发出一个数据包（包含在以太网数据包中），其中包含它所要查询主机的IP地址，在对方的MAC地址这一栏，填的是FF:FF:FF:FF:FF:FF，表示这是一个”广播”地址。它所在子网络的每一台主机，都会收到这个数据包，从中取出IP地址，与自身的IP地址进行比较。如果两者相同，都做出回复，向对方报告自己的MAC地址，否则就丢弃这个包。&lt;/p&gt;
&lt;p&gt;总之，有了ARP协议之后，我们就可以得到同一个子网络内的主机MAC地址，可以把数据包发送到任意一台主机之上了。&lt;/p&gt;
&lt;p&gt;五、传输层&lt;/p&gt;
&lt;p&gt;5.1 传输层的由来&lt;/p&gt;
&lt;p&gt;有了MAC地址和IP地址，我们已经可以在互联网上任意两台主机上建立通信。&lt;/p&gt;
&lt;p&gt;接下来的问题是，同一台主机上有许多程序都需要用到网络，比如，你一边浏览网页，一边与朋友在线聊天。当一个数据包从互联网上发来的时候，你怎么知道，它是表示网页的内容，还是表示在线聊天的内容？&lt;/p&gt;
&lt;p&gt;也就是说，我们还需要一个参数，表示这个数据包到底供哪个程序（进程）使用。这个参数就叫做”端口”（port），它其实是每一个使用网卡的程序的编号。每个数据包都发到主机的特定端口，所以不同的程序就能取到自己所需要的数据。&lt;/p&gt;
&lt;p&gt;“端口”是0到65535之间的一个整数，正好16个二进制位。0到1023的端口被系统占用，用户只能选用大于1023的端口。不管是浏览网页还是在线聊天，应用程序会随机选用一个端口，然后与服务器的相应端口联系。&lt;/p&gt;
&lt;p&gt;“传输层”的功能，就是建立”端口到端口”的通信。相比之下，”网络层”的功能是建立”主机到主机”的通信。只要确定主机和端口，我们就能实现程序之间的交流。因此，Unix系统就把主机+端口，叫做”套接字”（socket）。有了它，就可以进行网络应用程序开发了。&lt;/p&gt;
&lt;p&gt;5.2 UDP协议&lt;/p&gt;
&lt;p&gt;现在，我们必须在数据包中加入端口信息，这就需要新的协议。最简单的实现叫做UDP协议，它的格式几乎就是在数据前面，加上端口号。&lt;/p&gt;
&lt;p&gt;UDP数据包，也是由”标头”和”数据”两部分组成。&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/5ab85e4f82b18b4bb69762585a74239b.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;“标头”部分主要定义了发出端口和接收端口，”数据”部分就是具体的内容。然后，把整个UDP数据包放入IP数据包的”数据”部分，而前面说过，IP数据包又是放在以太网数据包之中的，所以整个以太网数据包现在变成了下面这样：&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/7cda488ed620f5b4808494bbadc15fe9.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;UDP数据包非常简单，”标头”部分一共只有8个字节，总长度不超过65,535字节，正好放进一个IP数据包。&lt;/p&gt;
&lt;p&gt;5.3 TCP协议&lt;/p&gt;
&lt;p&gt;UDP协议的优点是比较简单，容易实现，但是缺点是可靠性较差，一旦数据包发出，无法知道对方是否收到。&lt;/p&gt;
&lt;p&gt;为了解决这个问题，提高网络可靠性，TCP协议就诞生了。这个协议非常复杂，但可以近似认为，它就是有确认机制的UDP协议，每发出一个数据包都要求确认。如果有一个数据包遗失，就收不到确认，发出方就知道有必要重发这个数据包了。&lt;/p&gt;
&lt;p&gt;因此，TCP协议能够确保数据不会遗失。它的缺点是过程复杂、实现困难、消耗较多的资源。&lt;/p&gt;
&lt;p&gt;TCP数据包和UDP数据包一样，都是内嵌在IP数据包的”数据”部分。TCP数据包没有长度限制，理论上可以无限长，但是为了保证网络的效率，通常TCP数据包的长度不会超过IP数据包的长度，以确保单个TCP数据包不必再分割。&lt;/p&gt;
&lt;p&gt;六、应用层&lt;/p&gt;
&lt;p&gt;应用程序收到”传输层”的数据，接下来就要进行解读。由于互联网是开放架构，数据来源五花八门，必须事先规定好格式，否则根本无法解读。&lt;/p&gt;
&lt;p&gt;“应用层”的作用，就是规定应用程序的数据格式。&lt;/p&gt;
&lt;p&gt;举例来说，TCP协议可以为各种各样的程序传递数据，比如Email、WWW、FTP等等。那么，必须有不同协议规定电子邮件、网页、FTP数据的格式，这些应用程序协议就构成了”应用层”。&lt;/p&gt;
&lt;p&gt;这是最高的一层，直接面对用户。它的数据就放在TCP数据包的”数据”部分。因此，现在的以太网的数据包就变成下面这样。&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/381d55ee5bc30d802596c65de8bb24d7.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;至此，整个互联网的五层结构，自下而上全部讲完了。这是从系统的角度，解释互联网是如何构成的。&lt;a href=&quot;http://blog.jobbole.com/77931/&quot; target=&quot;_blank&quot;&gt;下一篇&lt;/a&gt;，我反过来，从用户的角度，自上而下看看这个结构是如何发挥作用，完成一次网络数据交换的。&lt;/p&gt;


&lt;!-- div id=&quot;ad1&quot;&gt;
&lt;/div --&gt;


	


	

</description>
        <pubDate>Wed, 08 Oct 2014 00:00:00 +0800</pubDate>
        <link>http://iftti.com/posts/2014/2014-10-08-77851-3c3456586.html</link>
        <guid isPermaLink="true">http://iftti.com/posts/2014/2014-10-08-77851-3c3456586.html</guid>
        
        
        <category>jobbole</category>
        
      </item>
    
      <item>
        <title>【翻译】Kibana 4 beta 1 发版日志</title>
        <description>

  
  &lt;div style=&quot;background-color: #FFF;&quot;&gt;
    &lt;p&gt;原文地址见：&lt;a href=&quot;http://www.elasticsearch.org/blog/kibana-4-beta-1-released/&quot;&gt;http://www.elasticsearch.org/blog/kibana-4-beta-1-released/&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;今天，我们&lt;del&gt;自豪高兴满意控制不住地兴奋过头欣喜若狂&lt;/del&gt;相当高兴得给大家分享一下 Kibana 项目的未来，以及 Kibana 4 的第一个 beta 版本。&lt;/p&gt;
&lt;h2 id=&quot;section&quot;&gt;我现在就要！快给我！&lt;/h2&gt;
&lt;p&gt;从&lt;a href=&quot;http://www.elasticsearch.org/overview/kibana/installation/&quot;&gt;这里&lt;/a&gt;下载，然后看 &lt;a href=&quot;https://github.com/elasticsearch/kibana/blob/master/README.md&quot;&gt;README.md&lt;/a&gt; 里新的而且更简单的安装流程。当然，你最好还是读一下本文剩下的内容，有很多超棒的秘诀呢！&lt;/p&gt;
&lt;h2 id=&quot;kibana-4&quot;&gt;欢迎来到 kibana 4&lt;/h2&gt;
&lt;p&gt;我们正走在 Kibana 4 的漫漫长路上：可以预见还会有好几个 beta 版本，每个都有新的特性，可视化和改善。我们梳理了各种反馈、邮件列表、IRC 以及 Github 的 issue ，把特性加入到这个 beta1 版本中，真是罪孽深重。我们已经在为 beta2 版本努力工作，在此，很高兴分享一下我们的 roadmap，查看 Github 上打有 “&lt;a href=&quot;https://github.com/elasticsearch/kibana/issues?q=is%3Aopen+is%3Aissue+label%3Aroadmap&quot;&gt;Roadmap&lt;/a&gt;” 标签的 issue。你们的反馈是我们永远做正确的事的保证。&lt;/p&gt;
&lt;p&gt;反馈之外，我们回头想了想人们是怎么看数据的，更进一步，人们是怎么解决真实问题的。我们发现一个问题总是能引出另一些问题，而这些问题又能引出更多其他问题。如果你参加了 Monitotama，或者其他 Elasticsearch 见面会，你可能已经看到过 Kibana 4 概念性的原型演示。它可以让你创建更复杂的图标，Kibana 4 从 PoC 出发，扩展出一大堆新特性，让你编写问题，得到解答，然后解决之前从来没这么解决过的问题。&lt;/p&gt;
&lt;p&gt;这种组合方式在 Kibana 4 中体现为聚合、搜索、可视化和仪表板融合在一起的方式。为了简化组成，我们把 Kibana 4 分成 3 个不同的界面，虽然一起工作，但是每个负责解决不同的一部分问题。&lt;/p&gt;
&lt;h2 id=&quot;section-1&quot;&gt;熟悉的界面&lt;/h2&gt;
&lt;p&gt;如果你是 Kibana 老用户，你会发现主页上 Discover 标签页的样子很熟悉。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/images/chenlinux.com/5a81f0bdb5b0982c0f9c0179529485a5.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;Discover 功能跟原先的带有一个文档表格和事件时间轴的搜索界面很像。在搜索框里输入，敲回车，然后让 Kibana 去挖掘你的 Elasticsearch 索引。说到索引，有一个快速下拉菜单让你在搜索的时候灵活的在多个索引之间切换。要切换回上一个索引，点击浏览器的回退按钮即可。不喜欢新的搜索关键词？同样点击回退按钮就能返回原来的搜索词了。当然，搜索框的历史中也存着过去的记录。&lt;/p&gt;
&lt;p&gt;说道搜索，你既可以输入 Lucene Query String 语法，也可以用上一个经常被要求的特性，&lt;strong&gt;Elasticsearch JSON 搜索&lt;/strong&gt; 到搜索框里。我们知道 JSON 格式可能比较难输对，所以不管你输入的是 Lucene Query String 还是 JSON，我们都会在发送给 Elasticsearch 之前替你验证一遍语法。不管你在 Kibana 4 的任何位置输入请求，这点都是生效的。&lt;/p&gt;
&lt;p&gt;这样搜索也可以保存下来留待后用。重要的是：搜索不在绑定在仪表板上，他们可以在 Discover 页上再次调用，也可以运用在可能稍后才添加到仪表板上的可视化页里。因为，不管你在仪表板的哪一屏，&lt;strong&gt;搜索一直都会通过 URL 传递&lt;/strong&gt;，所以链接到搜索非常简单。&lt;/p&gt;
&lt;h2 id=&quot;section-2&quot;&gt;画图的在这里&lt;/h2&gt;
&lt;p&gt;Kibana 4 的 Visualize 标签是之前说的概念原型里最高潮的地方。Kibana 4 把 Elasticsearch 的 nested 聚合函数的威力带到鼠标点击上。比如我想知道哪些国家访问我的网站，什么时候访问的，他们是否登录认证了？通过一个 canvas 上的单一请求，我就可以问出上面这些问题，然后看到结果是怎么相互联系的：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/images/chenlinux.com/a4a93b13ab5897dfc7ccc580eb7ed395.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;Kibana 3 的时候，时间只能在 histogram 面板上显示，而 terms 只能在柱状图上显示。Kibana 4 可以利用多个 &lt;strong&gt;Elasticsearch 聚合函数&lt;/strong&gt;。这包括 bucket 和 metric 聚合函数，其中有备受期待的&lt;strong&gt;基数&lt;/strong&gt;(又叫唯一计数)聚合函数，更多支持还在实现中。我们不得不创建了一个全新的可视化框架来处理复杂的聚合函数。目前有三种支持的类型：柱状图，线状图和光圈图。同样，更多支持还在实现中。未来每个 Kibana 4 的 beta 版本都值得你期待。&lt;/p&gt;
&lt;p&gt;光圈图类似多层次的饼图。理论上它可以有无限的环：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/images/chenlinux.com/98999f08bb400c82e886fb6d2371d6a0.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;柱状图现在还不单单可以做时间。这里我们展示根据文件后缀名分解文件大小范围。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/images/chenlinux.com/aa6816aa66f1177f81edee20367e35f6.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;现在你可能已经注意到每个可视化页底部的灰色小条。点击它，就可以看到图背后的源数据，然后，在大众要求下，提供了&lt;strong&gt;导出到CSV&lt;/strong&gt; 以便后续分析的功能。你还可以看到 Elasticsearch 请求和响应的内容，以及请求的处理耗时。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/images/chenlinux.com/504bcf3ceb9a309d0469e3bb5679c3ad.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;Visualization 既可以互动式搜索创建，让你在建图的时候修改请求，也可以关联到一个之前通过 Discover 标签创建保存的请求上。这样你可以关联一个请求到多个可视化页，如果需要更新一个搜索参数，只需要更新单独一个请求就行了。比如，假设你有多个图表，是用下面语句搜索图片内容的：&lt;/p&gt;
&lt;p&gt;&lt;code&gt;
png OR jpg
&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;保存成 “Images”。然后你打算支持动态 GIF 格式，你只需要更新 “Images” 的内容然后保存即可。所有关联了 “Images” 请求的图都会自动应用变更。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/images/chenlinux.com/cb00f30b6ef8165bbac7c089dfbe6c5b.jpg&quot; alt=&quot;&quot;&gt;]&lt;/p&gt;
&lt;h2 id=&quot;section-3&quot;&gt;给我看更多的图！&lt;/h2&gt;
&lt;p&gt;当然，你依然可以创建令人惊叹的仪表板，而且它们现在更方便创建和管理了。过去那堆凌乱的配置框一去不复返了。添加进仪表板的每个面板都可以在 Visualize 标签页理创建、保存，并且重复利用。就像保存了的搜索可以在多个 visualizations 里使用一样，保存了的 visualization 也可以在多个仪表板里使用。你需要更新一个 visualization 的话，只需要在一个地方修改好，每个仪表板里的都会应用变更。&lt;/p&gt;
&lt;p&gt;更进一步，虽然请求和可视化是绑定到一个选定的索引的，仪表板却不用。&lt;strong&gt;一个仪表板可以有从不同索引来的可视化&lt;/strong&gt;。这意味着，你可以从你的用户索引关联到网站流量索引，从销售数据关联到市场研究再关联到气象站日志。这些都可以在同一屏上！&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/images/chenlinux.com/5d03380f74f632ca72a9b8ad1860e21c.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;h2 id=&quot;section-4&quot;&gt;更多&lt;/h2&gt;
&lt;p&gt;一篇博客里完全不够说完全部内容，所以去下载安装然后亲自试试 &lt;a href=&quot;http://www.elasticsearch.org/overview/kibana/installation/&quot;&gt;HERE&lt;/a&gt; 吧。如果你来自 Kibana 3，我们收集了一个小小的 FAQ 解释：&lt;a href=&quot;https://github.com/elasticsearch/kibana/blob/master/K3_FAQ.md&quot;&gt;HERE&lt;/a&gt;。还是老话，我们需要你的反馈，构建 Kibana 4 的每一天，我们都用得着这些反馈，而我们也会继续让 Kibana 变得更好，更快，更简单。&lt;/p&gt;
    &lt;hr&gt;
    
    &lt;hr&gt;
  &lt;!-- UY BEGIN --&gt;


&lt;!-- UY END --&gt;
  &lt;/div&gt;

</description>
        <pubDate>Tue, 07 Oct 2014 00:00:00 +0800</pubDate>
        <link>http://iftti.com/posts/2014/2014-10-07-kibana-4-beta-1-released-bce65c2c2.html</link>
        <guid isPermaLink="true">http://iftti.com/posts/2014/2014-10-07-kibana-4-beta-1-released-bce65c2c2.html</guid>
        
        
        <category>chenlinux</category>
        
      </item>
    
      <item>
        <title>【翻译】Kibana 3 升级到 4 的常见问答</title>
        <description>

  
  &lt;div style=&quot;background-color: #FFF;&quot;&gt;
    &lt;p&gt;原文见&lt;a href=&quot;https://github.com/elasticsearch/kibana/blob/master/K3_FAQ.md&quot;&gt;https://github.com/elasticsearch/kibana/blob/master/K3_FAQ.md&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;问：我在 Kibana 3 里最想要的某某特性有了么？
答：就会有了！我们已经以 ticket 形式发布了目前的 roadmap。查看 GitHub 上的 beta 里程碑，看看有没有你想要的特性。&lt;/p&gt;
&lt;p&gt;问：仪表板模式是否兼容？
答：不好意思，不兼容了。要创建我们想要的新特性，还是用原先的模式是不可能的。Aggregation 跟 Facet 请求从根本上工作方式就不一样，新的仪表板不再绑定成行和列的样式，而且搜索框，可视化和仪表板的关系过于复杂，我们不得不重新设计一遍，来保证它的灵活可用。&lt;/p&gt;
&lt;p&gt;问：怎么做多项搜索？
答：”filters” Aggregation 可以运行你输入多项搜索条件然后完成可视化。甚至你可以在这里面自己写 JSON。&lt;/p&gt;
&lt;p&gt;问：模板化/脚本化仪表板还在么？
答：看看 URL 吧。每个应用的状态都记录在那里面，包括所有的过滤器，搜索和列。现在构建脚本化仪表板比过去简单多了。URL 是采用 RISON 编码的。&lt;/p&gt;
&lt;h3 id=&quot;section&quot;&gt;译者注：&lt;/h3&gt;
&lt;p&gt;RISON 是一个跟 JSON 很类似，还节省不少长度的东西。其官网见：&lt;a href=&quot;http://mjtemplate.org/examples/rison.html&quot;&gt;http://mjtemplate.org/examples/rison.html&lt;/a&gt;。但是我访问看似乎已经挂了，更多一点的说明可以看&lt;a href=&quot;https://github.com/Nanonid/rison&quot;&gt;https://github.com/Nanonid/rison&lt;/a&gt;。&lt;/p&gt;
    &lt;hr&gt;
    
    &lt;hr&gt;
  &lt;!-- UY BEGIN --&gt;


&lt;!-- UY END --&gt;
  &lt;/div&gt;

</description>
        <pubDate>Tue, 07 Oct 2014 00:00:00 +0800</pubDate>
        <link>http://iftti.com/posts/2014/2014-10-07-kibana-3-migration-faq-25c749394.html</link>
        <guid isPermaLink="true">http://iftti.com/posts/2014/2014-10-07-kibana-3-migration-faq-25c749394.html</guid>
        
        
        <category>chenlinux</category>
        
      </item>
    
      <item>
        <title>开源一个Key-Value存储工具类</title>
        <description>
&lt;h2&gt;前言&lt;/h2&gt;

&lt;p&gt;还记得大学刚学数据库那会儿，天真地以为世界上所有的存储都需要用数据库来做。后来毕业后，正值NOSQL流行，那时我在网易参与了网易微博的开发，我们当时使用了有道自己做的“BigTable”— OMAP来存储微博数据，那个时候才发现，其实Key-Value这种简单的存储也能搞定微博这类不太简单的存储逻辑。&lt;/p&gt;

&lt;p&gt;相比MYSQL，当数据量上千万后，NOSQL的优势体现出来了：对于海量数据，NOSQL在存取速度上没有任何影响，另外，天生的多备份和分布式，也说数据安全和扩容变得异常容易。&lt;/p&gt;

&lt;h2&gt;iOS端的尝试&lt;/h2&gt;

&lt;p&gt;后来我从后台转做iOS端的开发，我就尝试了在iOS端直接使用Key-Value式的存储。经过在粉笔网、猿题库、小猿搜题三个客户端中的尝试后，我发现Key-Value式的存储不但完全能够满足大多数移动端开发的需求，而且非常适合移动端采用。主要原因是：移动端存储的数据量不会很大：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;如果是单机的应用（例如效率工具Clear），用户自己一个人创建的数据最多也就上万条。&lt;/li&gt;
&lt;li&gt;如果是有服务端的应用（例如网易新闻，微博），那移动端通常不会保存全量的数据，每次会从服务器上获取数据，本地只是做一些内容的缓存而已，所以也不会有很大的数据量。&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;如果数据量不大的话，那么在iOS端使用最简单直接的Key-Value存储就能带来开发上的效率优势。它能保证：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Model层的代码编写简单，易于测试。&lt;/li&gt;
&lt;li&gt;由于Value是JSON格式，所以在做Model字段更改时，易于扩展和兼容。&lt;/li&gt;
&lt;/ol&gt;


&lt;h2&gt;实现方案&lt;/h2&gt;

&lt;p&gt;在存储引擎上，2年前我直接选择了Sqlite当做存储引擎，相当于每个数据库表只有Key，Value两个字段。后来，随着LevelDB的流行，业界也有一些应用采用了LevelDB来做iOS端的Key-Value存储引擎，例如开源的&lt;a href=&quot;https://github.com/viewfinderco/viewfinder&quot;&gt;ViewFinder&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;因为LevelDB本身并不是为移动端设计的，我担心它过于占用内存，我自己也没有看到业界有在移动端针对LevelDB做很详细的测试，连LevelDB的iOS端移植都不是官方做的。加上我自己写的基于Sqlite的Key-Value存储用着也没有什么问题，所以我也就一直没有更换成LevelDB。&lt;/p&gt;

&lt;h2&gt;开源&lt;/h2&gt;

&lt;p&gt;经过两年的使用和测试，我认为它非常好用，而且代码也非常简单，只有不到400行。所以现在开源分享给大家，这个项目叫&lt;code&gt;YTKKeyValueStore&lt;/code&gt;，项目在&lt;a href=&quot;https://github.com/yuantiku/YTKKeyValueStore&quot;&gt;这里&lt;/a&gt;。以下是一个简单的使用示例：&lt;/p&gt;

&lt;figure class=&quot;code&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;table&gt;&lt;tr&gt;
&lt;td class=&quot;gutter&quot;&gt;&lt;pre class=&quot;line-numbers&quot;&gt;&lt;span class=&quot;line-number&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;line-number&quot;&gt;2&lt;/span&gt;
&lt;span class=&quot;line-number&quot;&gt;3&lt;/span&gt;
&lt;span class=&quot;line-number&quot;&gt;4&lt;/span&gt;
&lt;span class=&quot;line-number&quot;&gt;5&lt;/span&gt;
&lt;span class=&quot;line-number&quot;&gt;6&lt;/span&gt;
&lt;span class=&quot;line-number&quot;&gt;7&lt;/span&gt;
&lt;span class=&quot;line-number&quot;&gt;8&lt;/span&gt;
&lt;span class=&quot;line-number&quot;&gt;9&lt;/span&gt;
&lt;span class=&quot;line-number&quot;&gt;10&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;code class=&quot;&quot;&gt;&lt;span class=&quot;line&quot;&gt;YTKKeyValueStore *store = [[YTKKeyValueStore alloc] initDBWithName:@&quot;test.db&quot;];
&lt;/span&gt;&lt;span class=&quot;line&quot;&gt;NSString *tableName = @&quot;user_table&quot;;
&lt;/span&gt;&lt;span class=&quot;line&quot;&gt;[store createTableWithName:tableName];
&lt;/span&gt;&lt;span class=&quot;line&quot;&gt;// 保存
&lt;/span&gt;&lt;span class=&quot;line&quot;&gt;NSString *key = @&quot;1&quot;;
&lt;/span&gt;&lt;span class=&quot;line&quot;&gt;NSDictionary *user = @{@&quot;id&quot;: @1, @&quot;name&quot;: @&quot;tangqiao&quot;, @&quot;age&quot;: @30};
&lt;/span&gt;&lt;span class=&quot;line&quot;&gt;[store putObject:user withId:key intoTable:tableName];
&lt;/span&gt;&lt;span class=&quot;line&quot;&gt;// 查询
&lt;/span&gt;&lt;span class=&quot;line&quot;&gt;NSDictionary *queryUser = [store getObjectById:key fromTable:tableName];
&lt;/span&gt;&lt;span class=&quot;line&quot;&gt;NSLog(@&quot;query data result: %@&quot;, queryUser);&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;&lt;/figure&gt;


&lt;h2&gt;其它&lt;/h2&gt;

&lt;p&gt;两年前写过不少测试用例，后来给弄丢了，所以现在开项项目中还没有测试用例。由于时间关系，更详细的使用说明稍后会更新到项目中。&lt;/p&gt;

</description>
        <pubDate>Fri, 03 Oct 2014 00:00:00 +0800</pubDate>
        <link>http://iftti.com/posts/2014/2014-10-03-opensouce-a-key-value-storage-tool-07b219e11.html</link>
        <guid isPermaLink="true">http://iftti.com/posts/2014/2014-10-03-opensouce-a-key-value-storage-tool-07b219e11.html</guid>
        
        
        <category>devtang</category>
        
      </item>
    
      <item>
        <title>Mojolicious 应用的自定义子命令</title>
        <description>

  
  &lt;div style=&quot;background-color: #FFF;&quot;&gt;
    &lt;p&gt;Mojolicious 框架开发应用的时候，可以跟 RoR 一样通过一系列子命令简化很多复杂操作。最简单的来说，就是快速生成整个 web 项目目录：&lt;code&gt;mojo generate youapp&lt;/code&gt;。更多子命令见：&lt;a href=&quot;http://cpan.php-oa.com/perldoc/Mojolicious/Commands&quot;&gt;http://cpan.php-oa.com/perldoc/Mojolicious/Commands&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;其实我们还可以自己扩展这个子命令方式，实现自己的子命令。如果打算继续使用 &lt;code&gt;mojo subcommand&lt;/code&gt; 的方式，那就把自己的子命令模块叫做 &lt;code&gt;Mojolicious::Command::yourcommand&lt;/code&gt;，而如果打算在自己的名字空间下使用，比如叫 &lt;code&gt;MyApp::Command::mycommand&lt;/code&gt;，那么需要在 &lt;code&gt;MyApp.pm&lt;/code&gt; 里加一行代码，设置一下名字空间：&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-perl&quot; data-lang=&quot;perl&quot;&gt;&lt;span class=&quot;k&quot;&gt;sub &lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;startup&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;my&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$self&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;shift&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
        &lt;span class=&quot;nb&quot;&gt;push&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;@&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;commands&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;namespaces&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&#39;MyApp::Command&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
        &lt;span class=&quot;o&quot;&gt;...&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;};&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;然后就可以写自己的 &lt;code&gt;MyApp::Command::mycommand&lt;/code&gt; 了：&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-perl&quot; data-lang=&quot;perl&quot;&gt;&lt;span class=&quot;nb&quot;&gt;package&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;MyApp::Command::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mycommand&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;use&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;strict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;use&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;warnings&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;use&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;Mojo::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Base&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&#39;Mojolicious::Command&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;use&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;Mojo::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;UserAgent&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;has&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;usage&lt;/span&gt;       &lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;usage: $0 migratint [username] [dashboards...]\n&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;has&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;description&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;kibana-int index migration for auth users\n&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;has&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ua&lt;/span&gt;          &lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;sub &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;Mojo::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;UserAgent&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;};&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;sub &lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;run&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;my&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$user&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;@_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;my&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$config&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;app&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;my&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$ua&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ua&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;o&quot;&gt;...&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;大致就是这样：&lt;/p&gt;
&lt;p&gt;继承 &lt;strong&gt;Mojolicious::Command&lt;/strong&gt; 类。这样就会有 usage 和 description 两个属性和 run 方法。&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;usage 属性用来在你执行 &lt;code&gt;script/myapp help mycommand&lt;/code&gt; 的时候输出信息；&lt;/li&gt;
  &lt;li&gt;description 属性用来在你执行 &lt;code&gt;script/myapp help&lt;/code&gt; 罗列全部可用子命令的时候描述该命令的作用；&lt;/li&gt;
  &lt;li&gt;run 方法是命令的入口函数。命令行后面的参数都会传递给 run 方法。如果你的子命令需要复杂处理，这里依然可以用 &lt;a href=&quot;https://metacpan.org/pod/Getopt::Long#Parsing-options-from-an-arbitrary-array&quot;&gt;GetOpt::Long&lt;/a&gt; 模块中的 &lt;code&gt;GetOptionsFromArray&lt;/code&gt; 函数处理。&lt;/li&gt;
&lt;/ul&gt;
    &lt;hr&gt;
    
    &lt;hr&gt;
  &lt;!-- UY BEGIN --&gt;


&lt;!-- UY END --&gt;
  &lt;/div&gt;

</description>
        <pubDate>Wed, 01 Oct 2014 00:00:00 +0800</pubDate>
        <link>http://iftti.com/posts/2014/2014-10-01-custom-mojolicious-app-command-014a96277.html</link>
        <guid isPermaLink="true">http://iftti.com/posts/2014/2014-10-01-custom-mojolicious-app-command-014a96277.html</guid>
        
        
        <category>chenlinux</category>
        
      </item>
    
      <item>
        <title>实现键值对存储（二）：以现有键值对存储为模型</title>
        <description>

					
		
&lt;!-- div id=&quot;ad1&quot;&gt;
&lt;script type=&quot;text/javascript&quot;&gt;
google_ad_client = &quot;ca-pub-7056282119617872&quot;;
google_ad_slot = &quot;6645040531&quot;;
google_ad_width = 300;
google_ad_height = 250;
&lt;/script&gt;
&lt;script type=&quot;text/javascript&quot;
src=&quot;http://pagead2.googlesyndication.com/pagead/show_ads.js&quot;&gt;
&lt;/script&gt;
&lt;/div&gt;
&lt;br/ --&gt;

&lt;p&gt;本文中，开头我会解释使用现有模型而非重头开始此项目的原因。我会阐述一系列选择键值对存储模型的标准。最后我将对一些广为人知的键值对存储项目做一个概述，并用这些标准选择其中一些作为模型。本文将包含：&lt;/p&gt;
&lt;p&gt;1. 不重新发明轮子&lt;br&gt;
2. 备选模型和选择标准&lt;br&gt;
3. 选择的键值对存储的概述&lt;br&gt;
4. 参考文献&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h3 align=&quot;left&quot;&gt;1. 不重新发明轮子&lt;b&gt;&lt;/b&gt;
&lt;/h3&gt;
&lt;p&gt;键值对存储已经被人们唱好至少30年了&lt;a href=&quot;http://codecapsule.com/2012/12/03/implementing-a-key-value-store-part-2-using-existing-key-value-stores-as-models/#ref_1&quot;&gt;[1]&lt;/a&gt;。最著名的一个项目是DBM，Kenneth Thompson为Unix第七版编写的最早的数据库管理器并在1979年发布&lt;a href=&quot;http://codecapsule.com/2012/12/03/implementing-a-key-value-store-part-2-using-existing-key-value-stores-as-models/#ref_2&quot;&gt;[2]&lt;/a&gt;。工程师们遇到了和这些数据库系统相关的一些问题，并选择或放弃了各种设计和数据结构的想法。对实际生活中的问题进行试验并从中学习。如果不考虑他们的工作并从头开始是很愚蠢的，只会重复他们之前所犯过的错误。John Gall的系统学中的Gall定理：&lt;/p&gt;
&lt;p style=&quot;padding-left: 30px;&quot;&gt;&lt;span style=&quot;color: #888888;&quot;&gt;任何可以运作的复杂系统都是从可以运作的简单系统发展而来的。其逆命题同样是真命题：由无法正常运作的系统设计而来的复杂系统是不可能正常运作的。你必须重头再来，从一个可运作的简单系统开始。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;这段引述为我的键值对存储项目开发带来了两个基础思想。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;1. 使用模型。&lt;/strong&gt;我需要识别出那些存在了一段时间的键值对存储，甚至更进一步，先前成功的键值对存储的继任者。这是其可靠设计的证明，并随着时间在迭代中凝练。这些选择过的建筑的存储应该作为我现在正在工作的项目的模型。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;2.起点小。&lt;/strong&gt;这个项目的第一版必须小且简单，这样它的设计就能简单的测试并通过。如果需要的话，改进和额外功能必须在后续版本中加入。&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h3 align=&quot;left&quot;&gt;2. 待选模型和选择标准&lt;b&gt;&lt;/b&gt;
&lt;/h3&gt;
&lt;p&gt;在对键值对存储和NoSQL数据库做过一点研究后，我决定将下面的几个作为进一步选择的选项：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;DBM&lt;/li&gt;
&lt;li&gt;Berkeley DB&lt;/li&gt;
&lt;li&gt;Kyoto Cabinet&lt;/li&gt;
&lt;li&gt;Memcached and MemcacheDB&lt;/li&gt;
&lt;li&gt;LevelDB&lt;/li&gt;
&lt;li&gt;MongoDB&lt;/li&gt;
&lt;li&gt;Redis&lt;/li&gt;
&lt;li&gt;OpenLDAP&lt;/li&gt;
&lt;li&gt;SQLite&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;选择标准如下:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;我想使用面向对象编程来创建键值对存储，所以在设计上，我必须从由面向对象语言编写的项目中汲取灵感。&lt;/li&gt;
&lt;li&gt;至于底层数据结构，我想要一个存在硬盘上的哈希表，于是我需要选择一个提供读写信息到硬盘上的方法的项目。&lt;/li&gt;
&lt;li&gt;我同样想让这个数据存储能够有网络接入。&lt;/li&gt;
&lt;li&gt;我不需要查询引擎或者方法来访问结构化的数据.&lt;/li&gt;
&lt;li&gt;不必完全支持ACID规范。&lt;/li&gt;
&lt;li&gt;鉴于这个项目是我自己弄的，我想使用那些由小团队实现的项目模型，理想情况下是一两个人。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;3. 所选键值对的概览&lt;/h3&gt;
&lt;p&gt;三个获选的模型是Berkeley DB、Kyoto Cabinet 和LevelDB。Berkeley DB和Kyoto Cabinet作为DBM的继任者有着相同的历史。此外，Berkeley DB 和 Kyoto Cabinet 并非“初版”。这表示他俩与其他初次实现的键值对存储项目比较更加可靠。LevelDB则更加现代，并基于LSM树的数据结构，其对于哈希表模式来说是无用的。然而其代码是我见过最干净的。这三个项目都是由一两个人开发的。下面是他们各自的详细信息。&lt;/p&gt;
&lt;h4 align=&quot;left&quot;&gt;Berkeley DB&lt;/h4&gt;
&lt;p&gt;Berkeley DB的开发始于1986年，这表示我开始写这篇文章的时候它已经存在了26年了。Berkeley DB是作为DBM的继任者而开发的，并实现了一个哈希表。第一版是由Margo Seltzer &lt;a href=&quot;http://codecapsule.com/2012/12/03/implementing-a-key-value-store-part-2-using-existing-key-value-stores-as-models/#ref_22&quot;&gt;[22]&lt;/a&gt; 和 Ozan Yigit &lt;a href=&quot;http://codecapsule.com/2012/12/03/implementing-a-key-value-store-part-2-using-existing-key-value-stores-as-models/#ref_23&quot;&gt;[23]&lt;/a&gt; 在加州大学伯克利分校的时候编写的。这个项目后来被Oracle获得，并由其继续开发。&lt;/p&gt;
&lt;p&gt;Berkeley DB最初是由C实现的，并且现在仍然是只用C。其通过增量过程开发的，就是说在每个主版本增加新的功能。Berkeley DB从一个简单的键值对存储，进化到管理并行访问、事务及复原、及同步功能&lt;a href=&quot;http://codecapsule.com/2012/12/03/implementing-a-key-value-store-part-2-using-existing-key-value-stores-as-models/#ref_4&quot;&gt;[4]&lt;/a&gt;。Berkeley DB的使用非常广泛，有着数亿已部署的拷贝&lt;a href=&quot;http://codecapsule.com/2012/12/03/implementing-a-key-value-store-part-2-using-existing-key-value-stores-as-models/#ref_5&quot;&gt;[5]&lt;/a&gt;，这是可以相信其架构及其可靠的证据。关于其设计的更多信息可以在“&lt;i&gt;Berkeley DB Programmer’s Reference Guide&lt;/i&gt;”&lt;a href=&quot;http://codecapsule.com/2012/12/03/implementing-a-key-value-store-part-2-using-existing-key-value-stores-as-models/#ref_6&quot;&gt;[6]&lt;/a&gt; 的介绍和“&lt;i&gt;The Architecture of Open Source Applications, Volume 1&lt;/i&gt;” &lt;a href=&quot;http://codecapsule.com/2012/12/03/implementing-a-key-value-store-part-2-using-existing-key-value-stores-as-models/#ref_5&quot;&gt;[5]&lt;/a&gt;的开头中找到。&lt;/p&gt;
&lt;h4&gt;Kyoto Cabinet&lt;/h4&gt;
&lt;p&gt;Kyoto Cabinet在2009年由Mikio Hirabayashi &lt;a href=&quot;http://codecapsule.com/2012/12/03/implementing-a-key-value-store-part-2-using-existing-key-value-stores-as-models/#ref_24&quot;&gt;[24]&lt;/a&gt; 引进。其现在仍在积极进化中。Kyoto Cabinet是同一个作者的其它键值对存储：Tokyo Cabinet (2007发布) 和QDBM (2003发布, 2000开始)的继任者。QDBM打算作为DBM的高性能继任者&lt;a href=&quot;http://codecapsule.com/2012/12/03/implementing-a-key-value-store-part-2-using-existing-key-value-stores-as-models/#ref_7&quot;&gt;[7]&lt;/a&gt;。Kyoto Cabinet尤其有意思，因为它有着DBM的纯正血统，并且它的作者在键值对存储方向工作12年了。在浸淫三个键值对存储这么多年之后，没有理由怀疑作者有着对结构需求的坚实理解，以及随之的对性能瓶颈的成因的极强认识。&lt;/p&gt;
&lt;p&gt;Kyoto Cabinet是由C++实现的，并实现了一个哈希表，一个B+树，以及其他一些深奥的数据结构。其同样提供了出色的性能&lt;a href=&quot;http://codecapsule.com/2012/12/03/implementing-a-key-value-store-part-2-using-existing-key-value-stores-as-models/#ref_16&quot;&gt;[16]&lt;/a&gt;。然而，因其内部参数的原因，似乎有些性能问题。的确，很多人报道说只要数据条目的数量保持在某一特定的阈值（正比于桶数组大小，其由创建数据库文件时的参数所确定）以下，性能就很好。一旦超过这个阈值，性能似乎急剧下降&lt;a href=&quot;http://codecapsule.com/2012/12/03/implementing-a-key-value-store-part-2-using-existing-key-value-stores-as-models/#ref_18&quot;&gt;[18]&lt;/a&gt;&lt;a href=&quot;http://codecapsule.com/2012/12/03/implementing-a-key-value-store-part-2-using-existing-key-value-stores-as-models/#ref_19&quot;&gt;[19]&lt;/a&gt;。Tokyo Cabinet &lt;a href=&quot;http://codecapsule.com/2012/12/03/implementing-a-key-value-store-part-2-using-existing-key-value-stores-as-models/#ref_20&quot;&gt;[20]&lt;/a&gt; &lt;a href=&quot;http://codecapsule.com/2012/12/03/implementing-a-key-value-store-part-2-using-existing-key-value-stores-as-models/#ref_21&quot;&gt;[21]&lt;/a&gt; 中也有相同的问题。这表示如果某项目的需求在数据库使用的时候改变，你可能会遇到严重的问题。而我们都知道，软件中的改变是如此的频繁。&lt;/p&gt;
&lt;h4&gt;LevelDB&lt;/h4&gt;
&lt;p&gt;LevelDB是由Google职员Jeffrey Dean &lt;a href=&quot;http://codecapsule.com/2012/12/03/implementing-a-key-value-store-part-2-using-existing-key-value-stores-as-models/#ref_8&quot;&gt;[8]&lt;/a&gt; 和 Sanjay Ghemawat &lt;a href=&quot;http://codecapsule.com/2012/12/03/implementing-a-key-value-store-part-2-using-existing-key-value-stores-as-models/#ref_9&quot;&gt;[9]&lt;/a&gt; 开发，他们为Google传说中的基础建设项目MapReduce和BigTable工作。基于Dean和Ghemawat在在Google工作时获得的大规模问题上的经验，他们很有可能很了解他们正在做的东西。和大多数键值对存储项目相比，LevelDB有一个很有意思的不同点就是它不用哈希表或者B-树作为底层数据结构，而是基于一个日志结构的合并树&lt;a href=&quot;http://codecapsule.com/2012/12/03/implementing-a-key-value-store-part-2-using-existing-key-value-stores-as-models/#ref_12&quot;&gt;[12]&lt;/a&gt;。LSM结构据说是为SSD硬盘优化的&lt;a href=&quot;http://codecapsule.com/2012/12/03/implementing-a-key-value-store-part-2-using-existing-key-value-stores-as-models/#ref_13&quot;&gt;[13]&lt;/a&gt;。你可以在这个博客High Scalability blog &lt;a href=&quot;http://codecapsule.com/2012/12/03/implementing-a-key-value-store-part-2-using-existing-key-value-stores-as-models/#ref_17&quot;&gt;[17]&lt;/a&gt;找到成吨的关于LevelDB的信息。&lt;/p&gt;
&lt;p&gt;LevelDB是由C++实现，2011年发布，并设计作为高级存储系统的一部分&lt;a href=&quot;http://codecapsule.com/2012/12/03/implementing-a-key-value-store-part-2-using-existing-key-value-stores-as-models/#ref_10&quot;&gt;[10]&lt;/a&gt;。IndexedDB HTML5 API在Chrome将来版本的实现将使用LevelDB &lt;a href=&quot;http://codecapsule.com/2012/12/03/implementing-a-key-value-store-part-2-using-existing-key-value-stores-as-models/#ref_10&quot;&gt;[10]&lt;/a&gt; &lt;a href=&quot;http://codecapsule.com/2012/12/03/implementing-a-key-value-store-part-2-using-existing-key-value-stores-as-models/#ref_11&quot;&gt;[11]&lt;/a&gt;。其性能决定于&lt;i&gt;特定的工作负载&lt;/i&gt;，就像作者提供的基准测试中显示的那样&lt;a href=&quot;http://codecapsule.com/2012/12/03/implementing-a-key-value-store-part-2-using-existing-key-value-stores-as-models/#ref_14&quot;&gt;[14]&lt;/a&gt;。然而，Andy Twigg在Acunu的另外一个基于商用SSD的基准测试显示出，如果数据的条数超过1e6（1百万），并向1e9（10亿）前进的时候，性能将会显著下降&lt;a href=&quot;http://codecapsule.com/2012/12/03/implementing-a-key-value-store-part-2-using-existing-key-value-stores-as-models/#ref_15&quot;&gt;[15]&lt;/a&gt;。因此似乎LevelDB似乎并不是重工作负载或像实际后端项目需求那样的大数据库最好的选择。&lt;/p&gt;
&lt;p&gt;但这其实并不重要，对于我来说，LevelDB最好的部分不是其性能而是其架构。看它的源代码和东西组织的方式，那是纯粹的美。所有的东西都很清晰、简单、条理分明。访问LevelDB的源代码并把它作为模范是创建出色代码的绝好机遇。&lt;/p&gt;
&lt;h4&gt;那些没选中的键值对存储是什么情况？&lt;/h4&gt;
&lt;p&gt;没有选择其他键值对存储的原因并不表示我完全抛弃他们。我会记得他们并可能偶尔使用他们结构中元素。但是，当前项目受到这些键值对项目影响不会像已选择的这些那么多。&lt;/p&gt;
&lt;h3&gt;4. 参考文献&lt;/h3&gt;
&lt;p align=&quot;left&quot;&gt;[1] &lt;a href=&quot;http://blog.knuthaugen.no/2010/03/a-brief-history-of-nosql.html&quot; target=&quot;_blank&quot;&gt;http://blog.knuthaugen.no/2010/03/a-brief-history-of-nosql.html&lt;/a&gt;&lt;br&gt;
[2] &lt;a href=&quot;http://en.wikipedia.org/wiki/Dbm&quot; target=&quot;_blank&quot;&gt;http://en.wikipedia.org/wiki/Dbm&lt;/a&gt;&lt;br&gt;
[3] &lt;a href=&quot;http://en.wikipedia.org/wiki/Systemantics&quot; target=&quot;_blank&quot;&gt;http://en.wikipedia.org/wiki/Systemantics&lt;/a&gt;&lt;br&gt;
[4] &lt;a href=&quot;http://en.wikipedia.org/wiki/Berkeley_DB#Origin&quot; target=&quot;_blank&quot;&gt;http://en.wikipedia.org/wiki/Berkeley_DB#Origin&lt;/a&gt;&lt;br&gt;
[5] &lt;a href=&quot;http://www.aosabook.org/en/bdb.html&quot; target=&quot;_blank&quot;&gt;http://www.aosabook.org/en/bdb.html&lt;/a&gt;&lt;br&gt;
[6] &lt;a href=&quot;http://docs.oracle.com/cd/E17076_02/html/programmer_reference/intro.html&quot; target=&quot;_blank&quot;&gt;http://docs.oracle.com/cd/E17076_02/html/programmer_reference/intro.html&lt;/a&gt;&lt;br&gt;
[7] &lt;a href=&quot;http://fallabs.com/qdbm/&quot; target=&quot;_blank&quot;&gt;http://fallabs.com/qdbm/&lt;/a&gt;&lt;br&gt;
[8] &lt;a href=&quot;http://research.google.com/people/jeff/&quot; target=&quot;_blank&quot;&gt;http://research.google.com/people/jeff/&lt;/a&gt;&lt;br&gt;
[9] &lt;a href=&quot;http://research.google.com/pubs/SanjayGhemawat.html&quot; target=&quot;_blank&quot;&gt;http://research.google.com/pubs/SanjayGhemawat.html&lt;/a&gt;&lt;br&gt;
[10] &lt;a href=&quot;http://google-opensource.blogspot.com/2011/07/leveldb-fast-persistent-key-value-store.html&quot; target=&quot;_blank&quot;&gt;http://google-opensource.blogspot.com/2011/07/leveldb-fast-persistent-key-value-store.html&lt;/a&gt;&lt;br&gt;
[11] &lt;a href=&quot;http://www.w3.org/TR/IndexedDB/&quot; target=&quot;_blank&quot;&gt;http://www.w3.org/TR/IndexedDB/&lt;/a&gt;&lt;br&gt;
[12] &lt;a href=&quot;http://www.igvita.com/2012/02/06/sstable-and-log-structured-storage-leveldb/&quot; target=&quot;_blank&quot;&gt;http://www.igvita.com/2012/02/06/sstable-and-log-structured-storage-leveldb/&lt;/a&gt;&lt;br&gt;
[13] &lt;a href=&quot;http://www.acunu.com/2/post/2011/04/log-file-systems-and-ssds-made-for-each-other.html&quot; target=&quot;_blank&quot;&gt;http://www.acunu.com/2/post/2011/04/log-file-systems-and-ssds-made-for-each-other.html&lt;/a&gt;&lt;br&gt;
[14] &lt;a href=&quot;http://leveldb.googlecode.com/svn/trunk/doc/benchmark.html&quot; target=&quot;_blank&quot;&gt;http://leveldb.googlecode.com/svn/trunk/doc/benchmark.html&lt;/a&gt;&lt;br&gt;
[15] &lt;a href=&quot;http://www.acunu.com/2/post/2011/08/benchmarking-leveldb.html&quot; target=&quot;_blank&quot;&gt;http://www.acunu.com/2/post/2011/08/benchmarking-leveldb.html&lt;/a&gt;&lt;br&gt;
[16] &lt;a href=&quot;http://blog.creapptives.com/post/8330476086/leveldb-vs-kyoto-cabinet-my-findings&quot; target=&quot;_blank&quot;&gt;http://blog.creapptives.com/post/8330476086/leveldb-vs-kyoto-cabinet-my-findings&lt;/a&gt;&lt;br&gt;
[17] &lt;a href=&quot;http://highscalability.com/blog/2011/8/10/leveldb-fast-and-lightweight-keyvalue-database-from-the-auth.html&quot; target=&quot;_blank&quot;&gt;http://highscalability.com/blog/2011/8/10/leveldb-fast-and-lightweight-keyvalue-database-from-the-auth.html&lt;/a&gt;&lt;br&gt;
[18] &lt;a href=&quot;http://stackoverflow.com/questions/13054852/kyoto-cabinet-berkeley-db-hash-table-size-limitations&quot; target=&quot;_blank&quot;&gt;http://stackoverflow.com/questions/13054852/kyoto-cabinet-berkeley-db-hash-table-size-limitations&lt;/a&gt;&lt;br&gt;
[19] &lt;a href=&quot;https://groups.google.com/forum/#!topic/tokyocabinet-users/Bzp4fLbmcDw/discussion&quot; target=&quot;_blank&quot;&gt;https://groups.google.com/forum/#!topic/tokyocabinet-users/Bzp4fLbmcDw/discussion&lt;/a&gt;&lt;br&gt;
[20] &lt;a href=&quot;http://stackoverflow.com/questions/1051847/why-does-tokyo-tyrant-slow-down-exponentially-even-after-adjusting-bnum&quot; target=&quot;_blank&quot;&gt;http://stackoverflow.com/questions/1051847/why-does-tokyo-tyrant-slow-down-exponentially-even-after-adjusting-bnum&lt;/a&gt;&lt;br&gt;
[21] &lt;a href=&quot;https://groups.google.com/forum/#!topic/tokyocabinet-users/1E06DFQM8mI/discussion&quot; target=&quot;_blank&quot;&gt;https://groups.google.com/forum/#!topic/tokyocabinet-users/1E06DFQM8mI/discussion&lt;/a&gt;&lt;br&gt;
[22] &lt;a href=&quot;http://www.eecs.harvard.edu/margo/&quot; target=&quot;_blank&quot;&gt;http://www.eecs.harvard.edu/margo/&lt;/a&gt;&lt;br&gt;
[23] &lt;a href=&quot;http://www.cse.yorku.ca/~oz/&quot; target=&quot;_blank&quot;&gt;http://www.cse.yorku.ca/~oz/&lt;/a&gt;&lt;br&gt;
[24] &lt;a href=&quot;http://fallabs.com/mikio/profile.html&quot; target=&quot;_blank&quot;&gt;http://fallabs.com/mikio/profile.html&lt;/a&gt;&lt;/p&gt;


&lt;!-- div id=&quot;ad1&quot;&gt;
&lt;/div --&gt;


	


	

</description>
        <pubDate>Mon, 29 Sep 2014 00:00:00 +0800</pubDate>
        <link>http://iftti.com/posts/2014/2014-09-29-77750-1173d7c42.html</link>
        <guid isPermaLink="true">http://iftti.com/posts/2014/2014-09-29-77750-1173d7c42.html</guid>
        
        
        <category>jobbole</category>
        
      </item>
    
      <item>
        <title>大型网站系统架构的演化</title>
        <description>

					
		
&lt;!-- div id=&quot;ad1&quot;&gt;
&lt;script type=&quot;text/javascript&quot;&gt;
google_ad_client = &quot;ca-pub-7056282119617872&quot;;
google_ad_slot = &quot;6645040531&quot;;
google_ad_width = 300;
google_ad_height = 250;
&lt;/script&gt;
&lt;script type=&quot;text/javascript&quot;
src=&quot;http://pagead2.googlesyndication.com/pagead/show_ads.js&quot;&gt;
&lt;/script&gt;
&lt;/div&gt;
&lt;br/ --&gt;

&lt;p&gt;前言&lt;/p&gt;
&lt;p&gt;一个成熟的大型网站（如淘宝、京东等）的系统架构并不是开始设计就具备完整的高性能、高可用、安全等特性，它总是随着用户量的增加，业务功能的扩展逐渐演变完善的，在这个过程中，开发模式、技术架构、设计思想也发生了很大的变化，就连技术人员也从几个人发展到一个部门甚至一条产品线。所以成熟的系统架构是随业务扩展而完善出来的，并不是一蹴而就；不同业务特征的系统，会有各自的侧重点，例如淘宝，要解决海量的商品信息的搜索、下单、支付，例如腾讯，要解决数亿的用户实时消息传输，百度它要处理海量的搜索请求，他们都有各自的业务特性，系统架构也有所不同。尽管如此我们也可以从这些不同的网站背景下，找出其中共用的技术，这些技术和手段可以广泛运行在大型网站系统的架构中，下面就通过介绍大型网站系统的演化过程，来认识这些技术和手段。&lt;/p&gt;
&lt;p&gt;一、最开始的网站架构&lt;/p&gt;
&lt;p&gt;最初的架构，应用程序、数据库、文件都部署在一台服务器上，如图：&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/348e8a744ebf2c6a9a2abd6a024d6ee1.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;二、应用、数据、文件分离&lt;/p&gt;
&lt;p&gt;随着业务的扩展，一台服务器已经不能满足性能需求，故将应用程序、数据库、文件各自部署在独立的服务器上，并且根据服务器的用途配置不同的硬件，达到最佳的性能效果。&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/a8627bac155086fb738b7832a53eb31d.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;三、利用缓存改善网站性能&lt;/p&gt;
&lt;p&gt;在硬件优化性能的同时，同时也通过软件进行性能优化，在大部分的网站系统中，都会利用缓存技术改善系统的性能，使用缓存主要源于热点数据的存在，大部分网站访问都遵循28原则（即80%的访问请求，最终落在20%的数据上），所以我们可以对热点数据进行缓存，减少这些数据的访问路径，提高用户体验。&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/144837c8716f0480166a240c60eec197.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;缓存实现常见的方式是本地缓存、分布式缓存。当然还有CDN、反向代理等，这个后面再讲。本地缓存，顾名思义是将数据缓存在应用服务器本地，可以存在内存中，也可以存在文件，OSCache就是常用的本地缓存组件。本地缓存的特点是速度快，但因为本地空间有限所以缓存数据量也有限。分布式缓存的特点是，可以缓存海量的数据，并且扩展非常容易，在门户类网站中常常被使用，速度按理没有本地缓存快，常用的分布式缓存是Memcached、Redis。&lt;/p&gt;
&lt;p&gt;四、使用集群改善应用服务器性能&lt;/p&gt;
&lt;p&gt;应用服务器作为网站的入口，会承担大量的请求，我们往往通过应用服务器集群来分担请求数。应用服务器前面部署负载均衡服务器调度用户请求，根据分发策略将请求分发到多个应用服务器节点。&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/d33e063fbcb63afc30f1a472dbaac87b.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;常用的负载均衡技术硬件的有F5，价格比较贵，软件的有LVS、Nginx、HAProxy。LVS是四层负载均衡，根据目标地址和端口选择内部服务器，Nginx是七层负载均衡和HAProxy支持四层、七层负载均衡，可以根据报文内容选择内部服务器，因此LVS分发路径优于Nginx和HAProxy，性能要高些，而Nginx和HAProxy则更具配置性，如可以用来做动静分离（根据请求报文特征，选择静态资源服务器还是应用服务器）。&lt;/p&gt;
&lt;p&gt;五、数据库读写分离和分库分表&lt;/p&gt;
&lt;p&gt;随着用户量的增加，数据库成为最大的瓶颈，改善数据库性能常用的手段是进行读写分离以及分表，读写分离顾名思义就是将数据库分为读库和写库，通过主备功能实现数据同步。分库分表则分为水平切分和垂直切分，水平切换则是对一个数据库特大的表进行拆分，例如用户表。垂直切分则是根据业务不同来切换，如用户业务、商品业务相关的表放在不同的数据库中。&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/a6b7adb5a464862a14d9b308a3e5169f.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;六、使用CDN和反向代理提高网站性能&lt;/p&gt;
&lt;p&gt;假如我们的服务器都部署在成都的机房，对于四川的用户来说访问是较快的，而对于北京的用户访问是较慢的，这是由于四川和北京分别属于电信和联通的不同发达地区，北京用户访问需要通过互联路由器经过较长的路径才能访问到成都的服务器，返回路径也一样，所以数据传输时间比较长。对于这种情况，常常使用CDN解决，CDN将数据内容缓存到运营商的机房，用户访问时先从最近的运营商获取数据，这样大大减少了网络访问的路径。比较专业的CDN运营商有蓝汛、网宿。&lt;/p&gt;
&lt;p&gt;而反向代理，则是部署在网站的机房，当用户请求达到时首先访问反向代理服务器，反向代理服务器将缓存的数据返回给用户，如果没有没有缓存数据才会继续走应用服务器获取，也减少了获取数据的成本。反向代理有Squid，Nginx。&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/2e015f81e496c31405fe9c454ccc4885.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;七、使用分布式文件系统&lt;/p&gt;
&lt;p&gt;用户一天天增加，业务量越来越大，产生的文件越来越多，单台的文件服务器已经不能满足需求。需要分布式的文件系统支撑。常用的分布式文件系统有NFS。&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/873167c7c25a2fbb7ddb348abb6b30fc.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;八、使用NoSql和搜索引擎&lt;/p&gt;
&lt;p&gt;对于海量数据的查询，我们使用nosql数据库加上搜索引擎可以达到更好的性能。并不是所有的数据都要放在关系型数据中。常用的NOSQL有mongodb和redis，搜索引擎有lucene。&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/46cad6842e63ae82a904b14ed97ff340.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;九、将应用服务器进行业务拆分&lt;/p&gt;
&lt;p&gt;随着业务进一步扩展，应用程序变得非常臃肿，这时我们需要将应用程序进行业务拆分，如百度分为新闻、网页、图片等业务。每个业务应用负责相对独立的业务运作。业务之间通过消息进行通信或者同享数据库来实现。&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/2d44f88cdd2b6e89dc019a91405bb271.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;十、搭建分布式服务&lt;/p&gt;
&lt;p&gt;这时我们发现各个业务应用都会使用到一些基本的业务服务，例如用户服务、订单服务、支付服务、安全服务，这些服务是支撑各业务应用的基本要素。我们将这些服务抽取出来利用分部式服务框架搭建分布式服务。淘宝的Dubbo是一个不错的选择。&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/dffe5ecece7a4ae2a4d1a5485e4e1ad7.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;小结&lt;/p&gt;
&lt;p&gt;大型网站的架构是根据业务需求不断完善的，根据不同的业务特征会做特定的设计和考虑，本文只是讲述一个常规大型网站会涉及的一些技术和手段。&lt;/p&gt;
&lt;p&gt;参考资料：&lt;/p&gt;
&lt;p&gt;《大型网站技术架构》 ——李智慧&lt;/p&gt;
&lt;p&gt;《海量运维运营规划》 ——唐文&lt;/p&gt;


&lt;!-- div id=&quot;ad1&quot;&gt;
&lt;/div --&gt;


	


	

</description>
        <pubDate>Mon, 29 Sep 2014 00:00:00 +0800</pubDate>
        <link>http://iftti.com/posts/2014/2014-09-29-77748-3d935e7bf.html</link>
        <guid isPermaLink="true">http://iftti.com/posts/2014/2014-09-29-77748-3d935e7bf.html</guid>
        
        
        <category>jobbole</category>
        
      </item>
    
      <item>
        <title>经验谈：用cp命令复制大量文件（432,000,000个，共39TB）</title>
        <description>

					
		
&lt;!-- div id=&quot;ad1&quot;&gt;
&lt;script type=&quot;text/javascript&quot;&gt;
google_ad_client = &quot;ca-pub-7056282119617872&quot;;
google_ad_slot = &quot;6645040531&quot;;
google_ad_width = 300;
google_ad_height = 250;
&lt;/script&gt;
&lt;script type=&quot;text/javascript&quot;
src=&quot;http://pagead2.googlesyndication.com/pagead/show_ads.js&quot;&gt;
&lt;/script&gt;
&lt;/div&gt;
&lt;br/ --&gt;

&lt;p&gt;最近，我有一个复制大量文件的需求，虽然我已经在Unix各种变种上有超过20年的工作经验，但是我仍然被cp指令的行为震撼，我认为这些心得很值得和大家分享下。&lt;/p&gt;
&lt;p&gt;首先介绍下机器：一台戴尔服务器（双核，内存最初是2G，后来扩展到10G，运行ubuntu系统），服务器配备的是全新戴尔存储套件MD1200，该套件包括12个4TB硬盘，其中40TB采用RAID 6配置，这样整个系统就可以容忍两块硬盘同时down掉。这台服务器主要用作异地备份，唯一的操作是IO写，由于我使用rsnapshot完成这项工作，因此大部分文件有较高的链接计数（30+）。&lt;/p&gt;
&lt;p&gt;一天早上，我被告知一个硬盘down掉了。这不是什么大事，这种事情经常发生。我打电话给戴尔，戴尔第二天就给送来了一块替换盘。但我安装替换盘时发现，替换盘根本不能工作，并且另一块磁盘也down掉了。戴尔技术服务部门富有经验地建议我不要只换down掉的硬盘，因为整个磁盘阵列可能已经损毁了。就我所知，磁盘只有在有足够多坏块的情况下才会告警，设想这样一个场景：短时间内，一个文件位于三个磁盘上的备份块全部坏掉，那么很不幸，RAID引擎短时间内难以完成检测坏块、重新计算备份数据并存储这样一个流程，这样你就有丢失数据的风险了。因此，如果两个硬盘显式告警，你的数据可能已经丢失了。&lt;/p&gt;
&lt;p&gt;现有存储套件容量已经难以达到要求，我们决定扩容，把文件从旧存储套件拷贝到新套件中。正常情况下，我会在块级别拷贝用dd指令或者pvmove指令拷贝数据，但是考虑到坏块，我决定采用文件级别的拷贝，因为这样的话，我就可以知道那些文件包含坏块了。我上网搜了相关经验，发现cp可以完美地解决这个问题。若想保存硬链接信息，需要记录有哪些文件已经被复制，所以我预订了8G的RAM，并配置了更大的交换区。&lt;/p&gt;
&lt;p&gt;当新的存储套件到货，我就着手复制了，起初，根据iotop的测量数据，复制速度接近于300-400MB/s。不一会儿速度便显著下降了，因为大部分时间花在了建立硬链接上，并且要花时间去处理文件系统的一致性问题。为保持一致性，我们使用了XFS，由于没有关闭写屏障，我们深受性能之苦，但如果RAID控制器配置了带有备份电源的写缓存，一致性问题便可更好地解决。正如所料，cp的内存占用量稳步增加，很快便达到了GB级。&lt;/p&gt;
&lt;p&gt;经历了几天的拷贝，问题来了：我发现系统已经停止拷贝，根据strace的显示，cp指令没有调用任何的系统调用函数。阅读相关源码后发现，cp指令会以一个哈希表跟踪有哪些文件被复制了，这个表需要时常地扩展自身容量以避免太多的冲突。当RAM被耗尽，哈希表的扩容过程会成为性能瓶颈。&lt;/p&gt;
&lt;p&gt;我们相信cp的哈希表扩容后，cp指令会继续执行，果然不一会它又“复活”了，但是它会进入周期性的“扩容-拷贝-扩容-拷贝”的循环中，且扩容的时间变得越来越长。经过10天的拷贝，根据dd结果，新文件系统的块数量和inode节点的数量已经和原系统一样，但是让人吃惊的是cp指令并没有退出。再次阅读源码，我发现cp正在认真地解构哈希表（使用forget_all函数），由于cp进程需要的虚拟内存大小达到了17GB+，但服务器总共才10G内存，所以解构过程执行了很多RAM和swap区间的换页操作。&lt;/p&gt;
&lt;p&gt;上面整个过程中，我使用了cp的-v选项，并使用tee把日志重定向到日志文件（很大的文件）中。cp的输出信息会被缓存，为使这些缓存信息全部刷新到日志文件中，我给了cp多于一天的时间进行解构哈希表。&lt;/p&gt;
&lt;p&gt;运行”ls –laR”指令，查看下两个文件系统中的文件是否一致，发现除了日志文件的一些错误外，只有一个文件发生了io错误（还好，我们有它的另一个备份）。&lt;/p&gt;
&lt;p&gt;这种错误不会马上复现，但是如果cp能够设计一种数据结构，在等待io的时候处理那些已经被记录的文件，那样处理效率会更高。此外，对于cp最后解构哈希表的forget_all函数，除了那些缺少可用内存管理模块的老式服务器一定需要它之外，我还没发现去掉这个函数会对整个拷贝过程产生什么不好的影响。&lt;/p&gt;
&lt;p&gt;总结一下：&lt;/p&gt;
&lt;p&gt;1、拷贝整个文件系统时，如果确定你的硬件和文件系统都OK，应该使用块级别的拷贝。这种拷贝方式更快，除非你有许多空闲块，但是无论如何它耗费的内存都是较少的；&lt;/p&gt;
&lt;p&gt;2、如果要拷贝许多文件，并想要保留硬链接的信息，可以采用文件级拷贝。拷贝之前需要保证机器有足够多的内存；&lt;/p&gt;
&lt;p&gt;3、比起简单粗暴地停止进程，认真地解构数据结构会花费更多的时间；&lt;/p&gt;
&lt;p&gt;4、告警的硬盘的数量并不等同于有坏块的硬盘的数量。如果不走运，就算配置了RAID6，没来得及等3块磁盘告警，你的数据可能已经丢失。这个时候我们只能赌一把了。同样的情况适用于RAID 5，那怕只有一块磁盘甚至没有磁盘告警，如果不走运，你的数据照样丢！&lt;/p&gt;
&lt;p&gt;希望这篇文章对你有益！&lt;/p&gt;


&lt;!-- div id=&quot;ad1&quot;&gt;
&lt;/div --&gt;


	


	

</description>
        <pubDate>Sun, 28 Sep 2014 00:00:00 +0800</pubDate>
        <link>http://iftti.com/posts/2014/2014-09-28-77698-c8ce5869d.html</link>
        <guid isPermaLink="true">http://iftti.com/posts/2014/2014-09-28-77698-c8ce5869d.html</guid>
        
        
        <category>jobbole</category>
        
      </item>
    
      <item>
        <title>From Clouds to Roots: Performance Analysis at Netflix</title>
        <description>

&lt;p&gt;How do companies like Netflix, Google, and Facebook do root cause performance analysis in their cloud environments? These companies use Linux heavily, so are they using SystemTap or another tracer? Netflix is hosted on EC2, how do they do low level CPU profiling, if Xen guests can&#39;t access the CPU counters? ... And if these companies don&#39;t do these kinds of profiling, aren&#39;t they losing millions because of it?&lt;/p&gt;

&lt;p&gt;On Thursday it was my privilege to speak again at &lt;a href=&quot;http://surge.omniti.com/2014&quot;&gt;Surge&lt;/a&gt;, the scalability and performance conference. My talk was titled &lt;a href=&quot;http://surge.omniti.com/2014/speakers/speakers.html?gregg-brendan%5D&quot;&gt;From Clouds to Roots&lt;/a&gt;, and showed how Neflix does root cause performance analysis, and answers these questions and more. The slides are on &lt;a href=&quot;http://www.slideshare.net/brendangregg/netflix-from-clouds-to-roots&quot;&gt;slideshare&lt;/a&gt;:&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;
&lt;center&gt;
&lt;iframe src=&quot;//www.slideshare.net/slideshow/embed_code/39526755&quot; width=&quot;425&quot; height=&quot;355&quot; frameborder=&quot;0&quot; marginwidth=&quot;0&quot; marginheight=&quot;0&quot; scrolling=&quot;no&quot; style=&quot;border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;&quot; allowfullscreen&gt; &lt;/iframe&gt; &lt;/center&gt;

&lt;p&gt;It was also videoed; I&#39;ll update this post when the video is available.&lt;/p&gt;

&lt;p&gt;For background, I began by explaining the Netflix fault-tolerant architecture: how this can automatically work around performance issues, and what kinds of new issues this can introduce. Then I summarized key cloud-wide performance tools, and how we use these to isolate issues to an instance. And finally instance analysis, with the low-level tools we run to find the root cause.&lt;/p&gt;

&lt;p&gt;Summary of cloud-wide performance tools:&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;
&lt;center&gt;&lt;a href=&quot;/blog/images/2014/netflix_cloud_perf_analysis.png&quot;&gt;&lt;img src=&quot;/images/brendangregg.com/8117220f7d1a16ef6a80ac51b34c927f.jpg&quot; width=&quot;500&quot;&gt;&lt;/a&gt;&lt;/center&gt;

&lt;p&gt;Summary of instance performance tools:&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;
&lt;center&gt;&lt;a href=&quot;/blog/images/2014/linux_EC2_perf_tools.png&quot;&gt;&lt;img src=&quot;/images/brendangregg.com/30cb18d519c569b8273e6a5838904be8.jpg&quot; width=&quot;500&quot;&gt;&lt;/a&gt;&lt;/center&gt;

&lt;p&gt;People liked the talk, although it surprised some. One comment was &quot;it didn&#39;t feel like a Brendan talk&quot;. Yes, I&#39;ve expanded my scope at Netflix, and I&#39;m working on higher-level analysis now, not just low-level performance tools. It&#39;s exciting for me, and I feel I have a better and more balanced view of performance analysis, which I captured in this talk.&lt;/p&gt;

&lt;p&gt;Keep in mind that you aren&#39;t necessarily expected to do the low-level performance analysis yourself. I summarized the likely real scenarios at the end of the talk:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;A. Your company has one or more people who do advanced perf analysis (perf team). Ask them to do it.&lt;/li&gt;
&lt;li&gt;B. You actually are that person. Do it.&lt;/li&gt;
&lt;li&gt;C. You buy a product that does it. Ask them to add low-level capabilities.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Perhaps the most valuable take-away of this talk is that you be aware that such low-level analysis is possible, so that if you are in situation (A) or (C), you can ask for it. I&#39;ve found that performance monitoring companies often build products based on what customers think they want. However, what customers think they want is bounded by what they know is possible. You really want products that make low-level analysis easy, including flame graphs and latency heat maps. Some monitoring products already provide these (eg, Circonus, AppNeta), and at Netflix we&#39;re building Vector.&lt;/p&gt;

&lt;p&gt;Last year at Surge, I saw a great talk by &lt;a href=&quot;http://twitter.com/coburnw&quot;&gt;Coburn Watson&lt;/a&gt; titled &lt;a href=&quot;http://www.youtube.com/watch?v=7-13wV3WO8Q&quot;&gt;How Netflix Maximizes Scalability, Resilience, and Engineering Velocity in the Cloud&lt;/a&gt;, which I recommend. His relaxed style threw me at first: he&#39;s able to talk very casually about complex performance issues in distributed environments, because he actually understands them extremely well. This year, he&#39;s now my manager at Netflix, and he&#39;s also still hiring. Great at performance and want to work with both Coburn and myself? Let him &lt;a href=&quot;mailto:cwatson@netflix.com&quot;&gt;know&lt;/a&gt;. :-)&lt;/p&gt;

&lt;p&gt;For more about low level performance analysis tools, see my &lt;a href=&quot;/blog&quot;&gt;previous blog posts&lt;/a&gt;, and my &lt;a href=&quot;https://github.com/brendangregg/perf-tools&quot;&gt;perf-tools&lt;/a&gt; and &lt;a href=&quot;https://github.com/brendangregg/msr-cloud-tools&quot;&gt;msr-cloud-tools&lt;/a&gt; collections on github.&lt;/p&gt;


</description>
        <pubDate>Sat, 27 Sep 2014 00:00:00 +0800</pubDate>
        <link>http://iftti.com/posts/2014/2014-09-27-from-clouds-to-roots.html-880dc9b64.html</link>
        <guid isPermaLink="true">http://iftti.com/posts/2014/2014-09-27-from-clouds-to-roots.html-880dc9b64.html</guid>
        
        
        <category>brendangregg</category>
        
      </item>
    
      <item>
        <title>Linux后门入侵检测工具，附bash漏洞解决方法</title>
        <description>

					
		
&lt;!-- div id=&quot;ad1&quot;&gt;
&lt;script type=&quot;text/javascript&quot;&gt;
google_ad_client = &quot;ca-pub-7056282119617872&quot;;
google_ad_slot = &quot;6645040531&quot;;
google_ad_width = 300;
google_ad_height = 250;
&lt;/script&gt;
&lt;script type=&quot;text/javascript&quot;
src=&quot;http://pagead2.googlesyndication.com/pagead/show_ads.js&quot;&gt;
&lt;/script&gt;
&lt;/div&gt;
&lt;br/ --&gt;

&lt;p&gt;&lt;strong&gt;一、rootkit简介&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;rootkit是Linux平台下最常见的一种木马后门工具，它主要通过替换系统文件来达到入侵和和隐蔽的目的，这种木马比普通木马后门更加危险和隐蔽，普通的检测工具和检查手段很难发现这种木马。rootkit攻击能力极强，对系统的危害很大，它通过一套工具来建立后门和隐藏行迹，从而让攻击者保住权限，以使它在任何时候都可以使用root权限登录到系统。&lt;/p&gt;
&lt;p&gt;rootkit主要有两种类型：文件级别和内核级别，下面分别进行简单介绍。&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;1、文件级别rootkit&lt;/p&gt;
&lt;p&gt;文件级别的rootkit一般是通过程序漏洞或者系统漏洞进入系统后，通过修改系统的重要文件来达到隐藏自己的目的。在系统遭受rootkit攻击后，合法的文件被木马程序替代，变成了外壳程序，而其内部是隐藏着的后门程序。通常容易被rootkit替换的系统程序有login、ls、ps、ifconfig、du、find、netstat等，其中login程序是最经常被替换的，因为当访问Linux时，无论是通过本地登录还是远程登录，/bin/login程序都会运行，系统将通过/bin/login来收集并核对用户的账号和密码，而rootkit就是利用这个程序的特点，使用一个带有根权限后门密码的/bin/login来替换系统的/bin/login，这样攻击者通过输入设定好的密码就能轻松进入系统。此时，即使系统管理员修改root密码或者清除root密码，攻击者还是一样能通过root用户登录系统。攻击者通常在进入Linux系统后，会进行一系列的攻击动作，最常见的是安装嗅探器收集本机或者网络中其他服务器的重要数据。在默认情况下，Linux中也有一些系统文件会监控这些工具动作，例如ifconfig命令，所以，攻击者为了避免被发现，会想方设法替换其他系统文件，常见的就是ls、ps、ifconfig、du、find、netstat等。如果这些文件都被替换，那么在系统层面就很难发现rootkit已经在系统中运行了。&lt;/p&gt;
&lt;p&gt;这就是文件级别的rootkit，对系统维护很大，目前最有效的防御方法是定期对系统重要文件的完整性进行检查，如果发现文件被修改或者被替换，那么很可能系统已经遭受了rootkit入侵。检查件完整性的工具很多，常见的有Tripwire、 aide等，可以通过这些工具定期检查文件系统的完整性，以检测系统是否被rootkit入侵。&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;2、内核级别的rootkit&lt;/p&gt;
&lt;p&gt;内核级rootkit是比文件级rootkit更高级的一种入侵方式，它可以使攻击者获得对系统底层的完全控制权，此时攻击者可以修改系统内核，进而截获运行程序向内核提交的命令，并将其重定向到入侵者所选择的程序并运行此程序，也就是说，当用户要运行程序A时，被入侵者修改过的内核会假装执行A程序，而实际上却执行了程序B。&lt;/p&gt;
&lt;p&gt;内核级rootkit主要依附在内核上，它并不对系统文件做任何修改，因此一般的检测工具很难检测到它的存在，这样一旦系统内核被植入rootkit，攻击者就可以对系统为所欲为而不被发现。目前对于内核级的rootkit还没有很好的防御工具，因此，做好系统安全防范就非常重要，将系统维持在最小权限内工作，只要攻击者不能获取root权限，就无法在内核中植入rootkit。&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;二、rootkit后门检测工具chkrootkit&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;chkrootkit是一个Linux系统下查找并检测rootkit后门的工具，它的官方址: http://www.chkrootkit.org/。 chkrootkit没有包含在官方的CentOS源中，因此要采取手动编译的方法来安装，不过这种安装方法也更加安全。下面简单介绍下chkrootkit的安装过程。&lt;/p&gt;
&lt;p&gt;1.准备gcc编译环境&lt;/p&gt;
&lt;p&gt;对于CentOS系统，需要安装gcc编译环境，执行下述三条命令：&lt;/p&gt;
&lt;pre class=&quot;brush: actionscript3; gutter: true&quot;&gt;[root@server ~]# yum -y install gcc
[root@server ~]# yum -y install gcc-c++
[root@server ~]# yum -y install make&lt;/pre&gt;
&lt;p&gt;2、安装chkrootkit&lt;/p&gt;
&lt;p&gt;为了安全起见，建议直接从官方网站下载chkrootkit源码，然后进行安装，操作如下：&lt;/p&gt;
&lt;pre class=&quot;brush: actionscript3; gutter: true&quot;&gt;[root@server ~]# tar zxvf chkrootkit.tar.gz
[root@server ~]# cd chkrootkit-*
[root@server ~]# make sense&lt;/pre&gt;
&lt;p&gt;# 注意，上面的编译命令为make sense&lt;/p&gt;
&lt;pre class=&quot;brush: actionscript3; gutter: true&quot;&gt;[root@server ~]# cd ..
[root@server ~]# cp -r chkrootkit-* /usr/local/chkrootkit
[root@server ~]# rm -rf chkrootkit-*&lt;/pre&gt;
&lt;p&gt;3、使用chkrootkit&lt;/p&gt;
&lt;p&gt;安装完的chkrootkit程序位于/usr/local/chkrootkit目录下，执行如下命令即可显示chkrootkit的详细用法：&lt;/p&gt;
&lt;p&gt;[root@server chkrootkit]# /usr/local/chkrootkit/chkrootkit  -h&lt;/p&gt;
&lt;p&gt;chkrootkit各个参数的含义如下所示。&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;参数含义&lt;/p&gt;
&lt;p&gt;-h显示帮助信息&lt;/p&gt;
&lt;p&gt;-v显示版本信息&lt;/p&gt;
&lt;p&gt;-l显示测试内容&lt;/p&gt;
&lt;p&gt;-ddebug模式，显示检测过程的相关指令程序&lt;/p&gt;
&lt;p&gt;-q安静模式，只显示有问题的内容&lt;/p&gt;
&lt;p&gt;-x高级模式，显示所有检测结果&lt;/p&gt;
&lt;p&gt;-r dir设置指定的目录为根目录&lt;/p&gt;
&lt;p&gt;-p dir1:dir2:dirN指定chkrootkit检测时使用系统命令的目录&lt;/p&gt;
&lt;p&gt;-n跳过NFS连接的目录&lt;/p&gt;
&lt;p&gt;chkrootkit的使用比较简单，直接执行chkrootkit命令即可自动开始检测系统。下面是某个系统的检测结果：&lt;/p&gt;
&lt;pre class=&quot;brush: actionscript3; gutter: true&quot;&gt;[root@server chkrootkit]# /usr/local/chkrootkit/chkrootkit
Checking `ifconfig&#39;... INFECTED
Checking `ls&#39;... INFECTED
Checking `login&#39;... INFECTED
Checking `netstat&#39;... INFECTED
Checking `ps&#39;... INFECTED
Checking `top&#39;... INFECTED
Checking `sshd&#39;... not infected
Checking `syslogd&#39;... not tested
Checking `tar&#39;... not infected
Checking `tcpd&#39;... not infected
Checking `tcpdump&#39;... not infected
Checking `telnetd&#39;... not found&lt;/pre&gt;
&lt;p&gt;从输出可以看出，此系统的ifconfig、ls、login、netstat、ps和top命令已经被感染。针对被感染rootkit的系统，最安全而有效的方法就是备份数据重新安装系统。&lt;/p&gt;
&lt;p&gt;4、chkrootkit的缺点&lt;/p&gt;
&lt;p&gt;chkrootkit在检查rootkit的过程中使用了部分系统命令，因此，如果服务器被黑客入侵，那么依赖的系统命令可能也已经被入侵者替换，此时chkrootkit的检测结果将变得完全不可信。为了避免chkrootkit的这个问题，可以在服务器对外开放前，事先将chkrootkit使用的系统命令进行备份，在需要的时候使用备份的原始系统命令让chkrootkit对rootkit进行检测。这个过程可以通过下面的操作实现：&lt;/p&gt;
&lt;pre class=&quot;brush: actionscript3; gutter: true&quot;&gt;[root@server ~]# mkdir /usr/share/.commands
[root@server ~]# cp `which --skip-alias awk cut echo find egrep id head ls netstat ps strings sed uname` /usr/share/.commands
[root@server ~]# /usr/local/chkrootkit/chkrootkit -p /usr/share/.commands/
[root@server share]# cd /usr/share/
[root@server share]# tar zcvf commands.tar.gz .commands
[root@server share]#  rm -rf commands.tar.gz&lt;/pre&gt;
&lt;p&gt;上面这段操作是在/usr/share/下建立了一个.commands隐藏文件，然后将chkrootkit使用的系统命令进行备份到这个目录下。为了安全起见，可以将.commands目录压缩打包，然后下载到一个安全的地方进行备份，以后如果服务器遭受入侵，就可以将这个备份上传到服务器任意路径下，然后通过chkrootkit命令的“-p”参数指定这个路径进行检测即可。&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;三、rootkit后门检测工具RKHunter&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;RKHunter是一款专业的检测系统是否感染rootkit的工具，它通过执行一系列的脚本来确认服务器是否已经感染rootkit。在官方的资料中，RKHunter可以作的事情有：&lt;/p&gt;
&lt;p&gt;MD5校验测试，检测文件是否有改动&lt;/p&gt;
&lt;p&gt;检测rootkit使用的二进制和系统工具文件&lt;/p&gt;
&lt;p&gt;检测特洛伊木马程序的特征码&lt;/p&gt;
&lt;p&gt;检测常用程序的文件属性是否异常&lt;/p&gt;
&lt;p&gt;检测系统相关的测试&lt;/p&gt;
&lt;p&gt;检测隐藏文件&lt;/p&gt;
&lt;p&gt;检测可疑的核心模块LKM&lt;/p&gt;
&lt;p&gt;检测系统已启动的监听端口&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;下面详细讲述下RKHunter的安装与使用。&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;1、安装RKHunter&lt;/p&gt;
&lt;p&gt;RKHunter的官方网页地址为：http://www.rootkit.nl/projects/rootkit_hunter.html，建议从这个网站下载RKHunter，这里下载的版本是rkhunter-1.4.0.tar.gz。RKHunter的安装非常简单，过程如下：&lt;/p&gt;
&lt;pre class=&quot;brush: actionscript3; gutter: true&quot;&gt;[root@server ~]# ls
rkhunter-1.4.0.tar.gz
[root@server ~]# pwd
/root
[root@server ~]# tar -zxvf rkhunter-1.4.0.tar.gz 
[root@server ~]# cd rkhunter-1.4.0
[root@server rkhunter-1.4.0]# ./installer.sh  --layout default --install&lt;/pre&gt;
&lt;p&gt;这里采用RKHunter的默认安装方式，rkhunter命令被安装到了/usr/local/bin目录下。&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;2、使用rkhunter指令&lt;/p&gt;
&lt;p&gt;rkhunter命令的参数较多，但是使用非常简单，直接运行rkhunter即可显示此命令的用法。下面简单介绍下rkhunter常用的几个参数选项。&lt;/p&gt;
&lt;p&gt;[root@server ~]#/usr/local/bin/rkhunter–help&lt;/p&gt;
&lt;p&gt;Rkhunter常用参数以及含义如下所示。&lt;/p&gt;
&lt;p&gt;参数             含义&lt;/p&gt;
&lt;p&gt;-c, –check必选参数，表示检测当前系统&lt;/p&gt;
&lt;p&gt;–configfile &amp;lt;file&amp;gt;使用特定的配置文件&lt;/p&gt;
&lt;p&gt;–cronjob作为cron任务定期运行&lt;/p&gt;
&lt;p&gt;–sk, –skip-keypress自动完成所有检测，跳过键盘输入&lt;/p&gt;
&lt;p&gt;–summary显示检测结果的统计信息&lt;/p&gt;
&lt;p&gt;–update检测更新内容&lt;/p&gt;
&lt;p&gt;-V, –version显示版本信息&lt;/p&gt;
&lt;p&gt;–versioncheck检测最新版本&lt;/p&gt;
&lt;p&gt;下面是通过rkhunter对某个系统的检测示例：&lt;/p&gt;
&lt;pre class=&quot;brush: actionscript3; gutter: true&quot;&gt;[root@server rkhunter-1.4.0]# /usr/local/bin/rkhunter   -c 
[ Rootkit Hunter version 1.4.0 ]
#下面是第一部分，先进行系统命令的检查，主要是检测系统的二进制文件，因为这些文件最容易被rootkit攻击。显示OK字样表示正常，显示Warning表示有异常，需要引起注意，而显示“Not found”字样，一般无需理会
Checking system commands...
  Performing &#39;strings&#39; command checks
    Checking &#39;strings&#39; command                           [ OK ]
  Performing &#39;shared libraries&#39; checks
    Checking for preloading variables                        [ None found ]
    Checking for preloaded libraries                         [ None found ]
    Checking LD_LIBRARY_PATH variable                 [ Not found ]
  Performing file properties checks
    Checking for prerequisites                              [ Warning ]
    /usr/local/bin/rkhunter  [ OK ]
    /sbin/chkconfig                                       [ OK ]
....(略)....
[Press &amp;lt;ENTER&amp;gt; to continue]
#下面是第二部分，主要检测常见的rootkit程序，显示“Not found”表示系统未感染此rootkit
Checking for rootkits...
  Performing check of known rootkit files and directories
    55808 Trojan - Variant A                                 [ Not found ]
    ADM Worm                                           [ Not found ]
    AjaKit Rootkit                                         [ Not found ]
    Adore Rootkit                                          [ Not found ]
aPa Kit                                               [ Not found ]
    Apache Worm                                          [ Not found ]
    Ambient (ark) Rootkit                                    [ Not found ]
    Balaur Rootkit           [ Not found ]
    BeastKit Rootkit                                         [ Not found ]
beX2 Rootkit                                             [ Not found ]
    BOBKit Rootkit                    [ Not found ]
....(略)....
[Press &amp;lt;ENTER&amp;gt; to continue]
#下面是第三部分，主要是一些特殊或附加的检测，例如对rootkit文件或目录检测、对恶意软件检测以及对指定的内核模块检测
  Performing additional rootkit checks
    Suckit Rookit additional checks                          [ OK ]
    Checking for possible rootkit files and directories      [ None found ]
    Checking for possible rootkit strings                    [ None found ]
  Performing malware checks
    Checking running processes for suspicious files          [ None found ]
    Checking for login backdoors                          [ None found ]
    Checking for suspicious directories                     [ None found ]
    Checking for sniffer log files                          [ None found ]
  Performing Linux specific checks
    Checking loaded kernel modules                     [ OK ]
    Checking kernel module names                     [ OK ]
[Press &amp;lt;ENTER&amp;gt; to continue]
#下面是第四部分，主要对网络、系统端口、系统启动文件、系统用户和组配置、SSH配置、文件系统等进行检测
Checking the network...
  Performing checks on the network ports
    Checking for backdoor ports                         [ None found ]
  Performing checks on the network interfaces
    Checking for promiscuous interfaces                      [ None found ]
Checking the local host...
  Performing system boot checks
    Checking for local host name                         [ Found ]
    Checking for system startup files                        [ Found ]
    Checking system startup files for malware                [ None found ]
  Performing group and account checks
    Checking for passwd file [ Found ]
    Checking for root equivalent (UID 0) accounts            [ None found ]
    Checking for passwordless accounts                   [ None found ]
....(略)....
[Press &amp;lt;ENTER&amp;gt; to continue]
#下面是第五部分，主要是对应用程序版本进行检测
Checking application versions...
    Checking version of GnuPG[ OK ]
    Checking version of OpenSSL                        [ Warning ]
    Checking version of OpenSSH                        [ OK ]
#下面是最后一部分，这个部分其实是上面输出的一个总结，通过这个总结，可以大概了解服务器目录的安全状态。
System checks summary
=====================
File properties checks...
    Required commands check failed
    Files checked: 137
    Suspect files: 4
Rootkit checks...
    Rootkits checked : 311
    Possible rootkits: 0
Applications checks...
    Applications checked: 3
    Suspect applications: 1
The system checks took: 6 minutes and 41 seconds&lt;/pre&gt;
&lt;p&gt;在Linux终端使用rkhunter来检测，最大的好处在于每项的检测结果都有不同的颜色显示，如果是绿色的表示没有问题，如果是红色的，那就要引起关注了。另外，在上面执行检测的过程中，在每个部分检测完成后，需要以Enter键来继续。如果要让程序自动运行，可以执行如下命令：&lt;/p&gt;
&lt;p&gt;[root@server ~]# /usr/local/bin/rkhunter –check –skip-keypress&lt;/p&gt;
&lt;p&gt;同时，如果想让检测程序每天定时运行，那么可以在/etc/crontab中加入如下内容：&lt;/p&gt;
&lt;p&gt;30 09 * * * root /usr/local/bin/rkhunter –check –cronjob&lt;/p&gt;
&lt;p&gt;这样，rkhunter检测程序就会在每天的9:30分运行一次。&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;安全更新：&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;今天刚刚爆出Bash安全漏洞，SSH bash紧急安全补丁！重要！&lt;/p&gt;
&lt;p&gt;测试是否存在漏洞，执行以下命令：&lt;/p&gt;
&lt;pre class=&quot;brush: actionscript3; gutter: true&quot;&gt;$ env x=&#39;() { :;}; echo vulnerable&#39; bash -c &quot;echo this is a test&quot;
 vulnerable
 this is a test&lt;/pre&gt;
&lt;p&gt;如果显示如上，那么，很遗憾，必须立即打上安全补丁修复，&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;临时解决办法为：&lt;/p&gt;
&lt;pre class=&quot;brush: actionscript3; gutter: true&quot;&gt;yum -y update bash&lt;/pre&gt;
&lt;p&gt;升级bash后，执行测试：&lt;/p&gt;
&lt;pre class=&quot;brush: actionscript3; gutter: true&quot;&gt;$ env x=&#39;() { :;}; echo vulnerable&#39; bash -c &quot;echo this is a test&quot;
 bash: warning: x: ignoring function definition attempt
 bash: error importing function definition for `x&#39;
 this is a test&lt;/pre&gt;
&lt;p&gt;如果显示如上，表示已经修补了漏洞。&lt;/p&gt;


&lt;!-- div id=&quot;ad1&quot;&gt;
&lt;/div --&gt;


	


	

</description>
        <pubDate>Thu, 25 Sep 2014 00:00:00 +0800</pubDate>
        <link>http://iftti.com/posts/2014/2014-09-25-77663-4da093c4d.html</link>
        <guid isPermaLink="true">http://iftti.com/posts/2014/2014-09-25-77663-4da093c4d.html</guid>
        
        
        <category>jobbole</category>
        
      </item>
    
      <item>
        <title>SSL延迟有多大？</title>
        <description>

					
		
&lt;!-- div id=&quot;ad1&quot;&gt;
&lt;script type=&quot;text/javascript&quot;&gt;
google_ad_client = &quot;ca-pub-7056282119617872&quot;;
google_ad_slot = &quot;6645040531&quot;;
google_ad_width = 300;
google_ad_height = 250;
&lt;/script&gt;
&lt;script type=&quot;text/javascript&quot;
src=&quot;http://pagead2.googlesyndication.com/pagead/show_ads.js&quot;&gt;
&lt;/script&gt;
&lt;/div&gt;
&lt;br/ --&gt;

&lt;p&gt;据说，Netscape 公司当年设计 &lt;a href=&quot;http://blog.jobbole.com/77439/&quot; target=&quot;_blank&quot;&gt;SSL 协议&lt;/a&gt;的时候，有人提过，将互联网所有链接都变成 HTTPs 开头的加密链接。&lt;/p&gt;
&lt;p&gt;这个建议没有得到采纳，原因之一是 HTTPs 链接比不加密的 HTTP 链接慢很多。（另一个原因好像是，HTTPs 链接默认不能缓存。）&lt;/p&gt;
&lt;p&gt;自从我知道这个掌故以后，脑袋中就有一个观念：HTTPs 链接很慢。但是，它到底有多慢，我并没有一个精确的概念。直到今天我从一篇&lt;a href=&quot;http://www.semicomplete.com/blog/geekery/ssl-latency.html&quot; target=&quot;_blank&quot;&gt;文章&lt;/a&gt;中，学到了测量 HTTPs 链接耗时的方法。&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/869312f2433a8ddcc2d7086ab8e4f2a4.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;首先我解释一下，为什么 HTTPs 链接比较慢。&lt;/p&gt;
&lt;p&gt;HTTPs 链接和 HTTP 链接都建立在 TCP 协议之上。HTTP 链接比较单纯，使用三个握手数据包建立连接之后，就可以发送内容数据了。&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/f0b241592ba624dd41bd64fd480ddd51.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;上图中，客户端首先发送 SYN 数据包，然后服务器发送 SYN+ACK 数据包，最后客户端发送 ACK 数据包，接下来就可以发送内容了。这三个数据包的发送过程，叫做 TCP 握手。&lt;/p&gt;
&lt;p&gt;再来看 HTTPs 链接，它也采用 TCP 协议发送数据，所以它也需要上面的这三步握手过程。而且，在这三步结束以后，它还有一个 &lt;a href=&quot;http://www.ruanyifeng.com/blog/2014/02/ssl_tls.html&quot; target=&quot;_blank&quot;&gt;SSL 握手&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;总结一下，就是下面这两个式子。&lt;/p&gt;
&lt;p style=&quot;padding-left: 30px;&quot;&gt;&lt;span style=&quot;color: #888888;&quot;&gt;HTTP 耗时 = TCP 握手&lt;/span&gt;&lt;/p&gt;
&lt;p style=&quot;padding-left: 30px;&quot;&gt;&lt;span style=&quot;color: #888888;&quot;&gt;HTTPs 耗时 = TCP 握手 + SSL 握手&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;所以，HTTPs 肯定比 HTTP 耗时，这就叫 SSL 延迟。&lt;/p&gt;
&lt;p&gt;命令行工具 &lt;a href=&quot;http://www.ruanyifeng.com/blog/2011/09/curl.html&quot; target=&quot;_blank&quot;&gt;curl&lt;/a&gt; 有一个w参数，可以用来测量 TCP 握手和 SSL 握手的具体耗时，以访问支付宝为例。&lt;/p&gt;
&lt;pre class=&quot;brush: bash; gutter: true&quot;&gt;$ curl -w &quot;TCP handshake: %{time_connect}， SSL handshake: %{time_appconnect}\n&quot; -so /dev/null https://www.alipay.comTCP handshake: 0.022, SSL handshake: 0.064&lt;/pre&gt;
&lt;p&gt;上面命令中的w参数表示指定输出格式，timeconnect 变量表示 TCP 握手的耗时，timeappconnect 变量表示 SSL 握手的耗时（更多变量请查看&lt;a href=&quot;http://curl.haxx.se/docs/manpage.html&quot; target=&quot;_blank&quot;&gt;文档&lt;/a&gt;和&lt;a href=&quot;https://josephscott.org/archives/2011/10/timing-details-with-curl/&quot; target=&quot;_blank&quot;&gt;实例&lt;/a&gt;），s参数和o参数用来关闭标准输出。&lt;/p&gt;
&lt;p&gt;从运行结果可以看到，SSL 握手的耗时（64 毫秒）大概是 TCP 握手（22 毫秒）的三倍。也就是说，在建立连接的阶段，HTTPs 链接比 HTTP 链接要长 3 倍的时间，具体数字取决于 CPU 的快慢。&lt;/p&gt;
&lt;p&gt;所以，如果是对安全性要求不高的场合，为了提高网页性能，建议不要采用保密强度很高的数字证书。一般场合下，1024 位的证书已经足够了，2048 位和 4096 位的证书将进一步延长 SSL 握手的耗时。&lt;/p&gt;
&lt;p&gt;（完）&lt;/p&gt;


&lt;!-- div id=&quot;ad1&quot;&gt;
&lt;/div --&gt;


	


	

</description>
        <pubDate>Thu, 25 Sep 2014 00:00:00 +0800</pubDate>
        <link>http://iftti.com/posts/2014/2014-09-25-77624-79bfbf9ed.html</link>
        <guid isPermaLink="true">http://iftti.com/posts/2014/2014-09-25-77624-79bfbf9ed.html</guid>
        
        
        <category>jobbole</category>
        
      </item>
    
      <item>
        <title>机器学习常见算法分类汇总</title>
        <description>

					
		
&lt;!-- div id=&quot;ad1&quot;&gt;
&lt;script type=&quot;text/javascript&quot;&gt;
google_ad_client = &quot;ca-pub-7056282119617872&quot;;
google_ad_slot = &quot;6645040531&quot;;
google_ad_width = 300;
google_ad_height = 250;
&lt;/script&gt;
&lt;script type=&quot;text/javascript&quot;
src=&quot;http://pagead2.googlesyndication.com/pagead/show_ads.js&quot;&gt;
&lt;/script&gt;
&lt;/div&gt;
&lt;br/ --&gt;

&lt;p&gt;机器学习无疑是当前数据分析领域的一个热点内容。很多人在平时的工作中都或多或少会用到机器学习的算法。本文为您总结一下常见的机器学习算法，以供您在工作和学习中参考。&lt;/p&gt;
&lt;p&gt;机器学习的算法很多。很多时候困惑人们都是，很多算法是一类算法，而有些算法又是从其他算法中延伸出来的。这里，我们从两个方面来给大家介绍，第一个方面是学习的方式，第二个方面是算法的类似性。&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;学习方式&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;根据数据类型的不同，对一个问题的建模有不同的方式。在机器学习或者人工智能领域，人们首先会考虑算法的学习方式。在机器学习领域，有几种主要的学习方式。将算法按照学习方式分类是一个不错的想法，这样可以让人们在建模和算法选择的时候考虑能根据输入数据来选择最合适的算法来获得最好的结果。&lt;/p&gt;
&lt;h3&gt;&lt;strong&gt;监督式学习：&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/518f60357fccae7066adb5ddfbff32b3.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;在监督式学习下，输入数据被称为“训练数据”，每组训练数据有一个明确的标识或结果，如对防垃圾邮件系统中“垃圾邮件”“非垃圾邮件”，对手写数字识别中的“1“，”2“，”3“，”4“等。在建立预测模型的时候，监督式学习建立一个学习过程，将预测结果与“训练数据”的实际结果进行比较，不断的调整预测模型，直到模型的预测结果达到一个预期的准确率。监督式学习的常见应用场景如分类问题和回归问题。常见算法有逻辑回归（Logistic Regression）和反向传递神经网络（Back Propagation Neural Network）&lt;/p&gt;
&lt;h3&gt;&lt;strong&gt;非监督式学习：&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/821b54dcaae4924fc7f5c3f858a1a259.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;在非监督式学习中，数据并不被特别标识，学习模型是为了推断出数据的一些内在结构。常见的应用场景包括关联规则的学习以及聚类等。常见算法包括Apriori算法以及k-Means算法。&lt;/p&gt;
&lt;h3&gt;&lt;strong&gt;半监督式学习：&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/1aa9c3e3344924f9f55916696ee74145.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;在此学习方式下，输入数据部分被标识，部分没有被标识，这种学习模型可以用来进行预测，但是模型首先需要学习数据的内在结构以便合理的组织数据来进行预测。应用场景包括分类和回归，算法包括一些对常用监督式学习算法的延伸，这些算法首先试图对未标识数据进行建模，在此基础上再对标识的数据进行预测。如图论推理算法（Graph Inference）或者拉普拉斯支持向量机（Laplacian SVM.）等。&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h3&gt;&lt;strong&gt;强化学习：&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/f646e8a089f83a20baa223d0bf1e6e79.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;在这种学习模式下，输入数据作为对模型的反馈，不像监督模型那样，输入数据仅仅是作为一个检查模型对错的方式，在强化学习下，输入数据直接反馈到模型，模型必须对此立刻作出调整。常见的应用场景包括动态系统以及机器人控制等。常见算法包括Q-Learning以及时间差学习（Temporal difference learning）&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;在企业数据应用的场景下， 人们最常用的可能就是监督式学习和非监督式学习的模型。 在图像识别等领域，由于存在大量的非标识的数据和少量的可标识数据， 目前半监督式学习是一个很热的话题。 而强化学习更多的应用在机器人控制及其他需要进行系统控制的领域。&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;算法类似性&lt;/strong&gt;&lt;/h2&gt;
&lt;h2&gt;&lt;strong&gt; &lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;根据算法的功能和形式的类似性，我们可以把算法分类，比如说基于树的算法，基于神经网络的算法等等。当然，机器学习的范围非常庞大，有些算法很难明确归类到某一类。而对于有些分类来说，同一分类的算法可以针对不同类型的问题。这里，我们尽量把常用的算法按照最容易理解的方式进行分类。&lt;/p&gt;
&lt;h3&gt;&lt;strong&gt;回归算法：&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/e0ebe4e5043444d2e306ed38f58e10c4.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;回归算法是试图采用对误差的衡量来探索变量之间的关系的一类算法。回归算法是统计机器学习的利器。在机器学习领域，人们说起回归，有时候是指一类问题，有时候是指一类算法，这一点常常会使初学者有所困惑。常见的回归算法包括：最小二乘法（Ordinary Least Square），逻辑回归（Logistic Regression），逐步式回归（Stepwise Regression），多元自适应回归样条（Multivariate Adaptive Regression Splines）以及本地散点平滑估计（Locally Estimated Scatterplot Smoothing）&lt;/p&gt;
&lt;h3&gt;&lt;strong&gt;基于实例的算法&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt; &lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/22ff19a0c252f0c406db6091813259df.jpg&quot;&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;基于实例的算法常常用来对决策问题建立模型，这样的模型常常先选取一批样本数据，然后根据某些近似性把新数据与样本数据进行比较。通过这种方式来寻找最佳的匹配。因此，基于实例的算法常常也被称为“赢家通吃”学习或者“基于记忆的学习”。常见的算法包括 k-Nearest Neighbor(KNN), 学习矢量量化（Learning Vector Quantization， LVQ），以及自组织映射算法（Self-Organizing Map ， SOM）&lt;/p&gt;
&lt;p&gt;&lt;strong&gt; &lt;/strong&gt;&lt;/p&gt;
&lt;h3&gt;&lt;strong&gt;正则化方法&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt; &lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/5578e533ef041563ba24807346bf9d3d.jpg&quot;&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;正则化方法是其他算法（通常是回归算法）的延伸，根据算法的复杂度对算法进行调整。正则化方法通常对简单模型予以奖励而对复杂算法予以惩罚。常见的算法包括：Ridge Regression， Least Absolute Shrinkage and Selection Operator（LASSO），以及弹性网络（Elastic Net）。&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h3&gt;&lt;strong&gt;决策树学习&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/15b49c4d7aab77171d9475c3fb8f5006.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;决策树算法根据数据的属性采用树状结构建立决策模型， 决策树模型常常用来解决分类和回归问题。常见的算法包括：分类及回归树（Classification And Regression Tree， CART）， ID3 (Iterative Dichotomiser 3)， C4.5， Chi-squared Automatic Interaction Detection(CHAID), Decision Stump, 随机森林（Random Forest）， 多元自适应回归样条（MARS）以及梯度推进机（Gradient Boosting Machine， GBM）&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h3&gt;&lt;strong&gt;贝叶斯方法&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/de9da162e1249472a426750340a8ba98.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;贝叶斯方法算法是基于贝叶斯定理的一类算法，主要用来解决分类和回归问题。常见算法包括：朴素贝叶斯算法，平均单依赖估计（Averaged One-Dependence Estimators， AODE），以及Bayesian Belief Network（BBN）。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt; &lt;/strong&gt;&lt;/p&gt;
&lt;h3&gt;&lt;strong&gt;基于核的算法&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt; &lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/cc04d60c7b791f909c93cc7d46b26b4a.jpg&quot;&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;基于核的算法中最著名的莫过于支持向量机（SVM）了。 基于核的算法把输入数据映射到一个高阶的向量空间， 在这些高阶向量空间里， 有些分类或者回归问题能够更容易的解决。 常见的基于核的算法包括：支持向量机（Support Vector Machine， SVM）， 径向基函数（Radial Basis Function ，RBF)， 以及线性判别分析（Linear Discriminate Analysis ，LDA)等&lt;/p&gt;
&lt;p&gt;&lt;strong&gt; &lt;/strong&gt;&lt;/p&gt;
&lt;h3&gt;&lt;strong&gt;聚类算法&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt; &lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/e7a6904e41cdcfa6c52dfceb24fa7340.jpg&quot;&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;聚类，就像回归一样，有时候人们描述的是一类问题，有时候描述的是一类算法。聚类算法通常按照中心点或者分层的方式对输入数据进行归并。所以的聚类算法都试图找到数据的内在结构，以便按照最大的共同点将数据进行归类。常见的聚类算法包括 k-Means算法以及期望最大化算法（Expectation Maximization， EM）。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt; &lt;/strong&gt;&lt;/p&gt;
&lt;h3&gt;&lt;strong&gt;关联规则学习&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt; &lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/826c328582b8fc95e6cae70745948efa.jpg&quot;&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;关联规则学习通过寻找最能够解释数据变量之间关系的规则，来找出大量多元数据集中有用的关联规则。常见算法包括 Apriori算法和Eclat算法等。&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h3&gt;&lt;strong&gt;人工神经网络&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/b91613bff92bd1a503c763d43e60170a.jpg&quot;&gt; &lt;/strong&gt;&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;人工神经网络算法模拟生物神经网络，是一类模式匹配算法。通常用于解决分类和回归问题。人工神经网络是机器学习的一个庞大的分支，有几百种不同的算法。（其中深度学习就是其中的一类算法，我们会单独讨论），重要的人工神经网络算法包括：感知器神经网络（Perceptron Neural Network）, 反向传递（Back Propagation）， Hopfield网络，自组织映射（Self-Organizing Map, SOM）。学习矢量量化（Learning Vector Quantization， LVQ）&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h3&gt;&lt;strong&gt;深度学习&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt; &lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/d8d09008ac3a811775a1e29af125980f.jpg&quot;&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;深度学习算法是对人工神经网络的发展。 在近期赢得了很多关注， 特别是&lt;a href=&quot;http://www.ctocio.com/ccnews/15615.html&quot;&gt;百度也开始发力深度学习后&lt;/a&gt;， 更是在国内引起了很多关注。   在计算能力变得日益廉价的今天，深度学习试图建立大得多也复杂得多的神经网络。很多深度学习的算法是半监督式学习算法，用来处理存在少量未标识数据的大数据集。常见的深度学习算法包括：受限波尔兹曼机（Restricted Boltzmann Machine， RBN）， Deep Belief Networks（DBN），卷积网络（Convolutional Network）, 堆栈式自动编码器（Stacked Auto-encoders）。&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h3&gt;&lt;strong&gt;降低维度算法&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt; &lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/0595924b456f4a99c51cff3d93f915cf.jpg&quot;&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;像聚类算法一样，降低维度算法试图分析数据的内在结构，不过降低维度算法是以非监督学习的方式试图利用较少的信息来归纳或者解释数据。这类算法可以用于高维数据的可视化或者用来简化数据以便监督式学习使用。常见的算法包括：主成份分析（Principle Component Analysis， PCA），偏最小二乘回归（Partial Least Square Regression，PLS）， Sammon映射，多维尺度（Multi-Dimensional Scaling, MDS）,  投影追踪（Projection Pursuit）等。&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h3&gt;&lt;strong&gt;集成算法：&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/4452f447638f455116a2fa9f54f4b500.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;集成算法用一些相对较弱的学习模型独立地就同样的样本进行训练，然后把结果整合起来进行整体预测。集成算法的主要难点在于究竟集成哪些独立的较弱的学习模型以及如何把学习结果整合起来。这是一类非常强大的算法，同时也非常流行。常见的算法包括：Boosting， Bootstrapped Aggregation（Bagging）， AdaBoost，堆叠泛化（Stacked Generalization， Blending），梯度推进机（Gradient Boosting Machine, GBM），随机森林（Random Forest）。&lt;/p&gt;


&lt;!-- div id=&quot;ad1&quot;&gt;
&lt;/div --&gt;


	


	

</description>
        <pubDate>Thu, 25 Sep 2014 00:00:00 +0800</pubDate>
        <link>http://iftti.com/posts/2014/2014-09-25-77620-574adc179.html</link>
        <guid isPermaLink="true">http://iftti.com/posts/2014/2014-09-25-77620-574adc179.html</guid>
        
        
        <category>jobbole</category>
        
      </item>
    
      <item>
        <title>推荐！可视化垃圾回收算法</title>
        <description>

					
		
&lt;!-- div id=&quot;ad1&quot;&gt;
&lt;script type=&quot;text/javascript&quot;&gt;
google_ad_client = &quot;ca-pub-7056282119617872&quot;;
google_ad_slot = &quot;6645040531&quot;;
google_ad_width = 300;
google_ad_height = 250;
&lt;/script&gt;
&lt;script type=&quot;text/javascript&quot;
src=&quot;http://pagead2.googlesyndication.com/pagead/show_ads.js&quot;&gt;
&lt;/script&gt;
&lt;/div&gt;
&lt;br/ --&gt;

&lt;p&gt;大部分开发者都认为自动垃圾回收器是理所当然的。实际上，这只是语言运行时提供的一项实用功能，旨在简化我们的开发工作。&lt;/p&gt;
&lt;p&gt;但是如果尝试着了解垃圾回收器的内部原理，你会发现很难弄明白。除非熟悉它的工作流程和错误处理方式，否则内部成千上万的实现细节会让你不知所措。&lt;/p&gt;
&lt;p&gt;我编译了一个有五种不同的垃圾回收算法工具。程序运行时会创建一个动画界面。你可以从&lt;a title=&quot; &quot; href=&quot;https://github.com/kenfox/gc-viz&quot; target=&quot;_blank&quot;&gt;github.com/kenfox/gc-viz&lt;/a&gt;上获取动画和代码来实现。非常让我惊讶的是，这个简单的动画显现出这些重要的算法。&lt;/p&gt;
&lt;h1&gt;任务完成后清理: aka No GC&lt;/h1&gt;
&lt;p&gt;&lt;img class=&quot;alignright&quot; alt=&quot;&quot; src=&quot;/images/jobbole.com/2e2f5d7c83df63e658803cb8b2e0d18a.jpg&quot; width=&quot;125&quot; height=&quot;200&quot;&gt;&lt;/p&gt;
&lt;p&gt;清理垃圾最简单可行的方法就是等一项任务完成之后，一次性处理所有的垃圾。这项技术非常有用，特别是如果能将一项任务分解成许多小任务。例如，Apache网络服务器在每次请求时创建一个小内存池并在请求完成后将创建的整个内存池完全释放。&lt;/p&gt;
&lt;p&gt;右图的动画显示了一个正在运行的程序。整张图片代表程序的内存区。内存区在开始时是黑色，黑色表明内存尚未被使用。闪着鲜绿色和黄色的区域表明该内存区域正在读写。颜色随着时间变化，你可以观察内存的使用情况，也可以看到当前的活动情况。如果仔细观察，你会发现内存区域中开始出现一些程序执行过程中会忽略的区域。这些区域就成了所谓的垃圾——程序不能访问和使用。垃圾区域之外的的内存区域是可用的。&lt;/p&gt;
&lt;p&gt;该程序的内存充足，所以不必担心程序运行时垃圾的清理。在后面的例子中我将一直使用这个简单的程序。&lt;/p&gt;
&lt;h1&gt;引用计数回收器&lt;/h1&gt;
&lt;p&gt;&lt;img class=&quot;alignright&quot; alt=&quot;&quot; src=&quot;/images/jobbole.com/35a16ebeb493293bd869b38d310e06b8.jpg&quot; width=&quot;125&quot; height=&quot;200&quot;&gt;&lt;/p&gt;
&lt;p&gt;另一个简单的解决方案是对你使用的资源(此处指内存中的对象)进行计数，当计数值变为0时，对其进行处理。这是一项广泛使用的技术，当开发者将垃圾回收添加到现有系统中时——这是唯一一个容易与其他资源管理器和现有代码库集成的垃圾回收器。苹果在为Objective-C发布了标志-擦除垃圾回收器后明白这个事实。发布产品出现很多问题以致于他们不得不废弃该项特性，取而代之的是性能良好的自动引用计数回收器。&lt;/p&gt;
&lt;p&gt;上面的动画显示了相同的程序，但是此时它将通过对内存中每一对象引用计数来处理垃圾。红色闪烁表示引用计数行为。引用计数的优势在于垃圾会被很快检测到——你可以看到红色闪烁过后紧接着该区域变黑。&lt;/p&gt;
&lt;p&gt;遗憾的是引用计数存在诸多问题。最糟糕的是，它不能处理循环结构。而循环结构非常常见——继承或反向引用都将建立一个循环，该结构将造成内存泄露。引用计数的开销也很大 ——从动画中可以看到即使当内存使用不在增长时，红色闪烁一直持续。CPU运算速度很快，但内存读写很慢，而计数器不断被加载并保存至内存。所有这些计数器的更新很难保证数据的只读或线程安全。&lt;/p&gt;
&lt;p&gt;引用计数是一种分摊算法（开销遍布整个程序运行时），但这是种分摊算法具有偶然性，不能保证反应时间。例如，程序中存在一个很大的树型结构。最后一段使用树的程序将触发对整个树的处理，墨菲说过事情如果有变坏的可能，不管这种可能性有多小，它总会发生。这里没有其他的分摊算法,所以分摊的偶然特征可能取决于数据。（所有这些算法有并发或部分并发的命令,但这些都是超出了程序可演示的范围。）&lt;/p&gt;
&lt;h1&gt;标记-擦除回收器&lt;/h1&gt;
&lt;p&gt;&lt;img class=&quot;alignright&quot; alt=&quot;&quot; src=&quot;/images/jobbole.com/458f2e66da1c9e965bb69c25997edbf5.jpg&quot; width=&quot;125&quot; height=&quot;200&quot;&gt;&lt;/p&gt;
&lt;p&gt;标记-擦除消除了引用计数存在的一些问题。它能够轻松解决循环结构在引用技术中存在的问题，由于不需维持计数，系统开销比较低。&lt;/p&gt;
&lt;p&gt;该算法舍弃垃圾检测的实时性。动画中，有一段运行时间没有任何红色的闪烁，然后突然出现许多红色闪烁表明当前正在标记活动对象。在标记完成后，程序要遍历整个内存空间并处理垃圾。在动画中你还将注意到—— 许多区域立刻变黑而不像引用计数方式那样随着时间慢慢变黑。&lt;/p&gt;
&lt;p&gt;标记-擦除比引用计数要求更高的一致性实现，而且很难移植到现有系统中。在标记阶段需要遍历所有活动数据，甚至是封装在对像中的数据。如果一个对象不支持遍历，那么尝试将标记-擦除移植到代码中风险太大。标记-擦除的另一个不足之处在于擦除阶段必须遍历整个内存来查找垃圾。对于一个产生垃圾较少的系统，这不是问题，但现在的函数式编程风格产生了大量的垃圾。&lt;/p&gt;
&lt;h1&gt;标记-压缩回收器&lt;/h1&gt;
&lt;p&gt;&lt;img class=&quot;alignright&quot; alt=&quot;&quot; src=&quot;/images/jobbole.com/837d74ff492982d54f37dc3c3abc604a.jpg&quot; width=&quot;125&quot; height=&quot;200&quot;&gt;&lt;/p&gt;
&lt;p&gt;在前面的动画中你可能注意到一点，对象从不移动。一旦对象在内存中分配，该对象的存储位置就不会再改变，即使被散步在黑色区域的内存碎片包围。下面两种算法用完全不同的方式改变了这种现象。&lt;/p&gt;
&lt;p&gt;标记-压缩算法不是仅通过标记内存区域是否空闲来处理内存，而是通过将对象移动到空闲表来实现。对象通常按照内存顺序存储,先分配的对象在内存的低地址空间——但是处理对象造成的空缺将随着对象的移动变大。&lt;/p&gt;
&lt;p&gt;移动对象意味着新对象只能在已使用内存的末尾创建。这就是所谓的“bunp”分配器，和栈分配器一样，但不限制栈空间。有些使用bump分配器的系统甚至不用调用栈存储数据，他们只在堆中分配调用帧,像其他对象一样对待。&lt;/p&gt;
&lt;p&gt;有时理论高于实践，另一个优势是当对象被压缩后，程序能够像访问硬件高速缓存一样访问内存。不确定你能否看到这个好处——尽管引用计数和标记-擦除使用的内存分配器很复杂,但调试效果很好，效率也很高。&lt;/p&gt;
&lt;p&gt;标记-压缩是算法很复杂，需要多次遍历所有分配对象。在动画中可以看到紧随红色闪烁的活动对象其后的是大量读和写标记为目的地计算，对象被移动，最终引用固定指向移动后的对象。这个复杂程序背后最大的优点是内存开销非常小。Oracle的Hotspot JVM使用了多种不同垃圾回收算法。而全局对象空间使用标记-压缩回收算法。&lt;/p&gt;
&lt;h1&gt;拷贝回收器&lt;/h1&gt;
&lt;p&gt;&lt;img class=&quot;alignright&quot; alt=&quot;&quot; src=&quot;/images/jobbole.com/2e2f5d7c83df63e658803cb8b2e0d18a.jpg&quot; width=&quot;125&quot; height=&quot;200&quot;&gt;&lt;/p&gt;
&lt;p&gt;最后使用动画显示的算法是大多数高性能垃圾收集系统的基础。它和标记-压缩是一样的移动回收器，但是相比之下实现却非常简单。它使用两块内存空间，在两个内存间交替复制活动对象。实际上，空间不止两块，这些空间用于不同代对象，新的对象在一个空间中创建，如果生命周期没有结束就会被复制到另一个空间，如果长期存在就会被复制到一个永久性空间。如果你听说一个垃圾收集器是分代的或短暂的,通常是多空间拷贝回收器。&lt;/p&gt;
&lt;p&gt;除了简单性和灵活性，该算法的主要优势在于只要在活动对象上花时间。没有独立的标记阶段必须被擦除或压缩。在遍历活动对象期间，对象会被立即复制，弥补了以往对象在引用计数时的不足。&lt;/p&gt;
&lt;p&gt;在动画中，你可以看到回收过程中乎所有的数据从一个空间复制到另一个空间。对该算法来说是个糟糕的情况，这是人们谈论优化垃圾收集器的一个原因。如果你能调整内存并有优化分配，使得在回收开始前大部分对象都废弃了，那么你就能兼顾安全函数式编程风格和高性能。&lt;/p&gt;
&lt;p&gt;（注：限于&lt;a href=&quot;mailto:%22ashiontang@gmail.com%22&quot;&gt;译者&lt;/a&gt;水平有限，不足之处恳请指正。）&lt;/p&gt;


&lt;!-- div id=&quot;ad1&quot;&gt;
&lt;/div --&gt;


	


	

</description>
        <pubDate>Thu, 25 Sep 2014 00:00:00 +0800</pubDate>
        <link>http://iftti.com/posts/2014/2014-09-25-77280-5a4d17c33.html</link>
        <guid isPermaLink="true">http://iftti.com/posts/2014/2014-09-25-77280-5a4d17c33.html</guid>
        
        
        <category>jobbole</category>
        
      </item>
    
      <item>
        <title>数据库优化案例（一）</title>
        <description>

                &lt;h1 id=&quot;-&quot;&gt;&lt;/h1&gt;
&lt;h3 id=&quot;-&quot;&gt;现象：&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;程序大量SQL运行超时;&lt;/li&gt;
&lt;li&gt;数据库中大量SQL长时间处于Updating阶段；&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&quot;-&quot;&gt;初步检查：&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;查看系统状态，CPU有24个核，只有一个核user利用率100%，其余核的idle基本上100%；&lt;/li&gt;
&lt;li&gt;内存、IO.util都正常；&lt;/li&gt;
&lt;li&gt;查看processlist，所有UPDATE语句长时间处于Updating阶段，其它类型的SQL没有Block或者运行缓慢现象；&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&quot;-&quot;&gt;进一步排查：&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;分析运行缓慢的SQL，发现运行缓慢的Update语句操作的都是同一张表；&lt;/li&gt;
&lt;li&gt;执行缓慢的Update语句总共有两类：一类是通过主键更新一行数据；另一类是根据状态字段批量更新数据；Schema和SQL示例如下：&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;/p&gt;
&lt;!-- Crayon Syntax Highlighter v2.1.2 --&gt;

		&lt;div id=&quot;crayon-542531c3c2293&quot; class=&quot;crayon-syntax crayon-theme-github crayon-font-monaco crayon-os-pc print-yes&quot; data-settings=&quot; minimize scroll-mouseover wrap&quot; style=&quot; margin-top: 12px; margin-bottom: 12px; float: none; clear: both; font-size: 12px !important; line-height: 15px !important;&quot;&gt;
		
			&lt;div class=&quot;crayon-toolbar&quot; data-settings=&quot; mouseover overlay hide delay&quot; style=&quot;height: 18px !important; line-height: 18px !important;&quot;&gt;
&lt;span class=&quot;crayon-title&quot;&gt;&lt;/span&gt;
			&lt;div class=&quot;crayon-tools&quot;&gt;
&lt;div class=&quot;crayon-button crayon-nums-button&quot; title=&quot;切换显示行编号&quot;&gt;&lt;/div&gt;
&lt;div class=&quot;crayon-button crayon-plain-button&quot; title=&quot;纯文本显示代码&quot;&gt;&lt;/div&gt;
&lt;div class=&quot;crayon-button crayon-wrap-button&quot; title=&quot;切换自动换行&quot;&gt;&lt;/div&gt;
&lt;div class=&quot;crayon-button crayon-expand-button&quot; title=&quot;Expand Code&quot;&gt;&lt;/div&gt;
&lt;div class=&quot;crayon-button crayon-copy-button&quot; title=&quot;Expand Code&quot;&gt;&lt;/div&gt;
&lt;div class=&quot;crayon-button crayon-popup-button&quot; title=&quot;在新窗口中显示代码&quot;&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
			&lt;div class=&quot;crayon-info&quot; style=&quot;min-height: 15px !important; line-height: 15px !important;&quot;&gt;&lt;/div&gt;
			&lt;div class=&quot;crayon-plain-wrap&quot;&gt;&lt;textarea class=&quot;crayon-plain print-no&quot; data-settings=&quot;dblclick&quot; readonly style=&quot;-moz-tab-size:4; -o-tab-size:4; -webkit-tab-size:4; tab-size:4; font-size: 12px !important; line-height: 15px !important;&quot;&gt;
&amp;lt;code&amp;gt;-- Schema
CREATE TABLE `general_mail` (
  `id` bigint(20) unsigned NOT NULL AUTO_INCREMENT,
  `user_id` bigint(20) unsigned NOT NULL DEFAULT &#39;0&#39;,
  `value` text NOT NULL,
  `status` tinyint(4) unsigned NOT NULL DEFAULT &#39;0&#39;,
  `type` tinyint(4) unsigned NOT NULL DEFAULT &#39;0&#39;,
    ......
  PRIMARY KEY (`id`),
  KEY `idx_status_type` (`status`,`type`),
) ENGINE=InnoDB;

-- 根据主键进行更新
UPDATE tbl_A SET status=N, value=&#39;.....&#39;, user_id=N WHERE id = N;

-- 根据状态进行更新
UPDATE tbl_A SET status=N WHERE (status=N or status=N) and user_id=N limit N;
&amp;lt;/code&amp;gt;&lt;/textarea&gt;&lt;/div&gt;
			&lt;div class=&quot;crayon-main&quot; style=&quot;&quot;&gt;
				&lt;table class=&quot;crayon-table&quot;&gt;
					&lt;tr class=&quot;crayon-row&quot;&gt;
				&lt;td class=&quot;crayon-nums &quot; data-settings=&quot;hide&quot;&gt;
					&lt;div class=&quot;crayon-nums-content&quot; style=&quot;font-size: 12px !important; line-height: 15px !important;&quot;&gt;
&lt;div class=&quot;crayon-num&quot; data-line=&quot;crayon-542531c3c2293-1&quot;&gt;1&lt;/div&gt;
&lt;div class=&quot;crayon-num crayon-striped-num&quot; data-line=&quot;crayon-542531c3c2293-2&quot;&gt;2&lt;/div&gt;
&lt;div class=&quot;crayon-num&quot; data-line=&quot;crayon-542531c3c2293-3&quot;&gt;3&lt;/div&gt;
&lt;div class=&quot;crayon-num crayon-striped-num&quot; data-line=&quot;crayon-542531c3c2293-4&quot;&gt;4&lt;/div&gt;
&lt;div class=&quot;crayon-num&quot; data-line=&quot;crayon-542531c3c2293-5&quot;&gt;5&lt;/div&gt;
&lt;div class=&quot;crayon-num crayon-striped-num&quot; data-line=&quot;crayon-542531c3c2293-6&quot;&gt;6&lt;/div&gt;
&lt;div class=&quot;crayon-num&quot; data-line=&quot;crayon-542531c3c2293-7&quot;&gt;7&lt;/div&gt;
&lt;div class=&quot;crayon-num crayon-striped-num&quot; data-line=&quot;crayon-542531c3c2293-8&quot;&gt;8&lt;/div&gt;
&lt;div class=&quot;crayon-num&quot; data-line=&quot;crayon-542531c3c2293-9&quot;&gt;9&lt;/div&gt;
&lt;div class=&quot;crayon-num crayon-striped-num&quot; data-line=&quot;crayon-542531c3c2293-10&quot;&gt;10&lt;/div&gt;
&lt;div class=&quot;crayon-num&quot; data-line=&quot;crayon-542531c3c2293-11&quot;&gt;11&lt;/div&gt;
&lt;div class=&quot;crayon-num crayon-striped-num&quot; data-line=&quot;crayon-542531c3c2293-12&quot;&gt;12&lt;/div&gt;
&lt;div class=&quot;crayon-num&quot; data-line=&quot;crayon-542531c3c2293-13&quot;&gt;13&lt;/div&gt;
&lt;div class=&quot;crayon-num crayon-striped-num&quot; data-line=&quot;crayon-542531c3c2293-14&quot;&gt;14&lt;/div&gt;
&lt;div class=&quot;crayon-num&quot; data-line=&quot;crayon-542531c3c2293-15&quot;&gt;15&lt;/div&gt;
&lt;div class=&quot;crayon-num crayon-striped-num&quot; data-line=&quot;crayon-542531c3c2293-16&quot;&gt;16&lt;/div&gt;
&lt;div class=&quot;crayon-num&quot; data-line=&quot;crayon-542531c3c2293-17&quot;&gt;17&lt;/div&gt;
&lt;div class=&quot;crayon-num crayon-striped-num&quot; data-line=&quot;crayon-542531c3c2293-18&quot;&gt;18&lt;/div&gt;
&lt;/div&gt;
				&lt;/td&gt;
						&lt;td class=&quot;crayon-code&quot;&gt;&lt;div class=&quot;crayon-pre&quot; style=&quot;font-size: 12px !important; line-height: 15px !important;&quot;&gt;
&lt;div class=&quot;crayon-line&quot; id=&quot;crayon-542531c3c2293-1&quot;&gt;
&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;i&quot;&gt;code&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;e&quot;&gt;Schema&lt;/span&gt;
&lt;/div&gt;
&lt;div class=&quot;crayon-line crayon-striped-line&quot; id=&quot;crayon-542531c3c2293-2&quot;&gt;
&lt;span class=&quot;e&quot;&gt;CREATE &lt;/span&gt;&lt;span class=&quot;i&quot;&gt;TABLE&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;`&lt;/span&gt;&lt;span class=&quot;i&quot;&gt;general_mail&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;`&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;(&lt;/span&gt;
&lt;/div&gt;
&lt;div class=&quot;crayon-line&quot; id=&quot;crayon-542531c3c2293-3&quot;&gt;
&lt;span class=&quot;h&quot;&gt;  &lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;`&lt;/span&gt;&lt;span class=&quot;i&quot;&gt;id&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;`&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;e&quot;&gt;bigint&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;cn&quot;&gt;20&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;t&quot;&gt;unsigned&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;st&quot;&gt;NOT&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;t&quot;&gt;NULL&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;i&quot;&gt;AUTO_INCREMENT&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;,&lt;/span&gt;
&lt;/div&gt;
&lt;div class=&quot;crayon-line crayon-striped-line&quot; id=&quot;crayon-542531c3c2293-4&quot;&gt;
&lt;span class=&quot;h&quot;&gt;  &lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;`&lt;/span&gt;&lt;span class=&quot;i&quot;&gt;user_id&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;`&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;e&quot;&gt;bigint&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;cn&quot;&gt;20&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;t&quot;&gt;unsigned&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;st&quot;&gt;NOT&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;t&quot;&gt;NULL&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;st&quot;&gt;DEFAULT&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;0&#39;&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;,&lt;/span&gt;
&lt;/div&gt;
&lt;div class=&quot;crayon-line&quot; id=&quot;crayon-542531c3c2293-5&quot;&gt;
&lt;span class=&quot;h&quot;&gt;  &lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;`&lt;/span&gt;&lt;span class=&quot;i&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;`&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;e&quot;&gt;text &lt;/span&gt;&lt;span class=&quot;st&quot;&gt;NOT&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;t&quot;&gt;NULL&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;,&lt;/span&gt;
&lt;/div&gt;
&lt;div class=&quot;crayon-line crayon-striped-line&quot; id=&quot;crayon-542531c3c2293-6&quot;&gt;
&lt;span class=&quot;h&quot;&gt;  &lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;`&lt;/span&gt;&lt;span class=&quot;i&quot;&gt;status&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;`&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;e&quot;&gt;tinyint&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;cn&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;t&quot;&gt;unsigned&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;st&quot;&gt;NOT&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;t&quot;&gt;NULL&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;st&quot;&gt;DEFAULT&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;0&#39;&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;,&lt;/span&gt;
&lt;/div&gt;
&lt;div class=&quot;crayon-line&quot; id=&quot;crayon-542531c3c2293-7&quot;&gt;
&lt;span class=&quot;h&quot;&gt;  &lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;`&lt;/span&gt;&lt;span class=&quot;i&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;`&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;e&quot;&gt;tinyint&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;cn&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;t&quot;&gt;unsigned&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;st&quot;&gt;NOT&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;t&quot;&gt;NULL&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;st&quot;&gt;DEFAULT&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;0&#39;&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;,&lt;/span&gt;
&lt;/div&gt;
&lt;div class=&quot;crayon-line crayon-striped-line&quot; id=&quot;crayon-542531c3c2293-8&quot;&gt;
&lt;span class=&quot;h&quot;&gt;    &lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;.&lt;/span&gt;
&lt;/div&gt;
&lt;div class=&quot;crayon-line&quot; id=&quot;crayon-542531c3c2293-9&quot;&gt;
&lt;span class=&quot;h&quot;&gt;  &lt;/span&gt;&lt;span class=&quot;e&quot;&gt;PRIMARY &lt;/span&gt;&lt;span class=&quot;e&quot;&gt;KEY&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;`&lt;/span&gt;&lt;span class=&quot;i&quot;&gt;id&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;`&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;,&lt;/span&gt;
&lt;/div&gt;
&lt;div class=&quot;crayon-line crayon-striped-line&quot; id=&quot;crayon-542531c3c2293-10&quot;&gt;
&lt;span class=&quot;h&quot;&gt;  &lt;/span&gt;&lt;span class=&quot;i&quot;&gt;KEY&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;`&lt;/span&gt;&lt;span class=&quot;i&quot;&gt;idx_status_type&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;`&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;`&lt;/span&gt;&lt;span class=&quot;i&quot;&gt;status&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;`&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;`&lt;/span&gt;&lt;span class=&quot;i&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;`&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;,&lt;/span&gt;
&lt;/div&gt;
&lt;div class=&quot;crayon-line&quot; id=&quot;crayon-542531c3c2293-11&quot;&gt;
&lt;span class=&quot;sy&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;v&quot;&gt;ENGINE&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;i&quot;&gt;InnoDB&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;;&lt;/span&gt;
&lt;/div&gt;
&lt;div class=&quot;crayon-line crayon-striped-line&quot; id=&quot;crayon-542531c3c2293-12&quot;&gt; &lt;/div&gt;
&lt;div class=&quot;crayon-line&quot; id=&quot;crayon-542531c3c2293-13&quot;&gt;
&lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;根据主键进行更新&lt;/div&gt;
&lt;div class=&quot;crayon-line crayon-striped-line&quot; id=&quot;crayon-542531c3c2293-14&quot;&gt;
&lt;span class=&quot;e&quot;&gt;UPDATE &lt;/span&gt;&lt;span class=&quot;e&quot;&gt;tbl_A &lt;/span&gt;&lt;span class=&quot;e&quot;&gt;SET &lt;/span&gt;&lt;span class=&quot;v&quot;&gt;status&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;i&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;v&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;.....&#39;&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;v&quot;&gt;user_id&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;i&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;e&quot;&gt;WHERE &lt;/span&gt;&lt;span class=&quot;v&quot;&gt;id&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;i&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;;&lt;/span&gt;
&lt;/div&gt;
&lt;div class=&quot;crayon-line&quot; id=&quot;crayon-542531c3c2293-15&quot;&gt; &lt;/div&gt;
&lt;div class=&quot;crayon-line crayon-striped-line&quot; id=&quot;crayon-542531c3c2293-16&quot;&gt;
&lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;根据状态进行更新&lt;/div&gt;
&lt;div class=&quot;crayon-line&quot; id=&quot;crayon-542531c3c2293-17&quot;&gt;
&lt;span class=&quot;e&quot;&gt;UPDATE &lt;/span&gt;&lt;span class=&quot;e&quot;&gt;tbl_A &lt;/span&gt;&lt;span class=&quot;e&quot;&gt;SET &lt;/span&gt;&lt;span class=&quot;v&quot;&gt;status&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;i&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;e&quot;&gt;WHERE&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;v&quot;&gt;status&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;i&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;st&quot;&gt;or&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;v&quot;&gt;status&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;i&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;st&quot;&gt;and&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;v&quot;&gt;user_id&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;i&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;i&quot;&gt;limit&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;i&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;;&lt;/span&gt;
&lt;/div&gt;
&lt;div class=&quot;crayon-line crayon-striped-line&quot; id=&quot;crayon-542531c3c2293-18&quot;&gt;
&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;i&quot;&gt;code&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;/td&gt;
					&lt;/tr&gt;
				&lt;/table&gt;
			&lt;/div&gt;
		&lt;/div&gt;
&lt;!-- [Format Time: 0.0059 seconds] --&gt;
&lt;p&gt;基本可以推测是根据状态字段更新数据的SQL导致的问题。进一步查看INNODB STATUS，发现如下记录：&lt;/p&gt;
&lt;!-- Crayon Syntax Highlighter v2.1.2 --&gt;

		&lt;div id=&quot;crayon-542531c3c2345&quot; class=&quot;crayon-syntax crayon-theme-github crayon-font-monaco crayon-os-pc print-yes&quot; data-settings=&quot; minimize scroll-mouseover wrap&quot; style=&quot; margin-top: 12px; margin-bottom: 12px; float: none; clear: both; font-size: 12px !important; line-height: 15px !important;&quot;&gt;
		
			&lt;div class=&quot;crayon-toolbar&quot; data-settings=&quot; mouseover overlay hide delay&quot; style=&quot;height: 18px !important; line-height: 18px !important;&quot;&gt;
&lt;span class=&quot;crayon-title&quot;&gt;&lt;/span&gt;
			&lt;div class=&quot;crayon-tools&quot;&gt;
&lt;div class=&quot;crayon-button crayon-nums-button&quot; title=&quot;切换显示行编号&quot;&gt;&lt;/div&gt;
&lt;div class=&quot;crayon-button crayon-plain-button&quot; title=&quot;纯文本显示代码&quot;&gt;&lt;/div&gt;
&lt;div class=&quot;crayon-button crayon-wrap-button&quot; title=&quot;切换自动换行&quot;&gt;&lt;/div&gt;
&lt;div class=&quot;crayon-button crayon-expand-button&quot; title=&quot;Expand Code&quot;&gt;&lt;/div&gt;
&lt;div class=&quot;crayon-button crayon-copy-button&quot; title=&quot;Expand Code&quot;&gt;&lt;/div&gt;
&lt;div class=&quot;crayon-button crayon-popup-button&quot; title=&quot;在新窗口中显示代码&quot;&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
			&lt;div class=&quot;crayon-info&quot; style=&quot;min-height: 15px !important; line-height: 15px !important;&quot;&gt;&lt;/div&gt;
			&lt;div class=&quot;crayon-plain-wrap&quot;&gt;&lt;textarea class=&quot;crayon-plain print-no&quot; data-settings=&quot;dblclick&quot; readonly style=&quot;-moz-tab-size:4; -o-tab-size:4; -webkit-tab-size:4; tab-size:4; font-size: 12px !important; line-height: 15px !important;&quot;&gt;
&amp;lt;code&amp;gt;------------------
---TRANSACTION 43DF0254C, ACTIVE 51 sec updating or deleting
mysql tables in use 1, locked 1
LOCK WAIT 3 lock struct(s), heap size 1248, 2 row lock(s), undo log entries 1
MySQL thread id 118879, OS thread handle 0x7f51950b6700, query id 18995841 10.0.0.0 mysql_user Updating
UPDATE tbl_A SET status=N, value=&#39;.....&#39;, user_id=N WHERE id = N
------- TRX HAS BEEN WAITING 51 SEC FOR THIS LOCK TO BE GRANTED:
RECORD LOCKS space id 2186 page no 188857 n bits 688 index `idx_status_type` of table `db`.`tbl_A` trx id 43DF0254C lock_mode X locks gap before rec insert intention waiting
Record lock, heap no 497 PHYSICAL RECORD: n_fields 3; compact format; info bits 0
 0: len 1; hex 02; asc  ;;
 1: len 1; hex 01; asc  ;;
 2: len 8; hex 00000000011c1f0f; asc         ;;

------------------
---TRANSACTION 43DF0216B, ACTIVE 65 sec fetching rows, thread declared inside InnoDB 155
mysql tables in use 1, locked 1
794767 lock struct(s), heap size 71694776, 13900037 row lock(s)
MySQL thread id 118812, OS thread handle 0x7f51951dc700, query id 18995134 10.0.0.1 mysql_user Searching rows for update
UPDATE tbl_A SET status=N WHERE (status=N or status=N) and user_id=N limit N
&amp;lt;/code&amp;gt;&lt;/textarea&gt;&lt;/div&gt;
			&lt;div class=&quot;crayon-main&quot; style=&quot;&quot;&gt;
				&lt;table class=&quot;crayon-table&quot;&gt;
					&lt;tr class=&quot;crayon-row&quot;&gt;
				&lt;td class=&quot;crayon-nums &quot; data-settings=&quot;hide&quot;&gt;
					&lt;div class=&quot;crayon-nums-content&quot; style=&quot;font-size: 12px !important; line-height: 15px !important;&quot;&gt;
&lt;div class=&quot;crayon-num&quot; data-line=&quot;crayon-542531c3c2345-1&quot;&gt;1&lt;/div&gt;
&lt;div class=&quot;crayon-num crayon-striped-num&quot; data-line=&quot;crayon-542531c3c2345-2&quot;&gt;2&lt;/div&gt;
&lt;div class=&quot;crayon-num&quot; data-line=&quot;crayon-542531c3c2345-3&quot;&gt;3&lt;/div&gt;
&lt;div class=&quot;crayon-num crayon-striped-num&quot; data-line=&quot;crayon-542531c3c2345-4&quot;&gt;4&lt;/div&gt;
&lt;div class=&quot;crayon-num&quot; data-line=&quot;crayon-542531c3c2345-5&quot;&gt;5&lt;/div&gt;
&lt;div class=&quot;crayon-num crayon-striped-num&quot; data-line=&quot;crayon-542531c3c2345-6&quot;&gt;6&lt;/div&gt;
&lt;div class=&quot;crayon-num&quot; data-line=&quot;crayon-542531c3c2345-7&quot;&gt;7&lt;/div&gt;
&lt;div class=&quot;crayon-num crayon-striped-num&quot; data-line=&quot;crayon-542531c3c2345-8&quot;&gt;8&lt;/div&gt;
&lt;div class=&quot;crayon-num&quot; data-line=&quot;crayon-542531c3c2345-9&quot;&gt;9&lt;/div&gt;
&lt;div class=&quot;crayon-num crayon-striped-num&quot; data-line=&quot;crayon-542531c3c2345-10&quot;&gt;10&lt;/div&gt;
&lt;div class=&quot;crayon-num&quot; data-line=&quot;crayon-542531c3c2345-11&quot;&gt;11&lt;/div&gt;
&lt;div class=&quot;crayon-num crayon-striped-num&quot; data-line=&quot;crayon-542531c3c2345-12&quot;&gt;12&lt;/div&gt;
&lt;div class=&quot;crayon-num&quot; data-line=&quot;crayon-542531c3c2345-13&quot;&gt;13&lt;/div&gt;
&lt;div class=&quot;crayon-num crayon-striped-num&quot; data-line=&quot;crayon-542531c3c2345-14&quot;&gt;14&lt;/div&gt;
&lt;div class=&quot;crayon-num&quot; data-line=&quot;crayon-542531c3c2345-15&quot;&gt;15&lt;/div&gt;
&lt;div class=&quot;crayon-num crayon-striped-num&quot; data-line=&quot;crayon-542531c3c2345-16&quot;&gt;16&lt;/div&gt;
&lt;div class=&quot;crayon-num&quot; data-line=&quot;crayon-542531c3c2345-17&quot;&gt;17&lt;/div&gt;
&lt;div class=&quot;crayon-num crayon-striped-num&quot; data-line=&quot;crayon-542531c3c2345-18&quot;&gt;18&lt;/div&gt;
&lt;div class=&quot;crayon-num&quot; data-line=&quot;crayon-542531c3c2345-19&quot;&gt;19&lt;/div&gt;
&lt;div class=&quot;crayon-num crayon-striped-num&quot; data-line=&quot;crayon-542531c3c2345-20&quot;&gt;20&lt;/div&gt;
&lt;/div&gt;
				&lt;/td&gt;
						&lt;td class=&quot;crayon-code&quot;&gt;&lt;div class=&quot;crayon-pre&quot; style=&quot;font-size: 12px !important; line-height: 15px !important;&quot;&gt;
&lt;div class=&quot;crayon-line&quot; id=&quot;crayon-542531c3c2345-1&quot;&gt;
&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;i&quot;&gt;code&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;
&lt;/div&gt;
&lt;div class=&quot;crayon-line crayon-striped-line&quot; id=&quot;crayon-542531c3c2345-2&quot;&gt;
&lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;i&quot;&gt;TRANSACTION&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;cn&quot;&gt;43DF0254C&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;i&quot;&gt;ACTIVE&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;cn&quot;&gt;51&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;e&quot;&gt;sec &lt;/span&gt;&lt;span class=&quot;e&quot;&gt;updating &lt;/span&gt;&lt;span class=&quot;st&quot;&gt;or&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;e&quot;&gt;deleting&lt;/span&gt;
&lt;/div&gt;
&lt;div class=&quot;crayon-line&quot; id=&quot;crayon-542531c3c2345-3&quot;&gt;
&lt;span class=&quot;e&quot;&gt;mysql &lt;/span&gt;&lt;span class=&quot;e&quot;&gt;tables &lt;/span&gt;&lt;span class=&quot;st&quot;&gt;in&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;st&quot;&gt;use&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;cn&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;i&quot;&gt;locked&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;cn&quot;&gt;1&lt;/span&gt;
&lt;/div&gt;
&lt;div class=&quot;crayon-line crayon-striped-line&quot; id=&quot;crayon-542531c3c2345-4&quot;&gt;
&lt;span class=&quot;e&quot;&gt;LOCK &lt;/span&gt;&lt;span class=&quot;i&quot;&gt;WAIT&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;cn&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;e&quot;&gt;lock &lt;/span&gt;&lt;span class=&quot;t&quot;&gt;struct&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;i&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;e&quot;&gt;heap &lt;/span&gt;&lt;span class=&quot;i&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;cn&quot;&gt;1248&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;cn&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;e&quot;&gt;row &lt;/span&gt;&lt;span class=&quot;e&quot;&gt;lock&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;i&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;e&quot;&gt;undo &lt;/span&gt;&lt;span class=&quot;e&quot;&gt;log &lt;/span&gt;&lt;span class=&quot;i&quot;&gt;entries&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;cn&quot;&gt;1&lt;/span&gt;
&lt;/div&gt;
&lt;div class=&quot;crayon-line&quot; id=&quot;crayon-542531c3c2345-5&quot;&gt;
&lt;span class=&quot;e&quot;&gt;MySQL &lt;/span&gt;&lt;span class=&quot;e&quot;&gt;thread &lt;/span&gt;&lt;span class=&quot;i&quot;&gt;id&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;cn&quot;&gt;118879&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;e&quot;&gt;OS &lt;/span&gt;&lt;span class=&quot;e&quot;&gt;thread &lt;/span&gt;&lt;span class=&quot;i&quot;&gt;handle&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;cn&quot;&gt;0x7f51950b6700&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;e&quot;&gt;query &lt;/span&gt;&lt;span class=&quot;i&quot;&gt;id&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;cn&quot;&gt;18995841&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;cn&quot;&gt;10.0.0.0&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;e&quot;&gt;mysql_user &lt;/span&gt;&lt;span class=&quot;e&quot;&gt;Updating&lt;/span&gt;
&lt;/div&gt;
&lt;div class=&quot;crayon-line crayon-striped-line&quot; id=&quot;crayon-542531c3c2345-6&quot;&gt;
&lt;span class=&quot;e&quot;&gt;UPDATE &lt;/span&gt;&lt;span class=&quot;e&quot;&gt;tbl_A &lt;/span&gt;&lt;span class=&quot;e&quot;&gt;SET &lt;/span&gt;&lt;span class=&quot;v&quot;&gt;status&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;i&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;v&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;.....&#39;&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;v&quot;&gt;user_id&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;i&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;e&quot;&gt;WHERE &lt;/span&gt;&lt;span class=&quot;v&quot;&gt;id&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;i&quot;&gt;N&lt;/span&gt;
&lt;/div&gt;
&lt;div class=&quot;crayon-line&quot; id=&quot;crayon-542531c3c2345-7&quot;&gt;
&lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;e&quot;&gt;TRX &lt;/span&gt;&lt;span class=&quot;e&quot;&gt;HAS &lt;/span&gt;&lt;span class=&quot;e&quot;&gt;BEEN &lt;/span&gt;&lt;span class=&quot;i&quot;&gt;WAITING&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;cn&quot;&gt;51&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;e&quot;&gt;SEC &lt;/span&gt;&lt;span class=&quot;st&quot;&gt;FOR&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;r&quot;&gt;THIS&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;e&quot;&gt;LOCK &lt;/span&gt;&lt;span class=&quot;st&quot;&gt;TO&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;e&quot;&gt;BE &lt;/span&gt;&lt;span class=&quot;i&quot;&gt;GRANTED&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;
&lt;/div&gt;
&lt;div class=&quot;crayon-line crayon-striped-line&quot; id=&quot;crayon-542531c3c2345-8&quot;&gt;
&lt;span class=&quot;e&quot;&gt;RECORD &lt;/span&gt;&lt;span class=&quot;e&quot;&gt;LOCKS &lt;/span&gt;&lt;span class=&quot;e&quot;&gt;space &lt;/span&gt;&lt;span class=&quot;i&quot;&gt;id&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;cn&quot;&gt;2186&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;e&quot;&gt;page &lt;/span&gt;&lt;span class=&quot;i&quot;&gt;no&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;cn&quot;&gt;188857&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;i&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;i&quot;&gt;bits&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;cn&quot;&gt;688&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;i&quot;&gt;index&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;`&lt;/span&gt;&lt;span class=&quot;i&quot;&gt;idx_status_type&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;`&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;e&quot;&gt;of &lt;/span&gt;&lt;span class=&quot;i&quot;&gt;table&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;`&lt;/span&gt;&lt;span class=&quot;i&quot;&gt;db&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;`&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;`&lt;/span&gt;&lt;span class=&quot;i&quot;&gt;tbl_A&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;`&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;e&quot;&gt;trx &lt;/span&gt;&lt;span class=&quot;i&quot;&gt;id&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;cn&quot;&gt;43DF0254C&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;i&quot;&gt;lock_mode&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;i&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;e&quot;&gt;locks &lt;/span&gt;&lt;span class=&quot;e&quot;&gt;gap &lt;/span&gt;&lt;span class=&quot;e&quot;&gt;before &lt;/span&gt;&lt;span class=&quot;e&quot;&gt;rec &lt;/span&gt;&lt;span class=&quot;e&quot;&gt;insert &lt;/span&gt;&lt;span class=&quot;e&quot;&gt;intention &lt;/span&gt;&lt;span class=&quot;e&quot;&gt;waiting&lt;/span&gt;
&lt;/div&gt;
&lt;div class=&quot;crayon-line&quot; id=&quot;crayon-542531c3c2345-9&quot;&gt;
&lt;span class=&quot;e&quot;&gt;Record &lt;/span&gt;&lt;span class=&quot;i&quot;&gt;lock&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;e&quot;&gt;heap &lt;/span&gt;&lt;span class=&quot;i&quot;&gt;no&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;cn&quot;&gt;497&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;e&quot;&gt;PHYSICAL &lt;/span&gt;&lt;span class=&quot;i&quot;&gt;RECORD&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;i&quot;&gt;n_fields&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;cn&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;;&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;e&quot;&gt;compact &lt;/span&gt;&lt;span class=&quot;i&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;;&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;e&quot;&gt;info &lt;/span&gt;&lt;span class=&quot;i&quot;&gt;bits&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;cn&quot;&gt;0&lt;/span&gt;
&lt;/div&gt;
&lt;div class=&quot;crayon-line crayon-striped-line&quot; id=&quot;crayon-542531c3c2345-10&quot;&gt;
&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;cn&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;i&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;cn&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;;&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;i&quot;&gt;hex&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;cn&quot;&gt;02&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;;&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;i&quot;&gt;asc&lt;/span&gt;&lt;span class=&quot;h&quot;&gt;  &lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;;&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;;&lt;/span&gt;
&lt;/div&gt;
&lt;div class=&quot;crayon-line&quot; id=&quot;crayon-542531c3c2345-11&quot;&gt;
&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;cn&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;i&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;cn&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;;&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;i&quot;&gt;hex&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;cn&quot;&gt;01&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;;&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;i&quot;&gt;asc&lt;/span&gt;&lt;span class=&quot;h&quot;&gt;  &lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;;&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;;&lt;/span&gt;
&lt;/div&gt;
&lt;div class=&quot;crayon-line crayon-striped-line&quot; id=&quot;crayon-542531c3c2345-12&quot;&gt;
&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;cn&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;i&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;cn&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;;&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;i&quot;&gt;hex&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;cn&quot;&gt;00000000011c1f0f&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;;&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;i&quot;&gt;asc&lt;/span&gt;&lt;span class=&quot;h&quot;&gt;         &lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;;&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;;&lt;/span&gt;
&lt;/div&gt;
&lt;div class=&quot;crayon-line&quot; id=&quot;crayon-542531c3c2345-13&quot;&gt; &lt;/div&gt;
&lt;div class=&quot;crayon-line crayon-striped-line&quot; id=&quot;crayon-542531c3c2345-14&quot;&gt;
&lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;
&lt;/div&gt;
&lt;div class=&quot;crayon-line&quot; id=&quot;crayon-542531c3c2345-15&quot;&gt;
&lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;i&quot;&gt;TRANSACTION&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;cn&quot;&gt;43DF0216B&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;i&quot;&gt;ACTIVE&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;cn&quot;&gt;65&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;e&quot;&gt;sec &lt;/span&gt;&lt;span class=&quot;e&quot;&gt;fetching &lt;/span&gt;&lt;span class=&quot;i&quot;&gt;rows&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;e&quot;&gt;thread &lt;/span&gt;&lt;span class=&quot;e&quot;&gt;declared &lt;/span&gt;&lt;span class=&quot;e&quot;&gt;inside &lt;/span&gt;&lt;span class=&quot;i&quot;&gt;InnoDB&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;cn&quot;&gt;155&lt;/span&gt;
&lt;/div&gt;
&lt;div class=&quot;crayon-line crayon-striped-line&quot; id=&quot;crayon-542531c3c2345-16&quot;&gt;
&lt;span class=&quot;e&quot;&gt;mysql &lt;/span&gt;&lt;span class=&quot;e&quot;&gt;tables &lt;/span&gt;&lt;span class=&quot;st&quot;&gt;in&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;st&quot;&gt;use&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;cn&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;i&quot;&gt;locked&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;cn&quot;&gt;1&lt;/span&gt;
&lt;/div&gt;
&lt;div class=&quot;crayon-line&quot; id=&quot;crayon-542531c3c2345-17&quot;&gt;
&lt;span class=&quot;cn&quot;&gt;794767&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;e&quot;&gt;lock &lt;/span&gt;&lt;span class=&quot;t&quot;&gt;struct&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;i&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;e&quot;&gt;heap &lt;/span&gt;&lt;span class=&quot;i&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;cn&quot;&gt;71694776&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;cn&quot;&gt;13900037&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;e&quot;&gt;row &lt;/span&gt;&lt;span class=&quot;e&quot;&gt;lock&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;i&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;)&lt;/span&gt;
&lt;/div&gt;
&lt;div class=&quot;crayon-line crayon-striped-line&quot; id=&quot;crayon-542531c3c2345-18&quot;&gt;
&lt;span class=&quot;e&quot;&gt;MySQL &lt;/span&gt;&lt;span class=&quot;e&quot;&gt;thread &lt;/span&gt;&lt;span class=&quot;i&quot;&gt;id&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;cn&quot;&gt;118812&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;e&quot;&gt;OS &lt;/span&gt;&lt;span class=&quot;e&quot;&gt;thread &lt;/span&gt;&lt;span class=&quot;i&quot;&gt;handle&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;cn&quot;&gt;0x7f51951dc700&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;e&quot;&gt;query &lt;/span&gt;&lt;span class=&quot;i&quot;&gt;id&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;cn&quot;&gt;18995134&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;cn&quot;&gt;10.0.0.1&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;e&quot;&gt;mysql_user &lt;/span&gt;&lt;span class=&quot;e&quot;&gt;Searching &lt;/span&gt;&lt;span class=&quot;e&quot;&gt;rows &lt;/span&gt;&lt;span class=&quot;st&quot;&gt;for&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;e&quot;&gt;update&lt;/span&gt;
&lt;/div&gt;
&lt;div class=&quot;crayon-line&quot; id=&quot;crayon-542531c3c2345-19&quot;&gt;
&lt;span class=&quot;e&quot;&gt;UPDATE &lt;/span&gt;&lt;span class=&quot;e&quot;&gt;tbl_A &lt;/span&gt;&lt;span class=&quot;e&quot;&gt;SET &lt;/span&gt;&lt;span class=&quot;v&quot;&gt;status&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;i&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;e&quot;&gt;WHERE&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;v&quot;&gt;status&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;i&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;st&quot;&gt;or&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;v&quot;&gt;status&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;i&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;st&quot;&gt;and&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;v&quot;&gt;user_id&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;i&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;i&quot;&gt;limit&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;i&quot;&gt;N&lt;/span&gt;
&lt;/div&gt;
&lt;div class=&quot;crayon-line crayon-striped-line&quot; id=&quot;crayon-542531c3c2345-20&quot;&gt;
&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;i&quot;&gt;code&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;/td&gt;
					&lt;/tr&gt;
				&lt;/table&gt;
			&lt;/div&gt;
		&lt;/div&gt;
&lt;!-- [Format Time: 0.0113 seconds] --&gt;
&lt;p&gt;这下可以解释为什么只有一个CPU核在运行了。第二条SQL通过索引加扫表的方式，寻找符合条件的数据，这条SQL消耗了一个CPU核。查找数据的同时，此SQL加了gap锁，导致其它更新同一张表的SQL一直在等待锁的释放，因此其它CPU核基本处于空闲状态。&lt;/p&gt;
&lt;h3 id=&quot;-&quot;&gt;解决&lt;/h3&gt;
&lt;p&gt;找到问题的根源，解决起来就比较简单了，分析一下数据，发现索引(user_id, status)的区分度还不错，新增索引(user_id, status)后，问题解决。&lt;/p&gt;
            

</description>
        <pubDate>Thu, 25 Sep 2014 00:00:00 +0800</pubDate>
        <link>http://iftti.com/posts/2014/2014-09-25--p=1608-f99e3b74d.html</link>
        <guid isPermaLink="true">http://iftti.com/posts/2014/2014-09-25--p=1608-f99e3b74d.html</guid>
        
        
        <category>noops</category>
        
      </item>
    
      <item>
        <title>Building assert() in Swift, Part 2: __FILE__ and __LINE__</title>
        <description>

						
						

						&lt;p&gt;Two occasionally useful features of C are the &lt;span class=&quot;keyword&quot;&gt;__FILE__&lt;/span&gt; and &lt;span class=&quot;keyword&quot;&gt;__LINE__&lt;/span&gt; magic macros. These are built into the preprocessor, and expanded out before the C parser is run. Despite not having a preprocessor, Swift provides very similar functionality with similar names, but Swift works quite differently under the covers.&lt;/p&gt;
&lt;h3&gt;Built-In Identifiers&lt;/h3&gt;
&lt;p&gt;As described in &lt;a href=&quot;http://developer.apple.com/library/prerelease/ios/documentation/swift/conceptual/swift_programming_language/LexicalStructure.html&quot;&gt;the Swift programming guide&lt;/a&gt;, Swift has a number of built-in identifiers, including  &lt;span class=&quot;keyword&quot;&gt;__FILE__&lt;/span&gt;, &lt;span class=&quot;keyword&quot;&gt;__LINE__&lt;/span&gt;, &lt;span class=&quot;keyword&quot;&gt;__COLUMN__&lt;/span&gt;, and &lt;span class=&quot;keyword&quot;&gt;__FUNCTION__&lt;/span&gt;. These expressions can be used anywhere and are expanded by the parser to string or integer literals that correspond to the current location in the source code. This is incredibly useful for manual logging, e.g. to print out the current position before quitting.&lt;/p&gt;
&lt;p&gt;However, this doesn’t help us in our quest to implement &lt;span class=&quot;keyword&quot;&gt;assert()&lt;/span&gt;.  If we defined assert like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&lt;span class=&quot;key&quot;&gt;func&lt;/span&gt; assert(predicate : &lt;span class=&quot;key&quot;&gt;@autoclosure&lt;/span&gt; () -&amp;gt; &lt;span class=&quot;title&quot;&gt;Bool&lt;/span&gt;) { 
	&lt;span class=&quot;preprocessor&quot;&gt;#if DEBUG&lt;/span&gt;
		&lt;span class=&quot;key&quot;&gt;if&lt;/span&gt; !predicate() {
			&lt;span class=&quot;method&quot;&gt;println&lt;/span&gt;(&lt;span class=&quot;string&quot;&gt;&quot;assertion failed at&lt;/span&gt; \(&lt;span class=&quot;key&quot;&gt;__FILE__&lt;/span&gt;)&lt;span class=&quot;string&quot;&gt;:&lt;/span&gt;\(&lt;span class=&quot;key&quot;&gt;__LINE__&lt;/span&gt;)&lt;span class=&quot;string&quot;&gt;&quot;&lt;/span&gt;)
			abort()
		}
	&lt;span class=&quot;preprocessor&quot;&gt;#endif&lt;/span&gt;
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The above code would print out of the file/line location that implements &lt;span class=&quot;keyword&quot;&gt;assert()&lt;/span&gt; itself, not the location from the caller. That isn’t helpful.&lt;/p&gt;
&lt;h3&gt;Getting the location of a caller&lt;/h3&gt;
&lt;p&gt;Swift borrows a clever feature from the D language: these identifiers expand to the location of the caller &lt;em&gt;when evaluated in a default argument list&lt;/em&gt;.  To enable this behavior, the &lt;span class=&quot;keyword&quot;&gt;assert()&lt;/span&gt; function is defined something like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&lt;span class=&quot;key&quot;&gt;func&lt;/span&gt; assert(condition: &lt;span class=&quot;key&quot;&gt;@autoclosure&lt;/span&gt; () -&amp;gt; &lt;span class=&quot;title&quot;&gt;Bool&lt;/span&gt;, &lt;span class=&quot;key&quot;&gt;_&lt;/span&gt; message: &lt;span class=&quot;title&quot;&gt;String&lt;/span&gt; =&lt;span class=&quot;string&quot;&gt; &quot;&quot;&lt;/span&gt;,
	file: &lt;span class=&quot;title&quot;&gt;String&lt;/span&gt; = &lt;span class=&quot;key&quot;&gt;__FILE__&lt;/span&gt;, line: &lt;span class=&quot;title&quot;&gt;Int&lt;/span&gt; = &lt;span class=&quot;key&quot;&gt;__LINE__&lt;/span&gt;) {
		&lt;span class=&quot;preprocessor&quot;&gt;#if DEBUG&lt;/span&gt;
			&lt;span class=&quot;key&quot;&gt;if&lt;/span&gt; !condition() {
				println(&lt;span class=&quot;string&quot;&gt;&quot;assertion failed at&lt;/span&gt; \(file)&lt;span class=&quot;string&quot;&gt;:&lt;/span&gt;\(line)&lt;span class=&quot;string&quot;&gt;:&lt;/span&gt; \(message)&lt;span class=&quot;string&quot;&gt;&quot;&lt;/span&gt;)
				abort()
			}
		&lt;span class=&quot;preprocessor&quot;&gt;#endif&lt;/span&gt;
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The second parameter to the Swift &lt;span class=&quot;keyword&quot;&gt;assert()&lt;/span&gt; function is an optional string that you can specify, and the third and forth arguments are defaulted to be the position in the caller’s context.  This allows &lt;span class=&quot;keyword&quot;&gt;assert()&lt;/span&gt; to pick up the source location of the caller by default, and if you want to define your own abstractions on top of assert, you can pass down locations from its caller.  As a trivial example, you could define a function that logs and asserts like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&lt;span class=&quot;key&quot;&gt;func&lt;/span&gt; logAndAssert(condition: &lt;span class=&quot;key&quot;&gt;@autoclosure&lt;/span&gt; () -&amp;gt; &lt;span class=&quot;title&quot;&gt;Bool&lt;/span&gt;, &lt;span class=&quot;key&quot;&gt;_&lt;/span&gt; message: &lt;span class=&quot;title&quot;&gt;StaticString&lt;/span&gt; = &lt;span class=&quot;string&quot;&gt;&quot;&quot;&lt;/span&gt;,
	file: &lt;span class=&quot;title&quot;&gt;StaticString&lt;/span&gt; = &lt;span class=&quot;key&quot;&gt;__FILE__&lt;/span&gt;, line: &lt;span class=&quot;title&quot;&gt;UWord&lt;/span&gt; = &lt;span class=&quot;key&quot;&gt;__LINE__&lt;/span&gt;) {

	logMessage(message)
	&lt;span class=&quot;method&quot;&gt;assert&lt;/span&gt;(condition, message, file: file, line: line)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This properly propagates the file/line location of the &lt;span class=&quot;keyword&quot;&gt;logAndAssert()&lt;/span&gt; caller down to the implementation of &lt;span class=&quot;keyword&quot;&gt;assert()&lt;/span&gt;. Note that &lt;span class=&quot;keyword&quot;&gt;StaticString&lt;/span&gt;, as shown in the code above, is a simple &lt;span class=&quot;nowrap&quot;&gt;String-like&lt;/span&gt; type used to store a string literal, such as one produced by &lt;span class=&quot;keyword&quot;&gt;__FILE__&lt;/span&gt;, with no &lt;span class=&quot;nowrap&quot;&gt;memory-management&lt;/span&gt; overhead.&lt;/p&gt;
&lt;p&gt;In addition to being useful for &lt;span class=&quot;keyword&quot;&gt;assert()&lt;/span&gt;, this functionality is used in the Swift implementation of the higher-level XCTest framework, and may be useful for your own libraries as well.&lt;/p&gt;

						
												
											

</description>
        <pubDate>Thu, 25 Sep 2014 00:00:00 +0800</pubDate>
        <link>http://iftti.com/posts/2014/2014-09-25--id=15-43f1860ed.html</link>
        <guid isPermaLink="true">http://iftti.com/posts/2014/2014-09-25--id=15-43f1860ed.html</guid>
        
        
        <category>apple_swift</category>
        
      </item>
    
      <item>
        <title>Python自然语言处理实践: 在NLTK中使用斯坦福中文分词器</title>
        <description>

						&lt;p&gt;斯坦福大学自然语言处理组是世界知名的NLP研究小组，他们提供了一系列开源的Java文本分析工具，包括分词器(&lt;a href=&quot;http://nlp.stanford.edu/software/segmenter.shtml&quot;&gt;Word Segmenter&lt;/a&gt;)，词性标注工具（&lt;a href=&quot;http://nlp.stanford.edu/software/tagger.shtml&quot;&gt;Part-Of-Speech Tagger&lt;/a&gt;），命名实体识别工具（&lt;a href=&quot;http://nlp.stanford.edu/software/CRF-NER.shtml&quot;&gt;Named Entity Recognizer&lt;/a&gt;），句法分析器（&lt;a href=&quot;http://nlp.stanford.edu/software/lex-parser.shtml&quot;&gt;Parser&lt;/a&gt;）等，可喜的事，他们还为这些工具训练了相应的中文模型，支持中文文本处理。在使用NLTK的过程中，发现当前版本的&lt;a href=&quot;https://github.com/nltk/nltk&quot;&gt;NLTK&lt;/a&gt;已经提供了相应的斯坦福文本处理工具接口，包括词性标注，命名实体识别和句法分析器的接口，不过可惜的是，没有提供&lt;a href=&quot;http://www.52nlp.cn/category/word-segmentation&quot;&gt;分词&lt;/a&gt;器的接口。在google无果和阅读了相应的代码后，我决定照猫画虎为NLTK写一个斯坦福&lt;a href=&quot;http://www.52nlp.cn/category/word-segmentation&quot;&gt;中文分词&lt;/a&gt;器接口，这样可以方便的在Python中调用斯坦福文本处理工具。&lt;/p&gt;
&lt;p&gt;首先需要做一些准备工作，第一步当然是安装NLTK，这个可以参考我们在gensim的相关文章中的介绍《&lt;a href=&quot;http://www.52nlp.cn/%E5%A6%82%E4%BD%95%E8%AE%A1%E7%AE%97%E4%B8%A4%E4%B8%AA%E6%96%87%E6%A1%A3%E7%9A%84%E7%9B%B8%E4%BC%BC%E5%BA%A6%E4%B8%89&quot;&gt;如何计算两个文档的相似度&lt;/a&gt;》，不过这里建议check github上最新的NLTK源代码并用“python setup.py install”的方式安装这个版本：&lt;a href=&quot;https://github.com/nltk/nltk&quot;&gt;https://github.com/nltk/nltk&lt;/a&gt;。这个版本新增了对于斯坦福句法分析器的接口，一些老的版本并没有，这个之后我们也许还会用来介绍。而我们也是在这个版本中添加的斯坦福分词器接口，其他版本也许会存在一些小问题。其次是安装Java运行环境，以Ubuntu 12.04为例，安装Java运行环境仅需要两步：&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;sudo apt-get install default-jre&lt;br&gt;
sudo apt-get install default-jdk&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;最后，当然是最重要的，你需要下载&lt;a href=&quot;http://nlp.stanford.edu/software/segmenter.shtml&quot;&gt;斯坦福分词器&lt;/a&gt;的相应文件，包括源代码，模型文件，词典文件等。注意斯坦福分词器并不仅仅支持中文分词，还支持阿拉伯语的分词，需要下载的zip打包文件是这个: &lt;a href=&quot;http://nlp.stanford.edu/software/stanford-segmenter-2014-08-27.zip&quot;&gt;Download Stanford Word Segmenter version 2014-08-27&lt;/a&gt;，下载后解压。&lt;/p&gt;
&lt;p&gt;准备工作就绪后，我们首先考虑的是在nltk源代码里的什么地方来添加这个接口文件。在nltk源代码包下，斯坦福词性标注器和命名实体识别工具的接口文件是这个：nltk/tag/stanford.py ，而句法分析器的接口文件是这个：nltk/parse/stanford.py , 虽然在nltk/tokenize/目录下有一个stanford.py文件，但是仅仅提供了一个针对英文的tokenizer工具PTBTokenizer的接口，没有针对斯坦福分词器的接口，于是我决定在nltk/tokenize下添加一个stanford_segmenter.py文件，作为nltk斯坦福中文分词器的接口文件。NLTK中的这些接口利用了Linux 下的管道（PIPE）机制和subprocess模块，这里直接贴源代码了，感兴趣的同学可以自行阅读:&lt;br&gt;
&lt;span id=&quot;more-6763&quot;&gt;&lt;/span&gt;&lt;/p&gt;
&lt;div class=&quot;codecolorer-container text default&quot; style=&quot;overflow:auto;white-space:nowrap;width:620px;height:450px;&quot;&gt;&lt;table cellspacing=&quot;0&quot; cellpadding=&quot;0&quot;&gt;&lt;tbody&gt;&lt;tr&gt;
&lt;td class=&quot;line-numbers&quot;&gt;&lt;div&gt;1&lt;br&gt;2&lt;br&gt;3&lt;br&gt;4&lt;br&gt;5&lt;br&gt;6&lt;br&gt;7&lt;br&gt;8&lt;br&gt;9&lt;br&gt;10&lt;br&gt;11&lt;br&gt;12&lt;br&gt;13&lt;br&gt;14&lt;br&gt;15&lt;br&gt;16&lt;br&gt;17&lt;br&gt;18&lt;br&gt;19&lt;br&gt;20&lt;br&gt;21&lt;br&gt;22&lt;br&gt;23&lt;br&gt;24&lt;br&gt;25&lt;br&gt;26&lt;br&gt;27&lt;br&gt;28&lt;br&gt;29&lt;br&gt;30&lt;br&gt;31&lt;br&gt;32&lt;br&gt;33&lt;br&gt;34&lt;br&gt;35&lt;br&gt;36&lt;br&gt;37&lt;br&gt;38&lt;br&gt;39&lt;br&gt;40&lt;br&gt;41&lt;br&gt;42&lt;br&gt;43&lt;br&gt;44&lt;br&gt;45&lt;br&gt;46&lt;br&gt;47&lt;br&gt;48&lt;br&gt;49&lt;br&gt;50&lt;br&gt;51&lt;br&gt;52&lt;br&gt;53&lt;br&gt;54&lt;br&gt;55&lt;br&gt;56&lt;br&gt;57&lt;br&gt;58&lt;br&gt;59&lt;br&gt;60&lt;br&gt;61&lt;br&gt;62&lt;br&gt;63&lt;br&gt;64&lt;br&gt;65&lt;br&gt;66&lt;br&gt;67&lt;br&gt;68&lt;br&gt;69&lt;br&gt;70&lt;br&gt;71&lt;br&gt;72&lt;br&gt;73&lt;br&gt;74&lt;br&gt;75&lt;br&gt;76&lt;br&gt;77&lt;br&gt;78&lt;br&gt;79&lt;br&gt;80&lt;br&gt;81&lt;br&gt;82&lt;br&gt;83&lt;br&gt;84&lt;br&gt;85&lt;br&gt;86&lt;br&gt;87&lt;br&gt;88&lt;br&gt;89&lt;br&gt;90&lt;br&gt;91&lt;br&gt;92&lt;br&gt;93&lt;br&gt;94&lt;br&gt;95&lt;br&gt;96&lt;br&gt;97&lt;br&gt;98&lt;br&gt;99&lt;br&gt;100&lt;br&gt;101&lt;br&gt;102&lt;br&gt;103&lt;br&gt;104&lt;br&gt;105&lt;br&gt;106&lt;br&gt;107&lt;br&gt;108&lt;br&gt;109&lt;br&gt;110&lt;br&gt;111&lt;br&gt;112&lt;br&gt;113&lt;br&gt;114&lt;br&gt;115&lt;br&gt;116&lt;br&gt;117&lt;br&gt;118&lt;br&gt;119&lt;br&gt;120&lt;br&gt;121&lt;br&gt;122&lt;br&gt;123&lt;br&gt;124&lt;br&gt;125&lt;br&gt;126&lt;br&gt;127&lt;br&gt;128&lt;br&gt;129&lt;br&gt;130&lt;br&gt;131&lt;br&gt;132&lt;br&gt;133&lt;br&gt;134&lt;br&gt;135&lt;br&gt;136&lt;br&gt;
&lt;/div&gt;&lt;/td&gt;
&lt;td&gt;&lt;div class=&quot;text codecolorer&quot;&gt;#!/usr/bin/env python&lt;br&gt;
# -*- coding: utf-8 -*-&lt;br&gt;
# Natural Language Toolkit: Interface to the Stanford Chinese Segmenter&lt;br&gt;
#&lt;br&gt;
# Copyright (C) 2001-2014 NLTK Project&lt;br&gt;
# Author: 52nlp &amp;lt;52nlpcn@gmail.com&amp;gt;&lt;br&gt;
#&lt;br&gt;
# URL: &amp;lt;http://nltk.org/&amp;gt;&lt;br&gt;
# For license information, see LICENSE.TXT&lt;br&gt;
&lt;br&gt;
from __future__ import unicode_literals, print_function&lt;br&gt;
&lt;br&gt;
import tempfile&lt;br&gt;
import os&lt;br&gt;
import json&lt;br&gt;
from subprocess import PIPE&lt;br&gt;
&lt;br&gt;
from nltk import compat&lt;br&gt;
from nltk.internals import find_jar, config_java, java, _java_options&lt;br&gt;
&lt;br&gt;
from nltk.tokenize.api import TokenizerI&lt;br&gt;
&lt;br&gt;
class StanfordSegmenter(TokenizerI):&lt;br&gt;
    r&quot;&quot;&quot;&lt;br&gt;
    Interface to the Stanford Segmenter&lt;br&gt;
&lt;br&gt;
    &amp;gt;&amp;gt;&amp;gt; from nltk.tokenize.stanford_segmenter import StanfordSegmenter&lt;br&gt;
    &amp;gt;&amp;gt;&amp;gt; segmenter = StanfordSegmenter(path_to_jar=&quot;stanford-segmenter-3.4.1.jar&quot;, path_to_sihan_corpora_dict=&quot;./data&quot;, path_to_model=&quot;./data/pku.gz&quot;, path_to_dict=&quot;./data/dict-chris6.ser.gz&quot;)&lt;br&gt;
    &amp;gt;&amp;gt;&amp;gt; sentence = u&quot;这是斯坦福中文分词器测试&quot;&lt;br&gt;
    &amp;gt;&amp;gt;&amp;gt; segmenter.segment(sentence)&lt;br&gt;
    &amp;gt;&amp;gt;&amp;gt; u&#39;\u8fd9 \u662f \u65af\u5766\u798f \u4e2d\u6587 \u5206\u8bcd\u5668 \u6d4b\u8bd5\n&#39;&lt;br&gt;
    &amp;gt;&amp;gt;&amp;gt; segmenter.segment_file(&quot;test.simp.utf8&quot;)&lt;br&gt;
    &amp;gt;&amp;gt;&amp;gt; u&#39;\u9762\u5bf9 \u65b0 \u4e16\u7eaa \uff0c \u4e16\u754c \u5404\u56fd ...&lt;br&gt;
    &quot;&quot;&quot;&lt;br&gt;
&lt;br&gt;
    _JAR = &#39;stanford-segmenter.jar&#39;&lt;br&gt;
&lt;br&gt;
    def __init__(self, path_to_jar=None,&lt;br&gt;
            path_to_sihan_corpora_dict=None,&lt;br&gt;
            path_to_model=None, path_to_dict=None,&lt;br&gt;
            encoding=&#39;UTF-8&#39;, options=None,&lt;br&gt;
            verbose=False, java_options=&#39;-mx2g&#39;):&lt;br&gt;
        self._stanford_jar = find_jar(&lt;br&gt;
            self._JAR, path_to_jar,&lt;br&gt;
            env_vars=(&#39;STANFORD_SEGMENTER&#39;,),&lt;br&gt;
            searchpath=(),&lt;br&gt;
            verbose=verbose&lt;br&gt;
        )&lt;br&gt;
        self._sihan_corpora_dict = path_to_sihan_corpora_dict&lt;br&gt;
        self._model = path_to_model&lt;br&gt;
        self._dict = path_to_dict&lt;br&gt;
&lt;br&gt;
        self._encoding = encoding&lt;br&gt;
        self.java_options = java_options&lt;br&gt;
        options = {} if options is None else options&lt;br&gt;
        self._options_cmd = &#39;,&#39;.join(&#39;{0}={1}&#39;.format(key, json.dumps(val)) for key, val in options.items())&lt;br&gt;
&lt;br&gt;
    def segment_file(self, input_file_path):&lt;br&gt;
        &quot;&quot;&quot;&lt;br&gt;
        &quot;&quot;&quot;&lt;br&gt;
        cmd = [&lt;br&gt;
            &#39;edu.stanford.nlp.ie.crf.CRFClassifier&#39;,&lt;br&gt;
            &#39;-sighanCorporaDict&#39;, self._sihan_corpora_dict,&lt;br&gt;
            &#39;-textFile&#39;, input_file_path,&lt;br&gt;
            &#39;-sighanPostProcessing&#39;, &#39;true&#39;,&lt;br&gt;
            &#39;-keepAllWhitespaces&#39;, &#39;false&#39;,&lt;br&gt;
            &#39;-loadClassifier&#39;, self._model,&lt;br&gt;
            &#39;-serDictionary&#39;, self._dict&lt;br&gt;
        ]&lt;br&gt;
&lt;br&gt;
        stdout = self._execute(cmd)&lt;br&gt;
&lt;br&gt;
        return stdout&lt;br&gt;
&lt;br&gt;
    def segment(self, tokens):&lt;br&gt;
        return self.segment_sents([tokens])&lt;br&gt;
&lt;br&gt;
    def segment_sents(self, sentences):&lt;br&gt;
        &quot;&quot;&quot;&lt;br&gt;
        &quot;&quot;&quot;&lt;br&gt;
        encoding = self._encoding&lt;br&gt;
        # Create a temporary input file&lt;br&gt;
        _input_fh, self._input_file_path = tempfile.mkstemp(text=True)&lt;br&gt;
&lt;br&gt;
        # Write the actural sentences to the temporary input file&lt;br&gt;
        _input_fh = os.fdopen(_input_fh, &#39;wb&#39;)&lt;br&gt;
        _input = &#39;\n&#39;.join((&#39; &#39;.join(x) for x in sentences))&lt;br&gt;
        if isinstance(_input, compat.text_type) and encoding:&lt;br&gt;
            _input = _input.encode(encoding)&lt;br&gt;
        _input_fh.write(_input)&lt;br&gt;
        _input_fh.close()&lt;br&gt;
&lt;br&gt;
        cmd = [&lt;br&gt;
            &#39;edu.stanford.nlp.ie.crf.CRFClassifier&#39;,&lt;br&gt;
            &#39;-sighanCorporaDict&#39;, self._sihan_corpora_dict,&lt;br&gt;
            &#39;-textFile&#39;, self._input_file_path,&lt;br&gt;
            &#39;-sighanPostProcessing&#39;, &#39;true&#39;,&lt;br&gt;
            &#39;-keepAllWhitespaces&#39;, &#39;false&#39;,&lt;br&gt;
            &#39;-loadClassifier&#39;, self._model,&lt;br&gt;
            &#39;-serDictionary&#39;, self._dict&lt;br&gt;
        ]&lt;br&gt;
&lt;br&gt;
        stdout = self._execute(cmd)&lt;br&gt;
&lt;br&gt;
        # Delete the temporary file&lt;br&gt;
        os.unlink(self._input_file_path)&lt;br&gt;
&lt;br&gt;
        return stdout&lt;br&gt;
&lt;br&gt;
    def _execute(self, cmd, verbose=False):&lt;br&gt;
        encoding = self._encoding&lt;br&gt;
        cmd.extend([&#39;-inputEncoding&#39;, encoding])&lt;br&gt;
        _options_cmd = self._options_cmd&lt;br&gt;
        if _options_cmd:&lt;br&gt;
            cmd.extend([&#39;-options&#39;, self._options_cmd])&lt;br&gt;
&lt;br&gt;
        default_options = &#39; &#39;.join(_java_options)&lt;br&gt;
&lt;br&gt;
        # Configure java.&lt;br&gt;
        config_java(options=self.java_options, verbose=verbose)&lt;br&gt;
&lt;br&gt;
        stdout, _stderr = java(cmd,classpath=self._stanford_jar, stdout=PIPE, stderr=PIPE)&lt;br&gt;
        stdout = stdout.decode(encoding)&lt;br&gt;
&lt;br&gt;
        # Return java configurations to their default values.&lt;br&gt;
        config_java(options=default_options, verbose=False)&lt;br&gt;
&lt;br&gt;
        return stdout&lt;br&gt;
&lt;br&gt;
def setup_module(module):&lt;br&gt;
    from nose import SkipTest&lt;br&gt;
&lt;br&gt;
    try:&lt;br&gt;
        StanfordSegmenter()&lt;br&gt;
    except LookupError:&lt;br&gt;
        raise SkipTest(&#39;doctests from nltk.tokenize.stanford_segmenter are skipped because the stanford segmenter jar doesn\&#39;t exist&#39;)&lt;/div&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/div&gt;
&lt;p&gt;我在github上fork了一个最新的NLTK版本，然后在这个版本中添加了&lt;a href=&quot;https://github.com/panyang/nltk/blob/develop/nltk/tokenize/stanford_segmenter.py&quot;&gt;stanford_segmenter.py&lt;/a&gt;，感兴趣的同学可以自行下载这个代码，放到nltk/tokenize/目录下，然后重新安装NLTK：sudo python setpy.py install. 或者直接clone我们的这个&lt;a href=&quot;https://github.com/panyang/nltk&quot;&gt;nltk&lt;/a&gt;版本，安装后就可以使用斯坦福中文分词器了。&lt;/p&gt;
&lt;p&gt;现在就可以在Python NLTK中调用这个斯坦福中文分词接口了。为了方便起见，建议首先进入到解压后的斯坦福分词工具目录下：cd stanford-segmenter-2014-08-27，然后在这个目录下启用ipython，当然默认python解释器也可：&lt;/p&gt;
&lt;p&gt;# 初始化斯坦福中文分词器&lt;br&gt;
In [1]: from nltk.tokenize.stanford_segmenter import StanfordSegmenter&lt;/p&gt;
&lt;p&gt;# 注意分词模型，词典等资源在data目录下，使用的是相对路径，&lt;br&gt;
# 在stanford-segmenter-2014-08-27的目录下执行有效&lt;br&gt;
# 注意，斯坦福中文分词器提供了两个中文分词模型：&lt;br&gt;
#     ctb.gz是基于宾州中文树库训练的模型&lt;br&gt;
#     pku.gz是基于北大在2005backoof上提供的人名日报语料库&lt;br&gt;
# 这里选用了pku.gz，方便最后的测试&lt;br&gt;
In [2]: segmenter = StanfordSegmenter(path_to_jar=”stanford-segmenter-3.4.1.jar”, path_to_sihan_corpora_dict=”./data”, path_to_model=”./data/pku.gz”, path_to_dict=”./data/dict-chris6.ser.gz”)&lt;/p&gt;
&lt;p&gt;# 测试一个中文句子，注意u&lt;br&gt;
In [3]: sentence = u”这是斯坦福中文分词器测试”&lt;/p&gt;
&lt;p&gt;# 调用segment方法来切分中文句子，这里隐藏了一个问题，我们最后来说明&lt;br&gt;
In [4]: segmenter.segment(sentence)&lt;br&gt;
Out[4]: u’\u8fd9 \u662f \u65af\u5766\u798f \u4e2d\u6587 \u5206\u8bcd\u5668 \u6d4b\u8bd5\n’&lt;/p&gt;
&lt;p&gt;# 由于分词后显示的是中文编码，我们把这个结果输出到文件中&lt;br&gt;
# 不知道有没有同学有在python解释器总显示中文的方法&lt;br&gt;
In [5]: outfile = open(‘outfile’, ‘w’)&lt;/p&gt;
&lt;p&gt;In [6]: result = segmenter.segment(sentence)&lt;/p&gt;
&lt;p&gt;# 注意写入到文件的时候要encode 为 UTF-8编码&lt;br&gt;
In [7]: outfile.write(result.encode(‘UTF-8′))&lt;/p&gt;
&lt;p&gt;In [8]: outfile.close()&lt;/p&gt;
&lt;p&gt;打开这个outfile文件：&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;这 是 斯坦福 中文 分词器 测试&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;这里同时提供了一个segment_file的调用方法，方便直接对文件进行切分，让我们来测试《&lt;a href=&quot;http://www.52nlp.cn/%E4%B8%AD%E6%96%87%E5%88%86%E8%AF%8D%E5%85%A5%E9%97%A8%E4%B9%8B%E8%B5%84%E6%BA%90&quot;&gt;中文分词入门之资源&lt;/a&gt;》中介绍的backoff2005的测试集pku_test.utf8，来看看斯坦福分词器的效果：&lt;/p&gt;
&lt;p&gt;In [9]: result = segmenter.segment_file(‘pku_test.utf8′)&lt;/p&gt;
&lt;p&gt;In [10]: outfile = open(‘pku_outfile’, ‘w’)&lt;/p&gt;
&lt;p&gt;In [11]: outfile.write(result.encode(‘UTF-8′))&lt;/p&gt;
&lt;p&gt;In [12]: outfile.close()&lt;/p&gt;
&lt;p&gt;打开结果文件pku_outfile：&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;共同 创造 美好 的 新 世纪 ——二○○一年 新年 贺词&lt;br&gt;
（ 二○○○年 十二月 三十一日 ） （ 附 图片 1 张 ）&lt;br&gt;
女士 们 ， 先生 们 ， 同志 们 ， 朋友 们 ：&lt;br&gt;
2001年 新年 钟声 即将 敲响 。 人类 社会 前进 的 航船 就要 驶入 21 世纪 的 新 航程 。 中国 人民 进入 了 向 现代化 建设 第三 步 战略 目标 迈进 的 新 征程 。&lt;br&gt;
在 这个 激动人心 的 时刻 ， 我 很 高兴 通过 中国 国际 广播 电台 、 中央 人民 广播 电台 和 中央 电视台 ， 向 全国 各族 人民 ， 向 香港 特别 行政区 同胞 、 澳门 特别 行政区 同胞 和 台湾 同胞 、 海外 侨胞 ， 向 世界 各国 的 朋友 们 ， 致以 新 世纪 第一 个 新年 的 祝贺 ！&lt;br&gt;
….
&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;我们用backoff2005的测试脚本来测试一下斯坦福中文分词器在这份测试语料上的效果：&lt;/p&gt;
&lt;p&gt;./icwb2-data/scripts/score ./icwb2-data/gold/pku_training_words.utf8 ./icwb2-data/gold/pku_test_gold.utf8 pku_outfile &amp;gt; stanford_pku_test.score&lt;/p&gt;
&lt;p&gt;结果如下：&lt;br&gt;
=== SUMMARY:&lt;br&gt;
=== TOTAL INSERTIONS:	1479&lt;br&gt;
=== TOTAL DELETIONS:	1974&lt;br&gt;
=== TOTAL SUBSTITUTIONS:	3638&lt;br&gt;
=== TOTAL NCHANGE:	7091&lt;br&gt;
=== TOTAL TRUE WORD COUNT:	104372&lt;br&gt;
=== TOTAL TEST WORD COUNT:	103877&lt;br&gt;
=== TOTAL TRUE WORDS RECALL:	0.946&lt;br&gt;
=== TOTAL TEST WORDS PRECISION:	0.951&lt;br&gt;
=== F MEASURE:	0.948&lt;br&gt;
=== OOV Rate:	0.058&lt;br&gt;
=== OOV Recall Rate:	0.769&lt;br&gt;
=== IV Recall Rate:	0.957&lt;br&gt;
###	pku_outfile	1479	1974	3638	7091	104372	103877	0.946	0.951	0.948	0.058	0.769	0.957&lt;/p&gt;
&lt;p&gt;准确率是95.1%， 召回率是94.6%, F值是94.8%, 相当不错。感兴趣的同学可以测试一下其他测试集，或者用宾州中文树库的模型来测试一下结果。&lt;/p&gt;
&lt;p&gt;最后我们再说明一下这个接口存在的问题，因为使用了Linux PIPE模式来调用斯坦福中文分词器，相当于在Python中执行相应的Java命令，导致每次在执行分词时会加载一遍分词所需的模型和词典，这个对文件操作时（segment_file)没有多大的问题，但是在对句子执行分词（segment)的时候会存在很大的问题，每次都加载数据，在实际产品中基本是不可用的。虽然发现斯坦福分词器提供了一个 –readStdin 的读入标准输入的参数，也尝试通过python subprocess中先load 文件，再用的communicate方法来读入标准输入，但是仍然没有解决问题，发现还是一次执行，一次结束。这个问题困扰了我很久，google了很多资料也没有解决问题，欢迎懂行的同学告知或者来解决这个问题，再此先谢过了。That’s all!&lt;/p&gt;
&lt;p&gt;注：原创文章，转载请注明出处“&lt;a href=&quot;http://www.52nlp.cn&quot;&gt;我爱自然语言处理&lt;/a&gt;”：&lt;a href=&quot;http://www.52nlp.cn&quot;&gt;www.52nlp.cn&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;本文链接地址：&lt;a href=&quot;http://www.52nlp.cn/?p=6763&quot;&gt;http://www.52nlp.cn/python自然语言处理实践-在nltk中使用斯坦福中文分词器&lt;/a&gt;&lt;/p&gt;

											

</description>
        <pubDate>Wed, 24 Sep 2014 00:00:00 +0800</pubDate>
        <link>http://iftti.com/posts/2014/2014-09-24-python%25e8%2587%25aa%25e7%2584%25b6%25e8%25af%25ad%25e8%25a8%2580%25e5%25a4%2584%25e7%2590%2586%25e5%25ae%259e%25e8%25b7%25b5-%25e5%259c%25a8nltk%25e4%25b8%25ad%25e4%25bd%25bf%25e7%2594%25a8%25e6%2596%25af%25e5%259d%25a6%25e7%25a6%258f%25e4%25b8%25ad%25e6%2596%2587%25e5%2588%2586%25e8%25af%258d%25e5%2599%25a8-5dd26d0a5.html</link>
        <guid isPermaLink="true">http://iftti.com/posts/2014/2014-09-24-python%25e8%2587%25aa%25e7%2584%25b6%25e8%25af%25ad%25e8%25a8%2580%25e5%25a4%2584%25e7%2590%2586%25e5%25ae%259e%25e8%25b7%25b5-%25e5%259c%25a8nltk%25e4%25b8%25ad%25e4%25bd%25bf%25e7%2594%25a8%25e6%2596%25af%25e5%259d%25a6%25e7%25a6%258f%25e4%25b8%25ad%25e6%2596%2587%25e5%2588%2586%25e8%25af%258d%25e5%2599%25a8-5dd26d0a5.html</guid>
        
        
        <category>52nlp</category>
        
      </item>
    
      <item>
        <title>在 logstash 里使用其他 RubyGems 模块</title>
        <description>

  
  &lt;div style=&quot;background-color: #FFF;&quot;&gt;
    &lt;p&gt;在开发和使用一些 logstash 自定义插件的时候，几乎不可避免会导入其他 RubyGems 模块 —— 因为都用不上模块的小型处理，直接写在 &lt;em&gt;filters/ruby&lt;/em&gt; 插件配置里就够了 —— 这时候，运行 logstash 命令可能会发现一个问题：这个 gem 模块一直是 “no found” 状态。&lt;/p&gt;
&lt;p&gt;这其实是因为我们一般是通过 java 命令来运行的 logstash，这时候它回去寻找的 Gem 路径跟我们预计中的是不一致的。&lt;/p&gt;
&lt;p&gt;要查看 logstash 运行时实际的 Gem 查找路径，首先要通过 &lt;code&gt;ps aux&lt;/code&gt; 命令确定 ruby 的实际运行方式：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ ps uax|grep logstash
raochenlin      27268  38.0  4.3  3268156 181344 s003  S+    7:10PM   0:22.36 /Library/Internet Plug-Ins/JavaAppletPlugin.plugin/Contents/Home/bin/java -Xmx500m -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -Djava.awt.headless=true -XX:CMSInitiatingOccupancyFraction=75 -XX:+UseCMSInitiatingOccupancyOnly -jar /Downloads/logstash-1.4.2/vendor/jar/jruby-complete-1.7.11.jar -I/Users/raochenlin/Downloads/logstash-1.4.2/lib /Users/raochenlin/Downloads/logstash-1.4.2/lib/logstash/runner.rb agent -f test.conf
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;看，实际的运行方式应该是：&lt;code&gt;java -jar logstash-1.4.2/vendor/jar/jruby-complete-1.7.11.jar -Ilogstash-1.4.2/lib logstash-1.4.2/lib/logstash/runner.rb&lt;/code&gt; 这样。&lt;/p&gt;
&lt;p&gt;那么我们查看 gem 路径的命令也就知道怎么写了：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;java -jar logstash-1.4.2/vendor/jar/jruby-complete-1.7.11.jar `which gem` env
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;你会看到这样的输出：&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;RubyGems Environment:
     - RUBYGEMS VERSION: 2.1.9
     - RUBY VERSION: 1.9.3 (2014-02-24 patchlevel 392) [java]
     - INSTALLATION DIRECTORY: file:/Downloads/logstash-1.4.2/vendor/jar/jruby-complete-1.7.11.jar!/META-INF/jruby.home/lib/ruby/gems/shared
     - RUBY EXECUTABLE: java -jar /Downloads/logstash-1.4.2/vendor/jar/jruby-complete-1.7.11.jar
     - EXECUTABLE DIRECTORY: file:/Downloads/logstash-1.4.2/vendor/jar/jruby-complete-1.7.11.jar!/META-INF/jruby.home/bin
     - SPEC CACHE DIRECTORY: /.gem/specs
     - RUBYGEMS PLATFORMS:
       - ruby
       - universal-java-1.7
     - GEM PATHS:
        - file:/Downloads/logstash-1.4.2/vendor/jar/jruby-complete-1.7.11.jar!/META-INF/jruby.home/lib/ruby/gems/shared
        - /.gem/jruby/1.9
     - GEM CONFIGURATION:
        - :update_sources =&amp;gt; true
        - :verbose =&amp;gt; true
        - :backtrace =&amp;gt; false
        - :bulk_threshold =&amp;gt; 1000
        - “install” =&amp;gt; “–no-rdoc –no-ri –env-shebang”
        - “update” =&amp;gt; “–no-rdoc –no-ri –env-shebang”
        - :sources =&amp;gt; [“http://ruby.taobao.org/”]
     - REMOTE SOURCES:
        - http://ruby.taobao.org/
     - SHELL PATH:
        - /usr/bin
        - /bin
        - /usr/sbin
        - /sbin
        - /usr/local/bin&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;看到其中的 GEM PATHS 部分，是一个以 &lt;strong&gt;file:&lt;/strong&gt; 开头的路径！也就是说，要求所有的 gem 包都打包在这个 jruby-complete-1.7.11.jar 里面才认。&lt;/p&gt;
&lt;p&gt;所以我们需要把额外的 gem 包，也加入这个 jar 里：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;jar uf jruby-completa-1.7.11.jar META-INF/jruby.home/lib/ruby/1.9/CUSTOM_RUBY_GEM_LIB
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;注：加入 jar 是用的相对路径，所以前面这串目录要提前创建然后复制文件进去。&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;当然，其实还有另一个办法。&lt;/p&gt;
&lt;p&gt;让我们返回去再看一次 logstash 的进程，在 jar 后面，还有一个 &lt;code&gt;-I&lt;/code&gt; 参数！所以，其实我们还可以把文件安装在 &lt;code&gt;logstash-1.4.2/lib&lt;/code&gt; 目录下去。&lt;/p&gt;
&lt;p&gt;最后，你可能会问：那 &lt;code&gt;--pluginpath&lt;/code&gt; 参数指定的位置可不可以呢？&lt;/p&gt;
&lt;p&gt;答案是：也可以。&lt;/p&gt;
&lt;p&gt;这个参数指定的位置在 &lt;em&gt;logstash-1.4.2/lib/logstash/agent.rb&lt;/em&gt; 中，被加入了 &lt;code&gt;$LOAD_PATH&lt;/code&gt; 中：&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-ruby&quot; data-lang=&quot;ruby&quot;&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;configure_plugin_path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;paths&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;paths&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;each&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;do&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;
      &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;!&lt;/span&gt;&lt;span class=&quot;no&quot;&gt;Dir&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;exists?&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;nb&quot;&gt;warn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;no&quot;&gt;I18n&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;logstash.agent.configuration.plugin_path_missing&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                    &lt;span class=&quot;ss&quot;&gt;:path&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
      &lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;plugin_glob&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;File&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;join&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;logstash&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;{inputs,codecs,filters,outputs}&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;*.rb&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;Dir&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;glob&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plugin_glob&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;empty?&lt;/span&gt;
        &lt;span class=&quot;vi&quot;&gt;@logger&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;warn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;no&quot;&gt;I18n&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;logstash.agent.configuration.no_plugins_found&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                    &lt;span class=&quot;ss&quot;&gt;:path&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;ss&quot;&gt;:plugin_glob&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plugin_glob&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
      &lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;
      &lt;span class=&quot;vi&quot;&gt;@logger&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;debug&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Adding plugin path&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;ss&quot;&gt;:path&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;vg&quot;&gt;$LOAD_PATH&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;unshift&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;code&gt;$LOAD_PATH&lt;/code&gt; 是 Ruby 的一个特殊变量，类似于 Perl 的 &lt;code&gt;@INC&lt;/code&gt; 或者 Java 的 &lt;code&gt;class_path&lt;/code&gt; 。在这个数组里的路径下的文件，都可以被 require 导入。&lt;/p&gt;
&lt;p&gt;可以运行如下命令查看：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ java -jar logstash-1.4.2/vendor/jar/jruby-complete-1.7.11.jar -e &#39;p $LOAD_PATH&#39;
[&quot;file:/Users/raochenlin/Downloads/logstash-1.4.2/vendor/jar/rar/jruby-complete-1.7.11.jar!/META-INF/jruby.home/lib/ruby/1.9/site_ruby&quot;, &quot;file:/Users/raochenlin/Downloads/logstash-1.4.2/vendor/jar/rar/jruby-complete-1.7.11.jar!/META-INF/jruby.home/lib/ruby/shared&quot;, &quot;file:/Users/raochenlin/Downloads/logstash-1.4.2/vendor/jar/rar/jruby-complete-1.7.11.jar!/META-INF/jruby.home/lib/ruby/1.9&quot;]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;这三种方式，你喜欢哪种呢？&lt;/p&gt;
    &lt;hr&gt;
    
    &lt;hr&gt;
  &lt;!-- UY BEGIN --&gt;


&lt;!-- UY END --&gt;
  &lt;/div&gt;

</description>
        <pubDate>Wed, 24 Sep 2014 00:00:00 +0800</pubDate>
        <link>http://iftti.com/posts/2014/2014-09-24-howto-use-custom-rubygem-in-logstash-205f7f438.html</link>
        <guid isPermaLink="true">http://iftti.com/posts/2014/2014-09-24-howto-use-custom-rubygem-in-logstash-205f7f438.html</guid>
        
        
        <category>chenlinux</category>
        
      </item>
    
      <item>
        <title>游戏引擎学习笔记：介绍、架构、设计及实现</title>
        <description>

					
		
&lt;!-- div id=&quot;ad1&quot;&gt;
&lt;script type=&quot;text/javascript&quot;&gt;
google_ad_client = &quot;ca-pub-7056282119617872&quot;;
google_ad_slot = &quot;6645040531&quot;;
google_ad_width = 300;
google_ad_height = 250;
&lt;/script&gt;
&lt;script type=&quot;text/javascript&quot;
src=&quot;http://pagead2.googlesyndication.com/pagead/show_ads.js&quot;&gt;
&lt;/script&gt;
&lt;/div&gt;
&lt;br/ --&gt;

&lt;p&gt;从小到大，虽然玩过的游戏不少，但是从写程序开始，目前为此仅仅写过2个游戏。其一是2011年在MTK平台下写的贪食蛇，其二是2010年在嵌入式开发板上写过一个迷宫的游戏。第一个代码量大概有3000行左右，第二个有2000行左右。&lt;/p&gt;
&lt;p&gt;这2个游戏都很简单，而且网上有很多现成的例子可供参考，因此难度也比较低。&lt;/p&gt;
&lt;p&gt;这2天把拖延了好久的&lt;strong&gt;《&lt;a href=&quot;http://www.amazon.cn/gp/product/B0033UX10A/ref=as_li_qf_sp_asin_il_tl?ie=UTF8&amp;amp;camp=536&amp;amp;creative=3200&amp;amp;creativeASIN=B0033UX10A&amp;amp;linkCode=as2&amp;amp;tag=vastwork-23&quot; target=&quot;_blank&quot;&gt;Android应用开发揭秘&lt;/a&gt;》&lt;/strong&gt;的游戏引擎的那一章看完了，收获还是很大，在此写一篇读书笔记。&lt;/p&gt;
&lt;p&gt;关于Game Engine，我能想到的几个问题：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;游戏引擎是什么？&lt;/li&gt;
&lt;li&gt;Game Engine是为了解决什么问题？&lt;/li&gt;
&lt;li&gt;Game Engine的架构是什么？&lt;/li&gt;
&lt;li&gt;如何设计一款游戏引擎？&lt;/li&gt;
&lt;li&gt;游戏引擎包含哪些模块？&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;下面就来探讨几个问题：&lt;/p&gt;
&lt;h3&gt;1. Game Engine是什么？&lt;/h3&gt;
&lt;p&gt;游戏产业在全球来看是一个很大的产业，一款游戏大作包含了非常多的元素。游戏涉及到剧情、人物、任务、关卡、地图、画质、美术、音乐、网络等多种元素。开发一款游戏实际上需要耗费非常多的资源，据说North Star的《GTA V》耗资几亿美元。正因为如此，在开发项目过程中，尽可能复用之前项目成功的东西就非常重要。&lt;/p&gt;
&lt;p&gt;一款游戏中，&lt;em&gt;Game Engine直接控制着剧情、关卡、美工、音乐、操作&lt;/em&gt;等内容，将游戏的所有元素捆绑在一起。&lt;/p&gt;
&lt;p&gt;一般来说，一款Game Engine需要包含以下模块：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;基本框架（渲染、逻辑、物理 等等各部分如何组装）&lt;/li&gt;
&lt;li&gt;资源管理&lt;/li&gt;
&lt;li&gt;渲染&lt;/li&gt;
&lt;li&gt;基本逻辑（网游还要解决逻辑的同步问题）&lt;/li&gt;
&lt;li&gt;物理（有时候和逻辑合并）&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;—————-分割线，以下是重要但较为独立的部分——————–&lt;br&gt;
6. UI&lt;br&gt;
7. 音乐音效&lt;br&gt;
8. 网络&lt;br&gt;
9. 脚本（有些类型的游戏引擎需要脚本和逻辑的关联性非常强，有些脚本则比较独立）&lt;/p&gt;
&lt;h3&gt;2. Game Engine为了解决什么问题？&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Game Engine实际上有效的减少开发者编写程序时的冗余劳动，同时增强游戏的可移植性&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;Engine就是游戏的框架，我们需要往框架中填充内容就可以形成一个游戏。&lt;/p&gt;
&lt;p&gt;引擎，就是一系列的工具和生产链，像Unreal 3，Unity这样的成熟引擎，用起来非常方便，就是因为它的关卡/场景编辑器十分宜用，支持多种脚本语言。这类引擎运用恰当的话，理论上能将关卡调试和物件流水线的大部分工作从程序员那里完全移出。&lt;/p&gt;
&lt;h3&gt;3. Game Engine的架构&lt;/h3&gt;
&lt;p&gt;游戏 = 引擎(程序) + 资源(图像、声音、动画等)&lt;br&gt;
目前的Game Engine的架构都是Model-View-Controller架构，逻辑和显示分开，由一个逻辑控制流来协调Client的请求和Server的行动。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;View: 负责界面回执&lt;/li&gt;
&lt;li&gt;Controller：处理工作流程的创建和种植，用户输入，各种事件的处理&lt;/li&gt;
&lt;li&gt;Model: 模型、逻辑，程序的功能实现&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;消息循环-&amp;gt;更新数据-&amp;gt;绘制各节点&lt;/strong&gt; 这是绘制的基本结构基本不会有大的改变。&lt;/p&gt;
&lt;p&gt;各种引擎的变种很大部分是在游戏逻辑上的封装。脚本也好，直接写代码也好。比如较为古老的数据与函数分离，以C语言为代表。大行其道的类结构。以c++为代表。以及现在光环日耀的CBSE，基于组件的架构&lt;/p&gt;
&lt;h3&gt;4. 如何设计一款游戏引擎？&lt;/h3&gt;
&lt;h4&gt;&amp;lt;1&amp;gt;. 结构设计及功能设计&lt;/h4&gt;
&lt;p&gt;Game Engine的设计包括结构设计、功能设计及注意事项。&lt;/p&gt;
&lt;p&gt;Game Engine包括&lt;strong&gt;图形引擎&lt;/strong&gt;、脚本引擎、物理引擎、工具模块、音效引擎、网络组件、事件组件等。&lt;/p&gt;
&lt;p&gt;Android游戏主要包括一个Activity类、流程控制类、游戏线程类和游戏对象类。Activity类是游戏的执行单元，负责游戏生命周期的控制。&lt;/p&gt;
&lt;p&gt;流程控制：提供在游戏中多个界面之间切换方法；&lt;/p&gt;
&lt;p&gt;游戏线程：不断监测可能发生的各种事件，计算游戏状态，刷新屏幕。&lt;/p&gt;
&lt;h4&gt;&amp;lt;2&amp;gt;. 注意事项：&lt;/h4&gt;
&lt;p&gt;手机游戏的主要问题是 硬件限制 及 电池瓶颈。CPU及内存不足，屏幕大小，音效等多方面限制，在设计时需要注意这些方面。&lt;/p&gt;
&lt;h3&gt;5. 实现一款游戏引擎&lt;/h3&gt;
&lt;p&gt;游戏引擎&lt;strong&gt;只是一款炒菜的炒菜锅，但有了好的炒菜锅不一定能保证炒出好的菜&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;游戏引擎的实现就很复杂了，需要按照上一节的架构及功能设计去编码实现，目前绝大部分都是面向对象编程，设计好各种类。比如人物、NPC、道具、动画、动植物等等。有余力的同学可以去研究研究。&lt;/p&gt;
&lt;p&gt;最近流行的一些游戏，其实也并不需要多么NB的游戏引擎，充分发掘用户的痛点才能设计出一款好的游戏。&lt;/p&gt;
&lt;p&gt;目前有很多开源的Game Engine可供大家研究，比如Unity3D, Box2D等，大家可以去网上搜索并研究。&lt;/p&gt;
&lt;h3&gt;6. 对手机游戏的展望&lt;/h3&gt;
&lt;p&gt;这2年玩过的手机游戏也不少，我也来谈谈一款好的手机游戏应该具备哪些特征：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;strong&gt;上手容易，精通不易&lt;/strong&gt;，且玩且珍惜。手游面向的是大众，所以上手难的游戏就一律pass，必须保证游戏具有简单性，让玩家一安装就可以玩的；&lt;/li&gt;
&lt;li&gt;
&lt;strong&gt;&lt;em&gt;可中断，时间短&lt;/em&gt;&lt;/strong&gt;。一般玩游戏，都是在公交地铁上等碎片时间里，所以提供的是短时间的娱乐效果，允许在游戏和工作模式之间顺利切换；&lt;/li&gt;
&lt;li&gt;
&lt;strong&gt;必须加入SNS元素&lt;/strong&gt;：一款好的手游应该具有社交元素，可以加入LBS寻找周围的玩家，或和好友一起玩游戏及互动，抑或者认识新的好友。因为手游都很简单，所以要留住玩家，加入SNS可以留住玩家；&lt;/li&gt;
&lt;li&gt;充分利用手机的各项优点：手机的优点比如便携性，私密性，即使抵达。手机是我们身体的延伸，所以一款好的游戏应该充分利用手机的一些传感器、摄像头、网络、蓝牙，找出特点，以便设计出一款优秀的游戏。&lt;/li&gt;
&lt;li&gt;…&lt;/li&gt;
&lt;/ol&gt;


&lt;!-- div id=&quot;ad1&quot;&gt;
&lt;/div --&gt;


	


	

</description>
        <pubDate>Wed, 24 Sep 2014 00:00:00 +0800</pubDate>
        <link>http://iftti.com/posts/2014/2014-09-24-77581-bc434c2db.html</link>
        <guid isPermaLink="true">http://iftti.com/posts/2014/2014-09-24-77581-bc434c2db.html</guid>
        
        
        <category>jobbole</category>
        
      </item>
    
      <item>
        <title>无损数据压缩算法的历史</title>
        <description>

					
		
&lt;!-- div id=&quot;ad1&quot;&gt;
&lt;script type=&quot;text/javascript&quot;&gt;
google_ad_client = &quot;ca-pub-7056282119617872&quot;;
google_ad_slot = &quot;6645040531&quot;;
google_ad_width = 300;
google_ad_height = 250;
&lt;/script&gt;
&lt;script type=&quot;text/javascript&quot;
src=&quot;http://pagead2.googlesyndication.com/pagead/show_ads.js&quot;&gt;
&lt;/script&gt;
&lt;/div&gt;
&lt;br/ --&gt;

&lt;h2&gt;
&lt;a class=&quot;anchor&quot; href=&quot;#%E5%BC%95%E8%A8%80&quot; name=&quot;user-content-%E5%BC%95%E8%A8%80&quot;&gt;&lt;/a&gt;引言&lt;/h2&gt;
&lt;p&gt;有两种主要的压缩算法：有损和无损。有损压缩算法通过移除在保真情形下需要大量的数据去存储的小细节，从而使文件变小。在有损压缩里，因某些必要数据的移除，恢复原文件是不可能的。有损压缩主要用来存储图像和音频文件，同时通过移除数据可以达到一个比较高的压缩率，不过本文不讨论有损压缩。无损压缩，也使文件变小，但对应的解压缩功能可以精确的恢复原文件，不丢失任何数据。无损数据压缩被广泛的应用在计算机领域，从节省你个人电脑的空间，到通过web发送数据。使用Secure Shell交流，查看PNG或GIF图片。&lt;/p&gt;
&lt;p&gt;无损压缩算法可行的基本原理是，任意一个非随机文件都含有重复数据，这些重复数据可以通过用来确定字符或短语出现概率的统计建模技术来压缩。统计模型可以用来为特定的字符或者短语生成代码，基于它们出现的频率，配置最短的代码给最常用的数据。这些技术包括熵编码(entropy encoding)、游程编码(run-length encoding)以及字典压缩。运用这些技术以及其它技术，一个8-bit长度的字符或者字符串可以用很少的bit来表示，从而大量的重复数据被移除。&lt;/p&gt;
&lt;h2&gt;
&lt;a class=&quot;anchor&quot; href=&quot;#%E5%8E%86%E5%8F%B2&quot; name=&quot;user-content-%E5%8E%86%E5%8F%B2&quot;&gt;&lt;/a&gt;历史&lt;/h2&gt;
&lt;p&gt;&lt;img id=&quot;pic&quot; alt=&quot;&quot; src=&quot;/images/jobbole.com/e657822be76c892f690831b084d88fbc.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;直到20世纪70年代，数据压缩才在计算机领域开始扮演重要角色，那时互联网变得更加流行，Lempel-Ziv算法被发明出来，但压缩算法在计算机领域之外有着更悠久的历史。发明于1838年的Morse code，是最早的数据压缩实例，为英语中最常用的字母比如”e”和”t”分配更短的Morse code。之后，随着大型机的兴起，Claude Shannon和Robert Fano发明了Shannon-Fano编码算法。他们的算法基于符号(symbol)出现的概率来给符号分配编码(code)。一个符号出现的概率大小与对应的编码成反比，从而用更短的方式来表示符号。&lt;/p&gt;
&lt;p&gt;两年后，David Huffman在MIT学习信息理论并上了一门Robert Fano老师的课，Fano给班级的同学两个选项，写一篇学期论文或者参加期末考试。Huffman选择的是写学期论文，题目是寻找二叉编码的最优算法。经过几个月的努力后依然没有任何成果，Huffman决定放弃所有论文相关的工作，开始学习为参加期末考试做准备。正在那时，灵感爆发，Huffman找到一个与Shannon-Fano编码相类似但是更有效的编码算法。Shannon-Fano编码和Huffman编码的主要区别是构建概率树的过程不同，前者是自下而上，得到一个次优结果，而后者是自上而下。&lt;/p&gt;
&lt;p&gt;早期的Shannon-Fano编码和Huffman编码算法实现是使用硬件和硬编码完成的。直到20世纪70年代互联网以及在线存储的出现，软件压缩才被实现为Huffman编码依据输入数据动态产生。随后，1977年Abraham Lempel 和 Jacob Ziv发表了他们独创性的LZ77算法，第一个使用字典来压缩数据的算法。特别的，LZ77使用了一个叫做slidingwindow的动态字典。1978年，这对搭档发表了同样使用字典的LZ78算法。与LZ77不同，LZ78解析输入数据，生成一个静态字典，不像LZ77动态产生。&lt;/p&gt;
&lt;h3&gt;
&lt;a class=&quot;anchor&quot; href=&quot;#%E6%B3%95%E5%BE%8B%E9%97%AE%E9%A2%98&quot; name=&quot;user-content-%E6%B3%95%E5%BE%8B%E9%97%AE%E9%A2%98&quot;&gt;&lt;/a&gt;法律问题&lt;/h3&gt;
&lt;p&gt;LZ77和LZ78都快速的流行开来，衍生出多个下图中所示的压缩算法。其中的大多数已经沉寂了，只有那么几个现在被大范围的使用，包括DEFLATE，LZMA以及LZX。绝大多数常用的压缩算法都衍生于LZ77，这不是因为LZ77技术更好，只是由于Sperry在1984年申请了LZ78衍生算法LZW的专利，从而发展受到了专利的阻碍，Sperry开始因专利侵权而起诉软件提供商，服务器管理员，甚至是使用GIF格式但没有License的终端用户。&lt;/p&gt;
&lt;p&gt;同时，UNIX压缩工具使用了一个叫LZC的LZW算法微调整版本，之后由于专利问题而被弃用。其他的UNIX开发者也开始放弃使用LZW。这导致UNIX社区采用基于DEFLATE的gzip和基于Burrows-Wheeler Transform的bzip2算法。长远来说，对于UNIX社区这是有好处的，因为gzip和bzip2格式几乎总是比LZW有更好的压缩比。围绕LZW的专利问题已经结束，因为LZW的专利2003年就到期了。尽管这样，LZW算法已经很大程度上被替代掉了，仅仅被使用于GIF压缩中。自那以后，也有一些LZW的衍生算法，不过都没有流行开来，LZ77算法仍然是主流。&lt;/p&gt;
&lt;p&gt;另外一场法律官司发生于1993，关于LZS算法。LZS是由Stac Electronics开发的，用于硬盘压缩软件，如Stacker。微软在开发影片压缩软件时使用了LZS算法，开发的软件随着MS-DOS 6.0一起发布，声称能够使硬盘容量翻倍。当Stac Electronics发现自己的知识财产被使用后，起诉了微软。微软随后被判专利侵权并赔偿Stac Electronics1亿2000万美元，后因微软上诉因非故意侵权而减少了1360万美元。尽管Stac Electronics和微软发生了一个那么大的官司，但它没有阻碍Lempel-Ziv算法的开发，不像LZW专利纠纷那样。唯一的结果就是LZS没有衍生出任何算法。&lt;/p&gt;
&lt;h3&gt;
&lt;a class=&quot;anchor&quot; href=&quot;#deflate%E7%9A%84%E5%B4%9B%E8%B5%B7&quot; name=&quot;user-content-deflate%E7%9A%84%E5%B4%9B%E8%B5%B7&quot;&gt;&lt;/a&gt;Deflate的崛起&lt;/h3&gt;
&lt;p&gt;自从Lempel-Ziv算法被发表以来，随着对存储需求的不断增长，一些公司及其他团体开始使用数据压缩技术，这能让他们满足这些需求。然而，数据压缩并没有被大范围的使用，这一局面直到20世纪80年代末期随着互联网的腾飞才开始改变，那时数据压缩的需求出现了。带宽限额，昂贵，数据压缩能够帮助缓解这些瓶颈。当万维网发展起来之后人们开始分享更多的图片以及其它格式的数据，这些数据远比文本大得多，压缩开始变得极其重要。为了满足这些需求，几个新的文件格式被开发出来，包括ZIP，GIF，和PNG。&lt;/p&gt;
&lt;p&gt;Thom Henderson通过他的公司发布了第一个成功的商业存档格式，叫做ARC，公司名为为System Enhancement Associates。ARC在BBS社区尤为流行，这是因为它是第一个既可以打包又可以压缩的程序，此外还开放了源代码。ARC格式使用一个LZW的衍生算法来压缩数据。一个叫做Phil Katz的家注意到了ARC的流行并决定用汇编语言来重写压缩和解压缩程序，希望改进ARC。他于1987发布了他的共享软件PKARC程序，不久被Henderson以侵犯版权为由起诉。Katz被认定为有罪，并被迫支付版权费用以及其它许可协议费用。他之所以被认定侵权，是由于PKARC是明显抄袭ARC，甚至于一些注释里面的错别字都相同。&lt;/p&gt;
&lt;p&gt;Phil Katz自1988年之后就因许可证问题不能继续出售PKARC，所以1989年他创建了一个PKARC的修改版，就是现在大家熟知的ZIP格式。由于使用了LZW，它被认为专利侵权的，之后Katz选择转而使用新的IMPLODE算法，这种格式于1993年再次被修改，那时Kata发布了PKZIP的2.0版本，那个版本实现了DEFLATE算法以及一些其它特性，如分割容量等。这个ZIP版本也是我们现在随处可见的格式，所有的ZIP文件都遵循PKZIP 2.0格式，尽管它年代久远。&lt;/p&gt;
&lt;p&gt;GIF格式，全称Graphics Interchange Format，于1987年由CompuServe创建，允许图像无失真地被共享(尽管这种格式被限定每一帧最多256种颜色)，同时减小文件的大小以允许通过数据机传输。然而，像ZIP格式一样，GIF也是基于LZW算法。尽管专利侵权，Unisys没有办法去阻止GIF的传播。即使是现在，20年后的今天，GIF仍然被使用着，特别是它的动画能力。&lt;/p&gt;
&lt;p&gt;尽管GIF没有办法被叫停，CompuServe需找一种不受专利束缚的格式，并于1994年引入了Portable Network Graphics (PNG) 格式。像ZIP一样，PNG使用DEFLATE算法来处理压缩。尽管DELLATE的专利属于Katz，这个专利并不是强性制的，正是这样，PNG以及其它基于DEFLATE的格式避免了专利侵权。尽管LZW在压缩历史的初期占据霸主位置，由于Unisys公司的好诉讼作为，LZW慢慢的淡出主流，大家转而使用更快更高效的DEFLATE算法。现在DEFLATE是使用得最多的算法，有些压缩世界里瑞士军刀的味道。&lt;/p&gt;
&lt;p&gt;除了用于PNG和ZIP格式之外，计算机世界里DEFLATE也被频繁的用在其它地方。例如gzip(.gz)文件格式也使用DEFLATE，gzip是ZIP的一个开源版本。其它还包括HTTP, SSL, 以及其它的高效压缩网络传输数据的技术。&lt;/p&gt;
&lt;p&gt;遗憾的是，Phil Katz英年早逝，没能看到他的DEFLATE算法统治计算机世界。有几年的时间他酗酒成性，生活也于20世纪90年代末期开始支离破碎，好几次因酒驾或者其它违法行为而被逮捕。Katz于2000年4月14号被发现死于一个酒店的房间，终年37岁。死因是酒精导致的严重胰腺出血，身旁是一堆的空酒瓶。&lt;/p&gt;
&lt;h3&gt;
&lt;a class=&quot;anchor&quot; href=&quot;#%E5%BD%93%E5%89%8D%E7%9A%84%E4%B8%80%E4%BA%9B%E5%BD%92%E6%A1%A3%E8%BD%AF%E4%BB%B6&quot; name=&quot;user-content-%E5%BD%93%E5%89%8D%E7%9A%84%E4%B8%80%E4%BA%9B%E5%BD%92%E6%A1%A3%E8%BD%AF%E4%BB%B6&quot;&gt;&lt;/a&gt;当前的一些归档软件&lt;/h3&gt;
&lt;p&gt;ZIP以及其它基于DEFLATE的格式一直占据主导地位，直到20世纪90年代中期，一些新的改进的格式开始出现。1993年，Eugene Roshal发布了一个叫做WinRAR的归档软件，该软件使用RAR格式。最新的RAR结合了PPM和LZSS算法，前面的版本就不太清楚了。RAR开始在互联网分享文件方面成为事实标准，特别是盗版影像的传播。1996年一个叫bzip2d的Burrows-Wheeler Transform算法开源实现发布，并很快在UNIX平台上面流行开来，大有对抗基于DEFLATE算法的gzip格式。1999年另外一个开源压缩程序发布了，以7-Zip或.7z的格式存在，7-Zip应该是第一个能够挑战ZIP和RAR霸主地位的格式，这源于它的高压缩比，模块化以及开放性。这种格式并不仅限于使用一种压缩算法，而是可以在bzip2, LZMA, LZMA2, 和 PPMd算法之间任意选择。最后，归档软件中较新的一个格式是PAQ*格式。第一个PAQ版本于2002年由Matt Mahoney发布，叫做PAQ1。PAQ主要使用一种叫做context mixing的技术来改进PPM算法，context mixing结合了两个甚至是多个统计模型来产生一个更好的符号概率预测，这要比其中的任意一个模型都要好。&lt;/p&gt;
&lt;h2&gt;
&lt;a class=&quot;anchor&quot; href=&quot;#%E5%8E%8B%E7%BC%A9%E6%8A%80%E6%9C%AF&quot; name=&quot;user-content-%E5%8E%8B%E7%BC%A9%E6%8A%80%E6%9C%AF&quot;&gt;&lt;/a&gt;压缩技术&lt;/h2&gt;
&lt;p&gt;有许多不同的技术被用来压缩数据。大多数技术都不能单独使用，需要结合起来形成一套算法。那些能够单独使用的&lt;br&gt;
技术比需要结合的技术通常更加有效。其中的绝大部分都归于entropy编码类别下面，但其它的一些技术也挺常用，如Run-Length Encoding和Burrows-Wheeler Transform。&lt;/p&gt;
&lt;h3&gt;
&lt;a class=&quot;anchor&quot; href=&quot;#run-length-encoding&quot; name=&quot;user-content-run-length-encoding&quot;&gt;&lt;/a&gt;Run-Length Encoding&lt;/h3&gt;
&lt;p&gt;Run-Length Encoding是一个非常简单的压缩技术，把重复出现的多个字符替换为重复次数外加字符。单个字符次数为1。RLE非常适合数据重复度比较高的数据，同一行有很多像素颜色相同的渐进图片，也可以结合Burrows-Wheeler Transform等其它技术一起使用。&lt;/p&gt;
&lt;p&gt;下面是RLE的一个简单例子:&lt;/p&gt;
&lt;p&gt;输入: AAABBCCCCDEEEEEEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA&lt;br&gt;
输出: 3A2B4C1D6E38A&lt;/p&gt;
&lt;h3&gt;
&lt;a class=&quot;anchor&quot; href=&quot;#burrows-wheeler-transform&quot; name=&quot;user-content-burrows-wheeler-transform&quot;&gt;&lt;/a&gt;Burrows-Wheeler Transform&lt;/h3&gt;
&lt;p&gt;Burrows-Wheeler Transform是1994年发明的技术，目的是可逆的处理一段输入数据，使得相同字符连续出现的次数最大化。BWT自身并不做任何的压缩操作，仅简单地转化数据，让Run-Length Encoder等压缩算法可以更有效的编码。&lt;/p&gt;
&lt;p&gt;BWT算法很简单:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;创建一个字符串数组。&lt;/li&gt;
&lt;li&gt;把输入字符串的所有排列组合塞入上述字符串数组。&lt;/li&gt;
&lt;li&gt;按照字符顺序为字符串数组排序。&lt;/li&gt;
&lt;li&gt;返回数组的最后一列。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;BWT通常处理有很多交叉重复字符的长字符串时效果很好。下面是一个有着理想输入的例子，注意&amp;amp;是文件结束符：&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/c169ca29581eb3fb92eaa37781c0fdad.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;因为交换相同的符号到一起，输入数据在BWT处理之后得到优化后的结果，另外一个算法可以对该结果进行压缩，比如RLE会得到”3H&amp;amp;3A”。尽管这个例子得到了一个较优的结果，不过现实世界中的数据它不总是这样。&lt;/p&gt;
&lt;h3&gt;
&lt;a class=&quot;anchor&quot; href=&quot;#entropy-encoding&quot; name=&quot;user-content-entropy-encoding&quot;&gt;&lt;/a&gt;Entropy Encoding&lt;/h3&gt;
&lt;p&gt;数据压缩中，平均来说为了表示一个字符或短语，Entropy意味着所需要的最少bit数。一个基本的entropy编码器包括一个分析模型以及一套编码。输入文件被解析，并产生一个由字符出现概率组成的统计模型。然后，编码器可以利用该统计模型去决定该给每一个字符多少个bit，从而使得最常用的字符用最短的编码，反之最不常用的字符用最长的编码。&lt;/p&gt;
&lt;h4&gt;
&lt;a class=&quot;anchor&quot; href=&quot;#shannon-fano-coding&quot; name=&quot;user-content-shannon-fano-coding&quot;&gt;&lt;/a&gt;Shannon-Fano Coding&lt;/h4&gt;
&lt;p&gt;这是最早的压缩技术，于1949年由Claude Shannon和Robert Fano发明。这个技术的其中一个步骤是产生一个代表字符出现概率的二叉树。字符以这样一种方式排序，出现得越频繁的字符越靠近树的顶端，越不常见的越靠近树的底部。&lt;/p&gt;
&lt;p&gt;一个字符对应的编码通过搜索Shannon-Fano来获得，此外，左分支后面加0，右分支加1。例如，”A”是两个左节点后接一个右节点，那么对于的编码为”0012″。Shannon-Fano coding不总是能够产生最优的编码，主要是由于二叉树是自下而上构建的。由于这个原因，使用的较多的还是对于任意输入都能够得到最优编码的Huffman coding。&lt;/p&gt;
&lt;p&gt;产生Shannon-Fano编码的算法很简单:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;解析输入，统计每一个字符出现的频率。&lt;/li&gt;
&lt;li&gt;根据是上述频率计算字符的概率。&lt;/li&gt;
&lt;li&gt;依据概率对字符降序排序。&lt;/li&gt;
&lt;li&gt;为每一个字符生成一个叶节点(LeafNode)&lt;/li&gt;
&lt;li&gt;把字符列表分为左右两部分，使得左边的概率与右边的概率大致相当。&lt;/li&gt;
&lt;li&gt;左节点加编码”0″，右节点加编码”1″。&lt;/li&gt;
&lt;li&gt;对两棵子树重复的步骤5和6，直到所有的字符节点都成为叶子节点。&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;
&lt;a class=&quot;anchor&quot; href=&quot;#huffman-coding&quot; name=&quot;user-content-huffman-coding&quot;&gt;&lt;/a&gt;Huffman Coding&lt;/h4&gt;
&lt;p&gt;Huffman Coding是另外一个entropy coding的例子，与Shannon-Fano Coding非常的相似，只是为了产生最优编码&lt;br&gt;
二叉树是自上而下构建的。&lt;/p&gt;
&lt;p&gt;生成Huffman编码的算法前面的三个步骤与Shannon-Fano完全相同:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;解析输入，统计每一个字符出现的频率。&lt;/li&gt;
&lt;li&gt;根据是上述频率计算字符的概率。&lt;/li&gt;
&lt;li&gt;依据概率对字符降序排序。&lt;/li&gt;
&lt;li&gt;为每一个字符生成一个叶节点(LeafNode)，节点包含概率信息P，把节点存入一个队列Queue。&lt;/li&gt;
&lt;li&gt;While (Nodes in Queue &amp;gt; 1)
&lt;ul&gt;
&lt;li&gt;从队列里面取出概率最小的两个节点。&lt;/li&gt;
&lt;li&gt;给左节点分配编码”0″，右节点分配编码”1″。&lt;/li&gt;
&lt;li&gt;创建一个新的节点，其概率为上面步骤中的两个节点之和。&lt;/li&gt;
&lt;li&gt;把两个节点中的第一个设置为新建节点的左节点，第二个节点为新建节点的右节点。&lt;/li&gt;
&lt;li&gt;把新建节点存入队列&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;最后一个节点就是二叉树的根节点。&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;
&lt;a class=&quot;anchor&quot; href=&quot;#arithmetic-coding&quot; name=&quot;user-content-arithmetic-coding&quot;&gt;&lt;/a&gt;Arithmetic Coding&lt;/h4&gt;
&lt;p&gt;1979年该算法在IBM被开发出来，当时IBM正在调研一些压缩算法，以期用于它们的大型机上。如果单论压缩比，Arithmetic coding确实是一个最优的entropy coding技术，通常压缩比方面Arithmetic Coding要比Huffman Coding表现得更好。然而，它却也比其它技术复杂得多。&lt;/p&gt;
&lt;p&gt;不像其它技术会把字符概率构建成一棵树，arithmetic coding把输入转化为一个0到1之间的有理数，输入字符的个数记为base，里面每一个不同的字符都分配一个0到base之间的值。然后，最后转化为二进制得到最终的结果。结果也可以通过把base恢复为原来的base值，替换为对应字符而得到原输入值。&lt;/p&gt;
&lt;p&gt;一个基本的计算arithmetic code算法如下:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;计算输入数据里面不同字符的个数。这个数字记为base b(比如 base 2代表2二进制)。&lt;/li&gt;
&lt;li&gt;按字符出现的顺序分别给每一个字符分配一个0到b之间的值。&lt;/li&gt;
&lt;li&gt;使用步骤2中德值，把输入中的字符替换为对应的数字(编码)。&lt;/li&gt;
&lt;li&gt;把步骤3中得到的结果从b进制转化为2进制。&lt;/li&gt;
&lt;li&gt;如果解码需要的话，记录输入的字符总个数。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;下面是一个编码操作的例子，输入为”ABCDAABD”：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;找到共有4个不同的字符输入, base = 4, length = 8。&lt;/li&gt;
&lt;li&gt;按出现顺序为不同的字符赋值: A=0, B=1, C=2, D=3。&lt;/li&gt;
&lt;li&gt;用编码替换字符，得到“0.012300134”，注意最前面的”0.”是为了得到小数而加上去的。最后的4表示base=4。&lt;/li&gt;
&lt;li&gt;把“0.012300134”从4进制转化为2进制，得到“0.011011000001112”。最后的2表示base=2。&lt;/li&gt;
&lt;li&gt;在结果中标识输入的总字符数为8。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;假设字符为8个bit表示，输入共需要64个bit空间，然而对应的arithmetic coding只有15个bit，压缩比为24%，&lt;br&gt;
效果显著。这个例子展示了arithmetic coding是如何良好的压缩固定字符串的。&lt;/p&gt;
&lt;h2&gt;
&lt;a class=&quot;anchor&quot; href=&quot;#%E5%8E%8B%E7%BC%A9%E7%AE%97%E6%B3%95&quot; name=&quot;user-content-%E5%8E%8B%E7%BC%A9%E7%AE%97%E6%B3%95&quot;&gt;&lt;/a&gt;压缩算法&lt;/h2&gt;
&lt;h3&gt;
&lt;a class=&quot;anchor&quot; href=&quot;#sliding-window-algorithms&quot; name=&quot;user-content-sliding-window-algorithms&quot;&gt;&lt;/a&gt;Sliding Window Algorithms&lt;/h3&gt;
&lt;h4&gt;
&lt;a class=&quot;anchor&quot; href=&quot;#lz77&quot; name=&quot;user-content-lz77&quot;&gt;&lt;/a&gt;LZ77&lt;/h4&gt;
&lt;p&gt;LZ77发表于1977年，是名副其实的压缩算法开山之作。它首次引入’sliding window’的概念，相较几个主要的压缩算法，压缩比都有非常明显的提高。LZ77维护了一个字典，用一个三元组来表示offset，run length和分割字符。offset表示从文件起始位置到当前Phase的起始位置的距离，run length记录当前Phase有多少个字符，分割符仅用于分割不同的Phase。Phase就是offset到offset+length之间的子串减掉分隔符。随着文件解析的进行，基于sliding window字典会动态的变化。例如，64MB的sliding window意味着四点将包含64M的输入数据的信息。&lt;/p&gt;
&lt;p&gt;给定一个输入为”abbadabba”，那么输出可能像”abb(0,1,’d&#39;)(0,3,’a&#39;)”，如下图所示:&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/f5966eaefb9fa7f7485591c7b1b1ba87.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;尽管上述的替换看起来比原数据还要大，当输入数据更大一些的时候，效果会比较好。&lt;/p&gt;
&lt;h4&gt;
&lt;a class=&quot;anchor&quot; href=&quot;#lzr&quot; name=&quot;user-content-lzr&quot;&gt;&lt;/a&gt;LZR&lt;/h4&gt;
&lt;p&gt;LZR是LZ77的修改版本，于1981年由Michael Rodeh发明。这个算法目标是成为LZ77的一个线性时间替换算法。然而，编码后Udell指针可能指向文件的任意offset，意味着需要耗费可观的内存。加之压缩比表现也差强人意(LZ77好得多)，LZR算是一个不成功的LZ77衍生算法。&lt;/p&gt;
&lt;h4&gt;
&lt;a class=&quot;anchor&quot; href=&quot;#deflate&quot; name=&quot;user-content-deflate&quot;&gt;&lt;/a&gt;DEFLATE&lt;/h4&gt;
&lt;p&gt;DEFLATE于1993年由Phil Katz发明，是现代绝大多数压缩任务的基石。它仅仅结合了两种算法，先用LZ77或LZSS预处理，然后用Huffman编码，快速的得到不错的压缩结果。&lt;/p&gt;
&lt;h4&gt;
&lt;a class=&quot;anchor&quot; href=&quot;#deflate64&quot; name=&quot;user-content-deflate64&quot;&gt;&lt;/a&gt;DEFLATE64&lt;/h4&gt;
&lt;p&gt;DEFLATE64是DEFLATE的一个有专利的扩展，把字典的大小提高到64K(名字随之)，从而允许在sliding window里面有更大的距离。相比于DEFLATE，DEFLATE64在性能和压缩比方面都有提高。然而，由于DEFLATE64的专利保护以及相较DEFLATE并没有特别明显的提高，DEFLATE64很少被采用。相反一些开源算法如LZMA被大量的使用。&lt;/p&gt;
&lt;h4&gt;
&lt;a class=&quot;anchor&quot; href=&quot;#lzss&quot; name=&quot;user-content-lzss&quot;&gt;&lt;/a&gt;LZSS&lt;/h4&gt;
&lt;p&gt;LZSS，全称Lempel-Ziv-Storer-Szymanski，于1982年由James Storer发表。LZSS相较LZ77有所提高，它能检测到一个替换是否真的减小了文件大小。如果文件大小没有减小，不再替换输入值。此外，输入段被(offset, length)数据对替换，其中offset表示离输入起始位置的bytes数量，length表示从该位置读取了多少个字符。另外一个改进是去除了”next character”信息，仅仅使用offset-length数据对。&lt;/p&gt;
&lt;p&gt;下面是一个输入为” these theses”简单的例子，结果为” these(0,6)s”，仅仅节省了一个Byte，不过输入数据大的时候效果会好得多。&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/5de2f6e48b62da018014a831becfae94.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;LZSS依然被用在许多使用广泛的归档格式中，其中最知名的是RAR。LZSS有时也被用于网络数据压缩。&lt;/p&gt;
&lt;h4&gt;
&lt;a class=&quot;anchor&quot; href=&quot;#lzh&quot; name=&quot;user-content-lzh&quot;&gt;&lt;/a&gt;LZH&lt;/h4&gt;
&lt;p&gt;LZH发明于1987年，全称为”Lempel-Ziv Huffman”。它是LZSS的一个衍生算法，利用Huffman coding压缩指针，压缩效果有微小的提高。然而使用Huffman coding带来的提高实在是很有限，相较于使用Huffman coding带来的性能损失，不足为取。&lt;/p&gt;
&lt;h4&gt;
&lt;a class=&quot;anchor&quot; href=&quot;#lzb&quot; name=&quot;user-content-lzb&quot;&gt;&lt;/a&gt;LZB&lt;/h4&gt;
&lt;p&gt;LZB同样发明于1987年，同样是LZSS的衍生算法。如LZH一样，LZB也致力于通过更加有效的编码指针以达到更好的压缩效果。它的做法是随着sliding window变大，逐步的增大指针的数量。它的压缩效果确实比LZSS和LZH要好，不过因为额外的编码步骤，速度上比LZSS慢得多。&lt;/p&gt;
&lt;h4&gt;
&lt;a class=&quot;anchor&quot; href=&quot;#rolz&quot; name=&quot;user-content-rolz&quot;&gt;&lt;/a&gt;ROLZ&lt;/h4&gt;
&lt;p&gt;ROLZ全称”Reduced Offset Lempel-Ziv”，它的目标是提高LZ77的压缩效果，通过限制offset的大小，从而减少为offset-length数据对编码的数据量。这项LZ77的衍生技术于1991年首次出现在Ross Williams的LZRW4算法里面。其它的实现包括BALZ，QUAD，和 RZM。高度优化的ROLZ能够达到接近LZMA的压缩比，不过ROLZ不太流行。&lt;/p&gt;
&lt;h4&gt;
&lt;a class=&quot;anchor&quot; href=&quot;#lzp&quot; name=&quot;user-content-lzp&quot;&gt;&lt;/a&gt;LZP&lt;/h4&gt;
&lt;p&gt;LZP全称”Lempel-Ziv + Prediction”。它是ROLZ算法的一个特殊案例，offset减小到1。&lt;br&gt;
有几个衍生的算法使用不同的技术来实现或加快压缩速度，或提高压缩比的目标。LZW4实现了一个数字编码器&lt;br&gt;
达到了最好的压缩比，不过牺牲了部分速度。&lt;/p&gt;
&lt;h4&gt;
&lt;a class=&quot;anchor&quot; href=&quot;#lzrw1&quot; name=&quot;user-content-lzrw1&quot;&gt;&lt;/a&gt;LZRW1&lt;/h4&gt;
&lt;p&gt;Ron Williams于1991年发明了这个算法，第一次引入了Reduced-Offset Lempel-Ziv compressiond的概念。LZRW1&lt;br&gt;
能够达到很高的压缩比，同时保持快速有效。Ron Williams也发明另外几个基于LZRW1改进的衍生算法，如&lt;br&gt;
LZRW1-A, 2, 3, 3-A, 和4。&lt;/p&gt;
&lt;h4&gt;
&lt;a class=&quot;anchor&quot; href=&quot;#lzjb&quot; name=&quot;user-content-lzjb&quot;&gt;&lt;/a&gt;LZJB&lt;/h4&gt;
&lt;p&gt;Jeff Bonwick于1998年发明了Lempel-Ziv Jeff Bonwick算法，用于Solaris操作系统的Z文件系统(ZFS)。它被认为是&lt;br&gt;
LZRW算法的一个衍生算法，特别是LZRW1，目标是改进压缩速度。既然它是被用于操作系统，速度显得尤为重要，&lt;br&gt;
不能因为压缩算法的原因而使得磁盘操作成为瓶颈。&lt;/p&gt;
&lt;h4&gt;
&lt;a class=&quot;anchor&quot; href=&quot;#lzs&quot; name=&quot;user-content-lzs&quot;&gt;&lt;/a&gt;LZS&lt;/h4&gt;
&lt;p&gt;Lempel-Ziv-Stac算法于1994年由Stac Electronics发明，用于磁盘压缩软件。它是LZ77的一个修改版本，区分了输出的文字符号与offset-length数据对，此外还移除了分隔符。功能上来说，LZS与LZSS算法很相似。&lt;/p&gt;
&lt;h4&gt;
&lt;a class=&quot;anchor&quot; href=&quot;#lzx&quot; name=&quot;user-content-lzx&quot;&gt;&lt;/a&gt;LZX&lt;/h4&gt;
&lt;p&gt;LZX算法于1995年由Jonathan Forbes和Tomi Poutanen发明，用于Amiga计算机。LZX中X没有什么特殊的意义。Forbes于1996年把该算法出售给了微软，并且受雇于微软，那那儿该算法被继续优化并用于微软的cabinet(.CAB)格式。这个算法也被微软用于压缩Compressed HTML Help (CHM) 文件，Windows Imaging Format (WIM)&lt;br&gt;
文件，和 Xbox Live Avatars。&lt;/p&gt;
&lt;h4&gt;
&lt;a class=&quot;anchor&quot; href=&quot;#lzo&quot; name=&quot;user-content-lzo&quot;&gt;&lt;/a&gt;LZO&lt;/h4&gt;
&lt;p&gt;LZO于1996年由Markus发明，该算法的目标是快速的压缩和解压缩。它允许调整压缩级别，并且在最高级别下仍仅需64KB额外的内存空间，同时解压缩仅需要输入和输出的空间。LZO功能上非常类似LZSS，不过是为了速度，而非压缩比做的优化。&lt;/p&gt;
&lt;h4&gt;
&lt;a class=&quot;anchor&quot; href=&quot;#lzma&quot; name=&quot;user-content-lzma&quot;&gt;&lt;/a&gt;LZMA&lt;/h4&gt;
&lt;p&gt;Lempel-Ziv Markov chain Algorithm算法于1998年首次发表，是随着7-Zip归档软件一起发布的。大多数情况下它比bzip2, DEFLATE以及其它算法表现都要好。LZMA使用一系列技术来完成输出。首先时一个LZ77的修改版本，它操作的是bitwise级别，而非传统上的bytewise级别，用于解析数据。LZ77算法解析后的输出经过数字编码。更多的技术可被使用，这依赖于具体的LZMA实现。相比其它LZ衍生算法，LZMA在压缩比方面提高明显，这要归功于操作bitewise，而非bytewise。&lt;/p&gt;
&lt;h4&gt;
&lt;a class=&quot;anchor&quot; href=&quot;#lzma2&quot; name=&quot;user-content-lzma2&quot;&gt;&lt;/a&gt;LZMA2&lt;/h4&gt;
&lt;p&gt;LZMA2是LZMA的一个增量改进版本，于2009年在7-Zip归档软件的一个更新版本里面首次引入。LZMA2改进了多线程处理功能，同时优化对不可压缩数据的处理，这也稍微提高了压缩效果。&lt;/p&gt;
&lt;h4&gt;
&lt;a class=&quot;anchor&quot; href=&quot;#statistical-lempel-ziv&quot; name=&quot;user-content-statistical-lempel-ziv&quot;&gt;&lt;/a&gt;Statistical Lempel-Ziv&lt;/h4&gt;
&lt;p&gt;Statistical Lempel-Ziv是于2001年由Sam Kwong博士和Yu Fan Ho博士提出的一个概念。基本的原则是数据的统计分析结果可以与LZ77衍生算法结合起来，进一步优化什么样的编码将存储在字典中。&lt;/p&gt;
&lt;h3&gt;
&lt;a class=&quot;anchor&quot; href=&quot;#dictionary-algorithms&quot; name=&quot;user-content-dictionary-algorithms&quot;&gt;&lt;/a&gt;Dictionary Algorithms&lt;/h3&gt;
&lt;h4&gt;
&lt;a class=&quot;anchor&quot; href=&quot;#lz78&quot; name=&quot;user-content-lz78&quot;&gt;&lt;/a&gt;LZ78&lt;/h4&gt;
&lt;p&gt;LZ78于1978年由Lempel和Ziv发明，缩写正是来源于此。不再使用sliding window来生成字典，输入数据要么被预处理之后用来生成字典，或者字典在文件解析过程中逐渐形成。LZ78采用了后者。字典的大小通常被限定为几兆的大小，或者所有编码上限为几个比特，比如8个。这是出于减少对内存要求的考量。算法如何处理正是LZ78的各个衍生算法的区别所在。&lt;/p&gt;
&lt;p&gt;解析文件的时候，LZ78把新碰到的字符或者字符串加到字典中。针对每一个符号，形如(dictionary index, unknown symbol)的字典记录会对应地生成。如果符号已经存在于字典中，那么将从字典中搜索该符号的子字符串以及其后的其它符号。最长子串的位置即为字典索引(Index)。字典索引对应的数据被设置为最后一个未知子串。如果当前字符是未知的，那么字典索引设置为0，表示它是单字符对。这些数据对形成一个链表数据结构。&lt;/p&gt;
&lt;p&gt;形如”abbadabbaabaad”的输入，将会产生”(0,a)(0,b)(2,a)(0,d)(1,b)(3,a)(6,d)”这样的输出。&lt;br&gt;
你能从下面的例子里面看到它是如何演化的:&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://jbcdn2.b0.upaiyun.com/2014/09/0e12905c00473a9d9cfff86a163b2b12.jpg&quot; rel=&quot;lightbox[77247]&quot; title=&quot;无损数据压缩算法的历史&quot;&gt;&lt;img class=&quot;alignnone size-full wp-image-77253&quot; alt=&quot;LZ78&quot; src=&quot;/images/jobbole.com/aded11e78678bff55334d118bc8865dd.jpg&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;
&lt;a class=&quot;anchor&quot; href=&quot;#lzw&quot; name=&quot;user-content-lzw&quot;&gt;&lt;/a&gt;LZW&lt;/h4&gt;
&lt;p&gt;LZW于1984年由Terry Welch发明，全称为Lempel-Ziv-Welch。它是LZ78大家族中被用得最多的算法，尽管被专利严重的阻碍了使用。LZW改进LZ78的方法与LZSS类似。它删除输出中冗余的数据，使得输出中不再包含指针。压缩之前它就在字典里面包含了每一个字符，也引入了一些技术改进压缩效果，比如把每一个语句的最后一个字符编码为下一个语句的第一个字符。LZW在图像转换格式中较为常见，早期也曾用于ZIP格式里面，也包括一些其他的专业应用。LZW非常的快，不过相较于一些新的算法，压缩效果就显得比较平庸。一些算法会更快，压缩效果也更好。&lt;/p&gt;
&lt;h4&gt;
&lt;a class=&quot;anchor&quot; href=&quot;#lzc&quot; name=&quot;user-content-lzc&quot;&gt;&lt;/a&gt;LZC&lt;/h4&gt;
&lt;p&gt;LZC，全称Lempel-Ziv Compress，是LZW算法的一个微小修改版本，用于UNIX压缩工具中。LZC与LZW两者的区别在于，LZC会监控输出的压缩比。当压缩比超过某一个临界值的时候，字典被丢弃并重构。&lt;/p&gt;
&lt;h4&gt;
&lt;a class=&quot;anchor&quot; href=&quot;#lzt&quot; name=&quot;user-content-lzt&quot;&gt;&lt;/a&gt;LZT&lt;/h4&gt;
&lt;p&gt;Lempel-Ziv Tischer是LZC的修改版，当字典满了，删除最不常用的的语句，用新的记录替换它。还有一些其它的小改进，不过现在LZC和LZT都不常用了。&lt;/p&gt;
&lt;h4&gt;
&lt;a class=&quot;anchor&quot; href=&quot;#lzmw&quot; name=&quot;user-content-lzmw&quot;&gt;&lt;/a&gt;LZMW&lt;/h4&gt;
&lt;p&gt;于1984年由Victor Miller和Mark Wegman发明，LZMW算法非常类似于LZT，它也采用了替换最不常用语句的策略。然而，不是连接字典中相似的语句，而是连接LZMW最后被编码的两个语句并且存储为一条记录。因此，字典的容量能够快速扩展并且LRUs被更频繁的丢弃掉。LZMW压缩效果比LZT更好，然而它也是另外一个这个时代很难看到其应用的算法。&lt;/p&gt;
&lt;h4&gt;
&lt;a class=&quot;anchor&quot; href=&quot;#lzap&quot; name=&quot;user-content-lzap&quot;&gt;&lt;/a&gt;LZAP&lt;/h4&gt;
&lt;p&gt;LZAP于1988年由James Storer发明，是LZMW算法的修改版本。AP代表”all prefixes”，以其遍历时在字典里面存储单个语句，字典存储了语句的所有排列组合。例如，如果最后一个语句为”last”，当前语句是”next”，字典将存储”lastn”，”lastne”，”lastnex”，和 “lastnext”。&lt;/p&gt;
&lt;h4&gt;
&lt;a class=&quot;anchor&quot; href=&quot;#lzwl&quot; name=&quot;user-content-lzwl&quot;&gt;&lt;/a&gt;LZWL&lt;/h4&gt;
&lt;p&gt;LZWL是2006年发明的一个LZW修改版本，处理音节而非字符。LZWL是专为有很多常用音节的数据集而设计的，比如&lt;br&gt;
XML。这种算法通常会搭配一个前置处理器，用来把输入数据分解为音节。&lt;/p&gt;
&lt;h4&gt;
&lt;a class=&quot;anchor&quot; href=&quot;#lzj&quot; name=&quot;user-content-lzj&quot;&gt;&lt;/a&gt;LZJ&lt;/h4&gt;
&lt;p&gt;Matti Jakobsson于1985年发表了LZJ算法，它是LZ78大家族中唯一一位衍生于LZW的算法。LZJ的工作方式是，在字典中存储每一个经过预处理输入中的不同字符串，并为它们编码。当字典满了，移除所有只出现一次的记录。&lt;/p&gt;
&lt;h3&gt;
&lt;a class=&quot;anchor&quot; href=&quot;#non-dictionary-algorithms&quot; name=&quot;user-content-non-dictionary-algorithms&quot;&gt;&lt;/a&gt;Non-dictionary Algorithms&lt;/h3&gt;
&lt;h4&gt;
&lt;a class=&quot;anchor&quot; href=&quot;#ppm&quot; name=&quot;user-content-ppm&quot;&gt;&lt;/a&gt;PPM&lt;/h4&gt;
&lt;p&gt;通过抽样来预测数据是一项统计建模技术，使用输入中的一部分数据，来预测后续的符号将会是什么，通过这种算法来减少输出数据的entropy。该算法与字典算法不一样，PPM预测下一个符号将会是什么，而不是找出下一个符号来编码。PPM通常结合一个编码器来一起使用，如arithmetic编码或者适配的Huffman编码。PPM或者它的衍生算法被实现于许多归档格式中，包括7-Zip和RAR。&lt;/p&gt;
&lt;h4&gt;
&lt;a class=&quot;anchor&quot; href=&quot;#bzip2&quot; name=&quot;user-content-bzip2&quot;&gt;&lt;/a&gt;bzip2&lt;/h4&gt;
&lt;p&gt;bzip2是Burrows-Wheeler Transform算法的一个开源实现。它的操作原理很简单，不过却在压缩比和速度之间达到了一个平衡，表现良好，这让它在UNIX环境上很流行。首先，使用了一个Run-Length编码器，接下来，Burrows-Wheeler Transform算法加入进来，然后，使用move-to-front transform以达到产生大量相同符号的目标，为接下来的另一个Run-Length 编码器做准备。最后结果用Huffman编码，将一个消息头与其打包。&lt;/p&gt;
&lt;h4&gt;
&lt;a class=&quot;anchor&quot; href=&quot;#paq&quot; name=&quot;user-content-paq&quot;&gt;&lt;/a&gt;PAQ&lt;/h4&gt;
&lt;p&gt;PAQ于2002年由Matt Mahoney发明，是老版PPM的一个改进版。改进的方法是使用一项叫做context mixing的革命性技术。Context mixing是指智能地结合多个(PPM是单个模型)统计模型，来做出对下一个符号的更好预测，比其中的任何一个模型都要好。PAQ是最有前途的技术之一，它有很好的压缩比，开发也很活跃。自它出现后算起，有超过20个衍生算法被发明出来，其中一些在压缩比方面屡破记录。PAQ的最大缺点是速度，这源于使用了多个统计模型来获得更好的压缩比。然而随着硬件速度的不断提升，它或许会是未来的标准。PAQ在应用方面进步缓慢，一个叫PAQ8O的衍生算法可以在一个叫PeaZip的Windows程序里面找到，PAQ8O支持64-bit，速度也有较大提升。其它的PAQ格式仅仅用于命令行程序。&lt;/p&gt;


&lt;!-- div id=&quot;ad1&quot;&gt;
&lt;/div --&gt;


	


	

</description>
        <pubDate>Wed, 24 Sep 2014 00:00:00 +0800</pubDate>
        <link>http://iftti.com/posts/2014/2014-09-24-77247-7dc4d49c7.html</link>
        <guid isPermaLink="true">http://iftti.com/posts/2014/2014-09-24-77247-7dc4d49c7.html</guid>
        
        
        <category>jobbole</category>
        
      </item>
    
      <item>
        <title>常见面试之机器学习算法思想简单梳理</title>
        <description>

					
		
&lt;!-- div id=&quot;ad1&quot;&gt;
&lt;script type=&quot;text/javascript&quot;&gt;
google_ad_client = &quot;ca-pub-7056282119617872&quot;;
google_ad_slot = &quot;6645040531&quot;;
google_ad_width = 300;
google_ad_height = 250;
&lt;/script&gt;
&lt;script type=&quot;text/javascript&quot;
src=&quot;http://pagead2.googlesyndication.com/pagead/show_ads.js&quot;&gt;
&lt;/script&gt;
&lt;/div&gt;
&lt;br/ --&gt;

&lt;p&gt;&lt;strong&gt;前言：&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;找工作时（IT行业），除了常见的软件开发以外，机器学习岗位也可以当作是一个选择，不少计算机方向的研究生都会接触这个，如果你的研究方向是机器学习/数据挖掘之类，且又对其非常感兴趣的话，可以考虑考虑该岗位，毕竟在机器智能没达到人类水平之前，机器学习可以作为一种重要手段，而随着科技的不断发展，相信这方面的人才需求也会越来越大。&lt;/p&gt;
&lt;p&gt;纵观IT行业的招聘岗位，机器学习之类的岗位还是挺少的，国内大点的公司里百度，阿里，腾讯，网易，搜狐，华为（华为的岗位基本都是随机分配，机器学习等岗位基本面向的是博士）等会有相关职位，另外一些国内的中小型企业和外企也会招一小部分。当然了，其中大部分还是百度北京要人最多，上百人。阿里的算法岗位很大一部分也是搞机器学习相关的。另外本人有幸签约了网易杭州研究院的深度学习算法岗位，打算从事机器学习领域至少5年。非常感谢小易收留了我！&lt;/p&gt;
&lt;p&gt;下面是本人在找机器学习岗位工作时，总结的常见机器学习算法（主要是一些常规分类器）大概流程和主要思想，希望对大家找机器学习岗位时有点帮助。实际上在面试过程中，懂这些算法的基本思想和大概流程是远远不够的，那些面试官往往问的都是一些公司内部业务中的课题，往往要求你不仅要懂得这些算法的理论过程，而且要非常熟悉怎样使用它，什么场合用它，算法的优缺点，以及调参经验等等。说白了，就是既要会点理论，也要会点应用，既要有点深度，也要有点广度，否则运气不好的话很容易就被刷掉，因为每个面试官爱好不同。&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;朴素贝叶斯：&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;有以下几个地方需要注意：&lt;/p&gt;
&lt;p&gt;1. 如果给出的特征向量长度可能不同，这是需要归一化为通长度的向量（这里以文本分类为例），比如说是句子单词的话，则长度为整个词汇量的长度，对应位置是该单词出现的次数。&lt;/p&gt;
&lt;p&gt;2. 计算公式如下：&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/b8083cb5e9f6fc73bd4c31350d0463ab.jpg&quot; width=&quot;214&quot; height=&quot;69&quot;&gt;&lt;/p&gt;
&lt;p align=&quot;left&quot;&gt;其中一项条件概率可以通过朴素贝叶斯条件独立展开。要注意一点就是&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/4c56b13fde8c16b7ddf93202cb6f1446.jpg&quot;&gt;的计算方法，而由朴素贝叶斯的前提假设可知，&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/21b0b4b9470a5a5c93616f233e3d8b60.jpg&quot;&gt;=&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/a2275101ca10e4966b0cab8a292eb00d.jpg&quot;&gt;，因此一般有两种，一种是在类别为ci的那些样本集中，找到wj出现次数的总和，然后除以该样本的总和；第二种方法是类别为ci的那些样本集中，找到wj出现次数的总和，然后除以该样本中所有特征出现次数的总和。&lt;/p&gt;
&lt;p&gt;3. 如果&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/4c56b13fde8c16b7ddf93202cb6f1446.jpg&quot;&gt;中的某一项为0，则其联合概率的乘积也可能为0，即2中公式的分子为0，为了避免这种现象出现，一般情况下会将这一项初始化为1，当然为了保证概率相等，分母应对应初始化为2（这里因为是2类，所以加2，如果是k类就需要加k，术语上叫做laplace光滑, 分母加k的原因是使之满足全概率公式）。&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;朴素贝叶斯的优点：&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;对小规模的数据表现很好，适合多分类任务，适合增量式训练。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;缺点&lt;/em&gt;：&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;对输入数据的表达形式很敏感。&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;决策树：&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;决策树中很重要的一点就是选择一个属性进行分枝，因此要注意一下信息增益的计算公式，并深入理解它。&lt;/p&gt;
&lt;p&gt;信息熵的计算公式如下:&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/cd414a54f5e77b0eebd058c4dd8bd280.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;其中的n代表有n个分类类别（比如假设是2类问题，那么n=2）。分别计算这2类样本在总样本中出现的概率p1和p2，这样就可以计算出未选中属性分枝前的信息熵。&lt;/p&gt;
&lt;p&gt;现在选中一个属性xi用来进行分枝，此时分枝规则是：如果xi=vx的话，将样本分到树的一个分支；如果不相等则进入另一个分支。很显然，分支中的样本很有可能包括2个类别，分别计算这2个分支的熵H1和H2,计算出分枝后的总信息熵H’=p1*H1+p2*H2.，则此时的信息增益ΔH=H-H’。以信息增益为原则，把所有的属性都测试一边，选择一个使增益最大的属性作为本次分枝属性。&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;决策树的优点：&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;计算量简单，可解释性强，比较适合处理有缺失属性值的样本，能够处理不相关的特征；&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;缺点：&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;容易过拟合（后续出现了随机森林，减小了过拟合现象）；&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Logistic回归：&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Logistic是用来分类的，是一种线性分类器，需要注意的地方有：&lt;/p&gt;
&lt;p&gt;1. logistic函数表达式为：&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/20d645eb987a3c33094186fe1964ec86.jpg&quot; width=&quot;372&quot; height=&quot;104&quot;&gt;&lt;/p&gt;
&lt;p&gt;其导数形式为：&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/828a60323e7c280125c274e8fd34d095.jpg&quot; width=&quot;273&quot; height=&quot;154&quot;&gt;&lt;/p&gt;
&lt;p&gt;2. logsitc回归方法主要是用最大似然估计来学习的，所以单个样本的后验概率为：&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/e95dc64a6bceb2e7315fab4d63432469.jpg&quot; width=&quot;342&quot; height=&quot;47&quot;&gt;&lt;/p&gt;
&lt;p&gt;到整个样本的后验概率：&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/7c493852ff0a2dc402c7244891da04e3.jpg&quot; width=&quot;328&quot; height=&quot;141&quot;&gt;&lt;/p&gt;
&lt;p&gt;其中：&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/bf2054e251ff31be712fe68a21c3455c.jpg&quot; width=&quot;265&quot; height=&quot;64&quot;&gt;&lt;/p&gt;
&lt;p&gt;通过对数进一步化简为：&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/09c1f28f198bc9578dc8b1e7fd13cd31.jpg&quot; width=&quot;449&quot; height=&quot;89&quot;&gt;&lt;/p&gt;
&lt;p&gt;3. 其实它的loss function为-l(θ)，因此我们需使loss function最小，可采用梯度下降法得到。梯度下降法公式为:&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/eeb652d69cf54cb4f6ef590abc28cf61.jpg&quot; width=&quot;475&quot; height=&quot;129&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/00d8651e45c69d31591c40994e238a24.jpg&quot; width=&quot;277&quot; height=&quot;44&quot;&gt;&lt;/p&gt;
&lt;p align=&quot;left&quot;&gt;&lt;em&gt;&lt;strong&gt;Logistic回归优点：&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p align=&quot;left&quot;&gt;1、实现简单；&lt;/p&gt;
&lt;p align=&quot;left&quot;&gt;2、分类时计算量非常小，速度很快，存储资源低；&lt;/p&gt;
&lt;p align=&quot;left&quot;&gt;&lt;em&gt;&lt;strong&gt;缺点：&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p align=&quot;left&quot;&gt;1、容易欠拟合，一般准确度不太高&lt;/p&gt;
&lt;p align=&quot;left&quot;&gt;2、只能处理两分类问题（在此基础上衍生出来的softmax可以用于多分类），且必须线性可分；&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;线性回归：&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;线性回归才是真正用于回归的，而不像logistic回归是用于分类，其基本思想是用梯度下降法对最小二乘法形式的误差函数进行优化，当然也可以用normal equation直接求得参数的解，结果为：&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/58c9408336b88e2ac396eb8f3f3b0b83.jpg&quot; width=&quot;175&quot; height=&quot;51&quot;&gt;&lt;/p&gt;
&lt;p&gt;而在LWLR（局部加权线性回归）中，参数的计算表达式为:&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/524d23873126bdc590eb2d0438736b7a.jpg&quot; width=&quot;246&quot; height=&quot;53&quot;&gt;&lt;/p&gt;
&lt;p&gt;因为此时优化的是：&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/c2d064a957f65a1fe4a7bb50f4b5db7e.jpg&quot; width=&quot;389&quot; height=&quot;78&quot;&gt;&lt;/p&gt;
&lt;p&gt;由此可见LWLR与LR不同，LWLR是一个非参数模型，因为每次进行回归计算都要遍历训练样本至少一次。&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;线性回归优点：&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;实现简单，计算简单；&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;缺点：&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;不能拟合非线性数据；&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;KNN&lt;/strong&gt;&lt;strong&gt;算法：&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;KNN即最近邻算法，其主要过程为：&lt;/p&gt;
&lt;p&gt;1. 计算训练样本和测试样本中每个样本点的距离（常见的距离度量有欧式距离，马氏距离等）；&lt;/p&gt;
&lt;p&gt;2. 对上面所有的距离值进行排序；&lt;/p&gt;
&lt;p&gt;3. 选前k个最小距离的样本；&lt;/p&gt;
&lt;p&gt;4. 根据这k个样本的标签进行投票，得到最后的分类类别；&lt;/p&gt;
&lt;p&gt;如何选择一个最佳的K值，这取决于数据。一般情况下，在分类时较大的K值能够减小噪声的影响。但会使类别之间的界限变得模糊。一个较好的K值可通过各种启发式技术来获取，比如，交叉验证。另外噪声和非相关性特征向量的存在会使K近邻算法的准确性减小。&lt;/p&gt;
&lt;p&gt;近邻算法具有较强的一致性结果。随着数据趋于无限，算法保证错误率不会超过贝叶斯算法错误率的两倍。对于一些好的K值，K近邻保证错误率不会超过贝叶斯理论误差率。&lt;/p&gt;
&lt;p&gt;注：马氏距离一定要先给出样本集的统计性质，比如均值向量，协方差矩阵等。关于马氏距离的介绍如下：&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/d3ff6709b6fffe23f47046a2dcdabd51.jpg&quot; width=&quot;978&quot; height=&quot;270&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;KNN算法的优点：&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;1. 思想简单，理论成熟，既可以用来做分类也可以用来做回归；&lt;/p&gt;
&lt;p&gt;2. 可用于非线性分类；&lt;/p&gt;
&lt;p&gt;3. 训练时间复杂度为O(n)；&lt;/p&gt;
&lt;p&gt;4. 准确度高，对数据没有假设，对outlier不敏感；&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;缺点：&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;1. 计算量大；&lt;/p&gt;
&lt;p&gt;2. 样本不平衡问题（即有些类别的样本数量很多，而其它样本的数量很少）；&lt;/p&gt;
&lt;p&gt;3. 需要大量的内存；&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;SVM&lt;/strong&gt;&lt;strong&gt;：&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;要学会如何使用libsvm以及一些参数的调节经验，另外需要理清楚svm算法的一些思路：&lt;/p&gt;
&lt;p&gt;1. svm中的最优分类面是对所有样本的几何裕量最大（为什么要选择最大间隔分类器，请从数学角度上说明？网易深度学习岗位面试过程中有被问到。答案就是几何间隔与样本的误分次数间存在关系：&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/0f747083f984f607d9d27dfa08c13d19.jpg&quot; width=&quot;113&quot; height=&quot;43&quot;&gt;，其中的分母就是样本到分类间隔距离，分子中的R是所有样本中的最长向量值），即：&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/8c8db41026303a2b79c57050f9395f3e.jpg&quot; width=&quot;364&quot; height=&quot;85&quot;&gt;&lt;/p&gt;
&lt;p&gt;经过一系列推导可得为优化下面原始目标：&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/3e9d73851204cb5036ebde77a8239384.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;2. 下面来看看拉格朗日理论：&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/39fb79050fd8a64b773662cfca03be14.jpg&quot; width=&quot;426&quot; height=&quot;157&quot;&gt;&lt;/p&gt;
&lt;p&gt;可以将1中的优化目标转换为拉格朗日的形式（通过各种对偶优化，KKD条件），最后目标函数为：&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/c1e40c1e06abc7c66da00e81b3bd7165.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;我们只需要最小化上述目标函数，其中的α为原始优化问题中的不等式约束拉格朗日系数。&lt;/p&gt;
&lt;p&gt;3. 对2中最后的式子分别w和b求导可得：&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/1cc9dd4cac61d47e71d65dab3e7faa8f.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/9b22731a6e39eac8bad74bd575e6b543.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;由上面第1式子可以知道，如果我们优化出了α，则直接可以求出w了，即模型的参数搞定。而上面第2个式子可以作为后续优化的一个约束条件。&lt;/p&gt;
&lt;p&gt;4. 对2中最后一个目标函数用对偶优化理论可以转换为优化下面的目标函数：&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/bc6f9efec5200fb36f5524048050d570.jpg&quot; width=&quot;394&quot; height=&quot;136&quot;&gt;&lt;/p&gt;
&lt;p&gt;而这个函数可以用常用的优化方法求得α，进而求得w和b。&lt;/p&gt;
&lt;p&gt;5. 按照道理，svm简单理论应该到此结束。不过还是要补充一点，即在预测时有：&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/f7acf5a6a5844a5149bce2dc0298a295.jpg&quot; width=&quot;291&quot; height=&quot;121&quot;&gt;&lt;/p&gt;
&lt;p&gt;那个尖括号我们可以用核函数代替，这也是svm经常和核函数扯在一起的原因。&lt;/p&gt;
&lt;p&gt;6. 最后是关于松弛变量的引入，因此原始的目标优化公式为：&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/5b769689705749a5db1621498a025cb2.jpg&quot; width=&quot;389&quot; height=&quot;114&quot;&gt;&lt;/p&gt;
&lt;p&gt;此时对应的对偶优化公式为：&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/16d9a4b55cc7e6cdb8f3db3712017c5f.jpg&quot; width=&quot;382&quot; height=&quot;132&quot;&gt;&lt;/p&gt;
&lt;p&gt;与前面的相比只是α多了个上界。&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;SVM算法优点：&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;可用于线性/非线性分类，也可以用于回归；&lt;/p&gt;
&lt;p&gt;低泛化误差；&lt;/p&gt;
&lt;p&gt;容易解释；&lt;/p&gt;
&lt;p&gt;计算复杂度较低；&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;缺点：&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;对参数和核函数的选择比较敏感；&lt;/p&gt;
&lt;p&gt;原始的SVM只比较擅长处理二分类问题；&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Boosting&lt;/strong&gt;&lt;strong&gt;：&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;主要以Adaboost为例，首先来看看Adaboost的流程图，如下：&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/4c1db6a8672bc7b05ca34e98759931ad.jpg&quot; width=&quot;366&quot; height=&quot;281&quot;&gt;&lt;/p&gt;
&lt;p&gt;从图中可以看到，在训练过程中我们需要训练出多个弱分类器（图中为3个），每个弱分类器是由不同权重的样本（图中为5个训练样本）训练得到（其中第一个弱分类器对应输入样本的权值是一样的），而每个弱分类器对最终分类结果的作用也不同，是通过加权平均输出的，权值见上图中三角形里面的数值。那么这些弱分类器和其对应的权值是怎样训练出来的呢？&lt;/p&gt;
&lt;p&gt;下面通过一个例子来简单说明。&lt;/p&gt;
&lt;p&gt;书中（machine learning in action）假设的是5个训练样本，每个训练样本的维度为2，在训练第一个分类器时5个样本的权重各为0.2. 注意这里样本的权值和最终训练的弱分类器组对应的权值α是不同的，样本的权重只在训练过程中用到，而α在训练过程和测试过程都有用到。&lt;/p&gt;
&lt;p&gt;现在假设弱分类器是带一个节点的简单决策树，该决策树会选择2个属性（假设只有2个属性）的一个，然后计算出这个属性中的最佳值用来分类。&lt;/p&gt;
&lt;p&gt;Adaboost的简单版本训练过程如下：&lt;/p&gt;
&lt;p&gt;1. 训练第一个分类器，样本的权值D为相同的均值。通过一个弱分类器，得到这5个样本（请对应书中的例子来看，依旧是machine learning in action）的分类预测标签。与给出的样本真实标签对比，就可能出现误差(即错误)。如果某个样本预测错误，则它对应的错误值为该样本的权重，如果分类正确，则错误值为0. 最后累加5个样本的错误率之和，记为ε。&lt;/p&gt;
&lt;p&gt;2. 通过ε来计算该弱分类器的权重α，公式如下：&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/9b96480798fabb5e957e460b2eccab4e.jpg&quot; width=&quot;165&quot; height=&quot;71&quot;&gt;&lt;/p&gt;
&lt;p&gt;3. 通过α来计算训练下一个弱分类器样本的权重D，如果对应样本分类正确，则减小该样本的权重，公式为：&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/e8bf668342cf3a21585a9baf15f8cb6c.jpg&quot; width=&quot;178&quot; height=&quot;79&quot;&gt;&lt;/p&gt;
&lt;p&gt;如果样本分类错误，则增加该样本的权重，公式为：&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/b496bd79b4a357b608628d041ec3172a.jpg&quot; width=&quot;166&quot; height=&quot;67&quot;&gt;&lt;/p&gt;
&lt;p&gt;4. 循环步骤1,2,3来继续训练多个分类器，只是其D值不同而已。&lt;/p&gt;
&lt;p&gt;测试过程如下：&lt;/p&gt;
&lt;p&gt;输入一个样本到训练好的每个弱分类中，则每个弱分类都对应一个输出标签，然后该标签乘以对应的α，最后求和得到值的符号即为预测标签值。&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;Boosting算法的优点：&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;低泛化误差；&lt;/p&gt;
&lt;p&gt;容易实现，分类准确率较高，没有太多参数可以调；&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;缺点：&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;对outlier比较敏感；&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;聚类：&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;根据聚类思想划分：&lt;/p&gt;
&lt;p&gt;1. 基于划分的聚类:&lt;/p&gt;
&lt;p&gt;K-means, k-medoids(每一个类别中找一个样本点来代表),CLARANS.&lt;/p&gt;
&lt;p&gt;k-means是使下面的表达式值最小：&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/07649c2bc46e9feb1e30be7962d5bb30.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;k-means算法的优点：&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;（1）k-means算法是解决聚类问题的一种经典算法，算法简单、快速。&lt;/p&gt;
&lt;p&gt;（2）对处理大数据集，该算法是相对可伸缩的和高效率的，因为它的复杂度大约是O(nkt)，其中n是所有对象的数目，k是簇的数目,t是迭代的次数。通常k&amp;lt;&amp;lt;n。这个算法通常局部收敛。&lt;/p&gt;
&lt;p&gt;（3）算法尝试找出使平方误差函数值最小的k个划分。当簇是密集的、球状或团状的，且簇与簇之间区别明显时，聚类效果较好。&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;缺点：&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;（1）k-平均方法只有在簇的平均值被定义的情况下才能使用，且对有些分类属性的数据不适合。&lt;/p&gt;
&lt;p&gt;（2）要求用户必须事先给出要生成的簇的数目k。&lt;/p&gt;
&lt;p&gt;（3）对初值敏感，对于不同的初始值，可能会导致不同的聚类结果。&lt;/p&gt;
&lt;p&gt;（4）不适合于发现非凸面形状的簇，或者大小差别很大的簇。&lt;/p&gt;
&lt;p&gt;（5）对于”噪声”和孤立点数据敏感，少量的该类数据能够对平均值产生极大影响。&lt;/p&gt;
&lt;p&gt;2. 基于层次的聚类：&lt;/p&gt;
&lt;p&gt;自底向上的凝聚方法，比如AGNES。&lt;/p&gt;
&lt;p&gt;自上向下的分裂方法，比如DIANA。&lt;/p&gt;
&lt;p&gt;3. 基于密度的聚类：&lt;/p&gt;
&lt;p&gt;DBSACN,OPTICS,BIRCH(CF-Tree),CURE.&lt;/p&gt;
&lt;p&gt;4. 基于网格的方法：&lt;/p&gt;
&lt;p&gt;STING, WaveCluster.&lt;/p&gt;
&lt;p&gt;5. 基于模型的聚类：&lt;/p&gt;
&lt;p&gt;EM,SOM,COBWEB.&lt;/p&gt;
&lt;p&gt;以上这些算法的简介可参考&lt;a href=&quot;http://baike.baidu.com/view/31801.htm&quot;&gt;聚类（百度百科）。&lt;/a&gt;&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;推荐系统：&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;推荐系统的实现主要分为两个方面：基于内容的实现和协同滤波的实现。&lt;/p&gt;
&lt;p&gt;基于内容的实现：&lt;/p&gt;
&lt;p&gt;不同人对不同电影的评分这个例子，可以看做是一个普通的回归问题，因此每部电影都需要提前提取出一个特征向量(即x值)，然后针对每个用户建模，即每个用户打的分值作为y值，利用这些已有的分值y和电影特征值x就可以训练回归模型了(最常见的就是线性回归)。这样就可以预测那些用户没有评分的电影的分数。（值得注意的是需对每个用户都建立他自己的回归模型）&lt;/p&gt;
&lt;p&gt;从另一个角度来看，也可以是先给定每个用户对某种电影的喜好程度（即权值），然后学出每部电影的特征，最后采用回归来预测那些没有被评分的电影。&lt;/p&gt;
&lt;p&gt;当然还可以是同时优化得到每个用户对不同类型电影的热爱程度以及每部电影的特征。具体可以参考Ng在coursera上的ml教程：&lt;a href=&quot;https://www.coursera.org/course/ml&quot;&gt;https://www.coursera.org/course/ml&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;基于协同滤波的实现：&lt;/p&gt;
&lt;p&gt;协同滤波（CF）可以看做是一个分类问题，也可以看做是矩阵分解问题。协同滤波主要是基于每个人自己的喜好都类似这一特征，它不依赖于个人的基本信息。比如刚刚那个电影评分的例子中，预测那些没有被评分的电影的分数只依赖于已经打分的那些分数，并不需要去学习那些电影的特征。&lt;/p&gt;
&lt;p&gt;SVD将矩阵分解为三个矩阵的乘积，公式如下所示：&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/70b15f485732721065229e4fbd513afc.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;中间的矩阵sigma为对角矩阵，对角元素的值为Data矩阵的奇异值(注意奇异值和特征值是不同的)，且已经从大到小排列好了。即使去掉特征值小的那些特征，依然可以很好的重构出原始矩阵。如下图所示：&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/98d03f9faad8dde7e137ec02b4c5979b.jpg&quot; width=&quot;431&quot; height=&quot;173&quot;&gt;&lt;/p&gt;
&lt;p&gt;其中更深的颜色代表去掉小特征值重构时的三个矩阵。&lt;/p&gt;
&lt;p&gt;果m代表商品的个数，n代表用户的个数，则U矩阵的每一行代表商品的属性，现在通过降维U矩阵（取深色部分）后，每一个商品的属性可以用更低的维度表示（假设为k维）。这样当新来一个用户的商品推荐向量X，则可以根据公式X’*U1*inv(S1)得到一个k维的向量，然后在V’中寻找最相似的那一个用户（相似度测量可用余弦公式等），根据这个用户的评分来推荐（主要是推荐新用户未打分的那些商品）。具体例子可以参考网页：&lt;a href=&quot;http://blog.csdn.net/wuyanyi/article/details/7964883&quot;&gt;SVD在推荐系统中的应用&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;另外关于SVD分解后每个矩阵的实际含义可以参考google吴军的《数学之美》一书（不过个人感觉吴军解释UV两个矩阵时好像弄反了，不知道大家怎样认为）。或者参考machine learning in action其中的svd章节。&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;pLSA:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;pLSA由LSA发展过来，而早期LSA的实现主要是通过SVD分解。pLSA的模型图如下：&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/5e66a98d735208a96823ac94a18b2049.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;公式中的意义如下：&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/f01167e3a30550ef633892b3a625c039.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;具体可以参考&lt;a href=&quot;http://bcmi.sjtu.edu.cn/ds/download.html&quot;&gt;2010龙星计划：机器学习中对应的主题模型那一讲&lt;/a&gt;&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;LDA&lt;/strong&gt;&lt;strong&gt;：&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;主题模型，概率图如下：&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/3b068ec4f6d0abbb1b29c4ac03c7201e.jpg&quot; width=&quot;257&quot; height=&quot;208&quot;&gt;&lt;/p&gt;
&lt;p&gt;和pLSA不同的是LDA中假设了很多先验分布，且一般参数的先验分布都假设为Dirichlet分布，其原因是共轭分布时先验概率和后验概率的形式相同。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;GDBT&lt;/strong&gt;&lt;strong&gt;：&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;GBDT(Gradient Boosting Decision Tree) 又叫 MART（Multiple Additive Regression Tree)，好像在阿里内部用得比较多（所以阿里算法岗位面试时可能会问到），它是一种迭代的决策树算法，该算法由多棵决策树组成，所有树的输出结果累加起来就是最终答案。它在被提出之初就和SVM一起被认为是泛化能力（generalization)较强的算法。近些年更因为被用于搜索排序的机器学习模型而引起大家关注。&lt;/p&gt;
&lt;p&gt;GBDT是回归树，不是分类树。其核心就在于，每一棵树是从之前所有树的残差中来学习的。为了防止过拟合，和Adaboosting一样，也加入了boosting这一项。&lt;/p&gt;
&lt;p&gt;关于GDBT的介绍可以可以参考：&lt;a href=&quot;http://hi.baidu.com/hehehehello/item/96cc42e45c16e7265a2d64ee&quot;&gt;GBDT（MART） 迭代决策树入门教程 | 简介&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Regularization:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;作用是（网易电话面试时有问到）：&lt;/p&gt;
&lt;p&gt;1. 数值上更容易求解；&lt;/p&gt;
&lt;p&gt;2. 特征数目太大时更稳定；&lt;/p&gt;
&lt;p&gt;3. 控制模型的复杂度，光滑性。复杂性越小且越光滑的目标函数泛化能力越强。而加入规则项能使目标函数复杂度减小，且更光滑。&lt;/p&gt;
&lt;p&gt;4. 减小参数空间；参数空间越小，复杂度越低。&lt;/p&gt;
&lt;p&gt;5. 系数越小，模型越简单，而模型越简单则泛化能力越强（Ng宏观上给出的解释）。&lt;/p&gt;
&lt;p&gt;6. 可以看出是权值的高斯先验。&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;异常检测：&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;可以估计样本的密度函数，对于新样本直接计算其密度，如果密度值小于某一阈值，则表示该样本异常。而密度函数一般采用多维的高斯分布。如果样本有n维，则每一维的特征都可以看作是符合高斯分布的，即使这些特征可视化出来不太符合高斯分布，也可以对该特征进行数学转换让其看起来像高斯分布，比如说x=log(x+c), x=x^(1/c)等。异常检测的算法流程如下：&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/8b8d0032d9f883a5caa2b93c4993c9f1.jpg&quot; width=&quot;482&quot; height=&quot;290&quot;&gt;&lt;/p&gt;
&lt;p&gt;其中的ε也是通过交叉验证得到的，也就是说在进行异常检测时，前面的p(x)的学习是用的无监督，后面的参数ε学习是用的有监督。那么为什么不全部使用普通有监督的方法来学习呢（即把它看做是一个普通的二分类问题）？主要是因为在异常检测中，异常的样本数量非常少而正常样本数量非常多，因此不足以学习到好的异常行为模型的参数，因为后面新来的异常样本可能完全是与训练样本中的模式不同。&lt;/p&gt;
&lt;p align=&quot;left&quot;&gt;另外，上面是将特征的每一维看成是相互独立的高斯分布，其实这样的近似并不是最好的，但是它的计算量较小，因此也常被使用。更好的方法应该是将特征拟合成多维高斯分布，这时有特征之间的相关性，但随之计算量会变复杂，且样本的协方差矩阵还可能出现不可逆的情况（主要在样本数比特征数小，或者样本特征维数之间有线性关系时）。&lt;/p&gt;
&lt;p align=&quot;left&quot;&gt;上面的内容可以参考Ng的&lt;a href=&quot;https://www.coursera.org/course/ml&quot;&gt;https://www.coursera.org/course/ml&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;EM&lt;/strong&gt;&lt;strong&gt;算法：&lt;/strong&gt;&lt;/p&gt;
&lt;p align=&quot;left&quot;&gt;有时候因为样本的产生和隐含变量有关（隐含变量是不能观察的），而求模型的参数时一般采用最大似然估计，由于含有了隐含变量，所以对似然函数参数求导是求不出来的，这时可以采用EM算法来求模型的参数的（对应模型参数个数可能有多个），EM算法一般分为2步：&lt;/p&gt;
&lt;p align=&quot;left&quot;&gt;E步：选取一组参数，求出在该参数下隐含变量的条件概率值；&lt;/p&gt;
&lt;p align=&quot;left&quot;&gt;M步：结合E步求出的隐含变量条件概率，求出似然函数下界函数（本质上是某个期望函数）的最大值。&lt;/p&gt;
&lt;p align=&quot;left&quot;&gt;重复上面2步直至收敛。&lt;/p&gt;
&lt;p align=&quot;left&quot;&gt;公式如下所示：&lt;/p&gt;
&lt;p align=&quot;left&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/cf4fd695f67722fb350963498da9b2de.jpg&quot; width=&quot;400&quot; height=&quot;150&quot;&gt;&lt;/p&gt;
&lt;p align=&quot;left&quot;&gt;M步公式中下界函数的推导过程：&lt;/p&gt;
&lt;p align=&quot;left&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/28ea30f56c7f1d9fbb6baa305cb8d99f.jpg&quot; width=&quot;434&quot; height=&quot;145&quot;&gt;&lt;/p&gt;
&lt;p align=&quot;left&quot;&gt;EM算法一个常见的例子就是GMM模型，每个样本都有可能由k个高斯产生，只不过由每个高斯产生的概率不同而已，因此每个样本都有对应的高斯分布（k个中的某一个），此时的隐含变量就是每个样本对应的某个高斯分布。&lt;/p&gt;
&lt;p align=&quot;left&quot;&gt;GMM的E步公式如下（计算每个样本对应每个高斯的概率）：&lt;/p&gt;
&lt;p align=&quot;left&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/213bce677dafac18f097a52b25caac73.jpg&quot; width=&quot;381&quot; height=&quot;71&quot;&gt;&lt;/p&gt;
&lt;p align=&quot;left&quot;&gt;更具体的计算公式为：&lt;/p&gt;
&lt;p align=&quot;left&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/73377ff094f50d0b5e7ca768fa68849d.jpg&quot; width=&quot;509&quot; height=&quot;56&quot;&gt;&lt;/p&gt;
&lt;p align=&quot;left&quot;&gt;M步公式如下（计算每个高斯的比重，均值，方差这3个参数）：&lt;/p&gt;
&lt;p align=&quot;left&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/0f680b38ad728be00844b8957f106e3a.jpg&quot; width=&quot;363&quot; height=&quot;192&quot;&gt;&lt;/p&gt;
&lt;p align=&quot;left&quot;&gt;关于EM算法可以参考&lt;a href=&quot;http://cs229.stanford.edu/&quot;&gt;Ng的cs229课程资料&lt;/a&gt;或者网易公开课：&lt;a href=&quot;http://v.163.com/special/opencourse/machinelearning.html&quot;&gt;斯坦福大学公开课 ：机器学习课程&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Apriori:&lt;/strong&gt;&lt;/p&gt;
&lt;p align=&quot;left&quot;&gt;Apriori是关联分析中比较早的一种方法，主要用来挖掘那些频繁项集合。其思想是：&lt;/p&gt;
&lt;p align=&quot;left&quot;&gt;1. 如果一个项目集合不是频繁集合，那么任何包含它的项目集合也一定不是频繁集合；&lt;/p&gt;
&lt;p align=&quot;left&quot;&gt;2. 如果一个项目集合是频繁集合，那么它的任何非空子集也是频繁集合；&lt;/p&gt;
&lt;p align=&quot;left&quot;&gt;Aprioir需要扫描项目表多遍，从一个项目开始扫描，舍去掉那些不是频繁的项目，得到的集合称为L，然后对L中的每个元素进行自组合，生成比上次扫描多一个项目的集合，该集合称为C，接着又扫描去掉那些非频繁的项目，重复…&lt;/p&gt;
&lt;p align=&quot;left&quot;&gt;看下面这个例子：&lt;/p&gt;
&lt;p align=&quot;left&quot;&gt;元素项目表格：&lt;/p&gt;
&lt;p align=&quot;left&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/7d14e78300024a017d3c4b0f8c888568.jpg&quot; width=&quot;448&quot; height=&quot;90&quot;&gt;&lt;/p&gt;
&lt;p align=&quot;left&quot;&gt;如果每个步骤不去掉非频繁项目集，则其扫描过程的树形结构如下：&lt;/p&gt;
&lt;p align=&quot;left&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/967c4b6d4992d5a3734b8bc523e50340.jpg&quot; width=&quot;252&quot; height=&quot;227&quot;&gt;&lt;/p&gt;
&lt;p align=&quot;left&quot;&gt;在其中某个过程中，可能出现非频繁的项目集，将其去掉（用阴影表示）为：&lt;/p&gt;
&lt;p align=&quot;left&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/ef1d35aacb6051a6d976398982a508d6.jpg&quot; width=&quot;261&quot; height=&quot;248&quot;&gt;&lt;/p&gt;
&lt;p align=&quot;left&quot;&gt;上面的内容主要参考的是machine learning in action这本书。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;FP Growth:&lt;/strong&gt;&lt;/p&gt;
&lt;p align=&quot;left&quot;&gt;FP Growth是一种比Apriori更高效的频繁项挖掘方法，它只需要扫描项目表2次。其中第1次扫描获得当个项目的频率，去掉不符合支持度要求的项，并对剩下的项排序。第2遍扫描是建立一颗FP-Tree(frequent-patten tree)。&lt;/p&gt;
&lt;p&gt;接下来的工作就是在FP-Tree上进行挖掘。&lt;/p&gt;
&lt;p align=&quot;left&quot;&gt;比如说有下表：&lt;/p&gt;
&lt;p align=&quot;left&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/d6380ab9e623777db1f74efd2fa86ca8.jpg&quot; width=&quot;339&quot; height=&quot;151&quot;&gt;&lt;/p&gt;
&lt;p align=&quot;left&quot;&gt;它所对应的FP_Tree如下：&lt;/p&gt;
&lt;p align=&quot;left&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/8fb9c89aa17e70bdc67c07d907718c9e.jpg&quot; width=&quot;405&quot; height=&quot;214&quot;&gt;&lt;/p&gt;
&lt;p align=&quot;left&quot;&gt;然后从频率最小的单项P开始，找出P的条件模式基，用构造FP_Tree同样的方法来构造P的条件模式基的FP_Tree，在这棵树上找出包含P的频繁项集。&lt;/p&gt;
&lt;p align=&quot;left&quot;&gt;依次从m,b,a,c,f的条件模式基上挖掘频繁项集，有些项需要递归的去挖掘，比较麻烦，比如m节点，具体的过程可以参考博客：&lt;a href=&quot;http://blog.sina.com.cn/s/blog_68ffc7a40100uebg.html&quot;&gt;Frequent Pattern 挖掘之二(FP Growth算法)&lt;/a&gt;，里面讲得很详细。&lt;/p&gt;
&lt;p align=&quot;left&quot;&gt;
&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;参考资料：&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Harrington, P. (2012). Machine Learning in Action, Manning Publications Co.&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://zh.wikipedia.org/wiki/%E6%9C%80%E8%BF%91%E9%84%B0%E5%B1%85%E6%B3%95&quot;&gt;最近邻算法（维基百科）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://zh.wikipedia.org/wiki/%E9%A9%AC%E6%B0%8F%E8%B7%9D%E7%A6%BB&quot;&gt;马氏距离（维基百科）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://baike.baidu.com/view/31801.htm&quot;&gt;聚类（百度百科）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://www.coursera.org/course/ml&quot;&gt;https://www.coursera.org/course/ml&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://blog.csdn.net/wuyanyi/article/details/7964883&quot;&gt;SVD在推荐系统中的应用&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;吴军 and 谷歌 (2012).数学之美, 人民邮电出版社.&lt;/li&gt;
&lt;li&gt;
&lt;a href=&quot;http://bcmi.sjtu.edu.cn/ds/download.html&quot;&gt;2010龙星计划：机器学习&lt;/a&gt;对应的视频教程：&lt;a href=&quot;http://pan.baidu.com/share/link?shareid=3053312914&amp;amp;uk=2620399451&quot;&gt; 2010龙星计划机器学习视频教程&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://hi.baidu.com/hehehehello/item/96cc42e45c16e7265a2d64ee&quot;&gt;GBDT（MART） 迭代决策树入门教程 | 简介&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://cs229.stanford.edu/&quot;&gt;Ng的cs229课程资料&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://v.163.com/special/opencourse/machinelearning.html&quot;&gt;斯坦福大学公开课 ：机器学习课程&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://blog.sina.com.cn/s/blog_68ffc7a40100uebg.html&quot;&gt;Frequent Pattern 挖掘之二(FP Growth算法)&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;


&lt;!-- div id=&quot;ad1&quot;&gt;
&lt;/div --&gt;


	


	

</description>
        <pubDate>Wed, 24 Sep 2014 00:00:00 +0800</pubDate>
        <link>http://iftti.com/posts/2014/2014-09-24-74438-8e0151f4b.html</link>
        <guid isPermaLink="true">http://iftti.com/posts/2014/2014-09-24-74438-8e0151f4b.html</guid>
        
        
        <category>jobbole</category>
        
      </item>
    
  </channel>
</rss>
