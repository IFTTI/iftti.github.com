<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>IT技术干货</title>
    <description>[IT技术干货iftti.com] @KernelHacks</description>
    <link>http://iftti.com/</link>
    <atom:link href="http://iftti.com/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Sat, 04 Oct 2014 12:33:26 +0800</pubDate>
    <lastBuildDate>Sat, 04 Oct 2014 12:33:26 +0800</lastBuildDate>
    <generator>Jekyll v2.2.0</generator>
    
      <item>
        <title>开源一个Key-Value存储工具类</title>
        <description>
&lt;h2&gt;前言&lt;/h2&gt;

&lt;p&gt;还记得大学刚学数据库那会儿，天真地以为世界上所有的存储都需要用数据库来做。后来毕业后，正值NOSQL流行，那时我在网易参与了网易微博的开发，我们当时使用了有道自己做的“BigTable”— OMAP来存储微博数据，那个时候才发现，其实Key-Value这种简单的存储也能搞定微博这类不太简单的存储逻辑。&lt;/p&gt;

&lt;p&gt;相比MYSQL，当数据量上千万后，NOSQL的优势体现出来了：对于海量数据，NOSQL在存取速度上没有任何影响，另外，天生的多备份和分布式，也说数据安全和扩容变得异常容易。&lt;/p&gt;

&lt;h2&gt;iOS端的尝试&lt;/h2&gt;

&lt;p&gt;后来我从后台转做iOS端的开发，我就尝试了在iOS端直接使用Key-Value式的存储。经过在粉笔网、猿题库、小猿搜题三个客户端中的尝试后，我发现Key-Value式的存储不但完全能够满足大多数移动端开发的需求，而且非常适合移动端采用。主要原因是：移动端存储的数据量不会很大：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;如果是单机的应用（例如效率工具Clear），用户自己一个人创建的数据最多也就上万条。&lt;/li&gt;
&lt;li&gt;如果是有服务端的应用（例如网易新闻，微博），那移动端通常不会保存全量的数据，每次会从服务器上获取数据，本地只是做一些内容的缓存而已，所以也不会有很大的数据量。&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;如果数据量不大的话，那么在iOS端使用最简单直接的Key-Value存储就能带来开发上的效率优势。它能保证：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Model层的代码编写简单，易于测试。&lt;/li&gt;
&lt;li&gt;由于Value是JSON格式，所以在做Model字段更改时，易于扩展和兼容。&lt;/li&gt;
&lt;/ol&gt;


&lt;h2&gt;实现方案&lt;/h2&gt;

&lt;p&gt;在存储引擎上，2年前我直接选择了Sqlite当做存储引擎，相当于每个数据库表只有Key，Value两个字段。后来，随着LevelDB的流行，业界也有一些应用采用了LevelDB来做iOS端的Key-Value存储引擎，例如开源的&lt;a href=&quot;https://github.com/viewfinderco/viewfinder&quot;&gt;ViewFinder&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;因为LevelDB本身并不是为移动端设计的，我担心它过于占用内存，我自己也没有看到业界有在移动端针对LevelDB做很详细的测试，连LevelDB的iOS端移植都不是官方做的。加上我自己写的基于Sqlite的Key-Value存储用着也没有什么问题，所以我也就一直没有更换成LevelDB。&lt;/p&gt;

&lt;h2&gt;开源&lt;/h2&gt;

&lt;p&gt;经过两年的使用和测试，我认为它非常好用，而且代码也非常简单，只有不到400行。所以现在开源分享给大家，这个项目叫&lt;code&gt;YTKKeyValueStore&lt;/code&gt;，项目在&lt;a href=&quot;https://github.com/yuantiku/YTKKeyValueStore&quot;&gt;这里&lt;/a&gt;。以下是一个简单的使用示例：&lt;/p&gt;

&lt;figure class=&quot;code&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;table&gt;&lt;tr&gt;
&lt;td class=&quot;gutter&quot;&gt;&lt;pre class=&quot;line-numbers&quot;&gt;&lt;span class=&quot;line-number&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;line-number&quot;&gt;2&lt;/span&gt;
&lt;span class=&quot;line-number&quot;&gt;3&lt;/span&gt;
&lt;span class=&quot;line-number&quot;&gt;4&lt;/span&gt;
&lt;span class=&quot;line-number&quot;&gt;5&lt;/span&gt;
&lt;span class=&quot;line-number&quot;&gt;6&lt;/span&gt;
&lt;span class=&quot;line-number&quot;&gt;7&lt;/span&gt;
&lt;span class=&quot;line-number&quot;&gt;8&lt;/span&gt;
&lt;span class=&quot;line-number&quot;&gt;9&lt;/span&gt;
&lt;span class=&quot;line-number&quot;&gt;10&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;code class=&quot;&quot;&gt;&lt;span class=&quot;line&quot;&gt;YTKKeyValueStore *store = [[YTKKeyValueStore alloc] initDBWithName:@&quot;test.db&quot;];
&lt;/span&gt;&lt;span class=&quot;line&quot;&gt;NSString *tableName = @&quot;user_table&quot;;
&lt;/span&gt;&lt;span class=&quot;line&quot;&gt;[store createTableWithName:tableName];
&lt;/span&gt;&lt;span class=&quot;line&quot;&gt;// 保存
&lt;/span&gt;&lt;span class=&quot;line&quot;&gt;NSString *key = @&quot;1&quot;;
&lt;/span&gt;&lt;span class=&quot;line&quot;&gt;NSDictionary *user = @{@&quot;id&quot;: @1, @&quot;name&quot;: @&quot;tangqiao&quot;, @&quot;age&quot;: @30};
&lt;/span&gt;&lt;span class=&quot;line&quot;&gt;[store putObject:user withId:key intoTable:tableName];
&lt;/span&gt;&lt;span class=&quot;line&quot;&gt;// 查询
&lt;/span&gt;&lt;span class=&quot;line&quot;&gt;NSDictionary *queryUser = [store getObjectById:key fromTable:tableName];
&lt;/span&gt;&lt;span class=&quot;line&quot;&gt;NSLog(@&quot;query data result: %@&quot;, queryUser);&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;&lt;/figure&gt;


&lt;h2&gt;其它&lt;/h2&gt;

&lt;p&gt;两年前写过不少测试用例，后来给弄丢了，所以现在开项项目中还没有测试用例。由于时间关系，更详细的使用说明稍后会更新到项目中。&lt;/p&gt;

</description>
        <pubDate>Fri, 03 Oct 2014 00:00:00 +0800</pubDate>
        <link>http://iftti.com/posts/2014/2014-10-03-opensouce-a-key-value-storage-tool-07b219e11.html</link>
        <guid isPermaLink="true">http://iftti.com/posts/2014/2014-10-03-opensouce-a-key-value-storage-tool-07b219e11.html</guid>
        
        
        <category>devtang</category>
        
      </item>
    
      <item>
        <title>Mojolicious 应用的自定义子命令</title>
        <description>

  
  &lt;div style=&quot;background-color: #FFF;&quot;&gt;
    &lt;p&gt;Mojolicious 框架开发应用的时候，可以跟 RoR 一样通过一系列子命令简化很多复杂操作。最简单的来说，就是快速生成整个 web 项目目录：&lt;code&gt;mojo generate youapp&lt;/code&gt;。更多子命令见：&lt;a href=&quot;http://cpan.php-oa.com/perldoc/Mojolicious/Commands&quot;&gt;http://cpan.php-oa.com/perldoc/Mojolicious/Commands&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;其实我们还可以自己扩展这个子命令方式，实现自己的子命令。如果打算继续使用 &lt;code&gt;mojo subcommand&lt;/code&gt; 的方式，那就把自己的子命令模块叫做 &lt;code&gt;Mojolicious::Command::yourcommand&lt;/code&gt;，而如果打算在自己的名字空间下使用，比如叫 &lt;code&gt;MyApp::Command::mycommand&lt;/code&gt;，那么需要在 &lt;code&gt;MyApp.pm&lt;/code&gt; 里加一行代码，设置一下名字空间：&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-perl&quot; data-lang=&quot;perl&quot;&gt;&lt;span class=&quot;k&quot;&gt;sub &lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;startup&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;my&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$self&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;shift&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
        &lt;span class=&quot;nb&quot;&gt;push&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;@&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;commands&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;namespaces&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&#39;MyApp::Command&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
        &lt;span class=&quot;o&quot;&gt;...&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;};&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;然后就可以写自己的 &lt;code&gt;MyApp::Command::mycommand&lt;/code&gt; 了：&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-perl&quot; data-lang=&quot;perl&quot;&gt;&lt;span class=&quot;nb&quot;&gt;package&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;MyApp::Command::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mycommand&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;use&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;strict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;use&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;warnings&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;use&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;Mojo::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Base&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&#39;Mojolicious::Command&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;use&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;Mojo::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;UserAgent&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;has&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;usage&lt;/span&gt;       &lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;usage: $0 migratint [username] [dashboards...]\n&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;has&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;description&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;kibana-int index migration for auth users\n&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;has&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ua&lt;/span&gt;          &lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;sub &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;Mojo::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;UserAgent&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;};&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;sub &lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;run&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;my&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$user&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;@_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;my&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$config&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;app&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;my&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$ua&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ua&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;o&quot;&gt;...&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;大致就是这样：&lt;/p&gt;
&lt;p&gt;继承 &lt;strong&gt;Mojolicious::Command&lt;/strong&gt; 类。这样就会有 usage 和 description 两个属性和 run 方法。&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;usage 属性用来在你执行 &lt;code&gt;script/myapp help mycommand&lt;/code&gt; 的时候输出信息；&lt;/li&gt;
  &lt;li&gt;description 属性用来在你执行 &lt;code&gt;script/myapp help&lt;/code&gt; 罗列全部可用子命令的时候描述该命令的作用；&lt;/li&gt;
  &lt;li&gt;run 方法是命令的入口函数。命令行后面的参数都会传递给 run 方法。如果你的子命令需要复杂处理，这里依然可以用 &lt;a href=&quot;https://metacpan.org/pod/Getopt::Long#Parsing-options-from-an-arbitrary-array&quot;&gt;GetOpt::Long&lt;/a&gt; 模块中的 &lt;code&gt;GetOptionsFromArray&lt;/code&gt; 函数处理。&lt;/li&gt;
&lt;/ul&gt;
    &lt;hr&gt;
    
    &lt;hr&gt;
  &lt;!-- UY BEGIN --&gt;


&lt;!-- UY END --&gt;
  &lt;/div&gt;

</description>
        <pubDate>Wed, 01 Oct 2014 00:00:00 +0800</pubDate>
        <link>http://iftti.com/posts/2014/2014-10-01-custom-mojolicious-app-command-014a96277.html</link>
        <guid isPermaLink="true">http://iftti.com/posts/2014/2014-10-01-custom-mojolicious-app-command-014a96277.html</guid>
        
        
        <category>chenlinux</category>
        
      </item>
    
      <item>
        <title>实现键值对存储（二）：以现有键值对存储为模型</title>
        <description>

					
		
&lt;!-- div id=&quot;ad1&quot;&gt;
&lt;script type=&quot;text/javascript&quot;&gt;
google_ad_client = &quot;ca-pub-7056282119617872&quot;;
google_ad_slot = &quot;6645040531&quot;;
google_ad_width = 300;
google_ad_height = 250;
&lt;/script&gt;
&lt;script type=&quot;text/javascript&quot;
src=&quot;http://pagead2.googlesyndication.com/pagead/show_ads.js&quot;&gt;
&lt;/script&gt;
&lt;/div&gt;
&lt;br/ --&gt;

&lt;p&gt;本文中，开头我会解释使用现有模型而非重头开始此项目的原因。我会阐述一系列选择键值对存储模型的标准。最后我将对一些广为人知的键值对存储项目做一个概述，并用这些标准选择其中一些作为模型。本文将包含：&lt;/p&gt;
&lt;p&gt;1. 不重新发明轮子&lt;br&gt;
2. 备选模型和选择标准&lt;br&gt;
3. 选择的键值对存储的概述&lt;br&gt;
4. 参考文献&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h3 align=&quot;left&quot;&gt;1. 不重新发明轮子&lt;b&gt;&lt;/b&gt;
&lt;/h3&gt;
&lt;p&gt;键值对存储已经被人们唱好至少30年了&lt;a href=&quot;http://codecapsule.com/2012/12/03/implementing-a-key-value-store-part-2-using-existing-key-value-stores-as-models/#ref_1&quot;&gt;[1]&lt;/a&gt;。最著名的一个项目是DBM，Kenneth Thompson为Unix第七版编写的最早的数据库管理器并在1979年发布&lt;a href=&quot;http://codecapsule.com/2012/12/03/implementing-a-key-value-store-part-2-using-existing-key-value-stores-as-models/#ref_2&quot;&gt;[2]&lt;/a&gt;。工程师们遇到了和这些数据库系统相关的一些问题，并选择或放弃了各种设计和数据结构的想法。对实际生活中的问题进行试验并从中学习。如果不考虑他们的工作并从头开始是很愚蠢的，只会重复他们之前所犯过的错误。John Gall的系统学中的Gall定理：&lt;/p&gt;
&lt;p style=&quot;padding-left: 30px;&quot;&gt;&lt;span style=&quot;color: #888888;&quot;&gt;任何可以运作的复杂系统都是从可以运作的简单系统发展而来的。其逆命题同样是真命题：由无法正常运作的系统设计而来的复杂系统是不可能正常运作的。你必须重头再来，从一个可运作的简单系统开始。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;这段引述为我的键值对存储项目开发带来了两个基础思想。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;1. 使用模型。&lt;/strong&gt;我需要识别出那些存在了一段时间的键值对存储，甚至更进一步，先前成功的键值对存储的继任者。这是其可靠设计的证明，并随着时间在迭代中凝练。这些选择过的建筑的存储应该作为我现在正在工作的项目的模型。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;2.起点小。&lt;/strong&gt;这个项目的第一版必须小且简单，这样它的设计就能简单的测试并通过。如果需要的话，改进和额外功能必须在后续版本中加入。&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h3 align=&quot;left&quot;&gt;2. 待选模型和选择标准&lt;b&gt;&lt;/b&gt;
&lt;/h3&gt;
&lt;p&gt;在对键值对存储和NoSQL数据库做过一点研究后，我决定将下面的几个作为进一步选择的选项：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;DBM&lt;/li&gt;
&lt;li&gt;Berkeley DB&lt;/li&gt;
&lt;li&gt;Kyoto Cabinet&lt;/li&gt;
&lt;li&gt;Memcached and MemcacheDB&lt;/li&gt;
&lt;li&gt;LevelDB&lt;/li&gt;
&lt;li&gt;MongoDB&lt;/li&gt;
&lt;li&gt;Redis&lt;/li&gt;
&lt;li&gt;OpenLDAP&lt;/li&gt;
&lt;li&gt;SQLite&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;选择标准如下:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;我想使用面向对象编程来创建键值对存储，所以在设计上，我必须从由面向对象语言编写的项目中汲取灵感。&lt;/li&gt;
&lt;li&gt;至于底层数据结构，我想要一个存在硬盘上的哈希表，于是我需要选择一个提供读写信息到硬盘上的方法的项目。&lt;/li&gt;
&lt;li&gt;我同样想让这个数据存储能够有网络接入。&lt;/li&gt;
&lt;li&gt;我不需要查询引擎或者方法来访问结构化的数据.&lt;/li&gt;
&lt;li&gt;不必完全支持ACID规范。&lt;/li&gt;
&lt;li&gt;鉴于这个项目是我自己弄的，我想使用那些由小团队实现的项目模型，理想情况下是一两个人。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;3. 所选键值对的概览&lt;/h3&gt;
&lt;p&gt;三个获选的模型是Berkeley DB、Kyoto Cabinet 和LevelDB。Berkeley DB和Kyoto Cabinet作为DBM的继任者有着相同的历史。此外，Berkeley DB 和 Kyoto Cabinet 并非“初版”。这表示他俩与其他初次实现的键值对存储项目比较更加可靠。LevelDB则更加现代，并基于LSM树的数据结构，其对于哈希表模式来说是无用的。然而其代码是我见过最干净的。这三个项目都是由一两个人开发的。下面是他们各自的详细信息。&lt;/p&gt;
&lt;h4 align=&quot;left&quot;&gt;Berkeley DB&lt;/h4&gt;
&lt;p&gt;Berkeley DB的开发始于1986年，这表示我开始写这篇文章的时候它已经存在了26年了。Berkeley DB是作为DBM的继任者而开发的，并实现了一个哈希表。第一版是由Margo Seltzer &lt;a href=&quot;http://codecapsule.com/2012/12/03/implementing-a-key-value-store-part-2-using-existing-key-value-stores-as-models/#ref_22&quot;&gt;[22]&lt;/a&gt; 和 Ozan Yigit &lt;a href=&quot;http://codecapsule.com/2012/12/03/implementing-a-key-value-store-part-2-using-existing-key-value-stores-as-models/#ref_23&quot;&gt;[23]&lt;/a&gt; 在加州大学伯克利分校的时候编写的。这个项目后来被Oracle获得，并由其继续开发。&lt;/p&gt;
&lt;p&gt;Berkeley DB最初是由C实现的，并且现在仍然是只用C。其通过增量过程开发的，就是说在每个主版本增加新的功能。Berkeley DB从一个简单的键值对存储，进化到管理并行访问、事务及复原、及同步功能&lt;a href=&quot;http://codecapsule.com/2012/12/03/implementing-a-key-value-store-part-2-using-existing-key-value-stores-as-models/#ref_4&quot;&gt;[4]&lt;/a&gt;。Berkeley DB的使用非常广泛，有着数亿已部署的拷贝&lt;a href=&quot;http://codecapsule.com/2012/12/03/implementing-a-key-value-store-part-2-using-existing-key-value-stores-as-models/#ref_5&quot;&gt;[5]&lt;/a&gt;，这是可以相信其架构及其可靠的证据。关于其设计的更多信息可以在“&lt;i&gt;Berkeley DB Programmer’s Reference Guide&lt;/i&gt;”&lt;a href=&quot;http://codecapsule.com/2012/12/03/implementing-a-key-value-store-part-2-using-existing-key-value-stores-as-models/#ref_6&quot;&gt;[6]&lt;/a&gt; 的介绍和“&lt;i&gt;The Architecture of Open Source Applications, Volume 1&lt;/i&gt;” &lt;a href=&quot;http://codecapsule.com/2012/12/03/implementing-a-key-value-store-part-2-using-existing-key-value-stores-as-models/#ref_5&quot;&gt;[5]&lt;/a&gt;的开头中找到。&lt;/p&gt;
&lt;h4&gt;Kyoto Cabinet&lt;/h4&gt;
&lt;p&gt;Kyoto Cabinet在2009年由Mikio Hirabayashi &lt;a href=&quot;http://codecapsule.com/2012/12/03/implementing-a-key-value-store-part-2-using-existing-key-value-stores-as-models/#ref_24&quot;&gt;[24]&lt;/a&gt; 引进。其现在仍在积极进化中。Kyoto Cabinet是同一个作者的其它键值对存储：Tokyo Cabinet (2007发布) 和QDBM (2003发布, 2000开始)的继任者。QDBM打算作为DBM的高性能继任者&lt;a href=&quot;http://codecapsule.com/2012/12/03/implementing-a-key-value-store-part-2-using-existing-key-value-stores-as-models/#ref_7&quot;&gt;[7]&lt;/a&gt;。Kyoto Cabinet尤其有意思，因为它有着DBM的纯正血统，并且它的作者在键值对存储方向工作12年了。在浸淫三个键值对存储这么多年之后，没有理由怀疑作者有着对结构需求的坚实理解，以及随之的对性能瓶颈的成因的极强认识。&lt;/p&gt;
&lt;p&gt;Kyoto Cabinet是由C++实现的，并实现了一个哈希表，一个B+树，以及其他一些深奥的数据结构。其同样提供了出色的性能&lt;a href=&quot;http://codecapsule.com/2012/12/03/implementing-a-key-value-store-part-2-using-existing-key-value-stores-as-models/#ref_16&quot;&gt;[16]&lt;/a&gt;。然而，因其内部参数的原因，似乎有些性能问题。的确，很多人报道说只要数据条目的数量保持在某一特定的阈值（正比于桶数组大小，其由创建数据库文件时的参数所确定）以下，性能就很好。一旦超过这个阈值，性能似乎急剧下降&lt;a href=&quot;http://codecapsule.com/2012/12/03/implementing-a-key-value-store-part-2-using-existing-key-value-stores-as-models/#ref_18&quot;&gt;[18]&lt;/a&gt;&lt;a href=&quot;http://codecapsule.com/2012/12/03/implementing-a-key-value-store-part-2-using-existing-key-value-stores-as-models/#ref_19&quot;&gt;[19]&lt;/a&gt;。Tokyo Cabinet &lt;a href=&quot;http://codecapsule.com/2012/12/03/implementing-a-key-value-store-part-2-using-existing-key-value-stores-as-models/#ref_20&quot;&gt;[20]&lt;/a&gt; &lt;a href=&quot;http://codecapsule.com/2012/12/03/implementing-a-key-value-store-part-2-using-existing-key-value-stores-as-models/#ref_21&quot;&gt;[21]&lt;/a&gt; 中也有相同的问题。这表示如果某项目的需求在数据库使用的时候改变，你可能会遇到严重的问题。而我们都知道，软件中的改变是如此的频繁。&lt;/p&gt;
&lt;h4&gt;LevelDB&lt;/h4&gt;
&lt;p&gt;LevelDB是由Google职员Jeffrey Dean &lt;a href=&quot;http://codecapsule.com/2012/12/03/implementing-a-key-value-store-part-2-using-existing-key-value-stores-as-models/#ref_8&quot;&gt;[8]&lt;/a&gt; 和 Sanjay Ghemawat &lt;a href=&quot;http://codecapsule.com/2012/12/03/implementing-a-key-value-store-part-2-using-existing-key-value-stores-as-models/#ref_9&quot;&gt;[9]&lt;/a&gt; 开发，他们为Google传说中的基础建设项目MapReduce和BigTable工作。基于Dean和Ghemawat在在Google工作时获得的大规模问题上的经验，他们很有可能很了解他们正在做的东西。和大多数键值对存储项目相比，LevelDB有一个很有意思的不同点就是它不用哈希表或者B-树作为底层数据结构，而是基于一个日志结构的合并树&lt;a href=&quot;http://codecapsule.com/2012/12/03/implementing-a-key-value-store-part-2-using-existing-key-value-stores-as-models/#ref_12&quot;&gt;[12]&lt;/a&gt;。LSM结构据说是为SSD硬盘优化的&lt;a href=&quot;http://codecapsule.com/2012/12/03/implementing-a-key-value-store-part-2-using-existing-key-value-stores-as-models/#ref_13&quot;&gt;[13]&lt;/a&gt;。你可以在这个博客High Scalability blog &lt;a href=&quot;http://codecapsule.com/2012/12/03/implementing-a-key-value-store-part-2-using-existing-key-value-stores-as-models/#ref_17&quot;&gt;[17]&lt;/a&gt;找到成吨的关于LevelDB的信息。&lt;/p&gt;
&lt;p&gt;LevelDB是由C++实现，2011年发布，并设计作为高级存储系统的一部分&lt;a href=&quot;http://codecapsule.com/2012/12/03/implementing-a-key-value-store-part-2-using-existing-key-value-stores-as-models/#ref_10&quot;&gt;[10]&lt;/a&gt;。IndexedDB HTML5 API在Chrome将来版本的实现将使用LevelDB &lt;a href=&quot;http://codecapsule.com/2012/12/03/implementing-a-key-value-store-part-2-using-existing-key-value-stores-as-models/#ref_10&quot;&gt;[10]&lt;/a&gt; &lt;a href=&quot;http://codecapsule.com/2012/12/03/implementing-a-key-value-store-part-2-using-existing-key-value-stores-as-models/#ref_11&quot;&gt;[11]&lt;/a&gt;。其性能决定于&lt;i&gt;特定的工作负载&lt;/i&gt;，就像作者提供的基准测试中显示的那样&lt;a href=&quot;http://codecapsule.com/2012/12/03/implementing-a-key-value-store-part-2-using-existing-key-value-stores-as-models/#ref_14&quot;&gt;[14]&lt;/a&gt;。然而，Andy Twigg在Acunu的另外一个基于商用SSD的基准测试显示出，如果数据的条数超过1e6（1百万），并向1e9（10亿）前进的时候，性能将会显著下降&lt;a href=&quot;http://codecapsule.com/2012/12/03/implementing-a-key-value-store-part-2-using-existing-key-value-stores-as-models/#ref_15&quot;&gt;[15]&lt;/a&gt;。因此似乎LevelDB似乎并不是重工作负载或像实际后端项目需求那样的大数据库最好的选择。&lt;/p&gt;
&lt;p&gt;但这其实并不重要，对于我来说，LevelDB最好的部分不是其性能而是其架构。看它的源代码和东西组织的方式，那是纯粹的美。所有的东西都很清晰、简单、条理分明。访问LevelDB的源代码并把它作为模范是创建出色代码的绝好机遇。&lt;/p&gt;
&lt;h4&gt;那些没选中的键值对存储是什么情况？&lt;/h4&gt;
&lt;p&gt;没有选择其他键值对存储的原因并不表示我完全抛弃他们。我会记得他们并可能偶尔使用他们结构中元素。但是，当前项目受到这些键值对项目影响不会像已选择的这些那么多。&lt;/p&gt;
&lt;h3&gt;4. 参考文献&lt;/h3&gt;
&lt;p align=&quot;left&quot;&gt;[1] &lt;a href=&quot;http://blog.knuthaugen.no/2010/03/a-brief-history-of-nosql.html&quot; target=&quot;_blank&quot;&gt;http://blog.knuthaugen.no/2010/03/a-brief-history-of-nosql.html&lt;/a&gt;&lt;br&gt;
[2] &lt;a href=&quot;http://en.wikipedia.org/wiki/Dbm&quot; target=&quot;_blank&quot;&gt;http://en.wikipedia.org/wiki/Dbm&lt;/a&gt;&lt;br&gt;
[3] &lt;a href=&quot;http://en.wikipedia.org/wiki/Systemantics&quot; target=&quot;_blank&quot;&gt;http://en.wikipedia.org/wiki/Systemantics&lt;/a&gt;&lt;br&gt;
[4] &lt;a href=&quot;http://en.wikipedia.org/wiki/Berkeley_DB#Origin&quot; target=&quot;_blank&quot;&gt;http://en.wikipedia.org/wiki/Berkeley_DB#Origin&lt;/a&gt;&lt;br&gt;
[5] &lt;a href=&quot;http://www.aosabook.org/en/bdb.html&quot; target=&quot;_blank&quot;&gt;http://www.aosabook.org/en/bdb.html&lt;/a&gt;&lt;br&gt;
[6] &lt;a href=&quot;http://docs.oracle.com/cd/E17076_02/html/programmer_reference/intro.html&quot; target=&quot;_blank&quot;&gt;http://docs.oracle.com/cd/E17076_02/html/programmer_reference/intro.html&lt;/a&gt;&lt;br&gt;
[7] &lt;a href=&quot;http://fallabs.com/qdbm/&quot; target=&quot;_blank&quot;&gt;http://fallabs.com/qdbm/&lt;/a&gt;&lt;br&gt;
[8] &lt;a href=&quot;http://research.google.com/people/jeff/&quot; target=&quot;_blank&quot;&gt;http://research.google.com/people/jeff/&lt;/a&gt;&lt;br&gt;
[9] &lt;a href=&quot;http://research.google.com/pubs/SanjayGhemawat.html&quot; target=&quot;_blank&quot;&gt;http://research.google.com/pubs/SanjayGhemawat.html&lt;/a&gt;&lt;br&gt;
[10] &lt;a href=&quot;http://google-opensource.blogspot.com/2011/07/leveldb-fast-persistent-key-value-store.html&quot; target=&quot;_blank&quot;&gt;http://google-opensource.blogspot.com/2011/07/leveldb-fast-persistent-key-value-store.html&lt;/a&gt;&lt;br&gt;
[11] &lt;a href=&quot;http://www.w3.org/TR/IndexedDB/&quot; target=&quot;_blank&quot;&gt;http://www.w3.org/TR/IndexedDB/&lt;/a&gt;&lt;br&gt;
[12] &lt;a href=&quot;http://www.igvita.com/2012/02/06/sstable-and-log-structured-storage-leveldb/&quot; target=&quot;_blank&quot;&gt;http://www.igvita.com/2012/02/06/sstable-and-log-structured-storage-leveldb/&lt;/a&gt;&lt;br&gt;
[13] &lt;a href=&quot;http://www.acunu.com/2/post/2011/04/log-file-systems-and-ssds-made-for-each-other.html&quot; target=&quot;_blank&quot;&gt;http://www.acunu.com/2/post/2011/04/log-file-systems-and-ssds-made-for-each-other.html&lt;/a&gt;&lt;br&gt;
[14] &lt;a href=&quot;http://leveldb.googlecode.com/svn/trunk/doc/benchmark.html&quot; target=&quot;_blank&quot;&gt;http://leveldb.googlecode.com/svn/trunk/doc/benchmark.html&lt;/a&gt;&lt;br&gt;
[15] &lt;a href=&quot;http://www.acunu.com/2/post/2011/08/benchmarking-leveldb.html&quot; target=&quot;_blank&quot;&gt;http://www.acunu.com/2/post/2011/08/benchmarking-leveldb.html&lt;/a&gt;&lt;br&gt;
[16] &lt;a href=&quot;http://blog.creapptives.com/post/8330476086/leveldb-vs-kyoto-cabinet-my-findings&quot; target=&quot;_blank&quot;&gt;http://blog.creapptives.com/post/8330476086/leveldb-vs-kyoto-cabinet-my-findings&lt;/a&gt;&lt;br&gt;
[17] &lt;a href=&quot;http://highscalability.com/blog/2011/8/10/leveldb-fast-and-lightweight-keyvalue-database-from-the-auth.html&quot; target=&quot;_blank&quot;&gt;http://highscalability.com/blog/2011/8/10/leveldb-fast-and-lightweight-keyvalue-database-from-the-auth.html&lt;/a&gt;&lt;br&gt;
[18] &lt;a href=&quot;http://stackoverflow.com/questions/13054852/kyoto-cabinet-berkeley-db-hash-table-size-limitations&quot; target=&quot;_blank&quot;&gt;http://stackoverflow.com/questions/13054852/kyoto-cabinet-berkeley-db-hash-table-size-limitations&lt;/a&gt;&lt;br&gt;
[19] &lt;a href=&quot;https://groups.google.com/forum/#!topic/tokyocabinet-users/Bzp4fLbmcDw/discussion&quot; target=&quot;_blank&quot;&gt;https://groups.google.com/forum/#!topic/tokyocabinet-users/Bzp4fLbmcDw/discussion&lt;/a&gt;&lt;br&gt;
[20] &lt;a href=&quot;http://stackoverflow.com/questions/1051847/why-does-tokyo-tyrant-slow-down-exponentially-even-after-adjusting-bnum&quot; target=&quot;_blank&quot;&gt;http://stackoverflow.com/questions/1051847/why-does-tokyo-tyrant-slow-down-exponentially-even-after-adjusting-bnum&lt;/a&gt;&lt;br&gt;
[21] &lt;a href=&quot;https://groups.google.com/forum/#!topic/tokyocabinet-users/1E06DFQM8mI/discussion&quot; target=&quot;_blank&quot;&gt;https://groups.google.com/forum/#!topic/tokyocabinet-users/1E06DFQM8mI/discussion&lt;/a&gt;&lt;br&gt;
[22] &lt;a href=&quot;http://www.eecs.harvard.edu/margo/&quot; target=&quot;_blank&quot;&gt;http://www.eecs.harvard.edu/margo/&lt;/a&gt;&lt;br&gt;
[23] &lt;a href=&quot;http://www.cse.yorku.ca/~oz/&quot; target=&quot;_blank&quot;&gt;http://www.cse.yorku.ca/~oz/&lt;/a&gt;&lt;br&gt;
[24] &lt;a href=&quot;http://fallabs.com/mikio/profile.html&quot; target=&quot;_blank&quot;&gt;http://fallabs.com/mikio/profile.html&lt;/a&gt;&lt;/p&gt;


&lt;!-- div id=&quot;ad1&quot;&gt;
&lt;/div --&gt;


	


	

</description>
        <pubDate>Mon, 29 Sep 2014 00:00:00 +0800</pubDate>
        <link>http://iftti.com/posts/2014/2014-09-29-77750-1173d7c42.html</link>
        <guid isPermaLink="true">http://iftti.com/posts/2014/2014-09-29-77750-1173d7c42.html</guid>
        
        
        <category>jobbole</category>
        
      </item>
    
      <item>
        <title>大型网站系统架构的演化</title>
        <description>

					
		
&lt;!-- div id=&quot;ad1&quot;&gt;
&lt;script type=&quot;text/javascript&quot;&gt;
google_ad_client = &quot;ca-pub-7056282119617872&quot;;
google_ad_slot = &quot;6645040531&quot;;
google_ad_width = 300;
google_ad_height = 250;
&lt;/script&gt;
&lt;script type=&quot;text/javascript&quot;
src=&quot;http://pagead2.googlesyndication.com/pagead/show_ads.js&quot;&gt;
&lt;/script&gt;
&lt;/div&gt;
&lt;br/ --&gt;

&lt;p&gt;前言&lt;/p&gt;
&lt;p&gt;一个成熟的大型网站（如淘宝、京东等）的系统架构并不是开始设计就具备完整的高性能、高可用、安全等特性，它总是随着用户量的增加，业务功能的扩展逐渐演变完善的，在这个过程中，开发模式、技术架构、设计思想也发生了很大的变化，就连技术人员也从几个人发展到一个部门甚至一条产品线。所以成熟的系统架构是随业务扩展而完善出来的，并不是一蹴而就；不同业务特征的系统，会有各自的侧重点，例如淘宝，要解决海量的商品信息的搜索、下单、支付，例如腾讯，要解决数亿的用户实时消息传输，百度它要处理海量的搜索请求，他们都有各自的业务特性，系统架构也有所不同。尽管如此我们也可以从这些不同的网站背景下，找出其中共用的技术，这些技术和手段可以广泛运行在大型网站系统的架构中，下面就通过介绍大型网站系统的演化过程，来认识这些技术和手段。&lt;/p&gt;
&lt;p&gt;一、最开始的网站架构&lt;/p&gt;
&lt;p&gt;最初的架构，应用程序、数据库、文件都部署在一台服务器上，如图：&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/348e8a744ebf2c6a9a2abd6a024d6ee1.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;二、应用、数据、文件分离&lt;/p&gt;
&lt;p&gt;随着业务的扩展，一台服务器已经不能满足性能需求，故将应用程序、数据库、文件各自部署在独立的服务器上，并且根据服务器的用途配置不同的硬件，达到最佳的性能效果。&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/a8627bac155086fb738b7832a53eb31d.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;三、利用缓存改善网站性能&lt;/p&gt;
&lt;p&gt;在硬件优化性能的同时，同时也通过软件进行性能优化，在大部分的网站系统中，都会利用缓存技术改善系统的性能，使用缓存主要源于热点数据的存在，大部分网站访问都遵循28原则（即80%的访问请求，最终落在20%的数据上），所以我们可以对热点数据进行缓存，减少这些数据的访问路径，提高用户体验。&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/144837c8716f0480166a240c60eec197.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;缓存实现常见的方式是本地缓存、分布式缓存。当然还有CDN、反向代理等，这个后面再讲。本地缓存，顾名思义是将数据缓存在应用服务器本地，可以存在内存中，也可以存在文件，OSCache就是常用的本地缓存组件。本地缓存的特点是速度快，但因为本地空间有限所以缓存数据量也有限。分布式缓存的特点是，可以缓存海量的数据，并且扩展非常容易，在门户类网站中常常被使用，速度按理没有本地缓存快，常用的分布式缓存是Memcached、Redis。&lt;/p&gt;
&lt;p&gt;四、使用集群改善应用服务器性能&lt;/p&gt;
&lt;p&gt;应用服务器作为网站的入口，会承担大量的请求，我们往往通过应用服务器集群来分担请求数。应用服务器前面部署负载均衡服务器调度用户请求，根据分发策略将请求分发到多个应用服务器节点。&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/d33e063fbcb63afc30f1a472dbaac87b.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;常用的负载均衡技术硬件的有F5，价格比较贵，软件的有LVS、Nginx、HAProxy。LVS是四层负载均衡，根据目标地址和端口选择内部服务器，Nginx是七层负载均衡和HAProxy支持四层、七层负载均衡，可以根据报文内容选择内部服务器，因此LVS分发路径优于Nginx和HAProxy，性能要高些，而Nginx和HAProxy则更具配置性，如可以用来做动静分离（根据请求报文特征，选择静态资源服务器还是应用服务器）。&lt;/p&gt;
&lt;p&gt;五、数据库读写分离和分库分表&lt;/p&gt;
&lt;p&gt;随着用户量的增加，数据库成为最大的瓶颈，改善数据库性能常用的手段是进行读写分离以及分表，读写分离顾名思义就是将数据库分为读库和写库，通过主备功能实现数据同步。分库分表则分为水平切分和垂直切分，水平切换则是对一个数据库特大的表进行拆分，例如用户表。垂直切分则是根据业务不同来切换，如用户业务、商品业务相关的表放在不同的数据库中。&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/a6b7adb5a464862a14d9b308a3e5169f.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;六、使用CDN和反向代理提高网站性能&lt;/p&gt;
&lt;p&gt;假如我们的服务器都部署在成都的机房，对于四川的用户来说访问是较快的，而对于北京的用户访问是较慢的，这是由于四川和北京分别属于电信和联通的不同发达地区，北京用户访问需要通过互联路由器经过较长的路径才能访问到成都的服务器，返回路径也一样，所以数据传输时间比较长。对于这种情况，常常使用CDN解决，CDN将数据内容缓存到运营商的机房，用户访问时先从最近的运营商获取数据，这样大大减少了网络访问的路径。比较专业的CDN运营商有蓝汛、网宿。&lt;/p&gt;
&lt;p&gt;而反向代理，则是部署在网站的机房，当用户请求达到时首先访问反向代理服务器，反向代理服务器将缓存的数据返回给用户，如果没有没有缓存数据才会继续走应用服务器获取，也减少了获取数据的成本。反向代理有Squid，Nginx。&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/2e015f81e496c31405fe9c454ccc4885.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;七、使用分布式文件系统&lt;/p&gt;
&lt;p&gt;用户一天天增加，业务量越来越大，产生的文件越来越多，单台的文件服务器已经不能满足需求。需要分布式的文件系统支撑。常用的分布式文件系统有NFS。&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/873167c7c25a2fbb7ddb348abb6b30fc.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;八、使用NoSql和搜索引擎&lt;/p&gt;
&lt;p&gt;对于海量数据的查询，我们使用nosql数据库加上搜索引擎可以达到更好的性能。并不是所有的数据都要放在关系型数据中。常用的NOSQL有mongodb和redis，搜索引擎有lucene。&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/46cad6842e63ae82a904b14ed97ff340.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;九、将应用服务器进行业务拆分&lt;/p&gt;
&lt;p&gt;随着业务进一步扩展，应用程序变得非常臃肿，这时我们需要将应用程序进行业务拆分，如百度分为新闻、网页、图片等业务。每个业务应用负责相对独立的业务运作。业务之间通过消息进行通信或者同享数据库来实现。&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/2d44f88cdd2b6e89dc019a91405bb271.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;十、搭建分布式服务&lt;/p&gt;
&lt;p&gt;这时我们发现各个业务应用都会使用到一些基本的业务服务，例如用户服务、订单服务、支付服务、安全服务，这些服务是支撑各业务应用的基本要素。我们将这些服务抽取出来利用分部式服务框架搭建分布式服务。淘宝的Dubbo是一个不错的选择。&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/dffe5ecece7a4ae2a4d1a5485e4e1ad7.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;小结&lt;/p&gt;
&lt;p&gt;大型网站的架构是根据业务需求不断完善的，根据不同的业务特征会做特定的设计和考虑，本文只是讲述一个常规大型网站会涉及的一些技术和手段。&lt;/p&gt;
&lt;p&gt;参考资料：&lt;/p&gt;
&lt;p&gt;《大型网站技术架构》 ——李智慧&lt;/p&gt;
&lt;p&gt;《海量运维运营规划》 ——唐文&lt;/p&gt;


&lt;!-- div id=&quot;ad1&quot;&gt;
&lt;/div --&gt;


	


	

</description>
        <pubDate>Mon, 29 Sep 2014 00:00:00 +0800</pubDate>
        <link>http://iftti.com/posts/2014/2014-09-29-77748-3d935e7bf.html</link>
        <guid isPermaLink="true">http://iftti.com/posts/2014/2014-09-29-77748-3d935e7bf.html</guid>
        
        
        <category>jobbole</category>
        
      </item>
    
      <item>
        <title>经验谈：用cp命令复制大量文件（432,000,000个，共39TB）</title>
        <description>

					
		
&lt;!-- div id=&quot;ad1&quot;&gt;
&lt;script type=&quot;text/javascript&quot;&gt;
google_ad_client = &quot;ca-pub-7056282119617872&quot;;
google_ad_slot = &quot;6645040531&quot;;
google_ad_width = 300;
google_ad_height = 250;
&lt;/script&gt;
&lt;script type=&quot;text/javascript&quot;
src=&quot;http://pagead2.googlesyndication.com/pagead/show_ads.js&quot;&gt;
&lt;/script&gt;
&lt;/div&gt;
&lt;br/ --&gt;

&lt;p&gt;最近，我有一个复制大量文件的需求，虽然我已经在Unix各种变种上有超过20年的工作经验，但是我仍然被cp指令的行为震撼，我认为这些心得很值得和大家分享下。&lt;/p&gt;
&lt;p&gt;首先介绍下机器：一台戴尔服务器（双核，内存最初是2G，后来扩展到10G，运行ubuntu系统），服务器配备的是全新戴尔存储套件MD1200，该套件包括12个4TB硬盘，其中40TB采用RAID 6配置，这样整个系统就可以容忍两块硬盘同时down掉。这台服务器主要用作异地备份，唯一的操作是IO写，由于我使用rsnapshot完成这项工作，因此大部分文件有较高的链接计数（30+）。&lt;/p&gt;
&lt;p&gt;一天早上，我被告知一个硬盘down掉了。这不是什么大事，这种事情经常发生。我打电话给戴尔，戴尔第二天就给送来了一块替换盘。但我安装替换盘时发现，替换盘根本不能工作，并且另一块磁盘也down掉了。戴尔技术服务部门富有经验地建议我不要只换down掉的硬盘，因为整个磁盘阵列可能已经损毁了。就我所知，磁盘只有在有足够多坏块的情况下才会告警，设想这样一个场景：短时间内，一个文件位于三个磁盘上的备份块全部坏掉，那么很不幸，RAID引擎短时间内难以完成检测坏块、重新计算备份数据并存储这样一个流程，这样你就有丢失数据的风险了。因此，如果两个硬盘显式告警，你的数据可能已经丢失了。&lt;/p&gt;
&lt;p&gt;现有存储套件容量已经难以达到要求，我们决定扩容，把文件从旧存储套件拷贝到新套件中。正常情况下，我会在块级别拷贝用dd指令或者pvmove指令拷贝数据，但是考虑到坏块，我决定采用文件级别的拷贝，因为这样的话，我就可以知道那些文件包含坏块了。我上网搜了相关经验，发现cp可以完美地解决这个问题。若想保存硬链接信息，需要记录有哪些文件已经被复制，所以我预订了8G的RAM，并配置了更大的交换区。&lt;/p&gt;
&lt;p&gt;当新的存储套件到货，我就着手复制了，起初，根据iotop的测量数据，复制速度接近于300-400MB/s。不一会儿速度便显著下降了，因为大部分时间花在了建立硬链接上，并且要花时间去处理文件系统的一致性问题。为保持一致性，我们使用了XFS，由于没有关闭写屏障，我们深受性能之苦，但如果RAID控制器配置了带有备份电源的写缓存，一致性问题便可更好地解决。正如所料，cp的内存占用量稳步增加，很快便达到了GB级。&lt;/p&gt;
&lt;p&gt;经历了几天的拷贝，问题来了：我发现系统已经停止拷贝，根据strace的显示，cp指令没有调用任何的系统调用函数。阅读相关源码后发现，cp指令会以一个哈希表跟踪有哪些文件被复制了，这个表需要时常地扩展自身容量以避免太多的冲突。当RAM被耗尽，哈希表的扩容过程会成为性能瓶颈。&lt;/p&gt;
&lt;p&gt;我们相信cp的哈希表扩容后，cp指令会继续执行，果然不一会它又“复活”了，但是它会进入周期性的“扩容-拷贝-扩容-拷贝”的循环中，且扩容的时间变得越来越长。经过10天的拷贝，根据dd结果，新文件系统的块数量和inode节点的数量已经和原系统一样，但是让人吃惊的是cp指令并没有退出。再次阅读源码，我发现cp正在认真地解构哈希表（使用forget_all函数），由于cp进程需要的虚拟内存大小达到了17GB+，但服务器总共才10G内存，所以解构过程执行了很多RAM和swap区间的换页操作。&lt;/p&gt;
&lt;p&gt;上面整个过程中，我使用了cp的-v选项，并使用tee把日志重定向到日志文件（很大的文件）中。cp的输出信息会被缓存，为使这些缓存信息全部刷新到日志文件中，我给了cp多于一天的时间进行解构哈希表。&lt;/p&gt;
&lt;p&gt;运行”ls –laR”指令，查看下两个文件系统中的文件是否一致，发现除了日志文件的一些错误外，只有一个文件发生了io错误（还好，我们有它的另一个备份）。&lt;/p&gt;
&lt;p&gt;这种错误不会马上复现，但是如果cp能够设计一种数据结构，在等待io的时候处理那些已经被记录的文件，那样处理效率会更高。此外，对于cp最后解构哈希表的forget_all函数，除了那些缺少可用内存管理模块的老式服务器一定需要它之外，我还没发现去掉这个函数会对整个拷贝过程产生什么不好的影响。&lt;/p&gt;
&lt;p&gt;总结一下：&lt;/p&gt;
&lt;p&gt;1、拷贝整个文件系统时，如果确定你的硬件和文件系统都OK，应该使用块级别的拷贝。这种拷贝方式更快，除非你有许多空闲块，但是无论如何它耗费的内存都是较少的；&lt;/p&gt;
&lt;p&gt;2、如果要拷贝许多文件，并想要保留硬链接的信息，可以采用文件级拷贝。拷贝之前需要保证机器有足够多的内存；&lt;/p&gt;
&lt;p&gt;3、比起简单粗暴地停止进程，认真地解构数据结构会花费更多的时间；&lt;/p&gt;
&lt;p&gt;4、告警的硬盘的数量并不等同于有坏块的硬盘的数量。如果不走运，就算配置了RAID6，没来得及等3块磁盘告警，你的数据可能已经丢失。这个时候我们只能赌一把了。同样的情况适用于RAID 5，那怕只有一块磁盘甚至没有磁盘告警，如果不走运，你的数据照样丢！&lt;/p&gt;
&lt;p&gt;希望这篇文章对你有益！&lt;/p&gt;


&lt;!-- div id=&quot;ad1&quot;&gt;
&lt;/div --&gt;


	


	

</description>
        <pubDate>Sun, 28 Sep 2014 00:00:00 +0800</pubDate>
        <link>http://iftti.com/posts/2014/2014-09-28-77698-c8ce5869d.html</link>
        <guid isPermaLink="true">http://iftti.com/posts/2014/2014-09-28-77698-c8ce5869d.html</guid>
        
        
        <category>jobbole</category>
        
      </item>
    
      <item>
        <title>From Clouds to Roots: Performance Analysis at Netflix</title>
        <description>

&lt;p&gt;How do companies like Netflix, Google, and Facebook do root cause performance analysis in their cloud environments? These companies use Linux heavily, so are they using SystemTap or another tracer? Netflix is hosted on EC2, how do they do low level CPU profiling, if Xen guests can&#39;t access the CPU counters? ... And if these companies don&#39;t do these kinds of profiling, aren&#39;t they losing millions because of it?&lt;/p&gt;

&lt;p&gt;On Thursday it was my privilege to speak again at &lt;a href=&quot;http://surge.omniti.com/2014&quot;&gt;Surge&lt;/a&gt;, the scalability and performance conference. My talk was titled &lt;a href=&quot;http://surge.omniti.com/2014/speakers/speakers.html?gregg-brendan%5D&quot;&gt;From Clouds to Roots&lt;/a&gt;, and showed how Neflix does root cause performance analysis, and answers these questions and more. The slides are on &lt;a href=&quot;http://www.slideshare.net/brendangregg/netflix-from-clouds-to-roots&quot;&gt;slideshare&lt;/a&gt;:&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;
&lt;center&gt;
&lt;iframe src=&quot;//www.slideshare.net/slideshow/embed_code/39526755&quot; width=&quot;425&quot; height=&quot;355&quot; frameborder=&quot;0&quot; marginwidth=&quot;0&quot; marginheight=&quot;0&quot; scrolling=&quot;no&quot; style=&quot;border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;&quot; allowfullscreen&gt; &lt;/iframe&gt; &lt;/center&gt;

&lt;p&gt;It was also videoed; I&#39;ll update this post when the video is available.&lt;/p&gt;

&lt;p&gt;For background, I began by explaining the Netflix fault-tolerant architecture: how this can automatically work around performance issues, and what kinds of new issues this can introduce. Then I summarized key cloud-wide performance tools, and how we use these to isolate issues to an instance. And finally instance analysis, with the low-level tools we run to find the root cause.&lt;/p&gt;

&lt;p&gt;Summary of cloud-wide performance tools:&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;
&lt;center&gt;&lt;a href=&quot;/blog/images/2014/netflix_cloud_perf_analysis.png&quot;&gt;&lt;img src=&quot;/images/brendangregg.com/8117220f7d1a16ef6a80ac51b34c927f.jpg&quot; width=&quot;500&quot;&gt;&lt;/a&gt;&lt;/center&gt;

&lt;p&gt;Summary of instance performance tools:&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;
&lt;center&gt;&lt;a href=&quot;/blog/images/2014/linux_EC2_perf_tools.png&quot;&gt;&lt;img src=&quot;/images/brendangregg.com/30cb18d519c569b8273e6a5838904be8.jpg&quot; width=&quot;500&quot;&gt;&lt;/a&gt;&lt;/center&gt;

&lt;p&gt;People liked the talk, although it surprised some. One comment was &quot;it didn&#39;t feel like a Brendan talk&quot;. Yes, I&#39;ve expanded my scope at Netflix, and I&#39;m working on higher-level analysis now, not just low-level performance tools. It&#39;s exciting for me, and I feel I have a better and more balanced view of performance analysis, which I captured in this talk.&lt;/p&gt;

&lt;p&gt;Keep in mind that you aren&#39;t necessarily expected to do the low-level performance analysis yourself. I summarized the likely real scenarios at the end of the talk:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;A. Your company has one or more people who do advanced perf analysis (perf team). Ask them to do it.&lt;/li&gt;
&lt;li&gt;B. You actually are that person. Do it.&lt;/li&gt;
&lt;li&gt;C. You buy a product that does it. Ask them to add low-level capabilities.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Perhaps the most valuable take-away of this talk is that you be aware that such low-level analysis is possible, so that if you are in situation (A) or (C), you can ask for it. I&#39;ve found that performance monitoring companies often build products based on what customers think they want. However, what customers think they want is bounded by what they know is possible. You really want products that make low-level analysis easy, including flame graphs and latency heat maps. Some monitoring products already provide these (eg, Circonus, AppNeta), and at Netflix we&#39;re building Vector.&lt;/p&gt;

&lt;p&gt;Last year at Surge, I saw a great talk by &lt;a href=&quot;http://twitter.com/coburnw&quot;&gt;Coburn Watson&lt;/a&gt; titled &lt;a href=&quot;http://www.youtube.com/watch?v=7-13wV3WO8Q&quot;&gt;How Netflix Maximizes Scalability, Resilience, and Engineering Velocity in the Cloud&lt;/a&gt;, which I recommend. His relaxed style threw me at first: he&#39;s able to talk very casually about complex performance issues in distributed environments, because he actually understands them extremely well. This year, he&#39;s now my manager at Netflix, and he&#39;s also still hiring. Great at performance and want to work with both Coburn and myself? Let him &lt;a href=&quot;mailto:cwatson@netflix.com&quot;&gt;know&lt;/a&gt;. :-)&lt;/p&gt;

&lt;p&gt;For more about low level performance analysis tools, see my &lt;a href=&quot;/blog&quot;&gt;previous blog posts&lt;/a&gt;, and my &lt;a href=&quot;https://github.com/brendangregg/perf-tools&quot;&gt;perf-tools&lt;/a&gt; and &lt;a href=&quot;https://github.com/brendangregg/msr-cloud-tools&quot;&gt;msr-cloud-tools&lt;/a&gt; collections on github.&lt;/p&gt;


</description>
        <pubDate>Sat, 27 Sep 2014 00:00:00 +0800</pubDate>
        <link>http://iftti.com/posts/2014/2014-09-27-from-clouds-to-roots.html-880dc9b64.html</link>
        <guid isPermaLink="true">http://iftti.com/posts/2014/2014-09-27-from-clouds-to-roots.html-880dc9b64.html</guid>
        
        
        <category>brendangregg</category>
        
      </item>
    
      <item>
        <title>Linux后门入侵检测工具，附bash漏洞解决方法</title>
        <description>

					
		
&lt;!-- div id=&quot;ad1&quot;&gt;
&lt;script type=&quot;text/javascript&quot;&gt;
google_ad_client = &quot;ca-pub-7056282119617872&quot;;
google_ad_slot = &quot;6645040531&quot;;
google_ad_width = 300;
google_ad_height = 250;
&lt;/script&gt;
&lt;script type=&quot;text/javascript&quot;
src=&quot;http://pagead2.googlesyndication.com/pagead/show_ads.js&quot;&gt;
&lt;/script&gt;
&lt;/div&gt;
&lt;br/ --&gt;

&lt;p&gt;&lt;strong&gt;一、rootkit简介&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;rootkit是Linux平台下最常见的一种木马后门工具，它主要通过替换系统文件来达到入侵和和隐蔽的目的，这种木马比普通木马后门更加危险和隐蔽，普通的检测工具和检查手段很难发现这种木马。rootkit攻击能力极强，对系统的危害很大，它通过一套工具来建立后门和隐藏行迹，从而让攻击者保住权限，以使它在任何时候都可以使用root权限登录到系统。&lt;/p&gt;
&lt;p&gt;rootkit主要有两种类型：文件级别和内核级别，下面分别进行简单介绍。&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;1、文件级别rootkit&lt;/p&gt;
&lt;p&gt;文件级别的rootkit一般是通过程序漏洞或者系统漏洞进入系统后，通过修改系统的重要文件来达到隐藏自己的目的。在系统遭受rootkit攻击后，合法的文件被木马程序替代，变成了外壳程序，而其内部是隐藏着的后门程序。通常容易被rootkit替换的系统程序有login、ls、ps、ifconfig、du、find、netstat等，其中login程序是最经常被替换的，因为当访问Linux时，无论是通过本地登录还是远程登录，/bin/login程序都会运行，系统将通过/bin/login来收集并核对用户的账号和密码，而rootkit就是利用这个程序的特点，使用一个带有根权限后门密码的/bin/login来替换系统的/bin/login，这样攻击者通过输入设定好的密码就能轻松进入系统。此时，即使系统管理员修改root密码或者清除root密码，攻击者还是一样能通过root用户登录系统。攻击者通常在进入Linux系统后，会进行一系列的攻击动作，最常见的是安装嗅探器收集本机或者网络中其他服务器的重要数据。在默认情况下，Linux中也有一些系统文件会监控这些工具动作，例如ifconfig命令，所以，攻击者为了避免被发现，会想方设法替换其他系统文件，常见的就是ls、ps、ifconfig、du、find、netstat等。如果这些文件都被替换，那么在系统层面就很难发现rootkit已经在系统中运行了。&lt;/p&gt;
&lt;p&gt;这就是文件级别的rootkit，对系统维护很大，目前最有效的防御方法是定期对系统重要文件的完整性进行检查，如果发现文件被修改或者被替换，那么很可能系统已经遭受了rootkit入侵。检查件完整性的工具很多，常见的有Tripwire、 aide等，可以通过这些工具定期检查文件系统的完整性，以检测系统是否被rootkit入侵。&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;2、内核级别的rootkit&lt;/p&gt;
&lt;p&gt;内核级rootkit是比文件级rootkit更高级的一种入侵方式，它可以使攻击者获得对系统底层的完全控制权，此时攻击者可以修改系统内核，进而截获运行程序向内核提交的命令，并将其重定向到入侵者所选择的程序并运行此程序，也就是说，当用户要运行程序A时，被入侵者修改过的内核会假装执行A程序，而实际上却执行了程序B。&lt;/p&gt;
&lt;p&gt;内核级rootkit主要依附在内核上，它并不对系统文件做任何修改，因此一般的检测工具很难检测到它的存在，这样一旦系统内核被植入rootkit，攻击者就可以对系统为所欲为而不被发现。目前对于内核级的rootkit还没有很好的防御工具，因此，做好系统安全防范就非常重要，将系统维持在最小权限内工作，只要攻击者不能获取root权限，就无法在内核中植入rootkit。&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;二、rootkit后门检测工具chkrootkit&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;chkrootkit是一个Linux系统下查找并检测rootkit后门的工具，它的官方址: http://www.chkrootkit.org/。 chkrootkit没有包含在官方的CentOS源中，因此要采取手动编译的方法来安装，不过这种安装方法也更加安全。下面简单介绍下chkrootkit的安装过程。&lt;/p&gt;
&lt;p&gt;1.准备gcc编译环境&lt;/p&gt;
&lt;p&gt;对于CentOS系统，需要安装gcc编译环境，执行下述三条命令：&lt;/p&gt;
&lt;pre class=&quot;brush: actionscript3; gutter: true&quot;&gt;[root@server ~]# yum -y install gcc
[root@server ~]# yum -y install gcc-c++
[root@server ~]# yum -y install make&lt;/pre&gt;
&lt;p&gt;2、安装chkrootkit&lt;/p&gt;
&lt;p&gt;为了安全起见，建议直接从官方网站下载chkrootkit源码，然后进行安装，操作如下：&lt;/p&gt;
&lt;pre class=&quot;brush: actionscript3; gutter: true&quot;&gt;[root@server ~]# tar zxvf chkrootkit.tar.gz
[root@server ~]# cd chkrootkit-*
[root@server ~]# make sense&lt;/pre&gt;
&lt;p&gt;# 注意，上面的编译命令为make sense&lt;/p&gt;
&lt;pre class=&quot;brush: actionscript3; gutter: true&quot;&gt;[root@server ~]# cd ..
[root@server ~]# cp -r chkrootkit-* /usr/local/chkrootkit
[root@server ~]# rm -rf chkrootkit-*&lt;/pre&gt;
&lt;p&gt;3、使用chkrootkit&lt;/p&gt;
&lt;p&gt;安装完的chkrootkit程序位于/usr/local/chkrootkit目录下，执行如下命令即可显示chkrootkit的详细用法：&lt;/p&gt;
&lt;p&gt;[root@server chkrootkit]# /usr/local/chkrootkit/chkrootkit  -h&lt;/p&gt;
&lt;p&gt;chkrootkit各个参数的含义如下所示。&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;参数含义&lt;/p&gt;
&lt;p&gt;-h显示帮助信息&lt;/p&gt;
&lt;p&gt;-v显示版本信息&lt;/p&gt;
&lt;p&gt;-l显示测试内容&lt;/p&gt;
&lt;p&gt;-ddebug模式，显示检测过程的相关指令程序&lt;/p&gt;
&lt;p&gt;-q安静模式，只显示有问题的内容&lt;/p&gt;
&lt;p&gt;-x高级模式，显示所有检测结果&lt;/p&gt;
&lt;p&gt;-r dir设置指定的目录为根目录&lt;/p&gt;
&lt;p&gt;-p dir1:dir2:dirN指定chkrootkit检测时使用系统命令的目录&lt;/p&gt;
&lt;p&gt;-n跳过NFS连接的目录&lt;/p&gt;
&lt;p&gt;chkrootkit的使用比较简单，直接执行chkrootkit命令即可自动开始检测系统。下面是某个系统的检测结果：&lt;/p&gt;
&lt;pre class=&quot;brush: actionscript3; gutter: true&quot;&gt;[root@server chkrootkit]# /usr/local/chkrootkit/chkrootkit
Checking `ifconfig&#39;... INFECTED
Checking `ls&#39;... INFECTED
Checking `login&#39;... INFECTED
Checking `netstat&#39;... INFECTED
Checking `ps&#39;... INFECTED
Checking `top&#39;... INFECTED
Checking `sshd&#39;... not infected
Checking `syslogd&#39;... not tested
Checking `tar&#39;... not infected
Checking `tcpd&#39;... not infected
Checking `tcpdump&#39;... not infected
Checking `telnetd&#39;... not found&lt;/pre&gt;
&lt;p&gt;从输出可以看出，此系统的ifconfig、ls、login、netstat、ps和top命令已经被感染。针对被感染rootkit的系统，最安全而有效的方法就是备份数据重新安装系统。&lt;/p&gt;
&lt;p&gt;4、chkrootkit的缺点&lt;/p&gt;
&lt;p&gt;chkrootkit在检查rootkit的过程中使用了部分系统命令，因此，如果服务器被黑客入侵，那么依赖的系统命令可能也已经被入侵者替换，此时chkrootkit的检测结果将变得完全不可信。为了避免chkrootkit的这个问题，可以在服务器对外开放前，事先将chkrootkit使用的系统命令进行备份，在需要的时候使用备份的原始系统命令让chkrootkit对rootkit进行检测。这个过程可以通过下面的操作实现：&lt;/p&gt;
&lt;pre class=&quot;brush: actionscript3; gutter: true&quot;&gt;[root@server ~]# mkdir /usr/share/.commands
[root@server ~]# cp `which --skip-alias awk cut echo find egrep id head ls netstat ps strings sed uname` /usr/share/.commands
[root@server ~]# /usr/local/chkrootkit/chkrootkit -p /usr/share/.commands/
[root@server share]# cd /usr/share/
[root@server share]# tar zcvf commands.tar.gz .commands
[root@server share]#  rm -rf commands.tar.gz&lt;/pre&gt;
&lt;p&gt;上面这段操作是在/usr/share/下建立了一个.commands隐藏文件，然后将chkrootkit使用的系统命令进行备份到这个目录下。为了安全起见，可以将.commands目录压缩打包，然后下载到一个安全的地方进行备份，以后如果服务器遭受入侵，就可以将这个备份上传到服务器任意路径下，然后通过chkrootkit命令的“-p”参数指定这个路径进行检测即可。&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;三、rootkit后门检测工具RKHunter&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;RKHunter是一款专业的检测系统是否感染rootkit的工具，它通过执行一系列的脚本来确认服务器是否已经感染rootkit。在官方的资料中，RKHunter可以作的事情有：&lt;/p&gt;
&lt;p&gt;MD5校验测试，检测文件是否有改动&lt;/p&gt;
&lt;p&gt;检测rootkit使用的二进制和系统工具文件&lt;/p&gt;
&lt;p&gt;检测特洛伊木马程序的特征码&lt;/p&gt;
&lt;p&gt;检测常用程序的文件属性是否异常&lt;/p&gt;
&lt;p&gt;检测系统相关的测试&lt;/p&gt;
&lt;p&gt;检测隐藏文件&lt;/p&gt;
&lt;p&gt;检测可疑的核心模块LKM&lt;/p&gt;
&lt;p&gt;检测系统已启动的监听端口&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;下面详细讲述下RKHunter的安装与使用。&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;1、安装RKHunter&lt;/p&gt;
&lt;p&gt;RKHunter的官方网页地址为：http://www.rootkit.nl/projects/rootkit_hunter.html，建议从这个网站下载RKHunter，这里下载的版本是rkhunter-1.4.0.tar.gz。RKHunter的安装非常简单，过程如下：&lt;/p&gt;
&lt;pre class=&quot;brush: actionscript3; gutter: true&quot;&gt;[root@server ~]# ls
rkhunter-1.4.0.tar.gz
[root@server ~]# pwd
/root
[root@server ~]# tar -zxvf rkhunter-1.4.0.tar.gz 
[root@server ~]# cd rkhunter-1.4.0
[root@server rkhunter-1.4.0]# ./installer.sh  --layout default --install&lt;/pre&gt;
&lt;p&gt;这里采用RKHunter的默认安装方式，rkhunter命令被安装到了/usr/local/bin目录下。&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;2、使用rkhunter指令&lt;/p&gt;
&lt;p&gt;rkhunter命令的参数较多，但是使用非常简单，直接运行rkhunter即可显示此命令的用法。下面简单介绍下rkhunter常用的几个参数选项。&lt;/p&gt;
&lt;p&gt;[root@server ~]#/usr/local/bin/rkhunter–help&lt;/p&gt;
&lt;p&gt;Rkhunter常用参数以及含义如下所示。&lt;/p&gt;
&lt;p&gt;参数             含义&lt;/p&gt;
&lt;p&gt;-c, –check必选参数，表示检测当前系统&lt;/p&gt;
&lt;p&gt;–configfile &amp;lt;file&amp;gt;使用特定的配置文件&lt;/p&gt;
&lt;p&gt;–cronjob作为cron任务定期运行&lt;/p&gt;
&lt;p&gt;–sk, –skip-keypress自动完成所有检测，跳过键盘输入&lt;/p&gt;
&lt;p&gt;–summary显示检测结果的统计信息&lt;/p&gt;
&lt;p&gt;–update检测更新内容&lt;/p&gt;
&lt;p&gt;-V, –version显示版本信息&lt;/p&gt;
&lt;p&gt;–versioncheck检测最新版本&lt;/p&gt;
&lt;p&gt;下面是通过rkhunter对某个系统的检测示例：&lt;/p&gt;
&lt;pre class=&quot;brush: actionscript3; gutter: true&quot;&gt;[root@server rkhunter-1.4.0]# /usr/local/bin/rkhunter   -c 
[ Rootkit Hunter version 1.4.0 ]
#下面是第一部分，先进行系统命令的检查，主要是检测系统的二进制文件，因为这些文件最容易被rootkit攻击。显示OK字样表示正常，显示Warning表示有异常，需要引起注意，而显示“Not found”字样，一般无需理会
Checking system commands...
  Performing &#39;strings&#39; command checks
    Checking &#39;strings&#39; command                           [ OK ]
  Performing &#39;shared libraries&#39; checks
    Checking for preloading variables                        [ None found ]
    Checking for preloaded libraries                         [ None found ]
    Checking LD_LIBRARY_PATH variable                 [ Not found ]
  Performing file properties checks
    Checking for prerequisites                              [ Warning ]
    /usr/local/bin/rkhunter  [ OK ]
    /sbin/chkconfig                                       [ OK ]
....(略)....
[Press &amp;lt;ENTER&amp;gt; to continue]
#下面是第二部分，主要检测常见的rootkit程序，显示“Not found”表示系统未感染此rootkit
Checking for rootkits...
  Performing check of known rootkit files and directories
    55808 Trojan - Variant A                                 [ Not found ]
    ADM Worm                                           [ Not found ]
    AjaKit Rootkit                                         [ Not found ]
    Adore Rootkit                                          [ Not found ]
aPa Kit                                               [ Not found ]
    Apache Worm                                          [ Not found ]
    Ambient (ark) Rootkit                                    [ Not found ]
    Balaur Rootkit           [ Not found ]
    BeastKit Rootkit                                         [ Not found ]
beX2 Rootkit                                             [ Not found ]
    BOBKit Rootkit                    [ Not found ]
....(略)....
[Press &amp;lt;ENTER&amp;gt; to continue]
#下面是第三部分，主要是一些特殊或附加的检测，例如对rootkit文件或目录检测、对恶意软件检测以及对指定的内核模块检测
  Performing additional rootkit checks
    Suckit Rookit additional checks                          [ OK ]
    Checking for possible rootkit files and directories      [ None found ]
    Checking for possible rootkit strings                    [ None found ]
  Performing malware checks
    Checking running processes for suspicious files          [ None found ]
    Checking for login backdoors                          [ None found ]
    Checking for suspicious directories                     [ None found ]
    Checking for sniffer log files                          [ None found ]
  Performing Linux specific checks
    Checking loaded kernel modules                     [ OK ]
    Checking kernel module names                     [ OK ]
[Press &amp;lt;ENTER&amp;gt; to continue]
#下面是第四部分，主要对网络、系统端口、系统启动文件、系统用户和组配置、SSH配置、文件系统等进行检测
Checking the network...
  Performing checks on the network ports
    Checking for backdoor ports                         [ None found ]
  Performing checks on the network interfaces
    Checking for promiscuous interfaces                      [ None found ]
Checking the local host...
  Performing system boot checks
    Checking for local host name                         [ Found ]
    Checking for system startup files                        [ Found ]
    Checking system startup files for malware                [ None found ]
  Performing group and account checks
    Checking for passwd file [ Found ]
    Checking for root equivalent (UID 0) accounts            [ None found ]
    Checking for passwordless accounts                   [ None found ]
....(略)....
[Press &amp;lt;ENTER&amp;gt; to continue]
#下面是第五部分，主要是对应用程序版本进行检测
Checking application versions...
    Checking version of GnuPG[ OK ]
    Checking version of OpenSSL                        [ Warning ]
    Checking version of OpenSSH                        [ OK ]
#下面是最后一部分，这个部分其实是上面输出的一个总结，通过这个总结，可以大概了解服务器目录的安全状态。
System checks summary
=====================
File properties checks...
    Required commands check failed
    Files checked: 137
    Suspect files: 4
Rootkit checks...
    Rootkits checked : 311
    Possible rootkits: 0
Applications checks...
    Applications checked: 3
    Suspect applications: 1
The system checks took: 6 minutes and 41 seconds&lt;/pre&gt;
&lt;p&gt;在Linux终端使用rkhunter来检测，最大的好处在于每项的检测结果都有不同的颜色显示，如果是绿色的表示没有问题，如果是红色的，那就要引起关注了。另外，在上面执行检测的过程中，在每个部分检测完成后，需要以Enter键来继续。如果要让程序自动运行，可以执行如下命令：&lt;/p&gt;
&lt;p&gt;[root@server ~]# /usr/local/bin/rkhunter –check –skip-keypress&lt;/p&gt;
&lt;p&gt;同时，如果想让检测程序每天定时运行，那么可以在/etc/crontab中加入如下内容：&lt;/p&gt;
&lt;p&gt;30 09 * * * root /usr/local/bin/rkhunter –check –cronjob&lt;/p&gt;
&lt;p&gt;这样，rkhunter检测程序就会在每天的9:30分运行一次。&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;安全更新：&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;今天刚刚爆出Bash安全漏洞，SSH bash紧急安全补丁！重要！&lt;/p&gt;
&lt;p&gt;测试是否存在漏洞，执行以下命令：&lt;/p&gt;
&lt;pre class=&quot;brush: actionscript3; gutter: true&quot;&gt;$ env x=&#39;() { :;}; echo vulnerable&#39; bash -c &quot;echo this is a test&quot;
 vulnerable
 this is a test&lt;/pre&gt;
&lt;p&gt;如果显示如上，那么，很遗憾，必须立即打上安全补丁修复，&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;临时解决办法为：&lt;/p&gt;
&lt;pre class=&quot;brush: actionscript3; gutter: true&quot;&gt;yum -y update bash&lt;/pre&gt;
&lt;p&gt;升级bash后，执行测试：&lt;/p&gt;
&lt;pre class=&quot;brush: actionscript3; gutter: true&quot;&gt;$ env x=&#39;() { :;}; echo vulnerable&#39; bash -c &quot;echo this is a test&quot;
 bash: warning: x: ignoring function definition attempt
 bash: error importing function definition for `x&#39;
 this is a test&lt;/pre&gt;
&lt;p&gt;如果显示如上，表示已经修补了漏洞。&lt;/p&gt;


&lt;!-- div id=&quot;ad1&quot;&gt;
&lt;/div --&gt;


	


	

</description>
        <pubDate>Thu, 25 Sep 2014 00:00:00 +0800</pubDate>
        <link>http://iftti.com/posts/2014/2014-09-25-77663-4da093c4d.html</link>
        <guid isPermaLink="true">http://iftti.com/posts/2014/2014-09-25-77663-4da093c4d.html</guid>
        
        
        <category>jobbole</category>
        
      </item>
    
      <item>
        <title>SSL延迟有多大？</title>
        <description>

					
		
&lt;!-- div id=&quot;ad1&quot;&gt;
&lt;script type=&quot;text/javascript&quot;&gt;
google_ad_client = &quot;ca-pub-7056282119617872&quot;;
google_ad_slot = &quot;6645040531&quot;;
google_ad_width = 300;
google_ad_height = 250;
&lt;/script&gt;
&lt;script type=&quot;text/javascript&quot;
src=&quot;http://pagead2.googlesyndication.com/pagead/show_ads.js&quot;&gt;
&lt;/script&gt;
&lt;/div&gt;
&lt;br/ --&gt;

&lt;p&gt;据说，Netscape 公司当年设计 &lt;a href=&quot;http://blog.jobbole.com/77439/&quot; target=&quot;_blank&quot;&gt;SSL 协议&lt;/a&gt;的时候，有人提过，将互联网所有链接都变成 HTTPs 开头的加密链接。&lt;/p&gt;
&lt;p&gt;这个建议没有得到采纳，原因之一是 HTTPs 链接比不加密的 HTTP 链接慢很多。（另一个原因好像是，HTTPs 链接默认不能缓存。）&lt;/p&gt;
&lt;p&gt;自从我知道这个掌故以后，脑袋中就有一个观念：HTTPs 链接很慢。但是，它到底有多慢，我并没有一个精确的概念。直到今天我从一篇&lt;a href=&quot;http://www.semicomplete.com/blog/geekery/ssl-latency.html&quot; target=&quot;_blank&quot;&gt;文章&lt;/a&gt;中，学到了测量 HTTPs 链接耗时的方法。&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/869312f2433a8ddcc2d7086ab8e4f2a4.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;首先我解释一下，为什么 HTTPs 链接比较慢。&lt;/p&gt;
&lt;p&gt;HTTPs 链接和 HTTP 链接都建立在 TCP 协议之上。HTTP 链接比较单纯，使用三个握手数据包建立连接之后，就可以发送内容数据了。&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/f0b241592ba624dd41bd64fd480ddd51.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;上图中，客户端首先发送 SYN 数据包，然后服务器发送 SYN+ACK 数据包，最后客户端发送 ACK 数据包，接下来就可以发送内容了。这三个数据包的发送过程，叫做 TCP 握手。&lt;/p&gt;
&lt;p&gt;再来看 HTTPs 链接，它也采用 TCP 协议发送数据，所以它也需要上面的这三步握手过程。而且，在这三步结束以后，它还有一个 &lt;a href=&quot;http://www.ruanyifeng.com/blog/2014/02/ssl_tls.html&quot; target=&quot;_blank&quot;&gt;SSL 握手&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;总结一下，就是下面这两个式子。&lt;/p&gt;
&lt;p style=&quot;padding-left: 30px;&quot;&gt;&lt;span style=&quot;color: #888888;&quot;&gt;HTTP 耗时 = TCP 握手&lt;/span&gt;&lt;/p&gt;
&lt;p style=&quot;padding-left: 30px;&quot;&gt;&lt;span style=&quot;color: #888888;&quot;&gt;HTTPs 耗时 = TCP 握手 + SSL 握手&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;所以，HTTPs 肯定比 HTTP 耗时，这就叫 SSL 延迟。&lt;/p&gt;
&lt;p&gt;命令行工具 &lt;a href=&quot;http://www.ruanyifeng.com/blog/2011/09/curl.html&quot; target=&quot;_blank&quot;&gt;curl&lt;/a&gt; 有一个w参数，可以用来测量 TCP 握手和 SSL 握手的具体耗时，以访问支付宝为例。&lt;/p&gt;
&lt;pre class=&quot;brush: bash; gutter: true&quot;&gt;$ curl -w &quot;TCP handshake: %{time_connect}， SSL handshake: %{time_appconnect}\n&quot; -so /dev/null https://www.alipay.comTCP handshake: 0.022, SSL handshake: 0.064&lt;/pre&gt;
&lt;p&gt;上面命令中的w参数表示指定输出格式，timeconnect 变量表示 TCP 握手的耗时，timeappconnect 变量表示 SSL 握手的耗时（更多变量请查看&lt;a href=&quot;http://curl.haxx.se/docs/manpage.html&quot; target=&quot;_blank&quot;&gt;文档&lt;/a&gt;和&lt;a href=&quot;https://josephscott.org/archives/2011/10/timing-details-with-curl/&quot; target=&quot;_blank&quot;&gt;实例&lt;/a&gt;），s参数和o参数用来关闭标准输出。&lt;/p&gt;
&lt;p&gt;从运行结果可以看到，SSL 握手的耗时（64 毫秒）大概是 TCP 握手（22 毫秒）的三倍。也就是说，在建立连接的阶段，HTTPs 链接比 HTTP 链接要长 3 倍的时间，具体数字取决于 CPU 的快慢。&lt;/p&gt;
&lt;p&gt;所以，如果是对安全性要求不高的场合，为了提高网页性能，建议不要采用保密强度很高的数字证书。一般场合下，1024 位的证书已经足够了，2048 位和 4096 位的证书将进一步延长 SSL 握手的耗时。&lt;/p&gt;
&lt;p&gt;（完）&lt;/p&gt;


&lt;!-- div id=&quot;ad1&quot;&gt;
&lt;/div --&gt;


	


	

</description>
        <pubDate>Thu, 25 Sep 2014 00:00:00 +0800</pubDate>
        <link>http://iftti.com/posts/2014/2014-09-25-77624-79bfbf9ed.html</link>
        <guid isPermaLink="true">http://iftti.com/posts/2014/2014-09-25-77624-79bfbf9ed.html</guid>
        
        
        <category>jobbole</category>
        
      </item>
    
      <item>
        <title>机器学习常见算法分类汇总</title>
        <description>

					
		
&lt;!-- div id=&quot;ad1&quot;&gt;
&lt;script type=&quot;text/javascript&quot;&gt;
google_ad_client = &quot;ca-pub-7056282119617872&quot;;
google_ad_slot = &quot;6645040531&quot;;
google_ad_width = 300;
google_ad_height = 250;
&lt;/script&gt;
&lt;script type=&quot;text/javascript&quot;
src=&quot;http://pagead2.googlesyndication.com/pagead/show_ads.js&quot;&gt;
&lt;/script&gt;
&lt;/div&gt;
&lt;br/ --&gt;

&lt;p&gt;机器学习无疑是当前数据分析领域的一个热点内容。很多人在平时的工作中都或多或少会用到机器学习的算法。本文为您总结一下常见的机器学习算法，以供您在工作和学习中参考。&lt;/p&gt;
&lt;p&gt;机器学习的算法很多。很多时候困惑人们都是，很多算法是一类算法，而有些算法又是从其他算法中延伸出来的。这里，我们从两个方面来给大家介绍，第一个方面是学习的方式，第二个方面是算法的类似性。&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;学习方式&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;根据数据类型的不同，对一个问题的建模有不同的方式。在机器学习或者人工智能领域，人们首先会考虑算法的学习方式。在机器学习领域，有几种主要的学习方式。将算法按照学习方式分类是一个不错的想法，这样可以让人们在建模和算法选择的时候考虑能根据输入数据来选择最合适的算法来获得最好的结果。&lt;/p&gt;
&lt;h3&gt;&lt;strong&gt;监督式学习：&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/518f60357fccae7066adb5ddfbff32b3.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;在监督式学习下，输入数据被称为“训练数据”，每组训练数据有一个明确的标识或结果，如对防垃圾邮件系统中“垃圾邮件”“非垃圾邮件”，对手写数字识别中的“1“，”2“，”3“，”4“等。在建立预测模型的时候，监督式学习建立一个学习过程，将预测结果与“训练数据”的实际结果进行比较，不断的调整预测模型，直到模型的预测结果达到一个预期的准确率。监督式学习的常见应用场景如分类问题和回归问题。常见算法有逻辑回归（Logistic Regression）和反向传递神经网络（Back Propagation Neural Network）&lt;/p&gt;
&lt;h3&gt;&lt;strong&gt;非监督式学习：&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/821b54dcaae4924fc7f5c3f858a1a259.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;在非监督式学习中，数据并不被特别标识，学习模型是为了推断出数据的一些内在结构。常见的应用场景包括关联规则的学习以及聚类等。常见算法包括Apriori算法以及k-Means算法。&lt;/p&gt;
&lt;h3&gt;&lt;strong&gt;半监督式学习：&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/1aa9c3e3344924f9f55916696ee74145.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;在此学习方式下，输入数据部分被标识，部分没有被标识，这种学习模型可以用来进行预测，但是模型首先需要学习数据的内在结构以便合理的组织数据来进行预测。应用场景包括分类和回归，算法包括一些对常用监督式学习算法的延伸，这些算法首先试图对未标识数据进行建模，在此基础上再对标识的数据进行预测。如图论推理算法（Graph Inference）或者拉普拉斯支持向量机（Laplacian SVM.）等。&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h3&gt;&lt;strong&gt;强化学习：&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/f646e8a089f83a20baa223d0bf1e6e79.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;在这种学习模式下，输入数据作为对模型的反馈，不像监督模型那样，输入数据仅仅是作为一个检查模型对错的方式，在强化学习下，输入数据直接反馈到模型，模型必须对此立刻作出调整。常见的应用场景包括动态系统以及机器人控制等。常见算法包括Q-Learning以及时间差学习（Temporal difference learning）&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;在企业数据应用的场景下， 人们最常用的可能就是监督式学习和非监督式学习的模型。 在图像识别等领域，由于存在大量的非标识的数据和少量的可标识数据， 目前半监督式学习是一个很热的话题。 而强化学习更多的应用在机器人控制及其他需要进行系统控制的领域。&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;算法类似性&lt;/strong&gt;&lt;/h2&gt;
&lt;h2&gt;&lt;strong&gt; &lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;根据算法的功能和形式的类似性，我们可以把算法分类，比如说基于树的算法，基于神经网络的算法等等。当然，机器学习的范围非常庞大，有些算法很难明确归类到某一类。而对于有些分类来说，同一分类的算法可以针对不同类型的问题。这里，我们尽量把常用的算法按照最容易理解的方式进行分类。&lt;/p&gt;
&lt;h3&gt;&lt;strong&gt;回归算法：&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/e0ebe4e5043444d2e306ed38f58e10c4.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;回归算法是试图采用对误差的衡量来探索变量之间的关系的一类算法。回归算法是统计机器学习的利器。在机器学习领域，人们说起回归，有时候是指一类问题，有时候是指一类算法，这一点常常会使初学者有所困惑。常见的回归算法包括：最小二乘法（Ordinary Least Square），逻辑回归（Logistic Regression），逐步式回归（Stepwise Regression），多元自适应回归样条（Multivariate Adaptive Regression Splines）以及本地散点平滑估计（Locally Estimated Scatterplot Smoothing）&lt;/p&gt;
&lt;h3&gt;&lt;strong&gt;基于实例的算法&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt; &lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/22ff19a0c252f0c406db6091813259df.jpg&quot;&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;基于实例的算法常常用来对决策问题建立模型，这样的模型常常先选取一批样本数据，然后根据某些近似性把新数据与样本数据进行比较。通过这种方式来寻找最佳的匹配。因此，基于实例的算法常常也被称为“赢家通吃”学习或者“基于记忆的学习”。常见的算法包括 k-Nearest Neighbor(KNN), 学习矢量量化（Learning Vector Quantization， LVQ），以及自组织映射算法（Self-Organizing Map ， SOM）&lt;/p&gt;
&lt;p&gt;&lt;strong&gt; &lt;/strong&gt;&lt;/p&gt;
&lt;h3&gt;&lt;strong&gt;正则化方法&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt; &lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/5578e533ef041563ba24807346bf9d3d.jpg&quot;&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;正则化方法是其他算法（通常是回归算法）的延伸，根据算法的复杂度对算法进行调整。正则化方法通常对简单模型予以奖励而对复杂算法予以惩罚。常见的算法包括：Ridge Regression， Least Absolute Shrinkage and Selection Operator（LASSO），以及弹性网络（Elastic Net）。&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h3&gt;&lt;strong&gt;决策树学习&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/15b49c4d7aab77171d9475c3fb8f5006.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;决策树算法根据数据的属性采用树状结构建立决策模型， 决策树模型常常用来解决分类和回归问题。常见的算法包括：分类及回归树（Classification And Regression Tree， CART）， ID3 (Iterative Dichotomiser 3)， C4.5， Chi-squared Automatic Interaction Detection(CHAID), Decision Stump, 随机森林（Random Forest）， 多元自适应回归样条（MARS）以及梯度推进机（Gradient Boosting Machine， GBM）&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h3&gt;&lt;strong&gt;贝叶斯方法&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/de9da162e1249472a426750340a8ba98.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;贝叶斯方法算法是基于贝叶斯定理的一类算法，主要用来解决分类和回归问题。常见算法包括：朴素贝叶斯算法，平均单依赖估计（Averaged One-Dependence Estimators， AODE），以及Bayesian Belief Network（BBN）。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt; &lt;/strong&gt;&lt;/p&gt;
&lt;h3&gt;&lt;strong&gt;基于核的算法&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt; &lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/cc04d60c7b791f909c93cc7d46b26b4a.jpg&quot;&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;基于核的算法中最著名的莫过于支持向量机（SVM）了。 基于核的算法把输入数据映射到一个高阶的向量空间， 在这些高阶向量空间里， 有些分类或者回归问题能够更容易的解决。 常见的基于核的算法包括：支持向量机（Support Vector Machine， SVM）， 径向基函数（Radial Basis Function ，RBF)， 以及线性判别分析（Linear Discriminate Analysis ，LDA)等&lt;/p&gt;
&lt;p&gt;&lt;strong&gt; &lt;/strong&gt;&lt;/p&gt;
&lt;h3&gt;&lt;strong&gt;聚类算法&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt; &lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/e7a6904e41cdcfa6c52dfceb24fa7340.jpg&quot;&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;聚类，就像回归一样，有时候人们描述的是一类问题，有时候描述的是一类算法。聚类算法通常按照中心点或者分层的方式对输入数据进行归并。所以的聚类算法都试图找到数据的内在结构，以便按照最大的共同点将数据进行归类。常见的聚类算法包括 k-Means算法以及期望最大化算法（Expectation Maximization， EM）。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt; &lt;/strong&gt;&lt;/p&gt;
&lt;h3&gt;&lt;strong&gt;关联规则学习&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt; &lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/826c328582b8fc95e6cae70745948efa.jpg&quot;&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;关联规则学习通过寻找最能够解释数据变量之间关系的规则，来找出大量多元数据集中有用的关联规则。常见算法包括 Apriori算法和Eclat算法等。&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h3&gt;&lt;strong&gt;人工神经网络&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/b91613bff92bd1a503c763d43e60170a.jpg&quot;&gt; &lt;/strong&gt;&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;人工神经网络算法模拟生物神经网络，是一类模式匹配算法。通常用于解决分类和回归问题。人工神经网络是机器学习的一个庞大的分支，有几百种不同的算法。（其中深度学习就是其中的一类算法，我们会单独讨论），重要的人工神经网络算法包括：感知器神经网络（Perceptron Neural Network）, 反向传递（Back Propagation）， Hopfield网络，自组织映射（Self-Organizing Map, SOM）。学习矢量量化（Learning Vector Quantization， LVQ）&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h3&gt;&lt;strong&gt;深度学习&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt; &lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/d8d09008ac3a811775a1e29af125980f.jpg&quot;&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;深度学习算法是对人工神经网络的发展。 在近期赢得了很多关注， 特别是&lt;a href=&quot;http://www.ctocio.com/ccnews/15615.html&quot;&gt;百度也开始发力深度学习后&lt;/a&gt;， 更是在国内引起了很多关注。   在计算能力变得日益廉价的今天，深度学习试图建立大得多也复杂得多的神经网络。很多深度学习的算法是半监督式学习算法，用来处理存在少量未标识数据的大数据集。常见的深度学习算法包括：受限波尔兹曼机（Restricted Boltzmann Machine， RBN）， Deep Belief Networks（DBN），卷积网络（Convolutional Network）, 堆栈式自动编码器（Stacked Auto-encoders）。&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h3&gt;&lt;strong&gt;降低维度算法&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt; &lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/0595924b456f4a99c51cff3d93f915cf.jpg&quot;&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;像聚类算法一样，降低维度算法试图分析数据的内在结构，不过降低维度算法是以非监督学习的方式试图利用较少的信息来归纳或者解释数据。这类算法可以用于高维数据的可视化或者用来简化数据以便监督式学习使用。常见的算法包括：主成份分析（Principle Component Analysis， PCA），偏最小二乘回归（Partial Least Square Regression，PLS）， Sammon映射，多维尺度（Multi-Dimensional Scaling, MDS）,  投影追踪（Projection Pursuit）等。&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h3&gt;&lt;strong&gt;集成算法：&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/4452f447638f455116a2fa9f54f4b500.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;集成算法用一些相对较弱的学习模型独立地就同样的样本进行训练，然后把结果整合起来进行整体预测。集成算法的主要难点在于究竟集成哪些独立的较弱的学习模型以及如何把学习结果整合起来。这是一类非常强大的算法，同时也非常流行。常见的算法包括：Boosting， Bootstrapped Aggregation（Bagging）， AdaBoost，堆叠泛化（Stacked Generalization， Blending），梯度推进机（Gradient Boosting Machine, GBM），随机森林（Random Forest）。&lt;/p&gt;


&lt;!-- div id=&quot;ad1&quot;&gt;
&lt;/div --&gt;


	


	

</description>
        <pubDate>Thu, 25 Sep 2014 00:00:00 +0800</pubDate>
        <link>http://iftti.com/posts/2014/2014-09-25-77620-574adc179.html</link>
        <guid isPermaLink="true">http://iftti.com/posts/2014/2014-09-25-77620-574adc179.html</guid>
        
        
        <category>jobbole</category>
        
      </item>
    
      <item>
        <title>推荐！可视化垃圾回收算法</title>
        <description>

					
		
&lt;!-- div id=&quot;ad1&quot;&gt;
&lt;script type=&quot;text/javascript&quot;&gt;
google_ad_client = &quot;ca-pub-7056282119617872&quot;;
google_ad_slot = &quot;6645040531&quot;;
google_ad_width = 300;
google_ad_height = 250;
&lt;/script&gt;
&lt;script type=&quot;text/javascript&quot;
src=&quot;http://pagead2.googlesyndication.com/pagead/show_ads.js&quot;&gt;
&lt;/script&gt;
&lt;/div&gt;
&lt;br/ --&gt;

&lt;p&gt;大部分开发者都认为自动垃圾回收器是理所当然的。实际上，这只是语言运行时提供的一项实用功能，旨在简化我们的开发工作。&lt;/p&gt;
&lt;p&gt;但是如果尝试着了解垃圾回收器的内部原理，你会发现很难弄明白。除非熟悉它的工作流程和错误处理方式，否则内部成千上万的实现细节会让你不知所措。&lt;/p&gt;
&lt;p&gt;我编译了一个有五种不同的垃圾回收算法工具。程序运行时会创建一个动画界面。你可以从&lt;a title=&quot; &quot; href=&quot;https://github.com/kenfox/gc-viz&quot; target=&quot;_blank&quot;&gt;github.com/kenfox/gc-viz&lt;/a&gt;上获取动画和代码来实现。非常让我惊讶的是，这个简单的动画显现出这些重要的算法。&lt;/p&gt;
&lt;h1&gt;任务完成后清理: aka No GC&lt;/h1&gt;
&lt;p&gt;&lt;img class=&quot;alignright&quot; alt=&quot;&quot; src=&quot;/images/jobbole.com/2e2f5d7c83df63e658803cb8b2e0d18a.jpg&quot; width=&quot;125&quot; height=&quot;200&quot;&gt;&lt;/p&gt;
&lt;p&gt;清理垃圾最简单可行的方法就是等一项任务完成之后，一次性处理所有的垃圾。这项技术非常有用，特别是如果能将一项任务分解成许多小任务。例如，Apache网络服务器在每次请求时创建一个小内存池并在请求完成后将创建的整个内存池完全释放。&lt;/p&gt;
&lt;p&gt;右图的动画显示了一个正在运行的程序。整张图片代表程序的内存区。内存区在开始时是黑色，黑色表明内存尚未被使用。闪着鲜绿色和黄色的区域表明该内存区域正在读写。颜色随着时间变化，你可以观察内存的使用情况，也可以看到当前的活动情况。如果仔细观察，你会发现内存区域中开始出现一些程序执行过程中会忽略的区域。这些区域就成了所谓的垃圾——程序不能访问和使用。垃圾区域之外的的内存区域是可用的。&lt;/p&gt;
&lt;p&gt;该程序的内存充足，所以不必担心程序运行时垃圾的清理。在后面的例子中我将一直使用这个简单的程序。&lt;/p&gt;
&lt;h1&gt;引用计数回收器&lt;/h1&gt;
&lt;p&gt;&lt;img class=&quot;alignright&quot; alt=&quot;&quot; src=&quot;/images/jobbole.com/35a16ebeb493293bd869b38d310e06b8.jpg&quot; width=&quot;125&quot; height=&quot;200&quot;&gt;&lt;/p&gt;
&lt;p&gt;另一个简单的解决方案是对你使用的资源(此处指内存中的对象)进行计数，当计数值变为0时，对其进行处理。这是一项广泛使用的技术，当开发者将垃圾回收添加到现有系统中时——这是唯一一个容易与其他资源管理器和现有代码库集成的垃圾回收器。苹果在为Objective-C发布了标志-擦除垃圾回收器后明白这个事实。发布产品出现很多问题以致于他们不得不废弃该项特性，取而代之的是性能良好的自动引用计数回收器。&lt;/p&gt;
&lt;p&gt;上面的动画显示了相同的程序，但是此时它将通过对内存中每一对象引用计数来处理垃圾。红色闪烁表示引用计数行为。引用计数的优势在于垃圾会被很快检测到——你可以看到红色闪烁过后紧接着该区域变黑。&lt;/p&gt;
&lt;p&gt;遗憾的是引用计数存在诸多问题。最糟糕的是，它不能处理循环结构。而循环结构非常常见——继承或反向引用都将建立一个循环，该结构将造成内存泄露。引用计数的开销也很大 ——从动画中可以看到即使当内存使用不在增长时，红色闪烁一直持续。CPU运算速度很快，但内存读写很慢，而计数器不断被加载并保存至内存。所有这些计数器的更新很难保证数据的只读或线程安全。&lt;/p&gt;
&lt;p&gt;引用计数是一种分摊算法（开销遍布整个程序运行时），但这是种分摊算法具有偶然性，不能保证反应时间。例如，程序中存在一个很大的树型结构。最后一段使用树的程序将触发对整个树的处理，墨菲说过事情如果有变坏的可能，不管这种可能性有多小，它总会发生。这里没有其他的分摊算法,所以分摊的偶然特征可能取决于数据。（所有这些算法有并发或部分并发的命令,但这些都是超出了程序可演示的范围。）&lt;/p&gt;
&lt;h1&gt;标记-擦除回收器&lt;/h1&gt;
&lt;p&gt;&lt;img class=&quot;alignright&quot; alt=&quot;&quot; src=&quot;/images/jobbole.com/458f2e66da1c9e965bb69c25997edbf5.jpg&quot; width=&quot;125&quot; height=&quot;200&quot;&gt;&lt;/p&gt;
&lt;p&gt;标记-擦除消除了引用计数存在的一些问题。它能够轻松解决循环结构在引用技术中存在的问题，由于不需维持计数，系统开销比较低。&lt;/p&gt;
&lt;p&gt;该算法舍弃垃圾检测的实时性。动画中，有一段运行时间没有任何红色的闪烁，然后突然出现许多红色闪烁表明当前正在标记活动对象。在标记完成后，程序要遍历整个内存空间并处理垃圾。在动画中你还将注意到—— 许多区域立刻变黑而不像引用计数方式那样随着时间慢慢变黑。&lt;/p&gt;
&lt;p&gt;标记-擦除比引用计数要求更高的一致性实现，而且很难移植到现有系统中。在标记阶段需要遍历所有活动数据，甚至是封装在对像中的数据。如果一个对象不支持遍历，那么尝试将标记-擦除移植到代码中风险太大。标记-擦除的另一个不足之处在于擦除阶段必须遍历整个内存来查找垃圾。对于一个产生垃圾较少的系统，这不是问题，但现在的函数式编程风格产生了大量的垃圾。&lt;/p&gt;
&lt;h1&gt;标记-压缩回收器&lt;/h1&gt;
&lt;p&gt;&lt;img class=&quot;alignright&quot; alt=&quot;&quot; src=&quot;/images/jobbole.com/837d74ff492982d54f37dc3c3abc604a.jpg&quot; width=&quot;125&quot; height=&quot;200&quot;&gt;&lt;/p&gt;
&lt;p&gt;在前面的动画中你可能注意到一点，对象从不移动。一旦对象在内存中分配，该对象的存储位置就不会再改变，即使被散步在黑色区域的内存碎片包围。下面两种算法用完全不同的方式改变了这种现象。&lt;/p&gt;
&lt;p&gt;标记-压缩算法不是仅通过标记内存区域是否空闲来处理内存，而是通过将对象移动到空闲表来实现。对象通常按照内存顺序存储,先分配的对象在内存的低地址空间——但是处理对象造成的空缺将随着对象的移动变大。&lt;/p&gt;
&lt;p&gt;移动对象意味着新对象只能在已使用内存的末尾创建。这就是所谓的“bunp”分配器，和栈分配器一样，但不限制栈空间。有些使用bump分配器的系统甚至不用调用栈存储数据，他们只在堆中分配调用帧,像其他对象一样对待。&lt;/p&gt;
&lt;p&gt;有时理论高于实践，另一个优势是当对象被压缩后，程序能够像访问硬件高速缓存一样访问内存。不确定你能否看到这个好处——尽管引用计数和标记-擦除使用的内存分配器很复杂,但调试效果很好，效率也很高。&lt;/p&gt;
&lt;p&gt;标记-压缩是算法很复杂，需要多次遍历所有分配对象。在动画中可以看到紧随红色闪烁的活动对象其后的是大量读和写标记为目的地计算，对象被移动，最终引用固定指向移动后的对象。这个复杂程序背后最大的优点是内存开销非常小。Oracle的Hotspot JVM使用了多种不同垃圾回收算法。而全局对象空间使用标记-压缩回收算法。&lt;/p&gt;
&lt;h1&gt;拷贝回收器&lt;/h1&gt;
&lt;p&gt;&lt;img class=&quot;alignright&quot; alt=&quot;&quot; src=&quot;/images/jobbole.com/2e2f5d7c83df63e658803cb8b2e0d18a.jpg&quot; width=&quot;125&quot; height=&quot;200&quot;&gt;&lt;/p&gt;
&lt;p&gt;最后使用动画显示的算法是大多数高性能垃圾收集系统的基础。它和标记-压缩是一样的移动回收器，但是相比之下实现却非常简单。它使用两块内存空间，在两个内存间交替复制活动对象。实际上，空间不止两块，这些空间用于不同代对象，新的对象在一个空间中创建，如果生命周期没有结束就会被复制到另一个空间，如果长期存在就会被复制到一个永久性空间。如果你听说一个垃圾收集器是分代的或短暂的,通常是多空间拷贝回收器。&lt;/p&gt;
&lt;p&gt;除了简单性和灵活性，该算法的主要优势在于只要在活动对象上花时间。没有独立的标记阶段必须被擦除或压缩。在遍历活动对象期间，对象会被立即复制，弥补了以往对象在引用计数时的不足。&lt;/p&gt;
&lt;p&gt;在动画中，你可以看到回收过程中乎所有的数据从一个空间复制到另一个空间。对该算法来说是个糟糕的情况，这是人们谈论优化垃圾收集器的一个原因。如果你能调整内存并有优化分配，使得在回收开始前大部分对象都废弃了，那么你就能兼顾安全函数式编程风格和高性能。&lt;/p&gt;
&lt;p&gt;（注：限于&lt;a href=&quot;mailto:%22ashiontang@gmail.com%22&quot;&gt;译者&lt;/a&gt;水平有限，不足之处恳请指正。）&lt;/p&gt;


&lt;!-- div id=&quot;ad1&quot;&gt;
&lt;/div --&gt;


	


	

</description>
        <pubDate>Thu, 25 Sep 2014 00:00:00 +0800</pubDate>
        <link>http://iftti.com/posts/2014/2014-09-25-77280-5a4d17c33.html</link>
        <guid isPermaLink="true">http://iftti.com/posts/2014/2014-09-25-77280-5a4d17c33.html</guid>
        
        
        <category>jobbole</category>
        
      </item>
    
      <item>
        <title>数据库优化案例（一）</title>
        <description>

                &lt;h1 id=&quot;-&quot;&gt;&lt;/h1&gt;
&lt;h3 id=&quot;-&quot;&gt;现象：&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;程序大量SQL运行超时;&lt;/li&gt;
&lt;li&gt;数据库中大量SQL长时间处于Updating阶段；&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&quot;-&quot;&gt;初步检查：&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;查看系统状态，CPU有24个核，只有一个核user利用率100%，其余核的idle基本上100%；&lt;/li&gt;
&lt;li&gt;内存、IO.util都正常；&lt;/li&gt;
&lt;li&gt;查看processlist，所有UPDATE语句长时间处于Updating阶段，其它类型的SQL没有Block或者运行缓慢现象；&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&quot;-&quot;&gt;进一步排查：&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;分析运行缓慢的SQL，发现运行缓慢的Update语句操作的都是同一张表；&lt;/li&gt;
&lt;li&gt;执行缓慢的Update语句总共有两类：一类是通过主键更新一行数据；另一类是根据状态字段批量更新数据；Schema和SQL示例如下：&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;/p&gt;
&lt;!-- Crayon Syntax Highlighter v2.1.2 --&gt;

		&lt;div id=&quot;crayon-542531c3c2293&quot; class=&quot;crayon-syntax crayon-theme-github crayon-font-monaco crayon-os-pc print-yes&quot; data-settings=&quot; minimize scroll-mouseover wrap&quot; style=&quot; margin-top: 12px; margin-bottom: 12px; float: none; clear: both; font-size: 12px !important; line-height: 15px !important;&quot;&gt;
		
			&lt;div class=&quot;crayon-toolbar&quot; data-settings=&quot; mouseover overlay hide delay&quot; style=&quot;height: 18px !important; line-height: 18px !important;&quot;&gt;
&lt;span class=&quot;crayon-title&quot;&gt;&lt;/span&gt;
			&lt;div class=&quot;crayon-tools&quot;&gt;
&lt;div class=&quot;crayon-button crayon-nums-button&quot; title=&quot;切换显示行编号&quot;&gt;&lt;/div&gt;
&lt;div class=&quot;crayon-button crayon-plain-button&quot; title=&quot;纯文本显示代码&quot;&gt;&lt;/div&gt;
&lt;div class=&quot;crayon-button crayon-wrap-button&quot; title=&quot;切换自动换行&quot;&gt;&lt;/div&gt;
&lt;div class=&quot;crayon-button crayon-expand-button&quot; title=&quot;Expand Code&quot;&gt;&lt;/div&gt;
&lt;div class=&quot;crayon-button crayon-copy-button&quot; title=&quot;Expand Code&quot;&gt;&lt;/div&gt;
&lt;div class=&quot;crayon-button crayon-popup-button&quot; title=&quot;在新窗口中显示代码&quot;&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
			&lt;div class=&quot;crayon-info&quot; style=&quot;min-height: 15px !important; line-height: 15px !important;&quot;&gt;&lt;/div&gt;
			&lt;div class=&quot;crayon-plain-wrap&quot;&gt;&lt;textarea class=&quot;crayon-plain print-no&quot; data-settings=&quot;dblclick&quot; readonly style=&quot;-moz-tab-size:4; -o-tab-size:4; -webkit-tab-size:4; tab-size:4; font-size: 12px !important; line-height: 15px !important;&quot;&gt;
&amp;lt;code&amp;gt;-- Schema
CREATE TABLE `general_mail` (
  `id` bigint(20) unsigned NOT NULL AUTO_INCREMENT,
  `user_id` bigint(20) unsigned NOT NULL DEFAULT &#39;0&#39;,
  `value` text NOT NULL,
  `status` tinyint(4) unsigned NOT NULL DEFAULT &#39;0&#39;,
  `type` tinyint(4) unsigned NOT NULL DEFAULT &#39;0&#39;,
    ......
  PRIMARY KEY (`id`),
  KEY `idx_status_type` (`status`,`type`),
) ENGINE=InnoDB;

-- 根据主键进行更新
UPDATE tbl_A SET status=N, value=&#39;.....&#39;, user_id=N WHERE id = N;

-- 根据状态进行更新
UPDATE tbl_A SET status=N WHERE (status=N or status=N) and user_id=N limit N;
&amp;lt;/code&amp;gt;&lt;/textarea&gt;&lt;/div&gt;
			&lt;div class=&quot;crayon-main&quot; style=&quot;&quot;&gt;
				&lt;table class=&quot;crayon-table&quot;&gt;
					&lt;tr class=&quot;crayon-row&quot;&gt;
				&lt;td class=&quot;crayon-nums &quot; data-settings=&quot;hide&quot;&gt;
					&lt;div class=&quot;crayon-nums-content&quot; style=&quot;font-size: 12px !important; line-height: 15px !important;&quot;&gt;
&lt;div class=&quot;crayon-num&quot; data-line=&quot;crayon-542531c3c2293-1&quot;&gt;1&lt;/div&gt;
&lt;div class=&quot;crayon-num crayon-striped-num&quot; data-line=&quot;crayon-542531c3c2293-2&quot;&gt;2&lt;/div&gt;
&lt;div class=&quot;crayon-num&quot; data-line=&quot;crayon-542531c3c2293-3&quot;&gt;3&lt;/div&gt;
&lt;div class=&quot;crayon-num crayon-striped-num&quot; data-line=&quot;crayon-542531c3c2293-4&quot;&gt;4&lt;/div&gt;
&lt;div class=&quot;crayon-num&quot; data-line=&quot;crayon-542531c3c2293-5&quot;&gt;5&lt;/div&gt;
&lt;div class=&quot;crayon-num crayon-striped-num&quot; data-line=&quot;crayon-542531c3c2293-6&quot;&gt;6&lt;/div&gt;
&lt;div class=&quot;crayon-num&quot; data-line=&quot;crayon-542531c3c2293-7&quot;&gt;7&lt;/div&gt;
&lt;div class=&quot;crayon-num crayon-striped-num&quot; data-line=&quot;crayon-542531c3c2293-8&quot;&gt;8&lt;/div&gt;
&lt;div class=&quot;crayon-num&quot; data-line=&quot;crayon-542531c3c2293-9&quot;&gt;9&lt;/div&gt;
&lt;div class=&quot;crayon-num crayon-striped-num&quot; data-line=&quot;crayon-542531c3c2293-10&quot;&gt;10&lt;/div&gt;
&lt;div class=&quot;crayon-num&quot; data-line=&quot;crayon-542531c3c2293-11&quot;&gt;11&lt;/div&gt;
&lt;div class=&quot;crayon-num crayon-striped-num&quot; data-line=&quot;crayon-542531c3c2293-12&quot;&gt;12&lt;/div&gt;
&lt;div class=&quot;crayon-num&quot; data-line=&quot;crayon-542531c3c2293-13&quot;&gt;13&lt;/div&gt;
&lt;div class=&quot;crayon-num crayon-striped-num&quot; data-line=&quot;crayon-542531c3c2293-14&quot;&gt;14&lt;/div&gt;
&lt;div class=&quot;crayon-num&quot; data-line=&quot;crayon-542531c3c2293-15&quot;&gt;15&lt;/div&gt;
&lt;div class=&quot;crayon-num crayon-striped-num&quot; data-line=&quot;crayon-542531c3c2293-16&quot;&gt;16&lt;/div&gt;
&lt;div class=&quot;crayon-num&quot; data-line=&quot;crayon-542531c3c2293-17&quot;&gt;17&lt;/div&gt;
&lt;div class=&quot;crayon-num crayon-striped-num&quot; data-line=&quot;crayon-542531c3c2293-18&quot;&gt;18&lt;/div&gt;
&lt;/div&gt;
				&lt;/td&gt;
						&lt;td class=&quot;crayon-code&quot;&gt;&lt;div class=&quot;crayon-pre&quot; style=&quot;font-size: 12px !important; line-height: 15px !important;&quot;&gt;
&lt;div class=&quot;crayon-line&quot; id=&quot;crayon-542531c3c2293-1&quot;&gt;
&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;i&quot;&gt;code&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;e&quot;&gt;Schema&lt;/span&gt;
&lt;/div&gt;
&lt;div class=&quot;crayon-line crayon-striped-line&quot; id=&quot;crayon-542531c3c2293-2&quot;&gt;
&lt;span class=&quot;e&quot;&gt;CREATE &lt;/span&gt;&lt;span class=&quot;i&quot;&gt;TABLE&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;`&lt;/span&gt;&lt;span class=&quot;i&quot;&gt;general_mail&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;`&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;(&lt;/span&gt;
&lt;/div&gt;
&lt;div class=&quot;crayon-line&quot; id=&quot;crayon-542531c3c2293-3&quot;&gt;
&lt;span class=&quot;h&quot;&gt;  &lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;`&lt;/span&gt;&lt;span class=&quot;i&quot;&gt;id&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;`&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;e&quot;&gt;bigint&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;cn&quot;&gt;20&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;t&quot;&gt;unsigned&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;st&quot;&gt;NOT&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;t&quot;&gt;NULL&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;i&quot;&gt;AUTO_INCREMENT&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;,&lt;/span&gt;
&lt;/div&gt;
&lt;div class=&quot;crayon-line crayon-striped-line&quot; id=&quot;crayon-542531c3c2293-4&quot;&gt;
&lt;span class=&quot;h&quot;&gt;  &lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;`&lt;/span&gt;&lt;span class=&quot;i&quot;&gt;user_id&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;`&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;e&quot;&gt;bigint&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;cn&quot;&gt;20&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;t&quot;&gt;unsigned&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;st&quot;&gt;NOT&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;t&quot;&gt;NULL&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;st&quot;&gt;DEFAULT&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;0&#39;&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;,&lt;/span&gt;
&lt;/div&gt;
&lt;div class=&quot;crayon-line&quot; id=&quot;crayon-542531c3c2293-5&quot;&gt;
&lt;span class=&quot;h&quot;&gt;  &lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;`&lt;/span&gt;&lt;span class=&quot;i&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;`&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;e&quot;&gt;text &lt;/span&gt;&lt;span class=&quot;st&quot;&gt;NOT&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;t&quot;&gt;NULL&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;,&lt;/span&gt;
&lt;/div&gt;
&lt;div class=&quot;crayon-line crayon-striped-line&quot; id=&quot;crayon-542531c3c2293-6&quot;&gt;
&lt;span class=&quot;h&quot;&gt;  &lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;`&lt;/span&gt;&lt;span class=&quot;i&quot;&gt;status&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;`&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;e&quot;&gt;tinyint&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;cn&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;t&quot;&gt;unsigned&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;st&quot;&gt;NOT&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;t&quot;&gt;NULL&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;st&quot;&gt;DEFAULT&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;0&#39;&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;,&lt;/span&gt;
&lt;/div&gt;
&lt;div class=&quot;crayon-line&quot; id=&quot;crayon-542531c3c2293-7&quot;&gt;
&lt;span class=&quot;h&quot;&gt;  &lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;`&lt;/span&gt;&lt;span class=&quot;i&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;`&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;e&quot;&gt;tinyint&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;cn&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;t&quot;&gt;unsigned&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;st&quot;&gt;NOT&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;t&quot;&gt;NULL&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;st&quot;&gt;DEFAULT&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;0&#39;&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;,&lt;/span&gt;
&lt;/div&gt;
&lt;div class=&quot;crayon-line crayon-striped-line&quot; id=&quot;crayon-542531c3c2293-8&quot;&gt;
&lt;span class=&quot;h&quot;&gt;    &lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;.&lt;/span&gt;
&lt;/div&gt;
&lt;div class=&quot;crayon-line&quot; id=&quot;crayon-542531c3c2293-9&quot;&gt;
&lt;span class=&quot;h&quot;&gt;  &lt;/span&gt;&lt;span class=&quot;e&quot;&gt;PRIMARY &lt;/span&gt;&lt;span class=&quot;e&quot;&gt;KEY&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;`&lt;/span&gt;&lt;span class=&quot;i&quot;&gt;id&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;`&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;,&lt;/span&gt;
&lt;/div&gt;
&lt;div class=&quot;crayon-line crayon-striped-line&quot; id=&quot;crayon-542531c3c2293-10&quot;&gt;
&lt;span class=&quot;h&quot;&gt;  &lt;/span&gt;&lt;span class=&quot;i&quot;&gt;KEY&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;`&lt;/span&gt;&lt;span class=&quot;i&quot;&gt;idx_status_type&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;`&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;`&lt;/span&gt;&lt;span class=&quot;i&quot;&gt;status&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;`&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;`&lt;/span&gt;&lt;span class=&quot;i&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;`&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;,&lt;/span&gt;
&lt;/div&gt;
&lt;div class=&quot;crayon-line&quot; id=&quot;crayon-542531c3c2293-11&quot;&gt;
&lt;span class=&quot;sy&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;v&quot;&gt;ENGINE&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;i&quot;&gt;InnoDB&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;;&lt;/span&gt;
&lt;/div&gt;
&lt;div class=&quot;crayon-line crayon-striped-line&quot; id=&quot;crayon-542531c3c2293-12&quot;&gt; &lt;/div&gt;
&lt;div class=&quot;crayon-line&quot; id=&quot;crayon-542531c3c2293-13&quot;&gt;
&lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;根据主键进行更新&lt;/div&gt;
&lt;div class=&quot;crayon-line crayon-striped-line&quot; id=&quot;crayon-542531c3c2293-14&quot;&gt;
&lt;span class=&quot;e&quot;&gt;UPDATE &lt;/span&gt;&lt;span class=&quot;e&quot;&gt;tbl_A &lt;/span&gt;&lt;span class=&quot;e&quot;&gt;SET &lt;/span&gt;&lt;span class=&quot;v&quot;&gt;status&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;i&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;v&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;.....&#39;&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;v&quot;&gt;user_id&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;i&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;e&quot;&gt;WHERE &lt;/span&gt;&lt;span class=&quot;v&quot;&gt;id&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;i&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;;&lt;/span&gt;
&lt;/div&gt;
&lt;div class=&quot;crayon-line&quot; id=&quot;crayon-542531c3c2293-15&quot;&gt; &lt;/div&gt;
&lt;div class=&quot;crayon-line crayon-striped-line&quot; id=&quot;crayon-542531c3c2293-16&quot;&gt;
&lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;根据状态进行更新&lt;/div&gt;
&lt;div class=&quot;crayon-line&quot; id=&quot;crayon-542531c3c2293-17&quot;&gt;
&lt;span class=&quot;e&quot;&gt;UPDATE &lt;/span&gt;&lt;span class=&quot;e&quot;&gt;tbl_A &lt;/span&gt;&lt;span class=&quot;e&quot;&gt;SET &lt;/span&gt;&lt;span class=&quot;v&quot;&gt;status&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;i&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;e&quot;&gt;WHERE&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;v&quot;&gt;status&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;i&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;st&quot;&gt;or&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;v&quot;&gt;status&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;i&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;st&quot;&gt;and&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;v&quot;&gt;user_id&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;i&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;i&quot;&gt;limit&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;i&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;;&lt;/span&gt;
&lt;/div&gt;
&lt;div class=&quot;crayon-line crayon-striped-line&quot; id=&quot;crayon-542531c3c2293-18&quot;&gt;
&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;i&quot;&gt;code&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;/td&gt;
					&lt;/tr&gt;
				&lt;/table&gt;
			&lt;/div&gt;
		&lt;/div&gt;
&lt;!-- [Format Time: 0.0059 seconds] --&gt;
&lt;p&gt;基本可以推测是根据状态字段更新数据的SQL导致的问题。进一步查看INNODB STATUS，发现如下记录：&lt;/p&gt;
&lt;!-- Crayon Syntax Highlighter v2.1.2 --&gt;

		&lt;div id=&quot;crayon-542531c3c2345&quot; class=&quot;crayon-syntax crayon-theme-github crayon-font-monaco crayon-os-pc print-yes&quot; data-settings=&quot; minimize scroll-mouseover wrap&quot; style=&quot; margin-top: 12px; margin-bottom: 12px; float: none; clear: both; font-size: 12px !important; line-height: 15px !important;&quot;&gt;
		
			&lt;div class=&quot;crayon-toolbar&quot; data-settings=&quot; mouseover overlay hide delay&quot; style=&quot;height: 18px !important; line-height: 18px !important;&quot;&gt;
&lt;span class=&quot;crayon-title&quot;&gt;&lt;/span&gt;
			&lt;div class=&quot;crayon-tools&quot;&gt;
&lt;div class=&quot;crayon-button crayon-nums-button&quot; title=&quot;切换显示行编号&quot;&gt;&lt;/div&gt;
&lt;div class=&quot;crayon-button crayon-plain-button&quot; title=&quot;纯文本显示代码&quot;&gt;&lt;/div&gt;
&lt;div class=&quot;crayon-button crayon-wrap-button&quot; title=&quot;切换自动换行&quot;&gt;&lt;/div&gt;
&lt;div class=&quot;crayon-button crayon-expand-button&quot; title=&quot;Expand Code&quot;&gt;&lt;/div&gt;
&lt;div class=&quot;crayon-button crayon-copy-button&quot; title=&quot;Expand Code&quot;&gt;&lt;/div&gt;
&lt;div class=&quot;crayon-button crayon-popup-button&quot; title=&quot;在新窗口中显示代码&quot;&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
			&lt;div class=&quot;crayon-info&quot; style=&quot;min-height: 15px !important; line-height: 15px !important;&quot;&gt;&lt;/div&gt;
			&lt;div class=&quot;crayon-plain-wrap&quot;&gt;&lt;textarea class=&quot;crayon-plain print-no&quot; data-settings=&quot;dblclick&quot; readonly style=&quot;-moz-tab-size:4; -o-tab-size:4; -webkit-tab-size:4; tab-size:4; font-size: 12px !important; line-height: 15px !important;&quot;&gt;
&amp;lt;code&amp;gt;------------------
---TRANSACTION 43DF0254C, ACTIVE 51 sec updating or deleting
mysql tables in use 1, locked 1
LOCK WAIT 3 lock struct(s), heap size 1248, 2 row lock(s), undo log entries 1
MySQL thread id 118879, OS thread handle 0x7f51950b6700, query id 18995841 10.0.0.0 mysql_user Updating
UPDATE tbl_A SET status=N, value=&#39;.....&#39;, user_id=N WHERE id = N
------- TRX HAS BEEN WAITING 51 SEC FOR THIS LOCK TO BE GRANTED:
RECORD LOCKS space id 2186 page no 188857 n bits 688 index `idx_status_type` of table `db`.`tbl_A` trx id 43DF0254C lock_mode X locks gap before rec insert intention waiting
Record lock, heap no 497 PHYSICAL RECORD: n_fields 3; compact format; info bits 0
 0: len 1; hex 02; asc  ;;
 1: len 1; hex 01; asc  ;;
 2: len 8; hex 00000000011c1f0f; asc         ;;

------------------
---TRANSACTION 43DF0216B, ACTIVE 65 sec fetching rows, thread declared inside InnoDB 155
mysql tables in use 1, locked 1
794767 lock struct(s), heap size 71694776, 13900037 row lock(s)
MySQL thread id 118812, OS thread handle 0x7f51951dc700, query id 18995134 10.0.0.1 mysql_user Searching rows for update
UPDATE tbl_A SET status=N WHERE (status=N or status=N) and user_id=N limit N
&amp;lt;/code&amp;gt;&lt;/textarea&gt;&lt;/div&gt;
			&lt;div class=&quot;crayon-main&quot; style=&quot;&quot;&gt;
				&lt;table class=&quot;crayon-table&quot;&gt;
					&lt;tr class=&quot;crayon-row&quot;&gt;
				&lt;td class=&quot;crayon-nums &quot; data-settings=&quot;hide&quot;&gt;
					&lt;div class=&quot;crayon-nums-content&quot; style=&quot;font-size: 12px !important; line-height: 15px !important;&quot;&gt;
&lt;div class=&quot;crayon-num&quot; data-line=&quot;crayon-542531c3c2345-1&quot;&gt;1&lt;/div&gt;
&lt;div class=&quot;crayon-num crayon-striped-num&quot; data-line=&quot;crayon-542531c3c2345-2&quot;&gt;2&lt;/div&gt;
&lt;div class=&quot;crayon-num&quot; data-line=&quot;crayon-542531c3c2345-3&quot;&gt;3&lt;/div&gt;
&lt;div class=&quot;crayon-num crayon-striped-num&quot; data-line=&quot;crayon-542531c3c2345-4&quot;&gt;4&lt;/div&gt;
&lt;div class=&quot;crayon-num&quot; data-line=&quot;crayon-542531c3c2345-5&quot;&gt;5&lt;/div&gt;
&lt;div class=&quot;crayon-num crayon-striped-num&quot; data-line=&quot;crayon-542531c3c2345-6&quot;&gt;6&lt;/div&gt;
&lt;div class=&quot;crayon-num&quot; data-line=&quot;crayon-542531c3c2345-7&quot;&gt;7&lt;/div&gt;
&lt;div class=&quot;crayon-num crayon-striped-num&quot; data-line=&quot;crayon-542531c3c2345-8&quot;&gt;8&lt;/div&gt;
&lt;div class=&quot;crayon-num&quot; data-line=&quot;crayon-542531c3c2345-9&quot;&gt;9&lt;/div&gt;
&lt;div class=&quot;crayon-num crayon-striped-num&quot; data-line=&quot;crayon-542531c3c2345-10&quot;&gt;10&lt;/div&gt;
&lt;div class=&quot;crayon-num&quot; data-line=&quot;crayon-542531c3c2345-11&quot;&gt;11&lt;/div&gt;
&lt;div class=&quot;crayon-num crayon-striped-num&quot; data-line=&quot;crayon-542531c3c2345-12&quot;&gt;12&lt;/div&gt;
&lt;div class=&quot;crayon-num&quot; data-line=&quot;crayon-542531c3c2345-13&quot;&gt;13&lt;/div&gt;
&lt;div class=&quot;crayon-num crayon-striped-num&quot; data-line=&quot;crayon-542531c3c2345-14&quot;&gt;14&lt;/div&gt;
&lt;div class=&quot;crayon-num&quot; data-line=&quot;crayon-542531c3c2345-15&quot;&gt;15&lt;/div&gt;
&lt;div class=&quot;crayon-num crayon-striped-num&quot; data-line=&quot;crayon-542531c3c2345-16&quot;&gt;16&lt;/div&gt;
&lt;div class=&quot;crayon-num&quot; data-line=&quot;crayon-542531c3c2345-17&quot;&gt;17&lt;/div&gt;
&lt;div class=&quot;crayon-num crayon-striped-num&quot; data-line=&quot;crayon-542531c3c2345-18&quot;&gt;18&lt;/div&gt;
&lt;div class=&quot;crayon-num&quot; data-line=&quot;crayon-542531c3c2345-19&quot;&gt;19&lt;/div&gt;
&lt;div class=&quot;crayon-num crayon-striped-num&quot; data-line=&quot;crayon-542531c3c2345-20&quot;&gt;20&lt;/div&gt;
&lt;/div&gt;
				&lt;/td&gt;
						&lt;td class=&quot;crayon-code&quot;&gt;&lt;div class=&quot;crayon-pre&quot; style=&quot;font-size: 12px !important; line-height: 15px !important;&quot;&gt;
&lt;div class=&quot;crayon-line&quot; id=&quot;crayon-542531c3c2345-1&quot;&gt;
&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;i&quot;&gt;code&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;
&lt;/div&gt;
&lt;div class=&quot;crayon-line crayon-striped-line&quot; id=&quot;crayon-542531c3c2345-2&quot;&gt;
&lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;i&quot;&gt;TRANSACTION&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;cn&quot;&gt;43DF0254C&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;i&quot;&gt;ACTIVE&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;cn&quot;&gt;51&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;e&quot;&gt;sec &lt;/span&gt;&lt;span class=&quot;e&quot;&gt;updating &lt;/span&gt;&lt;span class=&quot;st&quot;&gt;or&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;e&quot;&gt;deleting&lt;/span&gt;
&lt;/div&gt;
&lt;div class=&quot;crayon-line&quot; id=&quot;crayon-542531c3c2345-3&quot;&gt;
&lt;span class=&quot;e&quot;&gt;mysql &lt;/span&gt;&lt;span class=&quot;e&quot;&gt;tables &lt;/span&gt;&lt;span class=&quot;st&quot;&gt;in&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;st&quot;&gt;use&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;cn&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;i&quot;&gt;locked&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;cn&quot;&gt;1&lt;/span&gt;
&lt;/div&gt;
&lt;div class=&quot;crayon-line crayon-striped-line&quot; id=&quot;crayon-542531c3c2345-4&quot;&gt;
&lt;span class=&quot;e&quot;&gt;LOCK &lt;/span&gt;&lt;span class=&quot;i&quot;&gt;WAIT&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;cn&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;e&quot;&gt;lock &lt;/span&gt;&lt;span class=&quot;t&quot;&gt;struct&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;i&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;e&quot;&gt;heap &lt;/span&gt;&lt;span class=&quot;i&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;cn&quot;&gt;1248&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;cn&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;e&quot;&gt;row &lt;/span&gt;&lt;span class=&quot;e&quot;&gt;lock&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;i&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;e&quot;&gt;undo &lt;/span&gt;&lt;span class=&quot;e&quot;&gt;log &lt;/span&gt;&lt;span class=&quot;i&quot;&gt;entries&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;cn&quot;&gt;1&lt;/span&gt;
&lt;/div&gt;
&lt;div class=&quot;crayon-line&quot; id=&quot;crayon-542531c3c2345-5&quot;&gt;
&lt;span class=&quot;e&quot;&gt;MySQL &lt;/span&gt;&lt;span class=&quot;e&quot;&gt;thread &lt;/span&gt;&lt;span class=&quot;i&quot;&gt;id&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;cn&quot;&gt;118879&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;e&quot;&gt;OS &lt;/span&gt;&lt;span class=&quot;e&quot;&gt;thread &lt;/span&gt;&lt;span class=&quot;i&quot;&gt;handle&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;cn&quot;&gt;0x7f51950b6700&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;e&quot;&gt;query &lt;/span&gt;&lt;span class=&quot;i&quot;&gt;id&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;cn&quot;&gt;18995841&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;cn&quot;&gt;10.0.0.0&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;e&quot;&gt;mysql_user &lt;/span&gt;&lt;span class=&quot;e&quot;&gt;Updating&lt;/span&gt;
&lt;/div&gt;
&lt;div class=&quot;crayon-line crayon-striped-line&quot; id=&quot;crayon-542531c3c2345-6&quot;&gt;
&lt;span class=&quot;e&quot;&gt;UPDATE &lt;/span&gt;&lt;span class=&quot;e&quot;&gt;tbl_A &lt;/span&gt;&lt;span class=&quot;e&quot;&gt;SET &lt;/span&gt;&lt;span class=&quot;v&quot;&gt;status&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;i&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;v&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;.....&#39;&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;v&quot;&gt;user_id&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;i&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;e&quot;&gt;WHERE &lt;/span&gt;&lt;span class=&quot;v&quot;&gt;id&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;i&quot;&gt;N&lt;/span&gt;
&lt;/div&gt;
&lt;div class=&quot;crayon-line&quot; id=&quot;crayon-542531c3c2345-7&quot;&gt;
&lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;e&quot;&gt;TRX &lt;/span&gt;&lt;span class=&quot;e&quot;&gt;HAS &lt;/span&gt;&lt;span class=&quot;e&quot;&gt;BEEN &lt;/span&gt;&lt;span class=&quot;i&quot;&gt;WAITING&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;cn&quot;&gt;51&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;e&quot;&gt;SEC &lt;/span&gt;&lt;span class=&quot;st&quot;&gt;FOR&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;r&quot;&gt;THIS&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;e&quot;&gt;LOCK &lt;/span&gt;&lt;span class=&quot;st&quot;&gt;TO&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;e&quot;&gt;BE &lt;/span&gt;&lt;span class=&quot;i&quot;&gt;GRANTED&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;
&lt;/div&gt;
&lt;div class=&quot;crayon-line crayon-striped-line&quot; id=&quot;crayon-542531c3c2345-8&quot;&gt;
&lt;span class=&quot;e&quot;&gt;RECORD &lt;/span&gt;&lt;span class=&quot;e&quot;&gt;LOCKS &lt;/span&gt;&lt;span class=&quot;e&quot;&gt;space &lt;/span&gt;&lt;span class=&quot;i&quot;&gt;id&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;cn&quot;&gt;2186&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;e&quot;&gt;page &lt;/span&gt;&lt;span class=&quot;i&quot;&gt;no&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;cn&quot;&gt;188857&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;i&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;i&quot;&gt;bits&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;cn&quot;&gt;688&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;i&quot;&gt;index&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;`&lt;/span&gt;&lt;span class=&quot;i&quot;&gt;idx_status_type&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;`&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;e&quot;&gt;of &lt;/span&gt;&lt;span class=&quot;i&quot;&gt;table&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;`&lt;/span&gt;&lt;span class=&quot;i&quot;&gt;db&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;`&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;`&lt;/span&gt;&lt;span class=&quot;i&quot;&gt;tbl_A&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;`&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;e&quot;&gt;trx &lt;/span&gt;&lt;span class=&quot;i&quot;&gt;id&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;cn&quot;&gt;43DF0254C&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;i&quot;&gt;lock_mode&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;i&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;e&quot;&gt;locks &lt;/span&gt;&lt;span class=&quot;e&quot;&gt;gap &lt;/span&gt;&lt;span class=&quot;e&quot;&gt;before &lt;/span&gt;&lt;span class=&quot;e&quot;&gt;rec &lt;/span&gt;&lt;span class=&quot;e&quot;&gt;insert &lt;/span&gt;&lt;span class=&quot;e&quot;&gt;intention &lt;/span&gt;&lt;span class=&quot;e&quot;&gt;waiting&lt;/span&gt;
&lt;/div&gt;
&lt;div class=&quot;crayon-line&quot; id=&quot;crayon-542531c3c2345-9&quot;&gt;
&lt;span class=&quot;e&quot;&gt;Record &lt;/span&gt;&lt;span class=&quot;i&quot;&gt;lock&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;e&quot;&gt;heap &lt;/span&gt;&lt;span class=&quot;i&quot;&gt;no&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;cn&quot;&gt;497&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;e&quot;&gt;PHYSICAL &lt;/span&gt;&lt;span class=&quot;i&quot;&gt;RECORD&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;i&quot;&gt;n_fields&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;cn&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;;&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;e&quot;&gt;compact &lt;/span&gt;&lt;span class=&quot;i&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;;&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;e&quot;&gt;info &lt;/span&gt;&lt;span class=&quot;i&quot;&gt;bits&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;cn&quot;&gt;0&lt;/span&gt;
&lt;/div&gt;
&lt;div class=&quot;crayon-line crayon-striped-line&quot; id=&quot;crayon-542531c3c2345-10&quot;&gt;
&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;cn&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;i&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;cn&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;;&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;i&quot;&gt;hex&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;cn&quot;&gt;02&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;;&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;i&quot;&gt;asc&lt;/span&gt;&lt;span class=&quot;h&quot;&gt;  &lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;;&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;;&lt;/span&gt;
&lt;/div&gt;
&lt;div class=&quot;crayon-line&quot; id=&quot;crayon-542531c3c2345-11&quot;&gt;
&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;cn&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;i&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;cn&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;;&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;i&quot;&gt;hex&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;cn&quot;&gt;01&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;;&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;i&quot;&gt;asc&lt;/span&gt;&lt;span class=&quot;h&quot;&gt;  &lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;;&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;;&lt;/span&gt;
&lt;/div&gt;
&lt;div class=&quot;crayon-line crayon-striped-line&quot; id=&quot;crayon-542531c3c2345-12&quot;&gt;
&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;cn&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;i&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;cn&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;;&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;i&quot;&gt;hex&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;cn&quot;&gt;00000000011c1f0f&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;;&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;i&quot;&gt;asc&lt;/span&gt;&lt;span class=&quot;h&quot;&gt;         &lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;;&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;;&lt;/span&gt;
&lt;/div&gt;
&lt;div class=&quot;crayon-line&quot; id=&quot;crayon-542531c3c2345-13&quot;&gt; &lt;/div&gt;
&lt;div class=&quot;crayon-line crayon-striped-line&quot; id=&quot;crayon-542531c3c2345-14&quot;&gt;
&lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;
&lt;/div&gt;
&lt;div class=&quot;crayon-line&quot; id=&quot;crayon-542531c3c2345-15&quot;&gt;
&lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;i&quot;&gt;TRANSACTION&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;cn&quot;&gt;43DF0216B&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;i&quot;&gt;ACTIVE&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;cn&quot;&gt;65&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;e&quot;&gt;sec &lt;/span&gt;&lt;span class=&quot;e&quot;&gt;fetching &lt;/span&gt;&lt;span class=&quot;i&quot;&gt;rows&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;e&quot;&gt;thread &lt;/span&gt;&lt;span class=&quot;e&quot;&gt;declared &lt;/span&gt;&lt;span class=&quot;e&quot;&gt;inside &lt;/span&gt;&lt;span class=&quot;i&quot;&gt;InnoDB&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;cn&quot;&gt;155&lt;/span&gt;
&lt;/div&gt;
&lt;div class=&quot;crayon-line crayon-striped-line&quot; id=&quot;crayon-542531c3c2345-16&quot;&gt;
&lt;span class=&quot;e&quot;&gt;mysql &lt;/span&gt;&lt;span class=&quot;e&quot;&gt;tables &lt;/span&gt;&lt;span class=&quot;st&quot;&gt;in&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;st&quot;&gt;use&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;cn&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;i&quot;&gt;locked&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;cn&quot;&gt;1&lt;/span&gt;
&lt;/div&gt;
&lt;div class=&quot;crayon-line&quot; id=&quot;crayon-542531c3c2345-17&quot;&gt;
&lt;span class=&quot;cn&quot;&gt;794767&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;e&quot;&gt;lock &lt;/span&gt;&lt;span class=&quot;t&quot;&gt;struct&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;i&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;e&quot;&gt;heap &lt;/span&gt;&lt;span class=&quot;i&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;cn&quot;&gt;71694776&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;cn&quot;&gt;13900037&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;e&quot;&gt;row &lt;/span&gt;&lt;span class=&quot;e&quot;&gt;lock&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;i&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;)&lt;/span&gt;
&lt;/div&gt;
&lt;div class=&quot;crayon-line crayon-striped-line&quot; id=&quot;crayon-542531c3c2345-18&quot;&gt;
&lt;span class=&quot;e&quot;&gt;MySQL &lt;/span&gt;&lt;span class=&quot;e&quot;&gt;thread &lt;/span&gt;&lt;span class=&quot;i&quot;&gt;id&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;cn&quot;&gt;118812&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;e&quot;&gt;OS &lt;/span&gt;&lt;span class=&quot;e&quot;&gt;thread &lt;/span&gt;&lt;span class=&quot;i&quot;&gt;handle&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;cn&quot;&gt;0x7f51951dc700&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;e&quot;&gt;query &lt;/span&gt;&lt;span class=&quot;i&quot;&gt;id&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;cn&quot;&gt;18995134&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;cn&quot;&gt;10.0.0.1&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;e&quot;&gt;mysql_user &lt;/span&gt;&lt;span class=&quot;e&quot;&gt;Searching &lt;/span&gt;&lt;span class=&quot;e&quot;&gt;rows &lt;/span&gt;&lt;span class=&quot;st&quot;&gt;for&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;e&quot;&gt;update&lt;/span&gt;
&lt;/div&gt;
&lt;div class=&quot;crayon-line&quot; id=&quot;crayon-542531c3c2345-19&quot;&gt;
&lt;span class=&quot;e&quot;&gt;UPDATE &lt;/span&gt;&lt;span class=&quot;e&quot;&gt;tbl_A &lt;/span&gt;&lt;span class=&quot;e&quot;&gt;SET &lt;/span&gt;&lt;span class=&quot;v&quot;&gt;status&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;i&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;e&quot;&gt;WHERE&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;v&quot;&gt;status&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;i&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;st&quot;&gt;or&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;v&quot;&gt;status&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;i&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;sy&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;st&quot;&gt;and&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;v&quot;&gt;user_id&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;i&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;i&quot;&gt;limit&lt;/span&gt;&lt;span class=&quot;h&quot;&gt; &lt;/span&gt;&lt;span class=&quot;i&quot;&gt;N&lt;/span&gt;
&lt;/div&gt;
&lt;div class=&quot;crayon-line crayon-striped-line&quot; id=&quot;crayon-542531c3c2345-20&quot;&gt;
&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;i&quot;&gt;code&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;/td&gt;
					&lt;/tr&gt;
				&lt;/table&gt;
			&lt;/div&gt;
		&lt;/div&gt;
&lt;!-- [Format Time: 0.0113 seconds] --&gt;
&lt;p&gt;这下可以解释为什么只有一个CPU核在运行了。第二条SQL通过索引加扫表的方式，寻找符合条件的数据，这条SQL消耗了一个CPU核。查找数据的同时，此SQL加了gap锁，导致其它更新同一张表的SQL一直在等待锁的释放，因此其它CPU核基本处于空闲状态。&lt;/p&gt;
&lt;h3 id=&quot;-&quot;&gt;解决&lt;/h3&gt;
&lt;p&gt;找到问题的根源，解决起来就比较简单了，分析一下数据，发现索引(user_id, status)的区分度还不错，新增索引(user_id, status)后，问题解决。&lt;/p&gt;
            

</description>
        <pubDate>Thu, 25 Sep 2014 00:00:00 +0800</pubDate>
        <link>http://iftti.com/posts/2014/2014-09-25--p=1608-f99e3b74d.html</link>
        <guid isPermaLink="true">http://iftti.com/posts/2014/2014-09-25--p=1608-f99e3b74d.html</guid>
        
        
        <category>noops</category>
        
      </item>
    
      <item>
        <title>Building assert() in Swift, Part 2: __FILE__ and __LINE__</title>
        <description>

						
						

						&lt;p&gt;Two occasionally useful features of C are the &lt;span class=&quot;keyword&quot;&gt;__FILE__&lt;/span&gt; and &lt;span class=&quot;keyword&quot;&gt;__LINE__&lt;/span&gt; magic macros. These are built into the preprocessor, and expanded out before the C parser is run. Despite not having a preprocessor, Swift provides very similar functionality with similar names, but Swift works quite differently under the covers.&lt;/p&gt;
&lt;h3&gt;Built-In Identifiers&lt;/h3&gt;
&lt;p&gt;As described in &lt;a href=&quot;http://developer.apple.com/library/prerelease/ios/documentation/swift/conceptual/swift_programming_language/LexicalStructure.html&quot;&gt;the Swift programming guide&lt;/a&gt;, Swift has a number of built-in identifiers, including  &lt;span class=&quot;keyword&quot;&gt;__FILE__&lt;/span&gt;, &lt;span class=&quot;keyword&quot;&gt;__LINE__&lt;/span&gt;, &lt;span class=&quot;keyword&quot;&gt;__COLUMN__&lt;/span&gt;, and &lt;span class=&quot;keyword&quot;&gt;__FUNCTION__&lt;/span&gt;. These expressions can be used anywhere and are expanded by the parser to string or integer literals that correspond to the current location in the source code. This is incredibly useful for manual logging, e.g. to print out the current position before quitting.&lt;/p&gt;
&lt;p&gt;However, this doesn’t help us in our quest to implement &lt;span class=&quot;keyword&quot;&gt;assert()&lt;/span&gt;.  If we defined assert like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&lt;span class=&quot;key&quot;&gt;func&lt;/span&gt; assert(predicate : &lt;span class=&quot;key&quot;&gt;@autoclosure&lt;/span&gt; () -&amp;gt; &lt;span class=&quot;title&quot;&gt;Bool&lt;/span&gt;) { 
	&lt;span class=&quot;preprocessor&quot;&gt;#if DEBUG&lt;/span&gt;
		&lt;span class=&quot;key&quot;&gt;if&lt;/span&gt; !predicate() {
			&lt;span class=&quot;method&quot;&gt;println&lt;/span&gt;(&lt;span class=&quot;string&quot;&gt;&quot;assertion failed at&lt;/span&gt; \(&lt;span class=&quot;key&quot;&gt;__FILE__&lt;/span&gt;)&lt;span class=&quot;string&quot;&gt;:&lt;/span&gt;\(&lt;span class=&quot;key&quot;&gt;__LINE__&lt;/span&gt;)&lt;span class=&quot;string&quot;&gt;&quot;&lt;/span&gt;)
			abort()
		}
	&lt;span class=&quot;preprocessor&quot;&gt;#endif&lt;/span&gt;
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The above code would print out of the file/line location that implements &lt;span class=&quot;keyword&quot;&gt;assert()&lt;/span&gt; itself, not the location from the caller. That isn’t helpful.&lt;/p&gt;
&lt;h3&gt;Getting the location of a caller&lt;/h3&gt;
&lt;p&gt;Swift borrows a clever feature from the D language: these identifiers expand to the location of the caller &lt;em&gt;when evaluated in a default argument list&lt;/em&gt;.  To enable this behavior, the &lt;span class=&quot;keyword&quot;&gt;assert()&lt;/span&gt; function is defined something like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&lt;span class=&quot;key&quot;&gt;func&lt;/span&gt; assert(condition: &lt;span class=&quot;key&quot;&gt;@autoclosure&lt;/span&gt; () -&amp;gt; &lt;span class=&quot;title&quot;&gt;Bool&lt;/span&gt;, &lt;span class=&quot;key&quot;&gt;_&lt;/span&gt; message: &lt;span class=&quot;title&quot;&gt;String&lt;/span&gt; =&lt;span class=&quot;string&quot;&gt; &quot;&quot;&lt;/span&gt;,
	file: &lt;span class=&quot;title&quot;&gt;String&lt;/span&gt; = &lt;span class=&quot;key&quot;&gt;__FILE__&lt;/span&gt;, line: &lt;span class=&quot;title&quot;&gt;Int&lt;/span&gt; = &lt;span class=&quot;key&quot;&gt;__LINE__&lt;/span&gt;) {
		&lt;span class=&quot;preprocessor&quot;&gt;#if DEBUG&lt;/span&gt;
			&lt;span class=&quot;key&quot;&gt;if&lt;/span&gt; !condition() {
				println(&lt;span class=&quot;string&quot;&gt;&quot;assertion failed at&lt;/span&gt; \(file)&lt;span class=&quot;string&quot;&gt;:&lt;/span&gt;\(line)&lt;span class=&quot;string&quot;&gt;:&lt;/span&gt; \(message)&lt;span class=&quot;string&quot;&gt;&quot;&lt;/span&gt;)
				abort()
			}
		&lt;span class=&quot;preprocessor&quot;&gt;#endif&lt;/span&gt;
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The second parameter to the Swift &lt;span class=&quot;keyword&quot;&gt;assert()&lt;/span&gt; function is an optional string that you can specify, and the third and forth arguments are defaulted to be the position in the caller’s context.  This allows &lt;span class=&quot;keyword&quot;&gt;assert()&lt;/span&gt; to pick up the source location of the caller by default, and if you want to define your own abstractions on top of assert, you can pass down locations from its caller.  As a trivial example, you could define a function that logs and asserts like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&lt;span class=&quot;key&quot;&gt;func&lt;/span&gt; logAndAssert(condition: &lt;span class=&quot;key&quot;&gt;@autoclosure&lt;/span&gt; () -&amp;gt; &lt;span class=&quot;title&quot;&gt;Bool&lt;/span&gt;, &lt;span class=&quot;key&quot;&gt;_&lt;/span&gt; message: &lt;span class=&quot;title&quot;&gt;StaticString&lt;/span&gt; = &lt;span class=&quot;string&quot;&gt;&quot;&quot;&lt;/span&gt;,
	file: &lt;span class=&quot;title&quot;&gt;StaticString&lt;/span&gt; = &lt;span class=&quot;key&quot;&gt;__FILE__&lt;/span&gt;, line: &lt;span class=&quot;title&quot;&gt;UWord&lt;/span&gt; = &lt;span class=&quot;key&quot;&gt;__LINE__&lt;/span&gt;) {

	logMessage(message)
	&lt;span class=&quot;method&quot;&gt;assert&lt;/span&gt;(condition, message, file: file, line: line)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This properly propagates the file/line location of the &lt;span class=&quot;keyword&quot;&gt;logAndAssert()&lt;/span&gt; caller down to the implementation of &lt;span class=&quot;keyword&quot;&gt;assert()&lt;/span&gt;. Note that &lt;span class=&quot;keyword&quot;&gt;StaticString&lt;/span&gt;, as shown in the code above, is a simple &lt;span class=&quot;nowrap&quot;&gt;String-like&lt;/span&gt; type used to store a string literal, such as one produced by &lt;span class=&quot;keyword&quot;&gt;__FILE__&lt;/span&gt;, with no &lt;span class=&quot;nowrap&quot;&gt;memory-management&lt;/span&gt; overhead.&lt;/p&gt;
&lt;p&gt;In addition to being useful for &lt;span class=&quot;keyword&quot;&gt;assert()&lt;/span&gt;, this functionality is used in the Swift implementation of the higher-level XCTest framework, and may be useful for your own libraries as well.&lt;/p&gt;

						
												
											

</description>
        <pubDate>Thu, 25 Sep 2014 00:00:00 +0800</pubDate>
        <link>http://iftti.com/posts/2014/2014-09-25--id=15-43f1860ed.html</link>
        <guid isPermaLink="true">http://iftti.com/posts/2014/2014-09-25--id=15-43f1860ed.html</guid>
        
        
        <category>apple_swift</category>
        
      </item>
    
      <item>
        <title>Python自然语言处理实践: 在NLTK中使用斯坦福中文分词器</title>
        <description>

						&lt;p&gt;斯坦福大学自然语言处理组是世界知名的NLP研究小组，他们提供了一系列开源的Java文本分析工具，包括分词器(&lt;a href=&quot;http://nlp.stanford.edu/software/segmenter.shtml&quot;&gt;Word Segmenter&lt;/a&gt;)，词性标注工具（&lt;a href=&quot;http://nlp.stanford.edu/software/tagger.shtml&quot;&gt;Part-Of-Speech Tagger&lt;/a&gt;），命名实体识别工具（&lt;a href=&quot;http://nlp.stanford.edu/software/CRF-NER.shtml&quot;&gt;Named Entity Recognizer&lt;/a&gt;），句法分析器（&lt;a href=&quot;http://nlp.stanford.edu/software/lex-parser.shtml&quot;&gt;Parser&lt;/a&gt;）等，可喜的事，他们还为这些工具训练了相应的中文模型，支持中文文本处理。在使用NLTK的过程中，发现当前版本的&lt;a href=&quot;https://github.com/nltk/nltk&quot;&gt;NLTK&lt;/a&gt;已经提供了相应的斯坦福文本处理工具接口，包括词性标注，命名实体识别和句法分析器的接口，不过可惜的是，没有提供&lt;a href=&quot;http://www.52nlp.cn/category/word-segmentation&quot;&gt;分词&lt;/a&gt;器的接口。在google无果和阅读了相应的代码后，我决定照猫画虎为NLTK写一个斯坦福&lt;a href=&quot;http://www.52nlp.cn/category/word-segmentation&quot;&gt;中文分词&lt;/a&gt;器接口，这样可以方便的在Python中调用斯坦福文本处理工具。&lt;/p&gt;
&lt;p&gt;首先需要做一些准备工作，第一步当然是安装NLTK，这个可以参考我们在gensim的相关文章中的介绍《&lt;a href=&quot;http://www.52nlp.cn/%E5%A6%82%E4%BD%95%E8%AE%A1%E7%AE%97%E4%B8%A4%E4%B8%AA%E6%96%87%E6%A1%A3%E7%9A%84%E7%9B%B8%E4%BC%BC%E5%BA%A6%E4%B8%89&quot;&gt;如何计算两个文档的相似度&lt;/a&gt;》，不过这里建议check github上最新的NLTK源代码并用“python setup.py install”的方式安装这个版本：&lt;a href=&quot;https://github.com/nltk/nltk&quot;&gt;https://github.com/nltk/nltk&lt;/a&gt;。这个版本新增了对于斯坦福句法分析器的接口，一些老的版本并没有，这个之后我们也许还会用来介绍。而我们也是在这个版本中添加的斯坦福分词器接口，其他版本也许会存在一些小问题。其次是安装Java运行环境，以Ubuntu 12.04为例，安装Java运行环境仅需要两步：&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;sudo apt-get install default-jre&lt;br&gt;
sudo apt-get install default-jdk&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;最后，当然是最重要的，你需要下载&lt;a href=&quot;http://nlp.stanford.edu/software/segmenter.shtml&quot;&gt;斯坦福分词器&lt;/a&gt;的相应文件，包括源代码，模型文件，词典文件等。注意斯坦福分词器并不仅仅支持中文分词，还支持阿拉伯语的分词，需要下载的zip打包文件是这个: &lt;a href=&quot;http://nlp.stanford.edu/software/stanford-segmenter-2014-08-27.zip&quot;&gt;Download Stanford Word Segmenter version 2014-08-27&lt;/a&gt;，下载后解压。&lt;/p&gt;
&lt;p&gt;准备工作就绪后，我们首先考虑的是在nltk源代码里的什么地方来添加这个接口文件。在nltk源代码包下，斯坦福词性标注器和命名实体识别工具的接口文件是这个：nltk/tag/stanford.py ，而句法分析器的接口文件是这个：nltk/parse/stanford.py , 虽然在nltk/tokenize/目录下有一个stanford.py文件，但是仅仅提供了一个针对英文的tokenizer工具PTBTokenizer的接口，没有针对斯坦福分词器的接口，于是我决定在nltk/tokenize下添加一个stanford_segmenter.py文件，作为nltk斯坦福中文分词器的接口文件。NLTK中的这些接口利用了Linux 下的管道（PIPE）机制和subprocess模块，这里直接贴源代码了，感兴趣的同学可以自行阅读:&lt;br&gt;
&lt;span id=&quot;more-6763&quot;&gt;&lt;/span&gt;&lt;/p&gt;
&lt;div class=&quot;codecolorer-container text default&quot; style=&quot;overflow:auto;white-space:nowrap;width:620px;height:450px;&quot;&gt;&lt;table cellspacing=&quot;0&quot; cellpadding=&quot;0&quot;&gt;&lt;tbody&gt;&lt;tr&gt;
&lt;td class=&quot;line-numbers&quot;&gt;&lt;div&gt;1&lt;br&gt;2&lt;br&gt;3&lt;br&gt;4&lt;br&gt;5&lt;br&gt;6&lt;br&gt;7&lt;br&gt;8&lt;br&gt;9&lt;br&gt;10&lt;br&gt;11&lt;br&gt;12&lt;br&gt;13&lt;br&gt;14&lt;br&gt;15&lt;br&gt;16&lt;br&gt;17&lt;br&gt;18&lt;br&gt;19&lt;br&gt;20&lt;br&gt;21&lt;br&gt;22&lt;br&gt;23&lt;br&gt;24&lt;br&gt;25&lt;br&gt;26&lt;br&gt;27&lt;br&gt;28&lt;br&gt;29&lt;br&gt;30&lt;br&gt;31&lt;br&gt;32&lt;br&gt;33&lt;br&gt;34&lt;br&gt;35&lt;br&gt;36&lt;br&gt;37&lt;br&gt;38&lt;br&gt;39&lt;br&gt;40&lt;br&gt;41&lt;br&gt;42&lt;br&gt;43&lt;br&gt;44&lt;br&gt;45&lt;br&gt;46&lt;br&gt;47&lt;br&gt;48&lt;br&gt;49&lt;br&gt;50&lt;br&gt;51&lt;br&gt;52&lt;br&gt;53&lt;br&gt;54&lt;br&gt;55&lt;br&gt;56&lt;br&gt;57&lt;br&gt;58&lt;br&gt;59&lt;br&gt;60&lt;br&gt;61&lt;br&gt;62&lt;br&gt;63&lt;br&gt;64&lt;br&gt;65&lt;br&gt;66&lt;br&gt;67&lt;br&gt;68&lt;br&gt;69&lt;br&gt;70&lt;br&gt;71&lt;br&gt;72&lt;br&gt;73&lt;br&gt;74&lt;br&gt;75&lt;br&gt;76&lt;br&gt;77&lt;br&gt;78&lt;br&gt;79&lt;br&gt;80&lt;br&gt;81&lt;br&gt;82&lt;br&gt;83&lt;br&gt;84&lt;br&gt;85&lt;br&gt;86&lt;br&gt;87&lt;br&gt;88&lt;br&gt;89&lt;br&gt;90&lt;br&gt;91&lt;br&gt;92&lt;br&gt;93&lt;br&gt;94&lt;br&gt;95&lt;br&gt;96&lt;br&gt;97&lt;br&gt;98&lt;br&gt;99&lt;br&gt;100&lt;br&gt;101&lt;br&gt;102&lt;br&gt;103&lt;br&gt;104&lt;br&gt;105&lt;br&gt;106&lt;br&gt;107&lt;br&gt;108&lt;br&gt;109&lt;br&gt;110&lt;br&gt;111&lt;br&gt;112&lt;br&gt;113&lt;br&gt;114&lt;br&gt;115&lt;br&gt;116&lt;br&gt;117&lt;br&gt;118&lt;br&gt;119&lt;br&gt;120&lt;br&gt;121&lt;br&gt;122&lt;br&gt;123&lt;br&gt;124&lt;br&gt;125&lt;br&gt;126&lt;br&gt;127&lt;br&gt;128&lt;br&gt;129&lt;br&gt;130&lt;br&gt;131&lt;br&gt;132&lt;br&gt;133&lt;br&gt;134&lt;br&gt;135&lt;br&gt;136&lt;br&gt;
&lt;/div&gt;&lt;/td&gt;
&lt;td&gt;&lt;div class=&quot;text codecolorer&quot;&gt;#!/usr/bin/env python&lt;br&gt;
# -*- coding: utf-8 -*-&lt;br&gt;
# Natural Language Toolkit: Interface to the Stanford Chinese Segmenter&lt;br&gt;
#&lt;br&gt;
# Copyright (C) 2001-2014 NLTK Project&lt;br&gt;
# Author: 52nlp &amp;lt;52nlpcn@gmail.com&amp;gt;&lt;br&gt;
#&lt;br&gt;
# URL: &amp;lt;http://nltk.org/&amp;gt;&lt;br&gt;
# For license information, see LICENSE.TXT&lt;br&gt;
&lt;br&gt;
from __future__ import unicode_literals, print_function&lt;br&gt;
&lt;br&gt;
import tempfile&lt;br&gt;
import os&lt;br&gt;
import json&lt;br&gt;
from subprocess import PIPE&lt;br&gt;
&lt;br&gt;
from nltk import compat&lt;br&gt;
from nltk.internals import find_jar, config_java, java, _java_options&lt;br&gt;
&lt;br&gt;
from nltk.tokenize.api import TokenizerI&lt;br&gt;
&lt;br&gt;
class StanfordSegmenter(TokenizerI):&lt;br&gt;
    r&quot;&quot;&quot;&lt;br&gt;
    Interface to the Stanford Segmenter&lt;br&gt;
&lt;br&gt;
    &amp;gt;&amp;gt;&amp;gt; from nltk.tokenize.stanford_segmenter import StanfordSegmenter&lt;br&gt;
    &amp;gt;&amp;gt;&amp;gt; segmenter = StanfordSegmenter(path_to_jar=&quot;stanford-segmenter-3.4.1.jar&quot;, path_to_sihan_corpora_dict=&quot;./data&quot;, path_to_model=&quot;./data/pku.gz&quot;, path_to_dict=&quot;./data/dict-chris6.ser.gz&quot;)&lt;br&gt;
    &amp;gt;&amp;gt;&amp;gt; sentence = u&quot;这是斯坦福中文分词器测试&quot;&lt;br&gt;
    &amp;gt;&amp;gt;&amp;gt; segmenter.segment(sentence)&lt;br&gt;
    &amp;gt;&amp;gt;&amp;gt; u&#39;\u8fd9 \u662f \u65af\u5766\u798f \u4e2d\u6587 \u5206\u8bcd\u5668 \u6d4b\u8bd5\n&#39;&lt;br&gt;
    &amp;gt;&amp;gt;&amp;gt; segmenter.segment_file(&quot;test.simp.utf8&quot;)&lt;br&gt;
    &amp;gt;&amp;gt;&amp;gt; u&#39;\u9762\u5bf9 \u65b0 \u4e16\u7eaa \uff0c \u4e16\u754c \u5404\u56fd ...&lt;br&gt;
    &quot;&quot;&quot;&lt;br&gt;
&lt;br&gt;
    _JAR = &#39;stanford-segmenter.jar&#39;&lt;br&gt;
&lt;br&gt;
    def __init__(self, path_to_jar=None,&lt;br&gt;
            path_to_sihan_corpora_dict=None,&lt;br&gt;
            path_to_model=None, path_to_dict=None,&lt;br&gt;
            encoding=&#39;UTF-8&#39;, options=None,&lt;br&gt;
            verbose=False, java_options=&#39;-mx2g&#39;):&lt;br&gt;
        self._stanford_jar = find_jar(&lt;br&gt;
            self._JAR, path_to_jar,&lt;br&gt;
            env_vars=(&#39;STANFORD_SEGMENTER&#39;,),&lt;br&gt;
            searchpath=(),&lt;br&gt;
            verbose=verbose&lt;br&gt;
        )&lt;br&gt;
        self._sihan_corpora_dict = path_to_sihan_corpora_dict&lt;br&gt;
        self._model = path_to_model&lt;br&gt;
        self._dict = path_to_dict&lt;br&gt;
&lt;br&gt;
        self._encoding = encoding&lt;br&gt;
        self.java_options = java_options&lt;br&gt;
        options = {} if options is None else options&lt;br&gt;
        self._options_cmd = &#39;,&#39;.join(&#39;{0}={1}&#39;.format(key, json.dumps(val)) for key, val in options.items())&lt;br&gt;
&lt;br&gt;
    def segment_file(self, input_file_path):&lt;br&gt;
        &quot;&quot;&quot;&lt;br&gt;
        &quot;&quot;&quot;&lt;br&gt;
        cmd = [&lt;br&gt;
            &#39;edu.stanford.nlp.ie.crf.CRFClassifier&#39;,&lt;br&gt;
            &#39;-sighanCorporaDict&#39;, self._sihan_corpora_dict,&lt;br&gt;
            &#39;-textFile&#39;, input_file_path,&lt;br&gt;
            &#39;-sighanPostProcessing&#39;, &#39;true&#39;,&lt;br&gt;
            &#39;-keepAllWhitespaces&#39;, &#39;false&#39;,&lt;br&gt;
            &#39;-loadClassifier&#39;, self._model,&lt;br&gt;
            &#39;-serDictionary&#39;, self._dict&lt;br&gt;
        ]&lt;br&gt;
&lt;br&gt;
        stdout = self._execute(cmd)&lt;br&gt;
&lt;br&gt;
        return stdout&lt;br&gt;
&lt;br&gt;
    def segment(self, tokens):&lt;br&gt;
        return self.segment_sents([tokens])&lt;br&gt;
&lt;br&gt;
    def segment_sents(self, sentences):&lt;br&gt;
        &quot;&quot;&quot;&lt;br&gt;
        &quot;&quot;&quot;&lt;br&gt;
        encoding = self._encoding&lt;br&gt;
        # Create a temporary input file&lt;br&gt;
        _input_fh, self._input_file_path = tempfile.mkstemp(text=True)&lt;br&gt;
&lt;br&gt;
        # Write the actural sentences to the temporary input file&lt;br&gt;
        _input_fh = os.fdopen(_input_fh, &#39;wb&#39;)&lt;br&gt;
        _input = &#39;\n&#39;.join((&#39; &#39;.join(x) for x in sentences))&lt;br&gt;
        if isinstance(_input, compat.text_type) and encoding:&lt;br&gt;
            _input = _input.encode(encoding)&lt;br&gt;
        _input_fh.write(_input)&lt;br&gt;
        _input_fh.close()&lt;br&gt;
&lt;br&gt;
        cmd = [&lt;br&gt;
            &#39;edu.stanford.nlp.ie.crf.CRFClassifier&#39;,&lt;br&gt;
            &#39;-sighanCorporaDict&#39;, self._sihan_corpora_dict,&lt;br&gt;
            &#39;-textFile&#39;, self._input_file_path,&lt;br&gt;
            &#39;-sighanPostProcessing&#39;, &#39;true&#39;,&lt;br&gt;
            &#39;-keepAllWhitespaces&#39;, &#39;false&#39;,&lt;br&gt;
            &#39;-loadClassifier&#39;, self._model,&lt;br&gt;
            &#39;-serDictionary&#39;, self._dict&lt;br&gt;
        ]&lt;br&gt;
&lt;br&gt;
        stdout = self._execute(cmd)&lt;br&gt;
&lt;br&gt;
        # Delete the temporary file&lt;br&gt;
        os.unlink(self._input_file_path)&lt;br&gt;
&lt;br&gt;
        return stdout&lt;br&gt;
&lt;br&gt;
    def _execute(self, cmd, verbose=False):&lt;br&gt;
        encoding = self._encoding&lt;br&gt;
        cmd.extend([&#39;-inputEncoding&#39;, encoding])&lt;br&gt;
        _options_cmd = self._options_cmd&lt;br&gt;
        if _options_cmd:&lt;br&gt;
            cmd.extend([&#39;-options&#39;, self._options_cmd])&lt;br&gt;
&lt;br&gt;
        default_options = &#39; &#39;.join(_java_options)&lt;br&gt;
&lt;br&gt;
        # Configure java.&lt;br&gt;
        config_java(options=self.java_options, verbose=verbose)&lt;br&gt;
&lt;br&gt;
        stdout, _stderr = java(cmd,classpath=self._stanford_jar, stdout=PIPE, stderr=PIPE)&lt;br&gt;
        stdout = stdout.decode(encoding)&lt;br&gt;
&lt;br&gt;
        # Return java configurations to their default values.&lt;br&gt;
        config_java(options=default_options, verbose=False)&lt;br&gt;
&lt;br&gt;
        return stdout&lt;br&gt;
&lt;br&gt;
def setup_module(module):&lt;br&gt;
    from nose import SkipTest&lt;br&gt;
&lt;br&gt;
    try:&lt;br&gt;
        StanfordSegmenter()&lt;br&gt;
    except LookupError:&lt;br&gt;
        raise SkipTest(&#39;doctests from nltk.tokenize.stanford_segmenter are skipped because the stanford segmenter jar doesn\&#39;t exist&#39;)&lt;/div&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/div&gt;
&lt;p&gt;我在github上fork了一个最新的NLTK版本，然后在这个版本中添加了&lt;a href=&quot;https://github.com/panyang/nltk/blob/develop/nltk/tokenize/stanford_segmenter.py&quot;&gt;stanford_segmenter.py&lt;/a&gt;，感兴趣的同学可以自行下载这个代码，放到nltk/tokenize/目录下，然后重新安装NLTK：sudo python setpy.py install. 或者直接clone我们的这个&lt;a href=&quot;https://github.com/panyang/nltk&quot;&gt;nltk&lt;/a&gt;版本，安装后就可以使用斯坦福中文分词器了。&lt;/p&gt;
&lt;p&gt;现在就可以在Python NLTK中调用这个斯坦福中文分词接口了。为了方便起见，建议首先进入到解压后的斯坦福分词工具目录下：cd stanford-segmenter-2014-08-27，然后在这个目录下启用ipython，当然默认python解释器也可：&lt;/p&gt;
&lt;p&gt;# 初始化斯坦福中文分词器&lt;br&gt;
In [1]: from nltk.tokenize.stanford_segmenter import StanfordSegmenter&lt;/p&gt;
&lt;p&gt;# 注意分词模型，词典等资源在data目录下，使用的是相对路径，&lt;br&gt;
# 在stanford-segmenter-2014-08-27的目录下执行有效&lt;br&gt;
# 注意，斯坦福中文分词器提供了两个中文分词模型：&lt;br&gt;
#     ctb.gz是基于宾州中文树库训练的模型&lt;br&gt;
#     pku.gz是基于北大在2005backoof上提供的人名日报语料库&lt;br&gt;
# 这里选用了pku.gz，方便最后的测试&lt;br&gt;
In [2]: segmenter = StanfordSegmenter(path_to_jar=”stanford-segmenter-3.4.1.jar”, path_to_sihan_corpora_dict=”./data”, path_to_model=”./data/pku.gz”, path_to_dict=”./data/dict-chris6.ser.gz”)&lt;/p&gt;
&lt;p&gt;# 测试一个中文句子，注意u&lt;br&gt;
In [3]: sentence = u”这是斯坦福中文分词器测试”&lt;/p&gt;
&lt;p&gt;# 调用segment方法来切分中文句子，这里隐藏了一个问题，我们最后来说明&lt;br&gt;
In [4]: segmenter.segment(sentence)&lt;br&gt;
Out[4]: u’\u8fd9 \u662f \u65af\u5766\u798f \u4e2d\u6587 \u5206\u8bcd\u5668 \u6d4b\u8bd5\n’&lt;/p&gt;
&lt;p&gt;# 由于分词后显示的是中文编码，我们把这个结果输出到文件中&lt;br&gt;
# 不知道有没有同学有在python解释器总显示中文的方法&lt;br&gt;
In [5]: outfile = open(‘outfile’, ‘w’)&lt;/p&gt;
&lt;p&gt;In [6]: result = segmenter.segment(sentence)&lt;/p&gt;
&lt;p&gt;# 注意写入到文件的时候要encode 为 UTF-8编码&lt;br&gt;
In [7]: outfile.write(result.encode(‘UTF-8′))&lt;/p&gt;
&lt;p&gt;In [8]: outfile.close()&lt;/p&gt;
&lt;p&gt;打开这个outfile文件：&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;这 是 斯坦福 中文 分词器 测试&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;这里同时提供了一个segment_file的调用方法，方便直接对文件进行切分，让我们来测试《&lt;a href=&quot;http://www.52nlp.cn/%E4%B8%AD%E6%96%87%E5%88%86%E8%AF%8D%E5%85%A5%E9%97%A8%E4%B9%8B%E8%B5%84%E6%BA%90&quot;&gt;中文分词入门之资源&lt;/a&gt;》中介绍的backoff2005的测试集pku_test.utf8，来看看斯坦福分词器的效果：&lt;/p&gt;
&lt;p&gt;In [9]: result = segmenter.segment_file(‘pku_test.utf8′)&lt;/p&gt;
&lt;p&gt;In [10]: outfile = open(‘pku_outfile’, ‘w’)&lt;/p&gt;
&lt;p&gt;In [11]: outfile.write(result.encode(‘UTF-8′))&lt;/p&gt;
&lt;p&gt;In [12]: outfile.close()&lt;/p&gt;
&lt;p&gt;打开结果文件pku_outfile：&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;共同 创造 美好 的 新 世纪 ——二○○一年 新年 贺词&lt;br&gt;
（ 二○○○年 十二月 三十一日 ） （ 附 图片 1 张 ）&lt;br&gt;
女士 们 ， 先生 们 ， 同志 们 ， 朋友 们 ：&lt;br&gt;
2001年 新年 钟声 即将 敲响 。 人类 社会 前进 的 航船 就要 驶入 21 世纪 的 新 航程 。 中国 人民 进入 了 向 现代化 建设 第三 步 战略 目标 迈进 的 新 征程 。&lt;br&gt;
在 这个 激动人心 的 时刻 ， 我 很 高兴 通过 中国 国际 广播 电台 、 中央 人民 广播 电台 和 中央 电视台 ， 向 全国 各族 人民 ， 向 香港 特别 行政区 同胞 、 澳门 特别 行政区 同胞 和 台湾 同胞 、 海外 侨胞 ， 向 世界 各国 的 朋友 们 ， 致以 新 世纪 第一 个 新年 的 祝贺 ！&lt;br&gt;
….
&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;我们用backoff2005的测试脚本来测试一下斯坦福中文分词器在这份测试语料上的效果：&lt;/p&gt;
&lt;p&gt;./icwb2-data/scripts/score ./icwb2-data/gold/pku_training_words.utf8 ./icwb2-data/gold/pku_test_gold.utf8 pku_outfile &amp;gt; stanford_pku_test.score&lt;/p&gt;
&lt;p&gt;结果如下：&lt;br&gt;
=== SUMMARY:&lt;br&gt;
=== TOTAL INSERTIONS:	1479&lt;br&gt;
=== TOTAL DELETIONS:	1974&lt;br&gt;
=== TOTAL SUBSTITUTIONS:	3638&lt;br&gt;
=== TOTAL NCHANGE:	7091&lt;br&gt;
=== TOTAL TRUE WORD COUNT:	104372&lt;br&gt;
=== TOTAL TEST WORD COUNT:	103877&lt;br&gt;
=== TOTAL TRUE WORDS RECALL:	0.946&lt;br&gt;
=== TOTAL TEST WORDS PRECISION:	0.951&lt;br&gt;
=== F MEASURE:	0.948&lt;br&gt;
=== OOV Rate:	0.058&lt;br&gt;
=== OOV Recall Rate:	0.769&lt;br&gt;
=== IV Recall Rate:	0.957&lt;br&gt;
###	pku_outfile	1479	1974	3638	7091	104372	103877	0.946	0.951	0.948	0.058	0.769	0.957&lt;/p&gt;
&lt;p&gt;准确率是95.1%， 召回率是94.6%, F值是94.8%, 相当不错。感兴趣的同学可以测试一下其他测试集，或者用宾州中文树库的模型来测试一下结果。&lt;/p&gt;
&lt;p&gt;最后我们再说明一下这个接口存在的问题，因为使用了Linux PIPE模式来调用斯坦福中文分词器，相当于在Python中执行相应的Java命令，导致每次在执行分词时会加载一遍分词所需的模型和词典，这个对文件操作时（segment_file)没有多大的问题，但是在对句子执行分词（segment)的时候会存在很大的问题，每次都加载数据，在实际产品中基本是不可用的。虽然发现斯坦福分词器提供了一个 –readStdin 的读入标准输入的参数，也尝试通过python subprocess中先load 文件，再用的communicate方法来读入标准输入，但是仍然没有解决问题，发现还是一次执行，一次结束。这个问题困扰了我很久，google了很多资料也没有解决问题，欢迎懂行的同学告知或者来解决这个问题，再此先谢过了。That’s all!&lt;/p&gt;
&lt;p&gt;注：原创文章，转载请注明出处“&lt;a href=&quot;http://www.52nlp.cn&quot;&gt;我爱自然语言处理&lt;/a&gt;”：&lt;a href=&quot;http://www.52nlp.cn&quot;&gt;www.52nlp.cn&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;本文链接地址：&lt;a href=&quot;http://www.52nlp.cn/?p=6763&quot;&gt;http://www.52nlp.cn/python自然语言处理实践-在nltk中使用斯坦福中文分词器&lt;/a&gt;&lt;/p&gt;

											

</description>
        <pubDate>Wed, 24 Sep 2014 00:00:00 +0800</pubDate>
        <link>http://iftti.com/posts/2014/2014-09-24-python%25e8%2587%25aa%25e7%2584%25b6%25e8%25af%25ad%25e8%25a8%2580%25e5%25a4%2584%25e7%2590%2586%25e5%25ae%259e%25e8%25b7%25b5-%25e5%259c%25a8nltk%25e4%25b8%25ad%25e4%25bd%25bf%25e7%2594%25a8%25e6%2596%25af%25e5%259d%25a6%25e7%25a6%258f%25e4%25b8%25ad%25e6%2596%2587%25e5%2588%2586%25e8%25af%258d%25e5%2599%25a8-5dd26d0a5.html</link>
        <guid isPermaLink="true">http://iftti.com/posts/2014/2014-09-24-python%25e8%2587%25aa%25e7%2584%25b6%25e8%25af%25ad%25e8%25a8%2580%25e5%25a4%2584%25e7%2590%2586%25e5%25ae%259e%25e8%25b7%25b5-%25e5%259c%25a8nltk%25e4%25b8%25ad%25e4%25bd%25bf%25e7%2594%25a8%25e6%2596%25af%25e5%259d%25a6%25e7%25a6%258f%25e4%25b8%25ad%25e6%2596%2587%25e5%2588%2586%25e8%25af%258d%25e5%2599%25a8-5dd26d0a5.html</guid>
        
        
        <category>52nlp</category>
        
      </item>
    
      <item>
        <title>在 logstash 里使用其他 RubyGems 模块</title>
        <description>

  
  &lt;div style=&quot;background-color: #FFF;&quot;&gt;
    &lt;p&gt;在开发和使用一些 logstash 自定义插件的时候，几乎不可避免会导入其他 RubyGems 模块 —— 因为都用不上模块的小型处理，直接写在 &lt;em&gt;filters/ruby&lt;/em&gt; 插件配置里就够了 —— 这时候，运行 logstash 命令可能会发现一个问题：这个 gem 模块一直是 “no found” 状态。&lt;/p&gt;
&lt;p&gt;这其实是因为我们一般是通过 java 命令来运行的 logstash，这时候它回去寻找的 Gem 路径跟我们预计中的是不一致的。&lt;/p&gt;
&lt;p&gt;要查看 logstash 运行时实际的 Gem 查找路径，首先要通过 &lt;code&gt;ps aux&lt;/code&gt; 命令确定 ruby 的实际运行方式：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ ps uax|grep logstash
raochenlin      27268  38.0  4.3  3268156 181344 s003  S+    7:10PM   0:22.36 /Library/Internet Plug-Ins/JavaAppletPlugin.plugin/Contents/Home/bin/java -Xmx500m -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -Djava.awt.headless=true -XX:CMSInitiatingOccupancyFraction=75 -XX:+UseCMSInitiatingOccupancyOnly -jar /Downloads/logstash-1.4.2/vendor/jar/jruby-complete-1.7.11.jar -I/Users/raochenlin/Downloads/logstash-1.4.2/lib /Users/raochenlin/Downloads/logstash-1.4.2/lib/logstash/runner.rb agent -f test.conf
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;看，实际的运行方式应该是：&lt;code&gt;java -jar logstash-1.4.2/vendor/jar/jruby-complete-1.7.11.jar -Ilogstash-1.4.2/lib logstash-1.4.2/lib/logstash/runner.rb&lt;/code&gt; 这样。&lt;/p&gt;
&lt;p&gt;那么我们查看 gem 路径的命令也就知道怎么写了：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;java -jar logstash-1.4.2/vendor/jar/jruby-complete-1.7.11.jar `which gem` env
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;你会看到这样的输出：&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;RubyGems Environment:
     - RUBYGEMS VERSION: 2.1.9
     - RUBY VERSION: 1.9.3 (2014-02-24 patchlevel 392) [java]
     - INSTALLATION DIRECTORY: file:/Downloads/logstash-1.4.2/vendor/jar/jruby-complete-1.7.11.jar!/META-INF/jruby.home/lib/ruby/gems/shared
     - RUBY EXECUTABLE: java -jar /Downloads/logstash-1.4.2/vendor/jar/jruby-complete-1.7.11.jar
     - EXECUTABLE DIRECTORY: file:/Downloads/logstash-1.4.2/vendor/jar/jruby-complete-1.7.11.jar!/META-INF/jruby.home/bin
     - SPEC CACHE DIRECTORY: /.gem/specs
     - RUBYGEMS PLATFORMS:
       - ruby
       - universal-java-1.7
     - GEM PATHS:
        - file:/Downloads/logstash-1.4.2/vendor/jar/jruby-complete-1.7.11.jar!/META-INF/jruby.home/lib/ruby/gems/shared
        - /.gem/jruby/1.9
     - GEM CONFIGURATION:
        - :update_sources =&amp;gt; true
        - :verbose =&amp;gt; true
        - :backtrace =&amp;gt; false
        - :bulk_threshold =&amp;gt; 1000
        - “install” =&amp;gt; “–no-rdoc –no-ri –env-shebang”
        - “update” =&amp;gt; “–no-rdoc –no-ri –env-shebang”
        - :sources =&amp;gt; [“http://ruby.taobao.org/”]
     - REMOTE SOURCES:
        - http://ruby.taobao.org/
     - SHELL PATH:
        - /usr/bin
        - /bin
        - /usr/sbin
        - /sbin
        - /usr/local/bin&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;看到其中的 GEM PATHS 部分，是一个以 &lt;strong&gt;file:&lt;/strong&gt; 开头的路径！也就是说，要求所有的 gem 包都打包在这个 jruby-complete-1.7.11.jar 里面才认。&lt;/p&gt;
&lt;p&gt;所以我们需要把额外的 gem 包，也加入这个 jar 里：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;jar uf jruby-completa-1.7.11.jar META-INF/jruby.home/lib/ruby/1.9/CUSTOM_RUBY_GEM_LIB
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;注：加入 jar 是用的相对路径，所以前面这串目录要提前创建然后复制文件进去。&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;当然，其实还有另一个办法。&lt;/p&gt;
&lt;p&gt;让我们返回去再看一次 logstash 的进程，在 jar 后面，还有一个 &lt;code&gt;-I&lt;/code&gt; 参数！所以，其实我们还可以把文件安装在 &lt;code&gt;logstash-1.4.2/lib&lt;/code&gt; 目录下去。&lt;/p&gt;
&lt;p&gt;最后，你可能会问：那 &lt;code&gt;--pluginpath&lt;/code&gt; 参数指定的位置可不可以呢？&lt;/p&gt;
&lt;p&gt;答案是：也可以。&lt;/p&gt;
&lt;p&gt;这个参数指定的位置在 &lt;em&gt;logstash-1.4.2/lib/logstash/agent.rb&lt;/em&gt; 中，被加入了 &lt;code&gt;$LOAD_PATH&lt;/code&gt; 中：&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-ruby&quot; data-lang=&quot;ruby&quot;&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;configure_plugin_path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;paths&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;paths&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;each&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;do&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;
      &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;!&lt;/span&gt;&lt;span class=&quot;no&quot;&gt;Dir&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;exists?&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;nb&quot;&gt;warn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;no&quot;&gt;I18n&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;logstash.agent.configuration.plugin_path_missing&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                    &lt;span class=&quot;ss&quot;&gt;:path&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
      &lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;plugin_glob&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;File&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;join&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;logstash&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;{inputs,codecs,filters,outputs}&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;*.rb&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;Dir&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;glob&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plugin_glob&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;empty?&lt;/span&gt;
        &lt;span class=&quot;vi&quot;&gt;@logger&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;warn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;no&quot;&gt;I18n&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;logstash.agent.configuration.no_plugins_found&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                    &lt;span class=&quot;ss&quot;&gt;:path&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;ss&quot;&gt;:plugin_glob&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plugin_glob&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
      &lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;
      &lt;span class=&quot;vi&quot;&gt;@logger&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;debug&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Adding plugin path&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;ss&quot;&gt;:path&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;vg&quot;&gt;$LOAD_PATH&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;unshift&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;code&gt;$LOAD_PATH&lt;/code&gt; 是 Ruby 的一个特殊变量，类似于 Perl 的 &lt;code&gt;@INC&lt;/code&gt; 或者 Java 的 &lt;code&gt;class_path&lt;/code&gt; 。在这个数组里的路径下的文件，都可以被 require 导入。&lt;/p&gt;
&lt;p&gt;可以运行如下命令查看：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ java -jar logstash-1.4.2/vendor/jar/jruby-complete-1.7.11.jar -e &#39;p $LOAD_PATH&#39;
[&quot;file:/Users/raochenlin/Downloads/logstash-1.4.2/vendor/jar/rar/jruby-complete-1.7.11.jar!/META-INF/jruby.home/lib/ruby/1.9/site_ruby&quot;, &quot;file:/Users/raochenlin/Downloads/logstash-1.4.2/vendor/jar/rar/jruby-complete-1.7.11.jar!/META-INF/jruby.home/lib/ruby/shared&quot;, &quot;file:/Users/raochenlin/Downloads/logstash-1.4.2/vendor/jar/rar/jruby-complete-1.7.11.jar!/META-INF/jruby.home/lib/ruby/1.9&quot;]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;这三种方式，你喜欢哪种呢？&lt;/p&gt;
    &lt;hr&gt;
    
    &lt;hr&gt;
  &lt;!-- UY BEGIN --&gt;


&lt;!-- UY END --&gt;
  &lt;/div&gt;

</description>
        <pubDate>Wed, 24 Sep 2014 00:00:00 +0800</pubDate>
        <link>http://iftti.com/posts/2014/2014-09-24-howto-use-custom-rubygem-in-logstash-205f7f438.html</link>
        <guid isPermaLink="true">http://iftti.com/posts/2014/2014-09-24-howto-use-custom-rubygem-in-logstash-205f7f438.html</guid>
        
        
        <category>chenlinux</category>
        
      </item>
    
      <item>
        <title>游戏引擎学习笔记：介绍、架构、设计及实现</title>
        <description>

					
		
&lt;!-- div id=&quot;ad1&quot;&gt;
&lt;script type=&quot;text/javascript&quot;&gt;
google_ad_client = &quot;ca-pub-7056282119617872&quot;;
google_ad_slot = &quot;6645040531&quot;;
google_ad_width = 300;
google_ad_height = 250;
&lt;/script&gt;
&lt;script type=&quot;text/javascript&quot;
src=&quot;http://pagead2.googlesyndication.com/pagead/show_ads.js&quot;&gt;
&lt;/script&gt;
&lt;/div&gt;
&lt;br/ --&gt;

&lt;p&gt;从小到大，虽然玩过的游戏不少，但是从写程序开始，目前为此仅仅写过2个游戏。其一是2011年在MTK平台下写的贪食蛇，其二是2010年在嵌入式开发板上写过一个迷宫的游戏。第一个代码量大概有3000行左右，第二个有2000行左右。&lt;/p&gt;
&lt;p&gt;这2个游戏都很简单，而且网上有很多现成的例子可供参考，因此难度也比较低。&lt;/p&gt;
&lt;p&gt;这2天把拖延了好久的&lt;strong&gt;《&lt;a href=&quot;http://www.amazon.cn/gp/product/B0033UX10A/ref=as_li_qf_sp_asin_il_tl?ie=UTF8&amp;amp;camp=536&amp;amp;creative=3200&amp;amp;creativeASIN=B0033UX10A&amp;amp;linkCode=as2&amp;amp;tag=vastwork-23&quot; target=&quot;_blank&quot;&gt;Android应用开发揭秘&lt;/a&gt;》&lt;/strong&gt;的游戏引擎的那一章看完了，收获还是很大，在此写一篇读书笔记。&lt;/p&gt;
&lt;p&gt;关于Game Engine，我能想到的几个问题：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;游戏引擎是什么？&lt;/li&gt;
&lt;li&gt;Game Engine是为了解决什么问题？&lt;/li&gt;
&lt;li&gt;Game Engine的架构是什么？&lt;/li&gt;
&lt;li&gt;如何设计一款游戏引擎？&lt;/li&gt;
&lt;li&gt;游戏引擎包含哪些模块？&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;下面就来探讨几个问题：&lt;/p&gt;
&lt;h3&gt;1. Game Engine是什么？&lt;/h3&gt;
&lt;p&gt;游戏产业在全球来看是一个很大的产业，一款游戏大作包含了非常多的元素。游戏涉及到剧情、人物、任务、关卡、地图、画质、美术、音乐、网络等多种元素。开发一款游戏实际上需要耗费非常多的资源，据说North Star的《GTA V》耗资几亿美元。正因为如此，在开发项目过程中，尽可能复用之前项目成功的东西就非常重要。&lt;/p&gt;
&lt;p&gt;一款游戏中，&lt;em&gt;Game Engine直接控制着剧情、关卡、美工、音乐、操作&lt;/em&gt;等内容，将游戏的所有元素捆绑在一起。&lt;/p&gt;
&lt;p&gt;一般来说，一款Game Engine需要包含以下模块：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;基本框架（渲染、逻辑、物理 等等各部分如何组装）&lt;/li&gt;
&lt;li&gt;资源管理&lt;/li&gt;
&lt;li&gt;渲染&lt;/li&gt;
&lt;li&gt;基本逻辑（网游还要解决逻辑的同步问题）&lt;/li&gt;
&lt;li&gt;物理（有时候和逻辑合并）&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;—————-分割线，以下是重要但较为独立的部分——————–&lt;br&gt;
6. UI&lt;br&gt;
7. 音乐音效&lt;br&gt;
8. 网络&lt;br&gt;
9. 脚本（有些类型的游戏引擎需要脚本和逻辑的关联性非常强，有些脚本则比较独立）&lt;/p&gt;
&lt;h3&gt;2. Game Engine为了解决什么问题？&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Game Engine实际上有效的减少开发者编写程序时的冗余劳动，同时增强游戏的可移植性&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;Engine就是游戏的框架，我们需要往框架中填充内容就可以形成一个游戏。&lt;/p&gt;
&lt;p&gt;引擎，就是一系列的工具和生产链，像Unreal 3，Unity这样的成熟引擎，用起来非常方便，就是因为它的关卡/场景编辑器十分宜用，支持多种脚本语言。这类引擎运用恰当的话，理论上能将关卡调试和物件流水线的大部分工作从程序员那里完全移出。&lt;/p&gt;
&lt;h3&gt;3. Game Engine的架构&lt;/h3&gt;
&lt;p&gt;游戏 = 引擎(程序) + 资源(图像、声音、动画等)&lt;br&gt;
目前的Game Engine的架构都是Model-View-Controller架构，逻辑和显示分开，由一个逻辑控制流来协调Client的请求和Server的行动。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;View: 负责界面回执&lt;/li&gt;
&lt;li&gt;Controller：处理工作流程的创建和种植，用户输入，各种事件的处理&lt;/li&gt;
&lt;li&gt;Model: 模型、逻辑，程序的功能实现&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;消息循环-&amp;gt;更新数据-&amp;gt;绘制各节点&lt;/strong&gt; 这是绘制的基本结构基本不会有大的改变。&lt;/p&gt;
&lt;p&gt;各种引擎的变种很大部分是在游戏逻辑上的封装。脚本也好，直接写代码也好。比如较为古老的数据与函数分离，以C语言为代表。大行其道的类结构。以c++为代表。以及现在光环日耀的CBSE，基于组件的架构&lt;/p&gt;
&lt;h3&gt;4. 如何设计一款游戏引擎？&lt;/h3&gt;
&lt;h4&gt;&amp;lt;1&amp;gt;. 结构设计及功能设计&lt;/h4&gt;
&lt;p&gt;Game Engine的设计包括结构设计、功能设计及注意事项。&lt;/p&gt;
&lt;p&gt;Game Engine包括&lt;strong&gt;图形引擎&lt;/strong&gt;、脚本引擎、物理引擎、工具模块、音效引擎、网络组件、事件组件等。&lt;/p&gt;
&lt;p&gt;Android游戏主要包括一个Activity类、流程控制类、游戏线程类和游戏对象类。Activity类是游戏的执行单元，负责游戏生命周期的控制。&lt;/p&gt;
&lt;p&gt;流程控制：提供在游戏中多个界面之间切换方法；&lt;/p&gt;
&lt;p&gt;游戏线程：不断监测可能发生的各种事件，计算游戏状态，刷新屏幕。&lt;/p&gt;
&lt;h4&gt;&amp;lt;2&amp;gt;. 注意事项：&lt;/h4&gt;
&lt;p&gt;手机游戏的主要问题是 硬件限制 及 电池瓶颈。CPU及内存不足，屏幕大小，音效等多方面限制，在设计时需要注意这些方面。&lt;/p&gt;
&lt;h3&gt;5. 实现一款游戏引擎&lt;/h3&gt;
&lt;p&gt;游戏引擎&lt;strong&gt;只是一款炒菜的炒菜锅，但有了好的炒菜锅不一定能保证炒出好的菜&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;游戏引擎的实现就很复杂了，需要按照上一节的架构及功能设计去编码实现，目前绝大部分都是面向对象编程，设计好各种类。比如人物、NPC、道具、动画、动植物等等。有余力的同学可以去研究研究。&lt;/p&gt;
&lt;p&gt;最近流行的一些游戏，其实也并不需要多么NB的游戏引擎，充分发掘用户的痛点才能设计出一款好的游戏。&lt;/p&gt;
&lt;p&gt;目前有很多开源的Game Engine可供大家研究，比如Unity3D, Box2D等，大家可以去网上搜索并研究。&lt;/p&gt;
&lt;h3&gt;6. 对手机游戏的展望&lt;/h3&gt;
&lt;p&gt;这2年玩过的手机游戏也不少，我也来谈谈一款好的手机游戏应该具备哪些特征：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;strong&gt;上手容易，精通不易&lt;/strong&gt;，且玩且珍惜。手游面向的是大众，所以上手难的游戏就一律pass，必须保证游戏具有简单性，让玩家一安装就可以玩的；&lt;/li&gt;
&lt;li&gt;
&lt;strong&gt;&lt;em&gt;可中断，时间短&lt;/em&gt;&lt;/strong&gt;。一般玩游戏，都是在公交地铁上等碎片时间里，所以提供的是短时间的娱乐效果，允许在游戏和工作模式之间顺利切换；&lt;/li&gt;
&lt;li&gt;
&lt;strong&gt;必须加入SNS元素&lt;/strong&gt;：一款好的手游应该具有社交元素，可以加入LBS寻找周围的玩家，或和好友一起玩游戏及互动，抑或者认识新的好友。因为手游都很简单，所以要留住玩家，加入SNS可以留住玩家；&lt;/li&gt;
&lt;li&gt;充分利用手机的各项优点：手机的优点比如便携性，私密性，即使抵达。手机是我们身体的延伸，所以一款好的游戏应该充分利用手机的一些传感器、摄像头、网络、蓝牙，找出特点，以便设计出一款优秀的游戏。&lt;/li&gt;
&lt;li&gt;…&lt;/li&gt;
&lt;/ol&gt;


&lt;!-- div id=&quot;ad1&quot;&gt;
&lt;/div --&gt;


	


	

</description>
        <pubDate>Wed, 24 Sep 2014 00:00:00 +0800</pubDate>
        <link>http://iftti.com/posts/2014/2014-09-24-77581-bc434c2db.html</link>
        <guid isPermaLink="true">http://iftti.com/posts/2014/2014-09-24-77581-bc434c2db.html</guid>
        
        
        <category>jobbole</category>
        
      </item>
    
      <item>
        <title>无损数据压缩算法的历史</title>
        <description>

					
		
&lt;!-- div id=&quot;ad1&quot;&gt;
&lt;script type=&quot;text/javascript&quot;&gt;
google_ad_client = &quot;ca-pub-7056282119617872&quot;;
google_ad_slot = &quot;6645040531&quot;;
google_ad_width = 300;
google_ad_height = 250;
&lt;/script&gt;
&lt;script type=&quot;text/javascript&quot;
src=&quot;http://pagead2.googlesyndication.com/pagead/show_ads.js&quot;&gt;
&lt;/script&gt;
&lt;/div&gt;
&lt;br/ --&gt;

&lt;h2&gt;
&lt;a class=&quot;anchor&quot; href=&quot;#%E5%BC%95%E8%A8%80&quot; name=&quot;user-content-%E5%BC%95%E8%A8%80&quot;&gt;&lt;/a&gt;引言&lt;/h2&gt;
&lt;p&gt;有两种主要的压缩算法：有损和无损。有损压缩算法通过移除在保真情形下需要大量的数据去存储的小细节，从而使文件变小。在有损压缩里，因某些必要数据的移除，恢复原文件是不可能的。有损压缩主要用来存储图像和音频文件，同时通过移除数据可以达到一个比较高的压缩率，不过本文不讨论有损压缩。无损压缩，也使文件变小，但对应的解压缩功能可以精确的恢复原文件，不丢失任何数据。无损数据压缩被广泛的应用在计算机领域，从节省你个人电脑的空间，到通过web发送数据。使用Secure Shell交流，查看PNG或GIF图片。&lt;/p&gt;
&lt;p&gt;无损压缩算法可行的基本原理是，任意一个非随机文件都含有重复数据，这些重复数据可以通过用来确定字符或短语出现概率的统计建模技术来压缩。统计模型可以用来为特定的字符或者短语生成代码，基于它们出现的频率，配置最短的代码给最常用的数据。这些技术包括熵编码(entropy encoding)、游程编码(run-length encoding)以及字典压缩。运用这些技术以及其它技术，一个8-bit长度的字符或者字符串可以用很少的bit来表示，从而大量的重复数据被移除。&lt;/p&gt;
&lt;h2&gt;
&lt;a class=&quot;anchor&quot; href=&quot;#%E5%8E%86%E5%8F%B2&quot; name=&quot;user-content-%E5%8E%86%E5%8F%B2&quot;&gt;&lt;/a&gt;历史&lt;/h2&gt;
&lt;p&gt;&lt;img id=&quot;pic&quot; alt=&quot;&quot; src=&quot;/images/jobbole.com/e657822be76c892f690831b084d88fbc.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;直到20世纪70年代，数据压缩才在计算机领域开始扮演重要角色，那时互联网变得更加流行，Lempel-Ziv算法被发明出来，但压缩算法在计算机领域之外有着更悠久的历史。发明于1838年的Morse code，是最早的数据压缩实例，为英语中最常用的字母比如”e”和”t”分配更短的Morse code。之后，随着大型机的兴起，Claude Shannon和Robert Fano发明了Shannon-Fano编码算法。他们的算法基于符号(symbol)出现的概率来给符号分配编码(code)。一个符号出现的概率大小与对应的编码成反比，从而用更短的方式来表示符号。&lt;/p&gt;
&lt;p&gt;两年后，David Huffman在MIT学习信息理论并上了一门Robert Fano老师的课，Fano给班级的同学两个选项，写一篇学期论文或者参加期末考试。Huffman选择的是写学期论文，题目是寻找二叉编码的最优算法。经过几个月的努力后依然没有任何成果，Huffman决定放弃所有论文相关的工作，开始学习为参加期末考试做准备。正在那时，灵感爆发，Huffman找到一个与Shannon-Fano编码相类似但是更有效的编码算法。Shannon-Fano编码和Huffman编码的主要区别是构建概率树的过程不同，前者是自下而上，得到一个次优结果，而后者是自上而下。&lt;/p&gt;
&lt;p&gt;早期的Shannon-Fano编码和Huffman编码算法实现是使用硬件和硬编码完成的。直到20世纪70年代互联网以及在线存储的出现，软件压缩才被实现为Huffman编码依据输入数据动态产生。随后，1977年Abraham Lempel 和 Jacob Ziv发表了他们独创性的LZ77算法，第一个使用字典来压缩数据的算法。特别的，LZ77使用了一个叫做slidingwindow的动态字典。1978年，这对搭档发表了同样使用字典的LZ78算法。与LZ77不同，LZ78解析输入数据，生成一个静态字典，不像LZ77动态产生。&lt;/p&gt;
&lt;h3&gt;
&lt;a class=&quot;anchor&quot; href=&quot;#%E6%B3%95%E5%BE%8B%E9%97%AE%E9%A2%98&quot; name=&quot;user-content-%E6%B3%95%E5%BE%8B%E9%97%AE%E9%A2%98&quot;&gt;&lt;/a&gt;法律问题&lt;/h3&gt;
&lt;p&gt;LZ77和LZ78都快速的流行开来，衍生出多个下图中所示的压缩算法。其中的大多数已经沉寂了，只有那么几个现在被大范围的使用，包括DEFLATE，LZMA以及LZX。绝大多数常用的压缩算法都衍生于LZ77，这不是因为LZ77技术更好，只是由于Sperry在1984年申请了LZ78衍生算法LZW的专利，从而发展受到了专利的阻碍，Sperry开始因专利侵权而起诉软件提供商，服务器管理员，甚至是使用GIF格式但没有License的终端用户。&lt;/p&gt;
&lt;p&gt;同时，UNIX压缩工具使用了一个叫LZC的LZW算法微调整版本，之后由于专利问题而被弃用。其他的UNIX开发者也开始放弃使用LZW。这导致UNIX社区采用基于DEFLATE的gzip和基于Burrows-Wheeler Transform的bzip2算法。长远来说，对于UNIX社区这是有好处的，因为gzip和bzip2格式几乎总是比LZW有更好的压缩比。围绕LZW的专利问题已经结束，因为LZW的专利2003年就到期了。尽管这样，LZW算法已经很大程度上被替代掉了，仅仅被使用于GIF压缩中。自那以后，也有一些LZW的衍生算法，不过都没有流行开来，LZ77算法仍然是主流。&lt;/p&gt;
&lt;p&gt;另外一场法律官司发生于1993，关于LZS算法。LZS是由Stac Electronics开发的，用于硬盘压缩软件，如Stacker。微软在开发影片压缩软件时使用了LZS算法，开发的软件随着MS-DOS 6.0一起发布，声称能够使硬盘容量翻倍。当Stac Electronics发现自己的知识财产被使用后，起诉了微软。微软随后被判专利侵权并赔偿Stac Electronics1亿2000万美元，后因微软上诉因非故意侵权而减少了1360万美元。尽管Stac Electronics和微软发生了一个那么大的官司，但它没有阻碍Lempel-Ziv算法的开发，不像LZW专利纠纷那样。唯一的结果就是LZS没有衍生出任何算法。&lt;/p&gt;
&lt;h3&gt;
&lt;a class=&quot;anchor&quot; href=&quot;#deflate%E7%9A%84%E5%B4%9B%E8%B5%B7&quot; name=&quot;user-content-deflate%E7%9A%84%E5%B4%9B%E8%B5%B7&quot;&gt;&lt;/a&gt;Deflate的崛起&lt;/h3&gt;
&lt;p&gt;自从Lempel-Ziv算法被发表以来，随着对存储需求的不断增长，一些公司及其他团体开始使用数据压缩技术，这能让他们满足这些需求。然而，数据压缩并没有被大范围的使用，这一局面直到20世纪80年代末期随着互联网的腾飞才开始改变，那时数据压缩的需求出现了。带宽限额，昂贵，数据压缩能够帮助缓解这些瓶颈。当万维网发展起来之后人们开始分享更多的图片以及其它格式的数据，这些数据远比文本大得多，压缩开始变得极其重要。为了满足这些需求，几个新的文件格式被开发出来，包括ZIP，GIF，和PNG。&lt;/p&gt;
&lt;p&gt;Thom Henderson通过他的公司发布了第一个成功的商业存档格式，叫做ARC，公司名为为System Enhancement Associates。ARC在BBS社区尤为流行，这是因为它是第一个既可以打包又可以压缩的程序，此外还开放了源代码。ARC格式使用一个LZW的衍生算法来压缩数据。一个叫做Phil Katz的家注意到了ARC的流行并决定用汇编语言来重写压缩和解压缩程序，希望改进ARC。他于1987发布了他的共享软件PKARC程序，不久被Henderson以侵犯版权为由起诉。Katz被认定为有罪，并被迫支付版权费用以及其它许可协议费用。他之所以被认定侵权，是由于PKARC是明显抄袭ARC，甚至于一些注释里面的错别字都相同。&lt;/p&gt;
&lt;p&gt;Phil Katz自1988年之后就因许可证问题不能继续出售PKARC，所以1989年他创建了一个PKARC的修改版，就是现在大家熟知的ZIP格式。由于使用了LZW，它被认为专利侵权的，之后Katz选择转而使用新的IMPLODE算法，这种格式于1993年再次被修改，那时Kata发布了PKZIP的2.0版本，那个版本实现了DEFLATE算法以及一些其它特性，如分割容量等。这个ZIP版本也是我们现在随处可见的格式，所有的ZIP文件都遵循PKZIP 2.0格式，尽管它年代久远。&lt;/p&gt;
&lt;p&gt;GIF格式，全称Graphics Interchange Format，于1987年由CompuServe创建，允许图像无失真地被共享(尽管这种格式被限定每一帧最多256种颜色)，同时减小文件的大小以允许通过数据机传输。然而，像ZIP格式一样，GIF也是基于LZW算法。尽管专利侵权，Unisys没有办法去阻止GIF的传播。即使是现在，20年后的今天，GIF仍然被使用着，特别是它的动画能力。&lt;/p&gt;
&lt;p&gt;尽管GIF没有办法被叫停，CompuServe需找一种不受专利束缚的格式，并于1994年引入了Portable Network Graphics (PNG) 格式。像ZIP一样，PNG使用DEFLATE算法来处理压缩。尽管DELLATE的专利属于Katz，这个专利并不是强性制的，正是这样，PNG以及其它基于DEFLATE的格式避免了专利侵权。尽管LZW在压缩历史的初期占据霸主位置，由于Unisys公司的好诉讼作为，LZW慢慢的淡出主流，大家转而使用更快更高效的DEFLATE算法。现在DEFLATE是使用得最多的算法，有些压缩世界里瑞士军刀的味道。&lt;/p&gt;
&lt;p&gt;除了用于PNG和ZIP格式之外，计算机世界里DEFLATE也被频繁的用在其它地方。例如gzip(.gz)文件格式也使用DEFLATE，gzip是ZIP的一个开源版本。其它还包括HTTP, SSL, 以及其它的高效压缩网络传输数据的技术。&lt;/p&gt;
&lt;p&gt;遗憾的是，Phil Katz英年早逝，没能看到他的DEFLATE算法统治计算机世界。有几年的时间他酗酒成性，生活也于20世纪90年代末期开始支离破碎，好几次因酒驾或者其它违法行为而被逮捕。Katz于2000年4月14号被发现死于一个酒店的房间，终年37岁。死因是酒精导致的严重胰腺出血，身旁是一堆的空酒瓶。&lt;/p&gt;
&lt;h3&gt;
&lt;a class=&quot;anchor&quot; href=&quot;#%E5%BD%93%E5%89%8D%E7%9A%84%E4%B8%80%E4%BA%9B%E5%BD%92%E6%A1%A3%E8%BD%AF%E4%BB%B6&quot; name=&quot;user-content-%E5%BD%93%E5%89%8D%E7%9A%84%E4%B8%80%E4%BA%9B%E5%BD%92%E6%A1%A3%E8%BD%AF%E4%BB%B6&quot;&gt;&lt;/a&gt;当前的一些归档软件&lt;/h3&gt;
&lt;p&gt;ZIP以及其它基于DEFLATE的格式一直占据主导地位，直到20世纪90年代中期，一些新的改进的格式开始出现。1993年，Eugene Roshal发布了一个叫做WinRAR的归档软件，该软件使用RAR格式。最新的RAR结合了PPM和LZSS算法，前面的版本就不太清楚了。RAR开始在互联网分享文件方面成为事实标准，特别是盗版影像的传播。1996年一个叫bzip2d的Burrows-Wheeler Transform算法开源实现发布，并很快在UNIX平台上面流行开来，大有对抗基于DEFLATE算法的gzip格式。1999年另外一个开源压缩程序发布了，以7-Zip或.7z的格式存在，7-Zip应该是第一个能够挑战ZIP和RAR霸主地位的格式，这源于它的高压缩比，模块化以及开放性。这种格式并不仅限于使用一种压缩算法，而是可以在bzip2, LZMA, LZMA2, 和 PPMd算法之间任意选择。最后，归档软件中较新的一个格式是PAQ*格式。第一个PAQ版本于2002年由Matt Mahoney发布，叫做PAQ1。PAQ主要使用一种叫做context mixing的技术来改进PPM算法，context mixing结合了两个甚至是多个统计模型来产生一个更好的符号概率预测，这要比其中的任意一个模型都要好。&lt;/p&gt;
&lt;h2&gt;
&lt;a class=&quot;anchor&quot; href=&quot;#%E5%8E%8B%E7%BC%A9%E6%8A%80%E6%9C%AF&quot; name=&quot;user-content-%E5%8E%8B%E7%BC%A9%E6%8A%80%E6%9C%AF&quot;&gt;&lt;/a&gt;压缩技术&lt;/h2&gt;
&lt;p&gt;有许多不同的技术被用来压缩数据。大多数技术都不能单独使用，需要结合起来形成一套算法。那些能够单独使用的&lt;br&gt;
技术比需要结合的技术通常更加有效。其中的绝大部分都归于entropy编码类别下面，但其它的一些技术也挺常用，如Run-Length Encoding和Burrows-Wheeler Transform。&lt;/p&gt;
&lt;h3&gt;
&lt;a class=&quot;anchor&quot; href=&quot;#run-length-encoding&quot; name=&quot;user-content-run-length-encoding&quot;&gt;&lt;/a&gt;Run-Length Encoding&lt;/h3&gt;
&lt;p&gt;Run-Length Encoding是一个非常简单的压缩技术，把重复出现的多个字符替换为重复次数外加字符。单个字符次数为1。RLE非常适合数据重复度比较高的数据，同一行有很多像素颜色相同的渐进图片，也可以结合Burrows-Wheeler Transform等其它技术一起使用。&lt;/p&gt;
&lt;p&gt;下面是RLE的一个简单例子:&lt;/p&gt;
&lt;p&gt;输入: AAABBCCCCDEEEEEEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA&lt;br&gt;
输出: 3A2B4C1D6E38A&lt;/p&gt;
&lt;h3&gt;
&lt;a class=&quot;anchor&quot; href=&quot;#burrows-wheeler-transform&quot; name=&quot;user-content-burrows-wheeler-transform&quot;&gt;&lt;/a&gt;Burrows-Wheeler Transform&lt;/h3&gt;
&lt;p&gt;Burrows-Wheeler Transform是1994年发明的技术，目的是可逆的处理一段输入数据，使得相同字符连续出现的次数最大化。BWT自身并不做任何的压缩操作，仅简单地转化数据，让Run-Length Encoder等压缩算法可以更有效的编码。&lt;/p&gt;
&lt;p&gt;BWT算法很简单:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;创建一个字符串数组。&lt;/li&gt;
&lt;li&gt;把输入字符串的所有排列组合塞入上述字符串数组。&lt;/li&gt;
&lt;li&gt;按照字符顺序为字符串数组排序。&lt;/li&gt;
&lt;li&gt;返回数组的最后一列。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;BWT通常处理有很多交叉重复字符的长字符串时效果很好。下面是一个有着理想输入的例子，注意&amp;amp;是文件结束符：&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/c169ca29581eb3fb92eaa37781c0fdad.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;因为交换相同的符号到一起，输入数据在BWT处理之后得到优化后的结果，另外一个算法可以对该结果进行压缩，比如RLE会得到”3H&amp;amp;3A”。尽管这个例子得到了一个较优的结果，不过现实世界中的数据它不总是这样。&lt;/p&gt;
&lt;h3&gt;
&lt;a class=&quot;anchor&quot; href=&quot;#entropy-encoding&quot; name=&quot;user-content-entropy-encoding&quot;&gt;&lt;/a&gt;Entropy Encoding&lt;/h3&gt;
&lt;p&gt;数据压缩中，平均来说为了表示一个字符或短语，Entropy意味着所需要的最少bit数。一个基本的entropy编码器包括一个分析模型以及一套编码。输入文件被解析，并产生一个由字符出现概率组成的统计模型。然后，编码器可以利用该统计模型去决定该给每一个字符多少个bit，从而使得最常用的字符用最短的编码，反之最不常用的字符用最长的编码。&lt;/p&gt;
&lt;h4&gt;
&lt;a class=&quot;anchor&quot; href=&quot;#shannon-fano-coding&quot; name=&quot;user-content-shannon-fano-coding&quot;&gt;&lt;/a&gt;Shannon-Fano Coding&lt;/h4&gt;
&lt;p&gt;这是最早的压缩技术，于1949年由Claude Shannon和Robert Fano发明。这个技术的其中一个步骤是产生一个代表字符出现概率的二叉树。字符以这样一种方式排序，出现得越频繁的字符越靠近树的顶端，越不常见的越靠近树的底部。&lt;/p&gt;
&lt;p&gt;一个字符对应的编码通过搜索Shannon-Fano来获得，此外，左分支后面加0，右分支加1。例如，”A”是两个左节点后接一个右节点，那么对于的编码为”0012″。Shannon-Fano coding不总是能够产生最优的编码，主要是由于二叉树是自下而上构建的。由于这个原因，使用的较多的还是对于任意输入都能够得到最优编码的Huffman coding。&lt;/p&gt;
&lt;p&gt;产生Shannon-Fano编码的算法很简单:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;解析输入，统计每一个字符出现的频率。&lt;/li&gt;
&lt;li&gt;根据是上述频率计算字符的概率。&lt;/li&gt;
&lt;li&gt;依据概率对字符降序排序。&lt;/li&gt;
&lt;li&gt;为每一个字符生成一个叶节点(LeafNode)&lt;/li&gt;
&lt;li&gt;把字符列表分为左右两部分，使得左边的概率与右边的概率大致相当。&lt;/li&gt;
&lt;li&gt;左节点加编码”0″，右节点加编码”1″。&lt;/li&gt;
&lt;li&gt;对两棵子树重复的步骤5和6，直到所有的字符节点都成为叶子节点。&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;
&lt;a class=&quot;anchor&quot; href=&quot;#huffman-coding&quot; name=&quot;user-content-huffman-coding&quot;&gt;&lt;/a&gt;Huffman Coding&lt;/h4&gt;
&lt;p&gt;Huffman Coding是另外一个entropy coding的例子，与Shannon-Fano Coding非常的相似，只是为了产生最优编码&lt;br&gt;
二叉树是自上而下构建的。&lt;/p&gt;
&lt;p&gt;生成Huffman编码的算法前面的三个步骤与Shannon-Fano完全相同:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;解析输入，统计每一个字符出现的频率。&lt;/li&gt;
&lt;li&gt;根据是上述频率计算字符的概率。&lt;/li&gt;
&lt;li&gt;依据概率对字符降序排序。&lt;/li&gt;
&lt;li&gt;为每一个字符生成一个叶节点(LeafNode)，节点包含概率信息P，把节点存入一个队列Queue。&lt;/li&gt;
&lt;li&gt;While (Nodes in Queue &amp;gt; 1)
&lt;ul&gt;
&lt;li&gt;从队列里面取出概率最小的两个节点。&lt;/li&gt;
&lt;li&gt;给左节点分配编码”0″，右节点分配编码”1″。&lt;/li&gt;
&lt;li&gt;创建一个新的节点，其概率为上面步骤中的两个节点之和。&lt;/li&gt;
&lt;li&gt;把两个节点中的第一个设置为新建节点的左节点，第二个节点为新建节点的右节点。&lt;/li&gt;
&lt;li&gt;把新建节点存入队列&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;最后一个节点就是二叉树的根节点。&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;
&lt;a class=&quot;anchor&quot; href=&quot;#arithmetic-coding&quot; name=&quot;user-content-arithmetic-coding&quot;&gt;&lt;/a&gt;Arithmetic Coding&lt;/h4&gt;
&lt;p&gt;1979年该算法在IBM被开发出来，当时IBM正在调研一些压缩算法，以期用于它们的大型机上。如果单论压缩比，Arithmetic coding确实是一个最优的entropy coding技术，通常压缩比方面Arithmetic Coding要比Huffman Coding表现得更好。然而，它却也比其它技术复杂得多。&lt;/p&gt;
&lt;p&gt;不像其它技术会把字符概率构建成一棵树，arithmetic coding把输入转化为一个0到1之间的有理数，输入字符的个数记为base，里面每一个不同的字符都分配一个0到base之间的值。然后，最后转化为二进制得到最终的结果。结果也可以通过把base恢复为原来的base值，替换为对应字符而得到原输入值。&lt;/p&gt;
&lt;p&gt;一个基本的计算arithmetic code算法如下:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;计算输入数据里面不同字符的个数。这个数字记为base b(比如 base 2代表2二进制)。&lt;/li&gt;
&lt;li&gt;按字符出现的顺序分别给每一个字符分配一个0到b之间的值。&lt;/li&gt;
&lt;li&gt;使用步骤2中德值，把输入中的字符替换为对应的数字(编码)。&lt;/li&gt;
&lt;li&gt;把步骤3中得到的结果从b进制转化为2进制。&lt;/li&gt;
&lt;li&gt;如果解码需要的话，记录输入的字符总个数。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;下面是一个编码操作的例子，输入为”ABCDAABD”：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;找到共有4个不同的字符输入, base = 4, length = 8。&lt;/li&gt;
&lt;li&gt;按出现顺序为不同的字符赋值: A=0, B=1, C=2, D=3。&lt;/li&gt;
&lt;li&gt;用编码替换字符，得到“0.012300134”，注意最前面的”0.”是为了得到小数而加上去的。最后的4表示base=4。&lt;/li&gt;
&lt;li&gt;把“0.012300134”从4进制转化为2进制，得到“0.011011000001112”。最后的2表示base=2。&lt;/li&gt;
&lt;li&gt;在结果中标识输入的总字符数为8。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;假设字符为8个bit表示，输入共需要64个bit空间，然而对应的arithmetic coding只有15个bit，压缩比为24%，&lt;br&gt;
效果显著。这个例子展示了arithmetic coding是如何良好的压缩固定字符串的。&lt;/p&gt;
&lt;h2&gt;
&lt;a class=&quot;anchor&quot; href=&quot;#%E5%8E%8B%E7%BC%A9%E7%AE%97%E6%B3%95&quot; name=&quot;user-content-%E5%8E%8B%E7%BC%A9%E7%AE%97%E6%B3%95&quot;&gt;&lt;/a&gt;压缩算法&lt;/h2&gt;
&lt;h3&gt;
&lt;a class=&quot;anchor&quot; href=&quot;#sliding-window-algorithms&quot; name=&quot;user-content-sliding-window-algorithms&quot;&gt;&lt;/a&gt;Sliding Window Algorithms&lt;/h3&gt;
&lt;h4&gt;
&lt;a class=&quot;anchor&quot; href=&quot;#lz77&quot; name=&quot;user-content-lz77&quot;&gt;&lt;/a&gt;LZ77&lt;/h4&gt;
&lt;p&gt;LZ77发表于1977年，是名副其实的压缩算法开山之作。它首次引入’sliding window’的概念，相较几个主要的压缩算法，压缩比都有非常明显的提高。LZ77维护了一个字典，用一个三元组来表示offset，run length和分割字符。offset表示从文件起始位置到当前Phase的起始位置的距离，run length记录当前Phase有多少个字符，分割符仅用于分割不同的Phase。Phase就是offset到offset+length之间的子串减掉分隔符。随着文件解析的进行，基于sliding window字典会动态的变化。例如，64MB的sliding window意味着四点将包含64M的输入数据的信息。&lt;/p&gt;
&lt;p&gt;给定一个输入为”abbadabba”，那么输出可能像”abb(0,1,’d&#39;)(0,3,’a&#39;)”，如下图所示:&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/f5966eaefb9fa7f7485591c7b1b1ba87.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;尽管上述的替换看起来比原数据还要大，当输入数据更大一些的时候，效果会比较好。&lt;/p&gt;
&lt;h4&gt;
&lt;a class=&quot;anchor&quot; href=&quot;#lzr&quot; name=&quot;user-content-lzr&quot;&gt;&lt;/a&gt;LZR&lt;/h4&gt;
&lt;p&gt;LZR是LZ77的修改版本，于1981年由Michael Rodeh发明。这个算法目标是成为LZ77的一个线性时间替换算法。然而，编码后Udell指针可能指向文件的任意offset，意味着需要耗费可观的内存。加之压缩比表现也差强人意(LZ77好得多)，LZR算是一个不成功的LZ77衍生算法。&lt;/p&gt;
&lt;h4&gt;
&lt;a class=&quot;anchor&quot; href=&quot;#deflate&quot; name=&quot;user-content-deflate&quot;&gt;&lt;/a&gt;DEFLATE&lt;/h4&gt;
&lt;p&gt;DEFLATE于1993年由Phil Katz发明，是现代绝大多数压缩任务的基石。它仅仅结合了两种算法，先用LZ77或LZSS预处理，然后用Huffman编码，快速的得到不错的压缩结果。&lt;/p&gt;
&lt;h4&gt;
&lt;a class=&quot;anchor&quot; href=&quot;#deflate64&quot; name=&quot;user-content-deflate64&quot;&gt;&lt;/a&gt;DEFLATE64&lt;/h4&gt;
&lt;p&gt;DEFLATE64是DEFLATE的一个有专利的扩展，把字典的大小提高到64K(名字随之)，从而允许在sliding window里面有更大的距离。相比于DEFLATE，DEFLATE64在性能和压缩比方面都有提高。然而，由于DEFLATE64的专利保护以及相较DEFLATE并没有特别明显的提高，DEFLATE64很少被采用。相反一些开源算法如LZMA被大量的使用。&lt;/p&gt;
&lt;h4&gt;
&lt;a class=&quot;anchor&quot; href=&quot;#lzss&quot; name=&quot;user-content-lzss&quot;&gt;&lt;/a&gt;LZSS&lt;/h4&gt;
&lt;p&gt;LZSS，全称Lempel-Ziv-Storer-Szymanski，于1982年由James Storer发表。LZSS相较LZ77有所提高，它能检测到一个替换是否真的减小了文件大小。如果文件大小没有减小，不再替换输入值。此外，输入段被(offset, length)数据对替换，其中offset表示离输入起始位置的bytes数量，length表示从该位置读取了多少个字符。另外一个改进是去除了”next character”信息，仅仅使用offset-length数据对。&lt;/p&gt;
&lt;p&gt;下面是一个输入为” these theses”简单的例子，结果为” these(0,6)s”，仅仅节省了一个Byte，不过输入数据大的时候效果会好得多。&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/5de2f6e48b62da018014a831becfae94.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;LZSS依然被用在许多使用广泛的归档格式中，其中最知名的是RAR。LZSS有时也被用于网络数据压缩。&lt;/p&gt;
&lt;h4&gt;
&lt;a class=&quot;anchor&quot; href=&quot;#lzh&quot; name=&quot;user-content-lzh&quot;&gt;&lt;/a&gt;LZH&lt;/h4&gt;
&lt;p&gt;LZH发明于1987年，全称为”Lempel-Ziv Huffman”。它是LZSS的一个衍生算法，利用Huffman coding压缩指针，压缩效果有微小的提高。然而使用Huffman coding带来的提高实在是很有限，相较于使用Huffman coding带来的性能损失，不足为取。&lt;/p&gt;
&lt;h4&gt;
&lt;a class=&quot;anchor&quot; href=&quot;#lzb&quot; name=&quot;user-content-lzb&quot;&gt;&lt;/a&gt;LZB&lt;/h4&gt;
&lt;p&gt;LZB同样发明于1987年，同样是LZSS的衍生算法。如LZH一样，LZB也致力于通过更加有效的编码指针以达到更好的压缩效果。它的做法是随着sliding window变大，逐步的增大指针的数量。它的压缩效果确实比LZSS和LZH要好，不过因为额外的编码步骤，速度上比LZSS慢得多。&lt;/p&gt;
&lt;h4&gt;
&lt;a class=&quot;anchor&quot; href=&quot;#rolz&quot; name=&quot;user-content-rolz&quot;&gt;&lt;/a&gt;ROLZ&lt;/h4&gt;
&lt;p&gt;ROLZ全称”Reduced Offset Lempel-Ziv”，它的目标是提高LZ77的压缩效果，通过限制offset的大小，从而减少为offset-length数据对编码的数据量。这项LZ77的衍生技术于1991年首次出现在Ross Williams的LZRW4算法里面。其它的实现包括BALZ，QUAD，和 RZM。高度优化的ROLZ能够达到接近LZMA的压缩比，不过ROLZ不太流行。&lt;/p&gt;
&lt;h4&gt;
&lt;a class=&quot;anchor&quot; href=&quot;#lzp&quot; name=&quot;user-content-lzp&quot;&gt;&lt;/a&gt;LZP&lt;/h4&gt;
&lt;p&gt;LZP全称”Lempel-Ziv + Prediction”。它是ROLZ算法的一个特殊案例，offset减小到1。&lt;br&gt;
有几个衍生的算法使用不同的技术来实现或加快压缩速度，或提高压缩比的目标。LZW4实现了一个数字编码器&lt;br&gt;
达到了最好的压缩比，不过牺牲了部分速度。&lt;/p&gt;
&lt;h4&gt;
&lt;a class=&quot;anchor&quot; href=&quot;#lzrw1&quot; name=&quot;user-content-lzrw1&quot;&gt;&lt;/a&gt;LZRW1&lt;/h4&gt;
&lt;p&gt;Ron Williams于1991年发明了这个算法，第一次引入了Reduced-Offset Lempel-Ziv compressiond的概念。LZRW1&lt;br&gt;
能够达到很高的压缩比，同时保持快速有效。Ron Williams也发明另外几个基于LZRW1改进的衍生算法，如&lt;br&gt;
LZRW1-A, 2, 3, 3-A, 和4。&lt;/p&gt;
&lt;h4&gt;
&lt;a class=&quot;anchor&quot; href=&quot;#lzjb&quot; name=&quot;user-content-lzjb&quot;&gt;&lt;/a&gt;LZJB&lt;/h4&gt;
&lt;p&gt;Jeff Bonwick于1998年发明了Lempel-Ziv Jeff Bonwick算法，用于Solaris操作系统的Z文件系统(ZFS)。它被认为是&lt;br&gt;
LZRW算法的一个衍生算法，特别是LZRW1，目标是改进压缩速度。既然它是被用于操作系统，速度显得尤为重要，&lt;br&gt;
不能因为压缩算法的原因而使得磁盘操作成为瓶颈。&lt;/p&gt;
&lt;h4&gt;
&lt;a class=&quot;anchor&quot; href=&quot;#lzs&quot; name=&quot;user-content-lzs&quot;&gt;&lt;/a&gt;LZS&lt;/h4&gt;
&lt;p&gt;Lempel-Ziv-Stac算法于1994年由Stac Electronics发明，用于磁盘压缩软件。它是LZ77的一个修改版本，区分了输出的文字符号与offset-length数据对，此外还移除了分隔符。功能上来说，LZS与LZSS算法很相似。&lt;/p&gt;
&lt;h4&gt;
&lt;a class=&quot;anchor&quot; href=&quot;#lzx&quot; name=&quot;user-content-lzx&quot;&gt;&lt;/a&gt;LZX&lt;/h4&gt;
&lt;p&gt;LZX算法于1995年由Jonathan Forbes和Tomi Poutanen发明，用于Amiga计算机。LZX中X没有什么特殊的意义。Forbes于1996年把该算法出售给了微软，并且受雇于微软，那那儿该算法被继续优化并用于微软的cabinet(.CAB)格式。这个算法也被微软用于压缩Compressed HTML Help (CHM) 文件，Windows Imaging Format (WIM)&lt;br&gt;
文件，和 Xbox Live Avatars。&lt;/p&gt;
&lt;h4&gt;
&lt;a class=&quot;anchor&quot; href=&quot;#lzo&quot; name=&quot;user-content-lzo&quot;&gt;&lt;/a&gt;LZO&lt;/h4&gt;
&lt;p&gt;LZO于1996年由Markus发明，该算法的目标是快速的压缩和解压缩。它允许调整压缩级别，并且在最高级别下仍仅需64KB额外的内存空间，同时解压缩仅需要输入和输出的空间。LZO功能上非常类似LZSS，不过是为了速度，而非压缩比做的优化。&lt;/p&gt;
&lt;h4&gt;
&lt;a class=&quot;anchor&quot; href=&quot;#lzma&quot; name=&quot;user-content-lzma&quot;&gt;&lt;/a&gt;LZMA&lt;/h4&gt;
&lt;p&gt;Lempel-Ziv Markov chain Algorithm算法于1998年首次发表，是随着7-Zip归档软件一起发布的。大多数情况下它比bzip2, DEFLATE以及其它算法表现都要好。LZMA使用一系列技术来完成输出。首先时一个LZ77的修改版本，它操作的是bitwise级别，而非传统上的bytewise级别，用于解析数据。LZ77算法解析后的输出经过数字编码。更多的技术可被使用，这依赖于具体的LZMA实现。相比其它LZ衍生算法，LZMA在压缩比方面提高明显，这要归功于操作bitewise，而非bytewise。&lt;/p&gt;
&lt;h4&gt;
&lt;a class=&quot;anchor&quot; href=&quot;#lzma2&quot; name=&quot;user-content-lzma2&quot;&gt;&lt;/a&gt;LZMA2&lt;/h4&gt;
&lt;p&gt;LZMA2是LZMA的一个增量改进版本，于2009年在7-Zip归档软件的一个更新版本里面首次引入。LZMA2改进了多线程处理功能，同时优化对不可压缩数据的处理，这也稍微提高了压缩效果。&lt;/p&gt;
&lt;h4&gt;
&lt;a class=&quot;anchor&quot; href=&quot;#statistical-lempel-ziv&quot; name=&quot;user-content-statistical-lempel-ziv&quot;&gt;&lt;/a&gt;Statistical Lempel-Ziv&lt;/h4&gt;
&lt;p&gt;Statistical Lempel-Ziv是于2001年由Sam Kwong博士和Yu Fan Ho博士提出的一个概念。基本的原则是数据的统计分析结果可以与LZ77衍生算法结合起来，进一步优化什么样的编码将存储在字典中。&lt;/p&gt;
&lt;h3&gt;
&lt;a class=&quot;anchor&quot; href=&quot;#dictionary-algorithms&quot; name=&quot;user-content-dictionary-algorithms&quot;&gt;&lt;/a&gt;Dictionary Algorithms&lt;/h3&gt;
&lt;h4&gt;
&lt;a class=&quot;anchor&quot; href=&quot;#lz78&quot; name=&quot;user-content-lz78&quot;&gt;&lt;/a&gt;LZ78&lt;/h4&gt;
&lt;p&gt;LZ78于1978年由Lempel和Ziv发明，缩写正是来源于此。不再使用sliding window来生成字典，输入数据要么被预处理之后用来生成字典，或者字典在文件解析过程中逐渐形成。LZ78采用了后者。字典的大小通常被限定为几兆的大小，或者所有编码上限为几个比特，比如8个。这是出于减少对内存要求的考量。算法如何处理正是LZ78的各个衍生算法的区别所在。&lt;/p&gt;
&lt;p&gt;解析文件的时候，LZ78把新碰到的字符或者字符串加到字典中。针对每一个符号，形如(dictionary index, unknown symbol)的字典记录会对应地生成。如果符号已经存在于字典中，那么将从字典中搜索该符号的子字符串以及其后的其它符号。最长子串的位置即为字典索引(Index)。字典索引对应的数据被设置为最后一个未知子串。如果当前字符是未知的，那么字典索引设置为0，表示它是单字符对。这些数据对形成一个链表数据结构。&lt;/p&gt;
&lt;p&gt;形如”abbadabbaabaad”的输入，将会产生”(0,a)(0,b)(2,a)(0,d)(1,b)(3,a)(6,d)”这样的输出。&lt;br&gt;
你能从下面的例子里面看到它是如何演化的:&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://jbcdn2.b0.upaiyun.com/2014/09/0e12905c00473a9d9cfff86a163b2b12.jpg&quot; rel=&quot;lightbox[77247]&quot; title=&quot;无损数据压缩算法的历史&quot;&gt;&lt;img class=&quot;alignnone size-full wp-image-77253&quot; alt=&quot;LZ78&quot; src=&quot;/images/jobbole.com/aded11e78678bff55334d118bc8865dd.jpg&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;
&lt;a class=&quot;anchor&quot; href=&quot;#lzw&quot; name=&quot;user-content-lzw&quot;&gt;&lt;/a&gt;LZW&lt;/h4&gt;
&lt;p&gt;LZW于1984年由Terry Welch发明，全称为Lempel-Ziv-Welch。它是LZ78大家族中被用得最多的算法，尽管被专利严重的阻碍了使用。LZW改进LZ78的方法与LZSS类似。它删除输出中冗余的数据，使得输出中不再包含指针。压缩之前它就在字典里面包含了每一个字符，也引入了一些技术改进压缩效果，比如把每一个语句的最后一个字符编码为下一个语句的第一个字符。LZW在图像转换格式中较为常见，早期也曾用于ZIP格式里面，也包括一些其他的专业应用。LZW非常的快，不过相较于一些新的算法，压缩效果就显得比较平庸。一些算法会更快，压缩效果也更好。&lt;/p&gt;
&lt;h4&gt;
&lt;a class=&quot;anchor&quot; href=&quot;#lzc&quot; name=&quot;user-content-lzc&quot;&gt;&lt;/a&gt;LZC&lt;/h4&gt;
&lt;p&gt;LZC，全称Lempel-Ziv Compress，是LZW算法的一个微小修改版本，用于UNIX压缩工具中。LZC与LZW两者的区别在于，LZC会监控输出的压缩比。当压缩比超过某一个临界值的时候，字典被丢弃并重构。&lt;/p&gt;
&lt;h4&gt;
&lt;a class=&quot;anchor&quot; href=&quot;#lzt&quot; name=&quot;user-content-lzt&quot;&gt;&lt;/a&gt;LZT&lt;/h4&gt;
&lt;p&gt;Lempel-Ziv Tischer是LZC的修改版，当字典满了，删除最不常用的的语句，用新的记录替换它。还有一些其它的小改进，不过现在LZC和LZT都不常用了。&lt;/p&gt;
&lt;h4&gt;
&lt;a class=&quot;anchor&quot; href=&quot;#lzmw&quot; name=&quot;user-content-lzmw&quot;&gt;&lt;/a&gt;LZMW&lt;/h4&gt;
&lt;p&gt;于1984年由Victor Miller和Mark Wegman发明，LZMW算法非常类似于LZT，它也采用了替换最不常用语句的策略。然而，不是连接字典中相似的语句，而是连接LZMW最后被编码的两个语句并且存储为一条记录。因此，字典的容量能够快速扩展并且LRUs被更频繁的丢弃掉。LZMW压缩效果比LZT更好，然而它也是另外一个这个时代很难看到其应用的算法。&lt;/p&gt;
&lt;h4&gt;
&lt;a class=&quot;anchor&quot; href=&quot;#lzap&quot; name=&quot;user-content-lzap&quot;&gt;&lt;/a&gt;LZAP&lt;/h4&gt;
&lt;p&gt;LZAP于1988年由James Storer发明，是LZMW算法的修改版本。AP代表”all prefixes”，以其遍历时在字典里面存储单个语句，字典存储了语句的所有排列组合。例如，如果最后一个语句为”last”，当前语句是”next”，字典将存储”lastn”，”lastne”，”lastnex”，和 “lastnext”。&lt;/p&gt;
&lt;h4&gt;
&lt;a class=&quot;anchor&quot; href=&quot;#lzwl&quot; name=&quot;user-content-lzwl&quot;&gt;&lt;/a&gt;LZWL&lt;/h4&gt;
&lt;p&gt;LZWL是2006年发明的一个LZW修改版本，处理音节而非字符。LZWL是专为有很多常用音节的数据集而设计的，比如&lt;br&gt;
XML。这种算法通常会搭配一个前置处理器，用来把输入数据分解为音节。&lt;/p&gt;
&lt;h4&gt;
&lt;a class=&quot;anchor&quot; href=&quot;#lzj&quot; name=&quot;user-content-lzj&quot;&gt;&lt;/a&gt;LZJ&lt;/h4&gt;
&lt;p&gt;Matti Jakobsson于1985年发表了LZJ算法，它是LZ78大家族中唯一一位衍生于LZW的算法。LZJ的工作方式是，在字典中存储每一个经过预处理输入中的不同字符串，并为它们编码。当字典满了，移除所有只出现一次的记录。&lt;/p&gt;
&lt;h3&gt;
&lt;a class=&quot;anchor&quot; href=&quot;#non-dictionary-algorithms&quot; name=&quot;user-content-non-dictionary-algorithms&quot;&gt;&lt;/a&gt;Non-dictionary Algorithms&lt;/h3&gt;
&lt;h4&gt;
&lt;a class=&quot;anchor&quot; href=&quot;#ppm&quot; name=&quot;user-content-ppm&quot;&gt;&lt;/a&gt;PPM&lt;/h4&gt;
&lt;p&gt;通过抽样来预测数据是一项统计建模技术，使用输入中的一部分数据，来预测后续的符号将会是什么，通过这种算法来减少输出数据的entropy。该算法与字典算法不一样，PPM预测下一个符号将会是什么，而不是找出下一个符号来编码。PPM通常结合一个编码器来一起使用，如arithmetic编码或者适配的Huffman编码。PPM或者它的衍生算法被实现于许多归档格式中，包括7-Zip和RAR。&lt;/p&gt;
&lt;h4&gt;
&lt;a class=&quot;anchor&quot; href=&quot;#bzip2&quot; name=&quot;user-content-bzip2&quot;&gt;&lt;/a&gt;bzip2&lt;/h4&gt;
&lt;p&gt;bzip2是Burrows-Wheeler Transform算法的一个开源实现。它的操作原理很简单，不过却在压缩比和速度之间达到了一个平衡，表现良好，这让它在UNIX环境上很流行。首先，使用了一个Run-Length编码器，接下来，Burrows-Wheeler Transform算法加入进来，然后，使用move-to-front transform以达到产生大量相同符号的目标，为接下来的另一个Run-Length 编码器做准备。最后结果用Huffman编码，将一个消息头与其打包。&lt;/p&gt;
&lt;h4&gt;
&lt;a class=&quot;anchor&quot; href=&quot;#paq&quot; name=&quot;user-content-paq&quot;&gt;&lt;/a&gt;PAQ&lt;/h4&gt;
&lt;p&gt;PAQ于2002年由Matt Mahoney发明，是老版PPM的一个改进版。改进的方法是使用一项叫做context mixing的革命性技术。Context mixing是指智能地结合多个(PPM是单个模型)统计模型，来做出对下一个符号的更好预测，比其中的任何一个模型都要好。PAQ是最有前途的技术之一，它有很好的压缩比，开发也很活跃。自它出现后算起，有超过20个衍生算法被发明出来，其中一些在压缩比方面屡破记录。PAQ的最大缺点是速度，这源于使用了多个统计模型来获得更好的压缩比。然而随着硬件速度的不断提升，它或许会是未来的标准。PAQ在应用方面进步缓慢，一个叫PAQ8O的衍生算法可以在一个叫PeaZip的Windows程序里面找到，PAQ8O支持64-bit，速度也有较大提升。其它的PAQ格式仅仅用于命令行程序。&lt;/p&gt;


&lt;!-- div id=&quot;ad1&quot;&gt;
&lt;/div --&gt;


	


	

</description>
        <pubDate>Wed, 24 Sep 2014 00:00:00 +0800</pubDate>
        <link>http://iftti.com/posts/2014/2014-09-24-77247-7dc4d49c7.html</link>
        <guid isPermaLink="true">http://iftti.com/posts/2014/2014-09-24-77247-7dc4d49c7.html</guid>
        
        
        <category>jobbole</category>
        
      </item>
    
      <item>
        <title>常见面试之机器学习算法思想简单梳理</title>
        <description>

					
		
&lt;!-- div id=&quot;ad1&quot;&gt;
&lt;script type=&quot;text/javascript&quot;&gt;
google_ad_client = &quot;ca-pub-7056282119617872&quot;;
google_ad_slot = &quot;6645040531&quot;;
google_ad_width = 300;
google_ad_height = 250;
&lt;/script&gt;
&lt;script type=&quot;text/javascript&quot;
src=&quot;http://pagead2.googlesyndication.com/pagead/show_ads.js&quot;&gt;
&lt;/script&gt;
&lt;/div&gt;
&lt;br/ --&gt;

&lt;p&gt;&lt;strong&gt;前言：&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;找工作时（IT行业），除了常见的软件开发以外，机器学习岗位也可以当作是一个选择，不少计算机方向的研究生都会接触这个，如果你的研究方向是机器学习/数据挖掘之类，且又对其非常感兴趣的话，可以考虑考虑该岗位，毕竟在机器智能没达到人类水平之前，机器学习可以作为一种重要手段，而随着科技的不断发展，相信这方面的人才需求也会越来越大。&lt;/p&gt;
&lt;p&gt;纵观IT行业的招聘岗位，机器学习之类的岗位还是挺少的，国内大点的公司里百度，阿里，腾讯，网易，搜狐，华为（华为的岗位基本都是随机分配，机器学习等岗位基本面向的是博士）等会有相关职位，另外一些国内的中小型企业和外企也会招一小部分。当然了，其中大部分还是百度北京要人最多，上百人。阿里的算法岗位很大一部分也是搞机器学习相关的。另外本人有幸签约了网易杭州研究院的深度学习算法岗位，打算从事机器学习领域至少5年。非常感谢小易收留了我！&lt;/p&gt;
&lt;p&gt;下面是本人在找机器学习岗位工作时，总结的常见机器学习算法（主要是一些常规分类器）大概流程和主要思想，希望对大家找机器学习岗位时有点帮助。实际上在面试过程中，懂这些算法的基本思想和大概流程是远远不够的，那些面试官往往问的都是一些公司内部业务中的课题，往往要求你不仅要懂得这些算法的理论过程，而且要非常熟悉怎样使用它，什么场合用它，算法的优缺点，以及调参经验等等。说白了，就是既要会点理论，也要会点应用，既要有点深度，也要有点广度，否则运气不好的话很容易就被刷掉，因为每个面试官爱好不同。&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;朴素贝叶斯：&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;有以下几个地方需要注意：&lt;/p&gt;
&lt;p&gt;1. 如果给出的特征向量长度可能不同，这是需要归一化为通长度的向量（这里以文本分类为例），比如说是句子单词的话，则长度为整个词汇量的长度，对应位置是该单词出现的次数。&lt;/p&gt;
&lt;p&gt;2. 计算公式如下：&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/b8083cb5e9f6fc73bd4c31350d0463ab.jpg&quot; width=&quot;214&quot; height=&quot;69&quot;&gt;&lt;/p&gt;
&lt;p align=&quot;left&quot;&gt;其中一项条件概率可以通过朴素贝叶斯条件独立展开。要注意一点就是&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/4c56b13fde8c16b7ddf93202cb6f1446.jpg&quot;&gt;的计算方法，而由朴素贝叶斯的前提假设可知，&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/21b0b4b9470a5a5c93616f233e3d8b60.jpg&quot;&gt;=&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/a2275101ca10e4966b0cab8a292eb00d.jpg&quot;&gt;，因此一般有两种，一种是在类别为ci的那些样本集中，找到wj出现次数的总和，然后除以该样本的总和；第二种方法是类别为ci的那些样本集中，找到wj出现次数的总和，然后除以该样本中所有特征出现次数的总和。&lt;/p&gt;
&lt;p&gt;3. 如果&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/4c56b13fde8c16b7ddf93202cb6f1446.jpg&quot;&gt;中的某一项为0，则其联合概率的乘积也可能为0，即2中公式的分子为0，为了避免这种现象出现，一般情况下会将这一项初始化为1，当然为了保证概率相等，分母应对应初始化为2（这里因为是2类，所以加2，如果是k类就需要加k，术语上叫做laplace光滑, 分母加k的原因是使之满足全概率公式）。&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;朴素贝叶斯的优点：&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;对小规模的数据表现很好，适合多分类任务，适合增量式训练。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;缺点&lt;/em&gt;：&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;对输入数据的表达形式很敏感。&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;决策树：&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;决策树中很重要的一点就是选择一个属性进行分枝，因此要注意一下信息增益的计算公式，并深入理解它。&lt;/p&gt;
&lt;p&gt;信息熵的计算公式如下:&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/cd414a54f5e77b0eebd058c4dd8bd280.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;其中的n代表有n个分类类别（比如假设是2类问题，那么n=2）。分别计算这2类样本在总样本中出现的概率p1和p2，这样就可以计算出未选中属性分枝前的信息熵。&lt;/p&gt;
&lt;p&gt;现在选中一个属性xi用来进行分枝，此时分枝规则是：如果xi=vx的话，将样本分到树的一个分支；如果不相等则进入另一个分支。很显然，分支中的样本很有可能包括2个类别，分别计算这2个分支的熵H1和H2,计算出分枝后的总信息熵H’=p1*H1+p2*H2.，则此时的信息增益ΔH=H-H’。以信息增益为原则，把所有的属性都测试一边，选择一个使增益最大的属性作为本次分枝属性。&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;决策树的优点：&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;计算量简单，可解释性强，比较适合处理有缺失属性值的样本，能够处理不相关的特征；&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;缺点：&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;容易过拟合（后续出现了随机森林，减小了过拟合现象）；&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Logistic回归：&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Logistic是用来分类的，是一种线性分类器，需要注意的地方有：&lt;/p&gt;
&lt;p&gt;1. logistic函数表达式为：&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/20d645eb987a3c33094186fe1964ec86.jpg&quot; width=&quot;372&quot; height=&quot;104&quot;&gt;&lt;/p&gt;
&lt;p&gt;其导数形式为：&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/828a60323e7c280125c274e8fd34d095.jpg&quot; width=&quot;273&quot; height=&quot;154&quot;&gt;&lt;/p&gt;
&lt;p&gt;2. logsitc回归方法主要是用最大似然估计来学习的，所以单个样本的后验概率为：&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/e95dc64a6bceb2e7315fab4d63432469.jpg&quot; width=&quot;342&quot; height=&quot;47&quot;&gt;&lt;/p&gt;
&lt;p&gt;到整个样本的后验概率：&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/7c493852ff0a2dc402c7244891da04e3.jpg&quot; width=&quot;328&quot; height=&quot;141&quot;&gt;&lt;/p&gt;
&lt;p&gt;其中：&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/bf2054e251ff31be712fe68a21c3455c.jpg&quot; width=&quot;265&quot; height=&quot;64&quot;&gt;&lt;/p&gt;
&lt;p&gt;通过对数进一步化简为：&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/09c1f28f198bc9578dc8b1e7fd13cd31.jpg&quot; width=&quot;449&quot; height=&quot;89&quot;&gt;&lt;/p&gt;
&lt;p&gt;3. 其实它的loss function为-l(θ)，因此我们需使loss function最小，可采用梯度下降法得到。梯度下降法公式为:&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/eeb652d69cf54cb4f6ef590abc28cf61.jpg&quot; width=&quot;475&quot; height=&quot;129&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/00d8651e45c69d31591c40994e238a24.jpg&quot; width=&quot;277&quot; height=&quot;44&quot;&gt;&lt;/p&gt;
&lt;p align=&quot;left&quot;&gt;&lt;em&gt;&lt;strong&gt;Logistic回归优点：&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p align=&quot;left&quot;&gt;1、实现简单；&lt;/p&gt;
&lt;p align=&quot;left&quot;&gt;2、分类时计算量非常小，速度很快，存储资源低；&lt;/p&gt;
&lt;p align=&quot;left&quot;&gt;&lt;em&gt;&lt;strong&gt;缺点：&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p align=&quot;left&quot;&gt;1、容易欠拟合，一般准确度不太高&lt;/p&gt;
&lt;p align=&quot;left&quot;&gt;2、只能处理两分类问题（在此基础上衍生出来的softmax可以用于多分类），且必须线性可分；&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;线性回归：&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;线性回归才是真正用于回归的，而不像logistic回归是用于分类，其基本思想是用梯度下降法对最小二乘法形式的误差函数进行优化，当然也可以用normal equation直接求得参数的解，结果为：&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/58c9408336b88e2ac396eb8f3f3b0b83.jpg&quot; width=&quot;175&quot; height=&quot;51&quot;&gt;&lt;/p&gt;
&lt;p&gt;而在LWLR（局部加权线性回归）中，参数的计算表达式为:&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/524d23873126bdc590eb2d0438736b7a.jpg&quot; width=&quot;246&quot; height=&quot;53&quot;&gt;&lt;/p&gt;
&lt;p&gt;因为此时优化的是：&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/c2d064a957f65a1fe4a7bb50f4b5db7e.jpg&quot; width=&quot;389&quot; height=&quot;78&quot;&gt;&lt;/p&gt;
&lt;p&gt;由此可见LWLR与LR不同，LWLR是一个非参数模型，因为每次进行回归计算都要遍历训练样本至少一次。&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;线性回归优点：&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;实现简单，计算简单；&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;缺点：&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;不能拟合非线性数据；&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;KNN&lt;/strong&gt;&lt;strong&gt;算法：&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;KNN即最近邻算法，其主要过程为：&lt;/p&gt;
&lt;p&gt;1. 计算训练样本和测试样本中每个样本点的距离（常见的距离度量有欧式距离，马氏距离等）；&lt;/p&gt;
&lt;p&gt;2. 对上面所有的距离值进行排序；&lt;/p&gt;
&lt;p&gt;3. 选前k个最小距离的样本；&lt;/p&gt;
&lt;p&gt;4. 根据这k个样本的标签进行投票，得到最后的分类类别；&lt;/p&gt;
&lt;p&gt;如何选择一个最佳的K值，这取决于数据。一般情况下，在分类时较大的K值能够减小噪声的影响。但会使类别之间的界限变得模糊。一个较好的K值可通过各种启发式技术来获取，比如，交叉验证。另外噪声和非相关性特征向量的存在会使K近邻算法的准确性减小。&lt;/p&gt;
&lt;p&gt;近邻算法具有较强的一致性结果。随着数据趋于无限，算法保证错误率不会超过贝叶斯算法错误率的两倍。对于一些好的K值，K近邻保证错误率不会超过贝叶斯理论误差率。&lt;/p&gt;
&lt;p&gt;注：马氏距离一定要先给出样本集的统计性质，比如均值向量，协方差矩阵等。关于马氏距离的介绍如下：&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/d3ff6709b6fffe23f47046a2dcdabd51.jpg&quot; width=&quot;978&quot; height=&quot;270&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;KNN算法的优点：&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;1. 思想简单，理论成熟，既可以用来做分类也可以用来做回归；&lt;/p&gt;
&lt;p&gt;2. 可用于非线性分类；&lt;/p&gt;
&lt;p&gt;3. 训练时间复杂度为O(n)；&lt;/p&gt;
&lt;p&gt;4. 准确度高，对数据没有假设，对outlier不敏感；&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;缺点：&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;1. 计算量大；&lt;/p&gt;
&lt;p&gt;2. 样本不平衡问题（即有些类别的样本数量很多，而其它样本的数量很少）；&lt;/p&gt;
&lt;p&gt;3. 需要大量的内存；&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;SVM&lt;/strong&gt;&lt;strong&gt;：&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;要学会如何使用libsvm以及一些参数的调节经验，另外需要理清楚svm算法的一些思路：&lt;/p&gt;
&lt;p&gt;1. svm中的最优分类面是对所有样本的几何裕量最大（为什么要选择最大间隔分类器，请从数学角度上说明？网易深度学习岗位面试过程中有被问到。答案就是几何间隔与样本的误分次数间存在关系：&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/0f747083f984f607d9d27dfa08c13d19.jpg&quot; width=&quot;113&quot; height=&quot;43&quot;&gt;，其中的分母就是样本到分类间隔距离，分子中的R是所有样本中的最长向量值），即：&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/8c8db41026303a2b79c57050f9395f3e.jpg&quot; width=&quot;364&quot; height=&quot;85&quot;&gt;&lt;/p&gt;
&lt;p&gt;经过一系列推导可得为优化下面原始目标：&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/3e9d73851204cb5036ebde77a8239384.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;2. 下面来看看拉格朗日理论：&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/39fb79050fd8a64b773662cfca03be14.jpg&quot; width=&quot;426&quot; height=&quot;157&quot;&gt;&lt;/p&gt;
&lt;p&gt;可以将1中的优化目标转换为拉格朗日的形式（通过各种对偶优化，KKD条件），最后目标函数为：&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/c1e40c1e06abc7c66da00e81b3bd7165.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;我们只需要最小化上述目标函数，其中的α为原始优化问题中的不等式约束拉格朗日系数。&lt;/p&gt;
&lt;p&gt;3. 对2中最后的式子分别w和b求导可得：&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/1cc9dd4cac61d47e71d65dab3e7faa8f.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/9b22731a6e39eac8bad74bd575e6b543.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;由上面第1式子可以知道，如果我们优化出了α，则直接可以求出w了，即模型的参数搞定。而上面第2个式子可以作为后续优化的一个约束条件。&lt;/p&gt;
&lt;p&gt;4. 对2中最后一个目标函数用对偶优化理论可以转换为优化下面的目标函数：&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/bc6f9efec5200fb36f5524048050d570.jpg&quot; width=&quot;394&quot; height=&quot;136&quot;&gt;&lt;/p&gt;
&lt;p&gt;而这个函数可以用常用的优化方法求得α，进而求得w和b。&lt;/p&gt;
&lt;p&gt;5. 按照道理，svm简单理论应该到此结束。不过还是要补充一点，即在预测时有：&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/f7acf5a6a5844a5149bce2dc0298a295.jpg&quot; width=&quot;291&quot; height=&quot;121&quot;&gt;&lt;/p&gt;
&lt;p&gt;那个尖括号我们可以用核函数代替，这也是svm经常和核函数扯在一起的原因。&lt;/p&gt;
&lt;p&gt;6. 最后是关于松弛变量的引入，因此原始的目标优化公式为：&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/5b769689705749a5db1621498a025cb2.jpg&quot; width=&quot;389&quot; height=&quot;114&quot;&gt;&lt;/p&gt;
&lt;p&gt;此时对应的对偶优化公式为：&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/16d9a4b55cc7e6cdb8f3db3712017c5f.jpg&quot; width=&quot;382&quot; height=&quot;132&quot;&gt;&lt;/p&gt;
&lt;p&gt;与前面的相比只是α多了个上界。&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;SVM算法优点：&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;可用于线性/非线性分类，也可以用于回归；&lt;/p&gt;
&lt;p&gt;低泛化误差；&lt;/p&gt;
&lt;p&gt;容易解释；&lt;/p&gt;
&lt;p&gt;计算复杂度较低；&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;缺点：&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;对参数和核函数的选择比较敏感；&lt;/p&gt;
&lt;p&gt;原始的SVM只比较擅长处理二分类问题；&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Boosting&lt;/strong&gt;&lt;strong&gt;：&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;主要以Adaboost为例，首先来看看Adaboost的流程图，如下：&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/4c1db6a8672bc7b05ca34e98759931ad.jpg&quot; width=&quot;366&quot; height=&quot;281&quot;&gt;&lt;/p&gt;
&lt;p&gt;从图中可以看到，在训练过程中我们需要训练出多个弱分类器（图中为3个），每个弱分类器是由不同权重的样本（图中为5个训练样本）训练得到（其中第一个弱分类器对应输入样本的权值是一样的），而每个弱分类器对最终分类结果的作用也不同，是通过加权平均输出的，权值见上图中三角形里面的数值。那么这些弱分类器和其对应的权值是怎样训练出来的呢？&lt;/p&gt;
&lt;p&gt;下面通过一个例子来简单说明。&lt;/p&gt;
&lt;p&gt;书中（machine learning in action）假设的是5个训练样本，每个训练样本的维度为2，在训练第一个分类器时5个样本的权重各为0.2. 注意这里样本的权值和最终训练的弱分类器组对应的权值α是不同的，样本的权重只在训练过程中用到，而α在训练过程和测试过程都有用到。&lt;/p&gt;
&lt;p&gt;现在假设弱分类器是带一个节点的简单决策树，该决策树会选择2个属性（假设只有2个属性）的一个，然后计算出这个属性中的最佳值用来分类。&lt;/p&gt;
&lt;p&gt;Adaboost的简单版本训练过程如下：&lt;/p&gt;
&lt;p&gt;1. 训练第一个分类器，样本的权值D为相同的均值。通过一个弱分类器，得到这5个样本（请对应书中的例子来看，依旧是machine learning in action）的分类预测标签。与给出的样本真实标签对比，就可能出现误差(即错误)。如果某个样本预测错误，则它对应的错误值为该样本的权重，如果分类正确，则错误值为0. 最后累加5个样本的错误率之和，记为ε。&lt;/p&gt;
&lt;p&gt;2. 通过ε来计算该弱分类器的权重α，公式如下：&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/9b96480798fabb5e957e460b2eccab4e.jpg&quot; width=&quot;165&quot; height=&quot;71&quot;&gt;&lt;/p&gt;
&lt;p&gt;3. 通过α来计算训练下一个弱分类器样本的权重D，如果对应样本分类正确，则减小该样本的权重，公式为：&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/e8bf668342cf3a21585a9baf15f8cb6c.jpg&quot; width=&quot;178&quot; height=&quot;79&quot;&gt;&lt;/p&gt;
&lt;p&gt;如果样本分类错误，则增加该样本的权重，公式为：&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/b496bd79b4a357b608628d041ec3172a.jpg&quot; width=&quot;166&quot; height=&quot;67&quot;&gt;&lt;/p&gt;
&lt;p&gt;4. 循环步骤1,2,3来继续训练多个分类器，只是其D值不同而已。&lt;/p&gt;
&lt;p&gt;测试过程如下：&lt;/p&gt;
&lt;p&gt;输入一个样本到训练好的每个弱分类中，则每个弱分类都对应一个输出标签，然后该标签乘以对应的α，最后求和得到值的符号即为预测标签值。&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;Boosting算法的优点：&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;低泛化误差；&lt;/p&gt;
&lt;p&gt;容易实现，分类准确率较高，没有太多参数可以调；&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;缺点：&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;对outlier比较敏感；&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;聚类：&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;根据聚类思想划分：&lt;/p&gt;
&lt;p&gt;1. 基于划分的聚类:&lt;/p&gt;
&lt;p&gt;K-means, k-medoids(每一个类别中找一个样本点来代表),CLARANS.&lt;/p&gt;
&lt;p&gt;k-means是使下面的表达式值最小：&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/07649c2bc46e9feb1e30be7962d5bb30.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;k-means算法的优点：&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;（1）k-means算法是解决聚类问题的一种经典算法，算法简单、快速。&lt;/p&gt;
&lt;p&gt;（2）对处理大数据集，该算法是相对可伸缩的和高效率的，因为它的复杂度大约是O(nkt)，其中n是所有对象的数目，k是簇的数目,t是迭代的次数。通常k&amp;lt;&amp;lt;n。这个算法通常局部收敛。&lt;/p&gt;
&lt;p&gt;（3）算法尝试找出使平方误差函数值最小的k个划分。当簇是密集的、球状或团状的，且簇与簇之间区别明显时，聚类效果较好。&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;缺点：&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;（1）k-平均方法只有在簇的平均值被定义的情况下才能使用，且对有些分类属性的数据不适合。&lt;/p&gt;
&lt;p&gt;（2）要求用户必须事先给出要生成的簇的数目k。&lt;/p&gt;
&lt;p&gt;（3）对初值敏感，对于不同的初始值，可能会导致不同的聚类结果。&lt;/p&gt;
&lt;p&gt;（4）不适合于发现非凸面形状的簇，或者大小差别很大的簇。&lt;/p&gt;
&lt;p&gt;（5）对于”噪声”和孤立点数据敏感，少量的该类数据能够对平均值产生极大影响。&lt;/p&gt;
&lt;p&gt;2. 基于层次的聚类：&lt;/p&gt;
&lt;p&gt;自底向上的凝聚方法，比如AGNES。&lt;/p&gt;
&lt;p&gt;自上向下的分裂方法，比如DIANA。&lt;/p&gt;
&lt;p&gt;3. 基于密度的聚类：&lt;/p&gt;
&lt;p&gt;DBSACN,OPTICS,BIRCH(CF-Tree),CURE.&lt;/p&gt;
&lt;p&gt;4. 基于网格的方法：&lt;/p&gt;
&lt;p&gt;STING, WaveCluster.&lt;/p&gt;
&lt;p&gt;5. 基于模型的聚类：&lt;/p&gt;
&lt;p&gt;EM,SOM,COBWEB.&lt;/p&gt;
&lt;p&gt;以上这些算法的简介可参考&lt;a href=&quot;http://baike.baidu.com/view/31801.htm&quot;&gt;聚类（百度百科）。&lt;/a&gt;&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;推荐系统：&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;推荐系统的实现主要分为两个方面：基于内容的实现和协同滤波的实现。&lt;/p&gt;
&lt;p&gt;基于内容的实现：&lt;/p&gt;
&lt;p&gt;不同人对不同电影的评分这个例子，可以看做是一个普通的回归问题，因此每部电影都需要提前提取出一个特征向量(即x值)，然后针对每个用户建模，即每个用户打的分值作为y值，利用这些已有的分值y和电影特征值x就可以训练回归模型了(最常见的就是线性回归)。这样就可以预测那些用户没有评分的电影的分数。（值得注意的是需对每个用户都建立他自己的回归模型）&lt;/p&gt;
&lt;p&gt;从另一个角度来看，也可以是先给定每个用户对某种电影的喜好程度（即权值），然后学出每部电影的特征，最后采用回归来预测那些没有被评分的电影。&lt;/p&gt;
&lt;p&gt;当然还可以是同时优化得到每个用户对不同类型电影的热爱程度以及每部电影的特征。具体可以参考Ng在coursera上的ml教程：&lt;a href=&quot;https://www.coursera.org/course/ml&quot;&gt;https://www.coursera.org/course/ml&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;基于协同滤波的实现：&lt;/p&gt;
&lt;p&gt;协同滤波（CF）可以看做是一个分类问题，也可以看做是矩阵分解问题。协同滤波主要是基于每个人自己的喜好都类似这一特征，它不依赖于个人的基本信息。比如刚刚那个电影评分的例子中，预测那些没有被评分的电影的分数只依赖于已经打分的那些分数，并不需要去学习那些电影的特征。&lt;/p&gt;
&lt;p&gt;SVD将矩阵分解为三个矩阵的乘积，公式如下所示：&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/70b15f485732721065229e4fbd513afc.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;中间的矩阵sigma为对角矩阵，对角元素的值为Data矩阵的奇异值(注意奇异值和特征值是不同的)，且已经从大到小排列好了。即使去掉特征值小的那些特征，依然可以很好的重构出原始矩阵。如下图所示：&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/98d03f9faad8dde7e137ec02b4c5979b.jpg&quot; width=&quot;431&quot; height=&quot;173&quot;&gt;&lt;/p&gt;
&lt;p&gt;其中更深的颜色代表去掉小特征值重构时的三个矩阵。&lt;/p&gt;
&lt;p&gt;果m代表商品的个数，n代表用户的个数，则U矩阵的每一行代表商品的属性，现在通过降维U矩阵（取深色部分）后，每一个商品的属性可以用更低的维度表示（假设为k维）。这样当新来一个用户的商品推荐向量X，则可以根据公式X’*U1*inv(S1)得到一个k维的向量，然后在V’中寻找最相似的那一个用户（相似度测量可用余弦公式等），根据这个用户的评分来推荐（主要是推荐新用户未打分的那些商品）。具体例子可以参考网页：&lt;a href=&quot;http://blog.csdn.net/wuyanyi/article/details/7964883&quot;&gt;SVD在推荐系统中的应用&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;另外关于SVD分解后每个矩阵的实际含义可以参考google吴军的《数学之美》一书（不过个人感觉吴军解释UV两个矩阵时好像弄反了，不知道大家怎样认为）。或者参考machine learning in action其中的svd章节。&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;pLSA:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;pLSA由LSA发展过来，而早期LSA的实现主要是通过SVD分解。pLSA的模型图如下：&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/5e66a98d735208a96823ac94a18b2049.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;公式中的意义如下：&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/f01167e3a30550ef633892b3a625c039.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;具体可以参考&lt;a href=&quot;http://bcmi.sjtu.edu.cn/ds/download.html&quot;&gt;2010龙星计划：机器学习中对应的主题模型那一讲&lt;/a&gt;&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;LDA&lt;/strong&gt;&lt;strong&gt;：&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;主题模型，概率图如下：&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/3b068ec4f6d0abbb1b29c4ac03c7201e.jpg&quot; width=&quot;257&quot; height=&quot;208&quot;&gt;&lt;/p&gt;
&lt;p&gt;和pLSA不同的是LDA中假设了很多先验分布，且一般参数的先验分布都假设为Dirichlet分布，其原因是共轭分布时先验概率和后验概率的形式相同。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;GDBT&lt;/strong&gt;&lt;strong&gt;：&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;GBDT(Gradient Boosting Decision Tree) 又叫 MART（Multiple Additive Regression Tree)，好像在阿里内部用得比较多（所以阿里算法岗位面试时可能会问到），它是一种迭代的决策树算法，该算法由多棵决策树组成，所有树的输出结果累加起来就是最终答案。它在被提出之初就和SVM一起被认为是泛化能力（generalization)较强的算法。近些年更因为被用于搜索排序的机器学习模型而引起大家关注。&lt;/p&gt;
&lt;p&gt;GBDT是回归树，不是分类树。其核心就在于，每一棵树是从之前所有树的残差中来学习的。为了防止过拟合，和Adaboosting一样，也加入了boosting这一项。&lt;/p&gt;
&lt;p&gt;关于GDBT的介绍可以可以参考：&lt;a href=&quot;http://hi.baidu.com/hehehehello/item/96cc42e45c16e7265a2d64ee&quot;&gt;GBDT（MART） 迭代决策树入门教程 | 简介&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Regularization:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;作用是（网易电话面试时有问到）：&lt;/p&gt;
&lt;p&gt;1. 数值上更容易求解；&lt;/p&gt;
&lt;p&gt;2. 特征数目太大时更稳定；&lt;/p&gt;
&lt;p&gt;3. 控制模型的复杂度，光滑性。复杂性越小且越光滑的目标函数泛化能力越强。而加入规则项能使目标函数复杂度减小，且更光滑。&lt;/p&gt;
&lt;p&gt;4. 减小参数空间；参数空间越小，复杂度越低。&lt;/p&gt;
&lt;p&gt;5. 系数越小，模型越简单，而模型越简单则泛化能力越强（Ng宏观上给出的解释）。&lt;/p&gt;
&lt;p&gt;6. 可以看出是权值的高斯先验。&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;异常检测：&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;可以估计样本的密度函数，对于新样本直接计算其密度，如果密度值小于某一阈值，则表示该样本异常。而密度函数一般采用多维的高斯分布。如果样本有n维，则每一维的特征都可以看作是符合高斯分布的，即使这些特征可视化出来不太符合高斯分布，也可以对该特征进行数学转换让其看起来像高斯分布，比如说x=log(x+c), x=x^(1/c)等。异常检测的算法流程如下：&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/8b8d0032d9f883a5caa2b93c4993c9f1.jpg&quot; width=&quot;482&quot; height=&quot;290&quot;&gt;&lt;/p&gt;
&lt;p&gt;其中的ε也是通过交叉验证得到的，也就是说在进行异常检测时，前面的p(x)的学习是用的无监督，后面的参数ε学习是用的有监督。那么为什么不全部使用普通有监督的方法来学习呢（即把它看做是一个普通的二分类问题）？主要是因为在异常检测中，异常的样本数量非常少而正常样本数量非常多，因此不足以学习到好的异常行为模型的参数，因为后面新来的异常样本可能完全是与训练样本中的模式不同。&lt;/p&gt;
&lt;p align=&quot;left&quot;&gt;另外，上面是将特征的每一维看成是相互独立的高斯分布，其实这样的近似并不是最好的，但是它的计算量较小，因此也常被使用。更好的方法应该是将特征拟合成多维高斯分布，这时有特征之间的相关性，但随之计算量会变复杂，且样本的协方差矩阵还可能出现不可逆的情况（主要在样本数比特征数小，或者样本特征维数之间有线性关系时）。&lt;/p&gt;
&lt;p align=&quot;left&quot;&gt;上面的内容可以参考Ng的&lt;a href=&quot;https://www.coursera.org/course/ml&quot;&gt;https://www.coursera.org/course/ml&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;EM&lt;/strong&gt;&lt;strong&gt;算法：&lt;/strong&gt;&lt;/p&gt;
&lt;p align=&quot;left&quot;&gt;有时候因为样本的产生和隐含变量有关（隐含变量是不能观察的），而求模型的参数时一般采用最大似然估计，由于含有了隐含变量，所以对似然函数参数求导是求不出来的，这时可以采用EM算法来求模型的参数的（对应模型参数个数可能有多个），EM算法一般分为2步：&lt;/p&gt;
&lt;p align=&quot;left&quot;&gt;E步：选取一组参数，求出在该参数下隐含变量的条件概率值；&lt;/p&gt;
&lt;p align=&quot;left&quot;&gt;M步：结合E步求出的隐含变量条件概率，求出似然函数下界函数（本质上是某个期望函数）的最大值。&lt;/p&gt;
&lt;p align=&quot;left&quot;&gt;重复上面2步直至收敛。&lt;/p&gt;
&lt;p align=&quot;left&quot;&gt;公式如下所示：&lt;/p&gt;
&lt;p align=&quot;left&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/cf4fd695f67722fb350963498da9b2de.jpg&quot; width=&quot;400&quot; height=&quot;150&quot;&gt;&lt;/p&gt;
&lt;p align=&quot;left&quot;&gt;M步公式中下界函数的推导过程：&lt;/p&gt;
&lt;p align=&quot;left&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/28ea30f56c7f1d9fbb6baa305cb8d99f.jpg&quot; width=&quot;434&quot; height=&quot;145&quot;&gt;&lt;/p&gt;
&lt;p align=&quot;left&quot;&gt;EM算法一个常见的例子就是GMM模型，每个样本都有可能由k个高斯产生，只不过由每个高斯产生的概率不同而已，因此每个样本都有对应的高斯分布（k个中的某一个），此时的隐含变量就是每个样本对应的某个高斯分布。&lt;/p&gt;
&lt;p align=&quot;left&quot;&gt;GMM的E步公式如下（计算每个样本对应每个高斯的概率）：&lt;/p&gt;
&lt;p align=&quot;left&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/213bce677dafac18f097a52b25caac73.jpg&quot; width=&quot;381&quot; height=&quot;71&quot;&gt;&lt;/p&gt;
&lt;p align=&quot;left&quot;&gt;更具体的计算公式为：&lt;/p&gt;
&lt;p align=&quot;left&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/73377ff094f50d0b5e7ca768fa68849d.jpg&quot; width=&quot;509&quot; height=&quot;56&quot;&gt;&lt;/p&gt;
&lt;p align=&quot;left&quot;&gt;M步公式如下（计算每个高斯的比重，均值，方差这3个参数）：&lt;/p&gt;
&lt;p align=&quot;left&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/0f680b38ad728be00844b8957f106e3a.jpg&quot; width=&quot;363&quot; height=&quot;192&quot;&gt;&lt;/p&gt;
&lt;p align=&quot;left&quot;&gt;关于EM算法可以参考&lt;a href=&quot;http://cs229.stanford.edu/&quot;&gt;Ng的cs229课程资料&lt;/a&gt;或者网易公开课：&lt;a href=&quot;http://v.163.com/special/opencourse/machinelearning.html&quot;&gt;斯坦福大学公开课 ：机器学习课程&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Apriori:&lt;/strong&gt;&lt;/p&gt;
&lt;p align=&quot;left&quot;&gt;Apriori是关联分析中比较早的一种方法，主要用来挖掘那些频繁项集合。其思想是：&lt;/p&gt;
&lt;p align=&quot;left&quot;&gt;1. 如果一个项目集合不是频繁集合，那么任何包含它的项目集合也一定不是频繁集合；&lt;/p&gt;
&lt;p align=&quot;left&quot;&gt;2. 如果一个项目集合是频繁集合，那么它的任何非空子集也是频繁集合；&lt;/p&gt;
&lt;p align=&quot;left&quot;&gt;Aprioir需要扫描项目表多遍，从一个项目开始扫描，舍去掉那些不是频繁的项目，得到的集合称为L，然后对L中的每个元素进行自组合，生成比上次扫描多一个项目的集合，该集合称为C，接着又扫描去掉那些非频繁的项目，重复…&lt;/p&gt;
&lt;p align=&quot;left&quot;&gt;看下面这个例子：&lt;/p&gt;
&lt;p align=&quot;left&quot;&gt;元素项目表格：&lt;/p&gt;
&lt;p align=&quot;left&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/7d14e78300024a017d3c4b0f8c888568.jpg&quot; width=&quot;448&quot; height=&quot;90&quot;&gt;&lt;/p&gt;
&lt;p align=&quot;left&quot;&gt;如果每个步骤不去掉非频繁项目集，则其扫描过程的树形结构如下：&lt;/p&gt;
&lt;p align=&quot;left&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/967c4b6d4992d5a3734b8bc523e50340.jpg&quot; width=&quot;252&quot; height=&quot;227&quot;&gt;&lt;/p&gt;
&lt;p align=&quot;left&quot;&gt;在其中某个过程中，可能出现非频繁的项目集，将其去掉（用阴影表示）为：&lt;/p&gt;
&lt;p align=&quot;left&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/ef1d35aacb6051a6d976398982a508d6.jpg&quot; width=&quot;261&quot; height=&quot;248&quot;&gt;&lt;/p&gt;
&lt;p align=&quot;left&quot;&gt;上面的内容主要参考的是machine learning in action这本书。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;FP Growth:&lt;/strong&gt;&lt;/p&gt;
&lt;p align=&quot;left&quot;&gt;FP Growth是一种比Apriori更高效的频繁项挖掘方法，它只需要扫描项目表2次。其中第1次扫描获得当个项目的频率，去掉不符合支持度要求的项，并对剩下的项排序。第2遍扫描是建立一颗FP-Tree(frequent-patten tree)。&lt;/p&gt;
&lt;p&gt;接下来的工作就是在FP-Tree上进行挖掘。&lt;/p&gt;
&lt;p align=&quot;left&quot;&gt;比如说有下表：&lt;/p&gt;
&lt;p align=&quot;left&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/d6380ab9e623777db1f74efd2fa86ca8.jpg&quot; width=&quot;339&quot; height=&quot;151&quot;&gt;&lt;/p&gt;
&lt;p align=&quot;left&quot;&gt;它所对应的FP_Tree如下：&lt;/p&gt;
&lt;p align=&quot;left&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/8fb9c89aa17e70bdc67c07d907718c9e.jpg&quot; width=&quot;405&quot; height=&quot;214&quot;&gt;&lt;/p&gt;
&lt;p align=&quot;left&quot;&gt;然后从频率最小的单项P开始，找出P的条件模式基，用构造FP_Tree同样的方法来构造P的条件模式基的FP_Tree，在这棵树上找出包含P的频繁项集。&lt;/p&gt;
&lt;p align=&quot;left&quot;&gt;依次从m,b,a,c,f的条件模式基上挖掘频繁项集，有些项需要递归的去挖掘，比较麻烦，比如m节点，具体的过程可以参考博客：&lt;a href=&quot;http://blog.sina.com.cn/s/blog_68ffc7a40100uebg.html&quot;&gt;Frequent Pattern 挖掘之二(FP Growth算法)&lt;/a&gt;，里面讲得很详细。&lt;/p&gt;
&lt;p align=&quot;left&quot;&gt;
&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;参考资料：&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Harrington, P. (2012). Machine Learning in Action, Manning Publications Co.&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://zh.wikipedia.org/wiki/%E6%9C%80%E8%BF%91%E9%84%B0%E5%B1%85%E6%B3%95&quot;&gt;最近邻算法（维基百科）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://zh.wikipedia.org/wiki/%E9%A9%AC%E6%B0%8F%E8%B7%9D%E7%A6%BB&quot;&gt;马氏距离（维基百科）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://baike.baidu.com/view/31801.htm&quot;&gt;聚类（百度百科）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://www.coursera.org/course/ml&quot;&gt;https://www.coursera.org/course/ml&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://blog.csdn.net/wuyanyi/article/details/7964883&quot;&gt;SVD在推荐系统中的应用&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;吴军 and 谷歌 (2012).数学之美, 人民邮电出版社.&lt;/li&gt;
&lt;li&gt;
&lt;a href=&quot;http://bcmi.sjtu.edu.cn/ds/download.html&quot;&gt;2010龙星计划：机器学习&lt;/a&gt;对应的视频教程：&lt;a href=&quot;http://pan.baidu.com/share/link?shareid=3053312914&amp;amp;uk=2620399451&quot;&gt; 2010龙星计划机器学习视频教程&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://hi.baidu.com/hehehehello/item/96cc42e45c16e7265a2d64ee&quot;&gt;GBDT（MART） 迭代决策树入门教程 | 简介&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://cs229.stanford.edu/&quot;&gt;Ng的cs229课程资料&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://v.163.com/special/opencourse/machinelearning.html&quot;&gt;斯坦福大学公开课 ：机器学习课程&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://blog.sina.com.cn/s/blog_68ffc7a40100uebg.html&quot;&gt;Frequent Pattern 挖掘之二(FP Growth算法)&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;


&lt;!-- div id=&quot;ad1&quot;&gt;
&lt;/div --&gt;


	


	

</description>
        <pubDate>Wed, 24 Sep 2014 00:00:00 +0800</pubDate>
        <link>http://iftti.com/posts/2014/2014-09-24-74438-8e0151f4b.html</link>
        <guid isPermaLink="true">http://iftti.com/posts/2014/2014-09-24-74438-8e0151f4b.html</guid>
        
        
        <category>jobbole</category>
        
      </item>
    
      <item>
        <title>Kibana 认证鉴权方案</title>
        <description>

  
  &lt;div style=&quot;background-color: #FFF;&quot;&gt;
    &lt;p&gt;Kibana 作为一个纯 JS 项目，一直都没有提供完整的权限控制方面的功能。只是附带了一个 &lt;code&gt;nginx.conf&lt;/code&gt; 做基本的 Basic Auth。社区另外有在 nodejs 上实现的方案，则使用了 CAS 方式做认证。&lt;/p&gt;
&lt;p&gt;不过我对这两种方案都不太满意。&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;认证方式太单一，适应性不强；&lt;/li&gt;
  &lt;li&gt;权限隔离不明确，只是通过修改 &lt;code&gt;kibana-int&lt;/code&gt; 成 &lt;code&gt;kiban-int-user&lt;/code&gt; 来区分不同用户的 dashboard，并不能限制用户对 ES 索引的访问。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;加上 nodejs 我也不熟，最终在多番考虑后，决定抽一个晚上自己写一版。&lt;/p&gt;
&lt;p&gt;最终代码见 &lt;a href=&quot;https://github.com/chenryn/kibana&quot;&gt;https://github.com/chenryn/kibana&lt;/a&gt;。&lt;/p&gt;
&lt;h2 id=&quot;section&quot;&gt;原理和实现&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;全站代理和虚拟响应&lt;/p&gt;
    &lt;p&gt;这里不单通过 config.js 限定了 kibana 默认连接的 Elasticsearch 服务器地址和端口，还拦截伪造了 &lt;code&gt;/_nodes&lt;/code&gt; 请求的 JSON 响应体。伪造的响应中也只包含自己这个带认证的 web 服务器地址和端口。&lt;/p&gt;
    &lt;p&gt;&lt;em&gt;这么做是因为我的 kibana 版本使用的 elasticjs 库比官方新增了 sniff 功能，默认会自动轮训所有 nodes 发送请求。&lt;/em&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;新增 &lt;code&gt;kibana-auth&lt;/code&gt; 鉴权索引&lt;/p&gt;
    &lt;p&gt;在通常的 &lt;code&gt;kibana-int-user&lt;/code&gt; 区分 dashboard 基础上，我新增加 &lt;code&gt;kibana-auth&lt;/code&gt; 索引，专门记录每个用户可以访问的 ES 集群地址和索引前缀。请求会固定代理到指定的 ES 集群上，并且确认是被允许访问的索引。&lt;/p&gt;
    &lt;p&gt;这样，多个用户通过一个 kibana auth 服务器网址，可以访问多个不同的 ES 集群后端。而同一个 ES 集群后端的索引，也不用担心被其他人访问到。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://metacpan.org/pod/Authen::Simple&quot;&gt;Authen::Simple&lt;/a&gt; 认证框架&lt;/p&gt;
    &lt;p&gt;这是 Perl 一个认证框架，支持十多种不同的认证方式。项目里默认采用最简单的 htpasswd 文件记录方式，实际我线上是使用了 LDAP 方式，都没问题。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&quot;section-1&quot;&gt;部署&lt;/h2&gt;
&lt;p&gt;方案采用了 Mojolicious 框架开发，代码少不说，最关键的是 Mojolicious 无额外的 CPAN 模块依赖，这对于不了解 Perl 但是又有 Kibana 权限控制需求的人来说，大大减少了部署方面的麻烦。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;
curl http://xrl.us/cpanm -o /usr/local/bin/cpanm
chmod +x /usr/local/bin/cpanm
cpanm Mojolicious Authen::Simple::Passwd
&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;三行命令，就可以完成整个项目的安装需求了。然后运行目录下的:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;
hypnotoad script/kbnauth
&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;就可以通过 80 端口访问这个带有权限控制的 kibana 了。&lt;/p&gt;
&lt;h2 id=&quot;section-2&quot;&gt;权限赋值&lt;/h2&gt;
&lt;p&gt;因为 &lt;code&gt;kibana-auth&lt;/code&gt; 结构很简单，kibana 一般又都是内部使用，所以暂时还没做权限控制的管理页面。直接通过命令行方式即可赋权：&lt;/p&gt;
&lt;p&gt;&lt;code&gt;
curl  -XPOST http://127.0.0.1:9200/kibana-auth/indices/sri -d &#39;{
  &quot;prefix&quot;:[&quot;logstash-sri&quot;,&quot;logstash-ops&quot;],
  &quot;server&quot;:&quot;192.168.0.2:9200&quot;
}&#39;
&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;这样，sri 用户，就只能访问 192.168.0.2 集群上的 logstash-sri 或 logstash-ops 开头的日期型索引(即后面可以-YYYY, -YYYY.MM, -YYYY.MM.dd 三种格式)了。&lt;/p&gt;
&lt;h2 id=&quot;section-3&quot;&gt;下一步&lt;/h2&gt;
&lt;p&gt;考虑到新方案下各用户都有自己的 &lt;code&gt;kibana-int-user&lt;/code&gt; 索引，已经用着官方 kibana 的用户大批量的 dashboard 有迁移成本，找个时间可能做一个迁移脚本辅助这个事情。&lt;/p&gt;
&lt;p&gt;开发完成后，得到了 &lt;a href=&quot;http://weibo.com/u/1808998161&quot;&gt;@高伟&lt;/a&gt; 童鞋的主动尝试和各种 bug 反馈支持，在此表示感谢~也希望我这个方案能帮到更多 kibana 用户。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;注：我的 kibana 仓库除了新增的这个 kbnauth 代理认证鉴权功能外，本身在 kibana 分析统计功能上也有一些改进，这方面已经得到多个小伙伴的试用和好评，自认在官方 Kibana v4 版本出来之前，应该会是最好用的版本。欢迎大家下载使用！&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;新增功能包括：&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;仿 stats 的百分比统计面板(利用 PercentileAggr 接口)&lt;/li&gt;
  &lt;li&gt;仿 terms 的区间比面板(利用 RangeFacets 接口)&lt;/li&gt;
  &lt;li&gt;给 bettermap 增强的高德地图支持(利用 leaflet provider 扩展)&lt;/li&gt;
  &lt;li&gt;给 map 增强的中国地图支持(利用 jvectormap 文件)&lt;/li&gt;
  &lt;li&gt;给 map 增强的 &lt;code&gt;term_stats&lt;/code&gt; 数据显示(利用 TermStatsFacets 接口)&lt;/li&gt;
  &lt;li&gt;给 query 增强的请求生成器(利用 getMapping/getFieldMapping 接口和 jQuery.multiSelect 扩展)&lt;/li&gt;
  &lt;li&gt;仿 terms 的 statisticstrend 面板(利用 TermStatsFacets 接口)&lt;/li&gt;
  &lt;li&gt;仿 histogram 增强的 multifieldhistogram 面板(可以给不同query定制不同的panel setting，比如设置某个抽样数据 * 1000 倍和另一个全量数据做对比)&lt;/li&gt;
  &lt;li&gt;仿 histogram 的 valuehistogram 面板(去除了 histogram 面板的 X 轴时间类型数据限制，可以用于做数据概率分布分析)&lt;/li&gt;
  &lt;li&gt;给 histogram 增强的 threshold 变色功能(利用了 &lt;code&gt;jquery.flot.threshold&lt;/code&gt; 扩展)&lt;/li&gt;
  &lt;li&gt;单个面板自己的刷新按钮(避免调试的时候全页面刷新的麻烦)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;效果截图同样在 &lt;a href=&quot;https://github.com/chenryn/kibana/blob/master/README.md&quot;&gt;README&lt;/a&gt; 里贴出。欢迎试用和反馈！&lt;/p&gt;
    &lt;hr&gt;
    
    &lt;hr&gt;
  &lt;!-- UY BEGIN --&gt;


&lt;!-- UY END --&gt;
  &lt;/div&gt;

</description>
        <pubDate>Tue, 23 Sep 2014 00:00:00 +0800</pubDate>
        <link>http://iftti.com/posts/2014/2014-09-23-kibana-auth-088ca6c86.html</link>
        <guid isPermaLink="true">http://iftti.com/posts/2014/2014-09-23-kibana-auth-088ca6c86.html</guid>
        
        
        <category>chenlinux</category>
        
      </item>
    
      <item>
        <title>深度学习之浅见</title>
        <description>

					
		
&lt;!-- div id=&quot;ad1&quot;&gt;
&lt;script type=&quot;text/javascript&quot;&gt;
google_ad_client = &quot;ca-pub-7056282119617872&quot;;
google_ad_slot = &quot;6645040531&quot;;
google_ad_width = 300;
google_ad_height = 250;
&lt;/script&gt;
&lt;script type=&quot;text/javascript&quot;
src=&quot;http://pagead2.googlesyndication.com/pagead/show_ads.js&quot;&gt;
&lt;/script&gt;
&lt;/div&gt;
&lt;br/ --&gt;

&lt;blockquote&gt;&lt;/blockquote&gt;
&lt;h2&gt;1.深度学习&lt;/h2&gt;
&lt;p&gt;深度学习，即Deep Learning，是一种学习算法（Learning algorithm）。学习算法这个很好理解，那么Deep指的是什么呢？这里的Deep是针对算法的结构而言的。&lt;/p&gt;
&lt;p&gt;譬如，SVMs及Logistic Regression被称为浅层学习，是因为其隐藏节点只有一层。也就是说，这些算法只通过了一层“思考”就得到了答案，所以它们是浅显的。然而，深度学习的结构中有多个隐藏层，所以起了一个名字叫做Deep。博文&lt;a href=&quot;http://www.cnblogs.com/tornadomeet/archive/2013/03/25/2980357.html&quot;&gt;Deep learning：十六(deep networks)&lt;/a&gt;中对这种结构的优缺点做了说明。&lt;/p&gt;
&lt;h2&gt;2.波尔兹曼机&lt;/h2&gt;
&lt;p&gt;深度学习是基于Deep Belief Nets提出的算法。Hinton的论文中，是从受限波尔兹曼机（RBMs）中引出的Deep Belief Nets。波尔兹曼机的结构图如下（来自百度图片，这个图实际上不对，隐层节点和可见层节点两两之间都有连线）。&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://jbcdn2.b0.upaiyun.com/2014/09/6b5f32a21b26dd7e8fd3d7f6799a0391.jpg&quot; rel=&quot;lightbox[77531]&quot; title=&quot;深度学习之浅见&quot;&gt;&lt;img class=&quot;alignnone size-full wp-image-77540&quot; alt=&quot;16095348-07e527b706494ee592db811dc3c0f39d (1)&quot; src=&quot;/images/jobbole.com/15a00e9d22eb3e970bfdc1173fec5567.jpg&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;受限波尔兹曼机去掉了同层之间节点的连线，结构图如下（来自&lt;a href=&quot;http://blog.csdn.net/celerychen2009/article/details/8984316&quot;&gt;受限波尔兹曼机&lt;/a&gt;）。&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://jbcdn2.b0.upaiyun.com/2014/09/d82ee29e1c7b6471edda2852bd8f3d72.png&quot; rel=&quot;lightbox[77531]&quot; title=&quot;深度学习之浅见&quot;&gt;&lt;img class=&quot;alignnone size-full wp-image-77533&quot; alt=&quot;20130528144727586&quot; src=&quot;/images/jobbole.com/d4e49519061741aeb825cded122ed4a6.jpg&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;波尔兹曼机的相关知识具体可以参考Simon Haykin的《神经网络原理》第三版。受限波尔兹曼机有很多好的博文，譬如&lt;a href=&quot;http://blog.csdn.net/celerychen2009/article/details/8984316&quot;&gt;受限波尔兹曼机&lt;/a&gt;。波尔兹曼机的学习规则是使得学习之后的似然函数取值最大，然而作为无监督学习，似然函数从何而来？&lt;/p&gt;
&lt;p&gt;Hinton原来是学物理的，《神经网络原理》中把波尔兹曼机分在了植根于统计力学的随机机器这一章中。波尔兹曼机中定义了一个系统能量，并基于此定义了各个状态的概率。这些都是基于统计热力学得出的。在这一基础上，可以写出似然函数，之后利用梯度方法对网络进行迭代。&lt;/p&gt;
&lt;p&gt;受限波尔兹曼机的叠加得到了深度信度网络（deep belief nets），可参考&lt;a href=&quot;http://www.cnblogs.com/xiaowanyer/p/3701944.html&quot;&gt;深度学习概述：从感知机到深度网络&lt;/a&gt;或Hinton的《A Fast Learning Algorithm for Deep Belief Nets》&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://jbcdn2.b0.upaiyun.com/2014/09/5449d0034c8f3a8cbb8c75cf31eb4817.png&quot; rel=&quot;lightbox[77531]&quot; title=&quot;深度学习之浅见&quot;&gt;&lt;img class=&quot;alignnone size-full wp-image-77534&quot; alt=&quot;221402481852529&quot; src=&quot;/images/jobbole.com/d6c77e296a0509404c4170d7211b3a5c.jpg&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;深度信度网络的训练方法被称为是一种 fast, greedy algorithm。就如上图（来自&lt;a href=&quot;http://blog.csdn.net/celerychen2009/article/details/8984316&quot;&gt;受限波尔兹曼机&lt;/a&gt;）所表示，首先，我们认为x和h1组成RBM，对其进行训练；之后，固定此处的w，训练h1和h2，并以此类推。&lt;/p&gt;
&lt;h2&gt;3.稀疏自编码器&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;http://deeplearning.stanford.edu/wiki/index.php/UFLDL%E6%95%99%E7%A8%8B&quot;&gt;UFLDL教程&lt;/a&gt;中是利用稀疏自编码其来导出的深度信度网络，以及深度学习算法的。个人觉得这比玻尔兹曼机好理解多了。稀疏自编码器如下所示（来自UFLDL教程），它的理论建立在多层感知器的反向传播（back propagate）算法上。&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://jbcdn2.b0.upaiyun.com/2014/09/a245748f66f27a9363bfdc50a88da269.png&quot; rel=&quot;lightbox[77531]&quot; title=&quot;深度学习之浅见&quot;&gt;&lt;img class=&quot;alignnone size-full wp-image-77535&quot; alt=&quot;400px-Autoencoder636&quot; src=&quot;/images/jobbole.com/f09cea626cc7bf8c5c59382d2a142345.jpg&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;和2中类似，深度网络可以表示为如下形式（来自&lt;a href=&quot;http://blog.csdn.net/celerychen2009/article/details/8984316&quot;&gt;受限波尔兹曼机&lt;/a&gt;）。同样可以采用逐层训练的方法。&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://jbcdn2.b0.upaiyun.com/2014/09/d37ff976dbd30ed8afb3b5c3a4c9fc5b.png&quot; rel=&quot;lightbox[77531]&quot; title=&quot;深度学习之浅见&quot;&gt;&lt;img class=&quot;alignnone size-full wp-image-77536&quot; alt=&quot;052136291049785 (1)&quot; src=&quot;/images/jobbole.com/bc0e4933fb18d0243a64d4ee911589c8.jpg&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h2&gt;4.我们所理解的，是我们看到的世界&lt;/h2&gt;
&lt;p&gt;记得之前看过一本叫做《数学基础》的书，其中有一段印象深刻。“为何在集论可推出的无数种结构中，只有少数几种得到了重要的地位？为何有些概念更加有趣？形形色色的数学理论盛衰由人类的实践决定。”那么，我们是否可以认为，数学从一无所有开始，通过限定公理，建立起一套合适的数学理论来拟合我们看到的世界？而深度学习拟合的又是什么？为何深度学习能够在包括计算机视觉在内的众多领域取得成功？深度学习——机器学习——人工智能，拟合的是我们看到的世界。&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;稀疏自编码器的解释&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;   &lt;/strong&gt;根据信息不增定理，在网络传播过程中熵减少。而稀疏编码器中的隐藏元个数小于输入维度。因此，隐藏元是一个自编码的过程，在这个过程中尽可能的保留输入的信息。在深度网络中，每一层的训练亦是如此。&lt;/p&gt;
&lt;p&gt;研究表明，人脑的神经元连接也是多层的。由此发展出的神经网络（多层感知器）和由稀疏自编码器构成的深度网络具有相同的结构。一个不恰当的表示是，网络的训练过程是使得每一层保留的熵最大，这样整个网络的输出才有可能有尽可能大的熵（在限定的网络结构下）。那么，为何这一条规则可以在网络训练中取得成功？&lt;/p&gt;
&lt;p&gt;因为熵是人定义出来的，并不是真实存在，却帮助人更好的去理解这个世界。由稀疏自编码器导出的深度学习网络利用这一条规则对人脑进行了成功的模拟。如果这一条规则（即人脑的对信息的处理不满足保留熵最大）不成立，那么就不符合人对世界的认知，在人类发展过程中，熵的定义就会被淘汰。所以这一条规则是成立的，这样训练出来的网络，和人的大脑有共同之处。&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;波尔兹曼机的解释&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;对于波尔兹曼机及其背后的物理思想，自己还没有搞明白。然而，物理是一门对自然万物运行的机理进行解释的科学，统计热力学也不例外。人自身即置于这个世界之内，建立的理论大概也是殊途同归吧。对于这一点，由于自己学识所限，不敢妄加揣测。&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h2&gt;5.道可道，非常道&lt;/h2&gt;
&lt;p&gt;看过《老子》之后后，大抵都会纠结于一个问题——什么是道？我个人倾向于把“道”理解为天地，或是天地之间的规则，而这个规则不是永恒不变的。“道”可以视作为一种机制，形形色色的生物是其运行的不同结果。那么，回到4中那句“形形色色的数学理论盛衰由人类的实践决定”，数学理论的盛衰真的是人类的实践决定的吗？只有在人意识到某一种理论的不足的时候，才会有新的理论的发展，有了发展和变化，才会有盛衰。人认识世界，世界改变人的认识，数学理论的盛衰由“道”决定。&lt;/p&gt;
&lt;p&gt;回归正题。&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;1.对于SVMs等浅层学习，什么是道？&lt;/p&gt;
&lt;p&gt;2.深度学习的道？&lt;/p&gt;
&lt;p&gt;3.道法自然、修道&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;很多浅层学习算法可以看作是从被标记的数据之间寻找规律的过程，对于网络结构本身而言，它的“道”是一组组人为建立的被标记的数据。我们试图去建立一个属于它们的世界，让它们在我们建立的世界中学习。然而，这个过程是困难的，不谈浅层网络的结构对其表示的影响，和真实的世界相比，我们建立的用来训练浅层网络的世界太简单，对真实世界的表示有太大的局限性。&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://jbcdn2.b0.upaiyun.com/2014/09/ea1abe65d97b2d6646201c8640a0e60a.png&quot; rel=&quot;lightbox[77531]&quot; title=&quot;深度学习之浅见&quot;&gt;&lt;img class=&quot;alignnone size-full wp-image-77537&quot; alt=&quot;221403098572516&quot; src=&quot;/images/jobbole.com/ab2b4d4a741b2343aaaf64610bc4aa79.jpg&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;深度学习的过程是不一样的。认识到建立一个世界的困难之后，试图利用世界本身去训练网络。然而这个过程是没有有反馈的（即无监督学习），我们把人本身作为桥梁，利用我们抽象出的规则，和合适的结构（多层感知器）去构造一个能够表达真实世界规律的人工网络。&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://jbcdn2.b0.upaiyun.com/2014/09/acb377d85dcdadebfb908c8c7fc99344.png&quot; rel=&quot;lightbox[77531]&quot; title=&quot;深度学习之浅见&quot;&gt;&lt;img class=&quot;alignnone size-full wp-image-77538&quot; alt=&quot;221403254823456&quot; src=&quot;/images/jobbole.com/f99acb2a279b6ccb7b9a68de26052ae3.jpg&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;对于Deep belief Nets，我们做的还是太多了，太“有为”了。无论从结构上，还是其规则的限定上（我们把网络的训练问题过多的限制成优化问题了）。修道，一般指突破天地限制，大彻大悟的过程。如果有一天，算法能够突破人为的限定，和人所在的世界交互，那算是修道成功了吧！&lt;/p&gt;
&lt;h2&gt;6.其他&lt;/h2&gt;
&lt;p&gt;这段时间烦心事很多，博客本来都不想写了，后来想想，还是应该坚持下去。这篇博文主要是想把我现在的看法表达出来，行文有点惨不忍睹，观点也有很多错误，还请大家见谅。&lt;/p&gt;


&lt;!-- div id=&quot;ad1&quot;&gt;
&lt;/div --&gt;


	


	

</description>
        <pubDate>Tue, 23 Sep 2014 00:00:00 +0800</pubDate>
        <link>http://iftti.com/posts/2014/2014-09-23-77531-61f37eb4a.html</link>
        <guid isPermaLink="true">http://iftti.com/posts/2014/2014-09-23-77531-61f37eb4a.html</guid>
        
        
        <category>jobbole</category>
        
      </item>
    
      <item>
        <title>用 C 语言编写一个简单的垃圾回收器</title>
        <description>

					
		
&lt;!-- div id=&quot;ad1&quot;&gt;
&lt;script type=&quot;text/javascript&quot;&gt;
google_ad_client = &quot;ca-pub-7056282119617872&quot;;
google_ad_slot = &quot;6645040531&quot;;
google_ad_width = 300;
google_ad_height = 250;
&lt;/script&gt;
&lt;script type=&quot;text/javascript&quot;
src=&quot;http://pagead2.googlesyndication.com/pagead/show_ads.js&quot;&gt;
&lt;/script&gt;
&lt;/div&gt;
&lt;br/ --&gt;

&lt;p&gt;人们似乎认为编写垃圾回收机制是很难的，是一种只有少数智者和&lt;a href=&quot;http://hboehm.info/gc/index.html&quot;&gt;Hans Boehm(et al)&lt;/a&gt;才能理解的高深魔法。我认为编写垃圾回收最难的地方就是内存分配，这和阅读K&amp;amp;R所写的malloc样例难度是相当的。&lt;/p&gt;
&lt;p&gt;在开始之前有一些重要的事情需要说明一下：第一，我们所写的代码是基于Linux Kernel的，注意是Linux Kernel而不是GNU/Linux。第二，我们的代码是32bit的。第三，请不要直接使用这些代码。我并不保证这些代码完全正确，可能其中有一些我还未发现的小的bug，但是整体思路仍然是正确的。好了，让我们开始吧。&lt;/p&gt;
&lt;p&gt;如果你看到任何有误的地方，请邮件联系我maplant2@illinois.edu&lt;/p&gt;
&lt;h2&gt;编写malloc&lt;/h2&gt;
&lt;p&gt;最开始，我们需要写一个内存分配器(memmory allocator)，也可以叫做内存分配函数(malloc function)。最简单的内存分配实现方法就是维护一个由空闲内存块组成的链表，这些空闲内存块在需要的时候被分割或分配。当用户请求一块内存时，一块合适大小的内存块就会从链表中被移除并分配给用户。如果链表中没有合适的空闲内存块存在，而且更大的空闲内存块已经被分割成小的内存块了或内核也正在请求更多的内存(译者注：就是链表中的空闲内存块都太小不足以分配给用户的情况)。那么此时，会释放掉一块内存并把它添加到空闲块链表中。&lt;/p&gt;
&lt;p&gt;在链表中的每个空闲内存块都有一个头(header)用来描述内存块的信息。我们的header包含两个部分，第一部分表示内存块的大小，第二部分指向下一个空闲内存块。&lt;/p&gt;
&lt;pre class=&quot;brush: c; gutter: true; first-line: 1; highlight: []; html-script: false&quot;&gt;    typedef struct header{
        unsigned int  size;
        struct block  *next;
    } header_t;&lt;/pre&gt;
&lt;p&gt;将头(header)内嵌进内存块中是唯一明智的做法，而且这样还可以享有字节自动对齐的好处，这很重要。&lt;/p&gt;
&lt;p&gt;由于我们需要同时跟踪我们“当前使用过的内存块”和“未使用的内存块”，因此除了维护空闲内存的链表外，我们还需要一条维护当前已用内存块的链表(为了方便，这两条链表后面分别写为“空闲块链表”和“已用块链表”)。我们从空闲块链表中移除的内存块会被添加到已用块链表中，反之亦然。&lt;/p&gt;
&lt;p&gt;现在我们差不多已经做好准备来完成malloc实现的第一步了。但是再那之前，我们需要知道怎样向内核申请内存。&lt;/p&gt;
&lt;p&gt;动态分配的内存会驻留在一个叫做堆(heap)的地方，堆是介于栈(stack)和BSS(未初始化的数据段－你所有的全局变量都存放在这里且具有默认值为0)之间的一块内存。堆(heap)的内存地址起始于(低地址)BSS段的边界，结束于一个分隔地址(这个分隔地址是已建立映射的内存和未建立映射的内存的分隔线)。为了能够从内核中获取更多的内存，我们只需提高这个分隔地址。为了提高这个分隔地址我们需要调用一个叫作 sbrk 的Unix系统的系统调用，这个函数可以根据我们提供的参数来提高分隔地址，如果函数执行成功则会返回以前的分隔地址，如果失败将会返回－1。&lt;/p&gt;
&lt;p&gt;利用我们现在知道的知识，我们可以创建两个函数：morecore()和add_to_free_list()。当空闲块链表缺少内存块时，我们调用morecore()函数来申请更多的内存。由于每次向内核申请内存的代价是昂贵的，我们以页(page-size)为单位申请内存。页的大小在这并不是很重要的知识点，不过这有一个很简单解释：页是虚拟内存映射到物理内存的最小内存单位。接下来我们就可以使用add_to_list()将申请到的内存块加入空闲块链表。&lt;/p&gt;
&lt;pre class=&quot;brush: c; gutter: true; first-line: 1; highlight: []; html-script: false&quot;&gt;/*
 * Scan the free list and look for a place to put the block. Basically, we&#39;re
 * looking for any block the to be freed block might have been partitioned from.
 */
static void
add_to_free_list(header_t *bp)
{
    header_t *p;

    for (p = freep; !(bp &amp;gt; p &amp;amp;&amp;amp; bp &amp;lt; p-&amp;gt;next); p = p-&amp;gt;next)
        if (p &amp;gt;= p-&amp;gt;next &amp;amp;&amp;amp; (bp &amp;gt; p || bp &amp;lt; p-&amp;gt;next))
            break;

    if (bp + bp-&amp;gt;size == p-&amp;gt;next) {
        bp-&amp;gt;size += p-&amp;gt;next-&amp;gt;size;
        bp-&amp;gt;next = p-&amp;gt;next-&amp;gt;next;
    } else
        bp-&amp;gt;next = p-&amp;gt;next;

    if (p + p-&amp;gt;size == bp) {
        p-&amp;gt;size += bp-&amp;gt;size;
        p-&amp;gt;next = bp-&amp;gt;next;
    } else
        p-&amp;gt;next = bp;

    freep = p;
}

#define MIN_ALLOC_SIZE 4096 /* We allocate blocks in page sized chunks. */

/*
 * Request more memory from the kernel.
 */
static header_t *
morecore(size_t num_units)
{
    void *vp;
    header_t *up;

    if (num_units &amp;lt; MIN_ALLOC_SIZE)
        num_units = MIN_ALLOC_SIZE / sizeof(header_t);

    if ((vp = sbrk(num_units * sizeof(header_t))) == (void *) -1)
        return NULL;
            
    up = (header_t *) vp;
    up-&amp;gt;size = num_units;
    add_to_free_list (up);
    return freep;
}&lt;/pre&gt;
&lt;p&gt;现在我们有了两个有力的函数，接下来我们就可以直接编写malloc函数了。我们扫描空闲块链表当遇到第一块满足要求的内存块(内存块比所需内存大即满足要求)时，停止扫描，而不是扫描整个链表来寻找大小最合适的内存块，我们所采用的这种算法思想其实就是首次适应(与最佳适应相对)。&lt;/p&gt;
&lt;p&gt;注意：有件事情需要说明一下，内存块头部结构中size这一部分的计数单位是块(Block)，而不是Byte。&lt;/p&gt;
&lt;pre class=&quot;brush: c; gutter: true; first-line: 1; highlight: []; html-script: false&quot;&gt;    

static header_t base; /* Zero sized block to get us started. */
static header_t *usedp, *freep;

/*
 * Find a chunk from the free list and put it in the used list.
 */
void *
GC_malloc(size_t alloc_size)
{
    size_t num_units;
    header_t *p, *prevp;

    num_units = (alloc_size + sizeof(header_t) - 1) / sizeof(header_t) + 1;  
    prevp = freep;

    for (p = prevp-&amp;gt;next;; prevp = p, p = p-&amp;gt;next) {
        if (p-&amp;gt;size &amp;gt;= num_units) { /* Big enough. */
            if (p-&amp;gt;size == num_units) /* Exact size. */
                prevp-&amp;gt;next = p-&amp;gt;next;
            else {
                p-&amp;gt;size -= num_units;
                p += p-&amp;gt;size;
                p-&amp;gt;size = num_units;
            }

            freep = prevp;
            
            /* Add to p to the used list. */
            if (usedp == NULL)  
                usedp = p-&amp;gt;next = p;
            else {
                p-&amp;gt;next = usedp-&amp;gt;next;
                usedp-&amp;gt;next = p;
            }

            return (void *) (p + 1);
        }
        if (p == freep) { /* Not enough memory. */
            p = morecore(num_units);
            if (p == NULL) /* Request for more memory failed. */
                return NULL;
        }
    }
}

&lt;/pre&gt;
&lt;p&gt;注意这个函数的成功与否，取决于我们第一次使用时是否使 freep = &amp;amp;base 。这点我们会在初始化函数中进行设置。&lt;/p&gt;
&lt;p&gt;尽管我们的代码完全没有考虑到内存碎片，但是它能工作。既然它可以工作，我们就可以开始下一个有趣的部分－垃圾回收！&lt;/p&gt;
&lt;h2&gt;标记和清扫&lt;/h2&gt;
&lt;p&gt;我们说过垃圾回收器会很简单，因此我们尽可能的使用简单的方法：标记和清除方式。这个算法分为两个部分：&lt;/p&gt;
&lt;p&gt;首先，我们需要扫描所有可能存在指向堆中数据(heap data)的变量的内存空间并确认这些内存空间中的变量是否指向堆中的数据。为了做到这点，对于可能内存空间中的每个字长(word-size)的数据块，我们遍历已用块链表中的内存块。如果数据块所指向的内存是在已用链表块中的某一内存块中，我们对这个内存块进行标记。&lt;/p&gt;
&lt;p&gt;第二部分是，当扫描完所有可能的内存空间后，我们遍历已用块链表将所有未被标记的内存块移到空闲块链表中。&lt;/p&gt;
&lt;p&gt;现在很多人会开始认为只是靠编写类似于malloc那样的简单函数来实现C的垃圾回收是不可行的，因为在函数中我们无法获得其外面的很多信息。例如，在C语言中没有函数可以返回分配到堆栈中的所有变量的哈希映射。但是只要我们意识到两个重要的事实，我们就可以绕过这些东西：&lt;/p&gt;
&lt;p&gt;第一，在C中，你可以尝试访问任何你想访问的内存地址。因为不可能有一个数据块编译器可以访问但是其地址却不能被表示成一个可以赋值给指针的整数。如果一块内存在C程序中被使用了，那么它一定可以被这个程序访问。这是一个令不熟悉C的编程者很困惑的概念，因为很多编程语言都会限制程序访问虚拟内存，但是C不会。&lt;/p&gt;
&lt;p&gt;第二，所有的变量都存储在内存的某个地方。这意味着如果我们可以知道变量们的通常存储位置，我们可以遍历这些内存位置来寻找每个变量的所有可能值。另外，因为内存的访问通常是字(word-size)对齐的，因此我们仅需要遍历内存区域中的每个字(word)即可。&lt;/p&gt;
&lt;p&gt;局部变量也可以被存储在寄存器中，但是我们并不需要担心这些因为寄存器经常会用于存储局部变量，而且当函数被调用的时候他们通常会被存储在堆栈中。&lt;/p&gt;
&lt;p&gt;现在我们有一个标记阶段的策略：遍历一系列的内存区域并查看是否有内存可能指向已用块链表。编写这样的一个函数非常的简洁明了：&lt;/p&gt;
&lt;pre class=&quot;brush: c; gutter: true; first-line: 1; highlight: []; html-script: false&quot;&gt;
#define UNTAG(p) (((unsigned int) (p)) &amp;amp; 0xfffffffc)

/*
 * Scan a region of memory and mark any items in the used list appropriately.
 * Both arguments should be word aligned.
 */
static void
mark_from_region(unsigned int *sp, unsigned int *end)
{
    header_t *bp;

    for (; sp &amp;lt; end; sp++) {
        unsigned int v = *sp;
        bp = usedp;
        do {
            if (bp + 1 &amp;lt;= v &amp;amp;&amp;amp;
                bp + 1 + bp-&amp;gt;size &amp;gt; v) {
                    bp-&amp;gt;next = ((unsigned int) bp-&amp;gt;next) | 1;
                    break;
            }
        } while ((bp = UNTAG(bp-&amp;gt;next)) != usedp);
    }
}&lt;/pre&gt;
&lt;p&gt;为了确保我们只使用头(header)中的两个字长(two words)我们使用一种叫做标记指针(tagged pointer)的技术。利用header中的next指针指向的地址总是字对齐(word aligned)这一特点，我们可以得出指针低位的几个有效位总会是0。因此我们将next指针的最低位进行标记来表示当前块是否被标记。&lt;/p&gt;
&lt;p&gt;现在，我们可以扫描内存区域了，但是我们应该扫描哪些内存区域呢？我们要扫描的有以下这些：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;BBS(未初始化数据段)和初始化数据段。这里包含了程序的全局变量和局部变量。因为他们有可能应用堆(heap)中的一些东西，所以我们需要扫描BSS与初始化数据段。        &lt;/li&gt;
&lt;li&gt;已用的数据块。当然，如果用户分配一个指针来指向另一个已经被分配的内存块，我们不会想去释放掉那个被指向的内存块。    &lt;/li&gt;
&lt;li&gt;堆栈。因为堆栈中包含所有的局部变量，因此这可以说是最需要扫描的区域了。     &lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;我们已经了解了关于堆(heap)的一切，因此编写一个mark_from_heap函数将会非常简单：&lt;/p&gt;
&lt;pre class=&quot;brush: c; gutter: true; first-line: 1; highlight: []; html-script: false&quot;&gt; /*
 * Scan the marked blocks for references to other unmarked blocks.
 */
static void
mark_from_heap(void)
{
    unsigned int *vp;
    header_t *bp, *up;

    for (bp = UNTAG(usedp-&amp;gt;next); bp != usedp; bp = UNTAG(bp-&amp;gt;next)) {
        if (!((unsigned int)bp-&amp;gt;next &amp;amp; 1))
            continue;
        for (vp = (unsigned int *)(bp + 1);
             vp &amp;lt; (bp + bp-&amp;gt;size + 1);
             vp++) {
            unsigned int v = *vp;
            up = UNTAG(bp-&amp;gt;next);
            do {
                if (up != bp &amp;amp;&amp;amp;
                    up + 1 &amp;lt;= v &amp;amp;&amp;amp;
                    up + 1 + up-&amp;gt;size &amp;gt; v) {
                    up-&amp;gt;next = ((unsigned int) up-&amp;gt;next) | 1;
                    break;
                }
            } while ((up = UNTAG(up-&amp;gt;next)) != bp);
        }
    }
}&lt;/pre&gt;
&lt;p&gt;幸运的是对于BSS段和已初始化数据段，大部分的现代unix链接器可以导出 etext 和 end 符号。etext符号的地址是初始化数据段的起点(the last address past the text segment，这个段中包含了程序的机器码)，end符号是堆(heap)的起点。因此，BSS和已初始化数据段位于 &amp;amp;etext 与 &amp;amp;end 之间。这个方法足够简单，当不是平台独立的。&lt;/p&gt;
&lt;p&gt;堆栈这部分有一点困难。堆栈的栈顶非常容易找到，只需要使用一点内联汇编即可，因为它存储在 sp 这个寄存器中。但是我们将会使用的是 bp 这个寄存器，因为它忽略了一些局部变量。&lt;/p&gt;
&lt;p&gt;寻找堆栈的的栈底(堆栈的起点)涉及到一些技巧。出于安全因素的考虑，内核倾向于将堆栈的起点随机化，因此我们很难得到一个地址。老实说，我在寻找栈底方面并不是专家，但是我有一些点子可以帮你找到一个准确的地址。一个可能的方法是，你可以扫描调用栈(call stack)来寻找 env 指针，这个指针会被作为一个参数传递给主程序。另一种方法是从栈顶开始读取每个更大的后续地址并处理inexorible SIGSEGV。但是我们并不打算采用这两种方法中的任何一种，我们将利用linux会将栈底放入一个字符串并存于proc目录下表示该进程的文件中这一事实。这听起来很愚蠢而且非常间接。值得庆幸的是，我并不感觉这样做是滑稽的，因为它和Boehm GC中寻找栈底所用的方法完全相同。&lt;/p&gt;
&lt;p&gt;现在我们可以编写一个简单的初始化函数。在函数中，我们打开proc文件并找到栈底。栈底是文件中第28个值，因此我们忽略前27个值。Boehm GC和我们的做法不同的是他仅使用系统调用来读取文件来避免让stdlib库使用堆(heap)，但是我们并不在意这些。&lt;/p&gt;
&lt;pre class=&quot;brush: c; gutter: true; first-line: 1; highlight: []; html-script: false&quot;&gt;/*
 * Find the absolute bottom of the stack and set stuff up.
 */
void
GC_init(void)
{
    static int initted;
    FILE *statfp;

    if (initted)
        return;

    initted = 1;

    statfp = fopen(&quot;/proc/self/stat&quot;, &quot;r&quot;);
    assert(statfp != NULL);
    fscanf(statfp,
           &quot;%*d %*s %*c %*d %*d %*d %*d %*d %*u &quot;
           &quot;%*lu %*lu %*lu %*lu %*lu %*lu %*ld %*ld &quot;
           &quot;%*ld %*ld %*ld %*ld %*llu %*lu %*ld &quot;
           &quot;%*lu %*lu %*lu %lu&quot;, &amp;amp;stack_bottom);
    fclose(statfp);

    usedp = NULL;
    base.next = freep = &amp;amp;base;
    base.size = 0;&lt;/pre&gt;
&lt;p&gt;现在我们知道了每个我们需要扫描的内存区域的位置，所以我们终于可以编写显示调用的回收函数了：&lt;/p&gt;
&lt;pre class=&quot;brush: c; gutter: true; first-line: 1; highlight: []; html-script: false&quot;&gt; /*
 * Mark blocks of memory in use and free the ones not in use.
 */
void
GC_collect(void)
{
    header_t *p, *prevp, *tp;
    unsigned long stack_top;
    extern char end, etext; /* Provided by the linker. */

    if (usedp == NULL)
        return;
    
    /* Scan the BSS and initialized data segments. */
    mark_from_region(&amp;amp;etext, &amp;amp;end);

    /* Scan the stack. */
    asm volatile (&quot;movl    %%ebp, %0&quot; : &quot;=r&quot; (stack_top));
    mark_from_region(stack_top, stack_bottom);

    /* Mark from the heap. */
    mark_from_heap();

    /* And now we collect! */
    for (prevp = usedp, p = UNTAG(usedp-&amp;gt;next);; prevp = p, p = UNTAG(p-&amp;gt;next)) {
    next_chunk:
        if (!((unsigned int)p-&amp;gt;next &amp;amp; 1)) {
            /*
             * The chunk hasn&#39;t been marked. Thus, it must be set free. 
             */
            tp = p;
            p = UNTAG(p-&amp;gt;next);
            add_to_free_list(tp);

            if (usedp == tp) { 
                usedp = NULL;
                break;
            }

            prevp-&amp;gt;next = (unsigned int)p | ((unsigned int) prevp-&amp;gt;next &amp;amp; 1);
            goto next_chunk;
        }
        p-&amp;gt;next = ((unsigned int) p-&amp;gt;next) &amp;amp; ~1;
        if (p == usedp)
            break;
    }
}&lt;/pre&gt;
&lt;p&gt;朋友们，所有的东西都已经在这了，一个用C为C程序编写的垃圾回收器。这些代码自身并不是完整的，它还需要一些微调来使它可以正常工作，但是大部分代码是可以独立工作的。&lt;/p&gt;
&lt;h2&gt;总结&lt;/h2&gt;
&lt;p&gt;从小学到高中，我一直在学习打鼓。每个星期三的下午4:30左右我都会更一个很棒的老师上打鼓教学课。&lt;/p&gt;
&lt;p&gt;每当我在学习一些新的打槽（groove）或节拍时，我的老师总会给我一个相同的告诫：我试图同时做所有的事情。我看着乐谱，我只是简单地尝试用双手将它全部演奏出来，但是我做不到。原因是因为我还不知道怎样打槽，但我却在学习打槽地时候同时学习其它东西而不是单纯地练习打槽。&lt;/p&gt;
&lt;p&gt;因此我的老师教导我该如何去学习：不要想着可以同时做所有地事情。先学习用你地右手打架子鼓，当你学会之后，再学习用你的左手打小鼓。用同样地方式学习贝斯、手鼓和其它部分。当你可以单独使用每个部分之后，慢慢开始同时练习它们，先两个同时练习，然后三个，最后你将可以可以同时完成所有部分。&lt;/p&gt;
&lt;p&gt;我在打鼓方面从来都不够优秀，但我在编程时始终记着这门课地教训。一开始就打算编写完整的程序是很困难的，你编程的唯一算法就是分而治之。先编写内存分配函数，然后编写查询内存的函数，然后是清除内存的函数。最后将它们合在一起。&lt;/p&gt;
&lt;p&gt;当你在编程方面克服这个障碍后，就再也没有困难的实践了。你可能有一个算法不太了解，但是任何人只要有足够的时间就肯定可以通过论文或书理解这个算法。如果有一个项目看起来令人生畏，那么将它分成完全独立的几个部分。你可能不懂如何编写一个解释器，但你绝对可以编写一个分析器，然后看一下你还有什么需要添加的，添上它。&lt;/p&gt;


&lt;!-- div id=&quot;ad1&quot;&gt;
&lt;/div --&gt;


	


	

</description>
        <pubDate>Tue, 23 Sep 2014 00:00:00 +0800</pubDate>
        <link>http://iftti.com/posts/2014/2014-09-23-77248-d39d792ab.html</link>
        <guid isPermaLink="true">http://iftti.com/posts/2014/2014-09-23-77248-d39d792ab.html</guid>
        
        
        <category>jobbole</category>
        
      </item>
    
      <item>
        <title>图解SSL/TLS协议</title>
        <description>

					
		
&lt;!-- div id=&quot;ad1&quot;&gt;
&lt;script type=&quot;text/javascript&quot;&gt;
google_ad_client = &quot;ca-pub-7056282119617872&quot;;
google_ad_slot = &quot;6645040531&quot;;
google_ad_width = 300;
google_ad_height = 250;
&lt;/script&gt;
&lt;script type=&quot;text/javascript&quot;
src=&quot;http://pagead2.googlesyndication.com/pagead/show_ads.js&quot;&gt;
&lt;/script&gt;
&lt;/div&gt;
&lt;br/ --&gt;

&lt;p&gt;上周，&lt;a href=&quot;https://www.cloudflare.com/&quot; target=&quot;_blank&quot;&gt;CloudFlare&lt;/a&gt;宣布，开始提供Keyless服务，即你把网站放到它们的CDN上，不用提供自己的私钥，也能使用SSL加密链接。&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/2824cf96b035e0f9579fe84fd2101cb7.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;我看了CloudFlare的说明（&lt;a href=&quot;https://blog.cloudflare.com/announcing-keyless-ssl-all-the-benefits-of-cloudflare-without-having-to-turn-over-your-private-ssl-keys/&quot; target=&quot;_blank&quot;&gt;这里&lt;/a&gt;和&lt;a href=&quot;http://blog.cloudflare.com/keyless-ssl-the-nitty-gritty-technical-details/&quot; target=&quot;_blank&quot;&gt;这里&lt;/a&gt;），突然意识到这是绝好的例子，可以用来说明SSL/TLS协议的运行机制。它配有插图，很容易看懂。&lt;/p&gt;
&lt;p&gt;下面，我就用这些图片作为例子，配合我半年前写的&lt;a href=&quot;http://www.ruanyifeng.com/blog/2014/02/ssl_tls.html&quot; target=&quot;_blank&quot;&gt;《SSL/TLS协议运行机制的概述》&lt;/a&gt;，来解释SSL协议。&lt;/p&gt;
&lt;h2&gt;一、SSL协议的握手过程&lt;/h2&gt;
&lt;p&gt;开始加密通信之前，客户端和服务器首先必须建立连接和交换参数，这个过程叫做握手（handshake）。&lt;/p&gt;
&lt;p&gt;假定客户端叫做爱丽丝，服务器叫做鲍勃，整个握手过程可以用下图说明（点击看大图）。&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/3adb9a41f76190f59c8e4dec2c459028.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;握手阶段分成五步。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;第一步，爱丽丝给出协议版本号、一个客户端生成的随机数（Client random），以及客户端支持的加密方法。&lt;/p&gt;
&lt;p&gt;第二步，鲍勃确认双方使用的加密方法，并给出数字证书、以及一个服务器生成的随机数（Server random）。&lt;/p&gt;
&lt;p&gt;第三步，爱丽丝确认数字证书有效，然后生成一个新的随机数（Premaster secret），并使用数字证书中的公钥，加密这个随机数，发给鲍勃。&lt;/p&gt;
&lt;p&gt;第四步，鲍勃使用自己的私钥，获取爱丽丝发来的随机数（即Premaster secret）。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;上面的五步，画成一张图，就是下面这样。&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/e6645a69e4f0a9b4e1885aff10bb09ed.jpg&quot;&gt;&lt;/p&gt;
&lt;h2&gt;二、私钥的作用&lt;/h2&gt;
&lt;p&gt;握手阶段有三点需要注意。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;（1）生成对话密钥一共需要三个随机数。&lt;/p&gt;
&lt;p&gt;（2）握手之后的对话使用”对话密钥”加密（对称加密），服务器的公钥和私钥只用于加密和解密”对话密钥”（非对称加密），无其他作用。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;从上面第二点可知，整个对话过程中（握手阶段和其后的对话），服务器的公钥和私钥只需要用到一次。这就是CloudFlare能够提供Keyless服务的根本原因。&lt;/p&gt;
&lt;p&gt;某些客户（比如银行）想要使用外部CDN，加快自家网站的访问速度，但是出于安全考虑，不能把私钥交给CDN服务商。这时，完全可以把私钥留在自家服务器，只用来解密对话密钥，其他步骤都让CDN服务商去完成。&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/7936dc1377cde2f97269bdd4c61a63bf.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;上图中，银行的服务器只参与第四步，后面的对话都不再会用到私钥了。&lt;/p&gt;
&lt;h2&gt;三、DH算法的握手阶段&lt;/h2&gt;
&lt;p&gt;整个握手阶段都不加密（也没法加密），都是明文的。因此，如果有人窃听通信，他可以知道双方选择的加密方法，以及三个随机数中的两个。整个通话的安全，只取决于第三个随机数（Premaster secret）能不能被破解。&lt;/p&gt;
&lt;p&gt;虽然理论上，只要服务器的公钥足够长（比如2048位），那么Premaster secret可以保证不被破解。但是为了足够安全，我们可以考虑把握手阶段的算法从默认的&lt;a href=&quot;http://www.ruanyifeng.com/blog/2013/06/rsa_algorithm_part_one.html&quot; target=&quot;_blank&quot;&gt;RSA算法&lt;/a&gt;，改为 &lt;a href=&quot;http://zh.wikipedia.org/wiki/%E8%BF%AA%E8%8F%B2%EF%BC%8D%E8%B5%AB%E5%B0%94%E6%9B%BC%E5%AF%86%E9%92%A5%E4%BA%A4%E6%8D%A2&quot; target=&quot;_blank&quot;&gt;Diffie-Hellman算法&lt;/a&gt;（简称DH算法）。&lt;/p&gt;
&lt;p&gt;采用DH算法后，Premaster secret不需要传递，双方只要交换各自的参数，就可以算出这个随机数。&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/94799c7ff94ef5901f2b7d3c54a981a6.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;上图中，第三步和第四步由传递Premaster secret变成了传递DH算法所需的参数，然后双方各自算出Premaster secret。这样就提高了安全性。&lt;/p&gt;
&lt;h2&gt;四、session的恢复&lt;/h2&gt;
&lt;p&gt;握手阶段用来建立SSL连接。如果出于某种原因，对话中断，就需要重新握手。&lt;/p&gt;
&lt;p&gt;这时有两种方法可以恢复原来的session：一种叫做session ID，另一种叫做session ticket。&lt;/p&gt;
&lt;p&gt;session ID的思想很简单，就是每一次对话都有一个编号（session ID）。如果对话中断，下次重连的时候，只要客户端给出这个编号，且服务器有这个编号的记录，双方就可以重新使用已有的”对话密钥”，而不必重新生成一把。&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/3b60786405357e0323c5a5183b5b0160.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;上图中，客户端给出session ID，服务器确认该编号存在，双方就不再进行握手阶段剩余的步骤，而直接用已有的对话密钥进行加密通信。&lt;/p&gt;
&lt;p&gt;session ID是目前所有浏览器都支持的方法，但是它的缺点在于session ID往往只保留在一台服务器上。所以，如果客户端的请求发到另一台服务器，就无法恢复对话。session ticket就是为了解决这个问题而诞生的，目前只有Firefox和Chrome浏览器支持。&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/c987c18ed256e3d3bf3c782d04d3577e.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;上图中，客户端不再发送session ID，而是发送一个服务器在上一次对话中发送过来的session ticket。这个session ticket是加密的，只有服务器才能解密，其中包括本次对话的主要信息，比如对话密钥和加密方法。当服务器收到session ticket以后，解密后就不必重新生成对话密钥了。&lt;/p&gt;


&lt;!-- div id=&quot;ad1&quot;&gt;
&lt;/div --&gt;


	


	

</description>
        <pubDate>Mon, 22 Sep 2014 00:00:00 +0800</pubDate>
        <link>http://iftti.com/posts/2014/2014-09-22-77439-a259dc1a2.html</link>
        <guid isPermaLink="true">http://iftti.com/posts/2014/2014-09-22-77439-a259dc1a2.html</guid>
        
        
        <category>jobbole</category>
        
      </item>
    
      <item>
        <title>node.js Flame Graphs on Linux</title>
        <description>

&lt;p&gt;CPU &lt;a href=&quot;/flamegraphs.html&quot;&gt;flame graphs&lt;/a&gt; are a useful visualization application stack traces, allowing you to quickly identify and quantify what to tune to improve performance. For Node.js they have solved countless problems on systems which have DTrace for sampling stack traces. But what about Linux?&lt;/p&gt;

&lt;p&gt;At Netflix we have node.js in production at scale, on Linux instances in AWS EC2, and we create flame graphs using Linux &lt;a href=&quot;/perf.html&quot;&gt;perf_events&lt;/a&gt; and v8&#39;s &lt;a href=&quot;https://codereview.chromium.org/70013002&quot;&gt;--perf-basic-prof option&lt;/a&gt;. In this quick blog post, I&#39;ll share how it works and how you can do it, and what needs to be fixed to improve it further.&lt;/p&gt;

&lt;h2&gt;1. The problem&lt;/h2&gt;

&lt;p&gt;Using perf_events to profile CPU usage on node.js 0.10.23:&lt;/p&gt;

&lt;p&gt;&lt;object data=&quot;/images/brendangregg.com/da8a526902cf4e359d30f4104fbae443.svg&quot; type=&quot;image/svg+xml&quot; width=&quot;720&quot; height=&quot;875&quot;&gt;
&lt;img src=&quot;/images/brendangregg.com/da8a526902cf4e359d30f4104fbae443.jpg&quot; width=&quot;720&quot;&gt;
&lt;/object&gt;&lt;/p&gt;

&lt;p&gt;It&#39;s interactive: mouse-over elements for details, and click the &lt;a href=&quot;/blog/images/2014/node.js_flamegraph_nosymbols_01023.svg&quot;&gt;SVG&lt;/a&gt; to zoom. The &lt;a href=&quot;/FlameGraphs/cpuflamegraphs.html&quot;&gt;CPU flame graphs&lt;/a&gt; page explains how to interpret these, and this was created using the instructions in the &lt;a href=&quot;/FlameGraphs/cpuflamegraphs.html#perf&quot;&gt;Linux perf section&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;This flame graph is partially-useful, as I can see system and v8 library symbols. However, it is missing JavaScript symbols (the blank rectangles), since v8, like the JVM, compiles and places symbols just in time (JIT).&lt;/p&gt;

&lt;h2&gt;2. Linux perf_events JIT support&lt;/h2&gt;

&lt;p&gt;In 2009, Linux &lt;a href=&quot;/perf.html&quot;&gt;perf_events&lt;/a&gt; added &lt;a href=&quot;https://lkml.org/lkml/2009/6/8/499&quot;&gt;JIT symbol support&lt;/a&gt;, so that symbols from language virtual machines like the JVM could be inspected. It works in the following amazingly simple way:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Your JIT application must be modified to create a /tmp/perf-&lt;em&gt;PID&lt;/em&gt;.map file, which is a simple text database containing symbol addresses (in hex), sizes, and symbol names.&lt;/li&gt;
&lt;li&gt;That&#39;s it.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;perf already looks for the /tmp/perf-&lt;em&gt;PID&lt;/em&gt;.map file, and if it finds it, it uses it for symbol translations. So only v8 needed to be modified.&lt;/p&gt;

&lt;h2&gt;3. v8 --perf-basic-prof support&lt;/h2&gt;

&lt;p&gt;In November 2013, &lt;a href=&quot;https://codereview.chromium.org/70013002&quot;&gt;v8 added perf_events support&lt;/a&gt;, enabled using the --perf-basic-prof option. This made it into node v0.11.13. It works like this:&lt;/p&gt;

&lt;pre&gt;
# &lt;b&gt;~/node-v0.11.13-linux-x64/bin/node --perf-basic-prof hello.js &amp;amp;&lt;/b&gt;
[1] 31441
# &lt;b&gt;ls -l /tmp/perf-31441.map&lt;/b&gt;
-rw-r--r-- 1 root root 81920 Sep 17 20:41 /tmp/perf-31441.map
# &lt;b&gt;tail /tmp/perf-31441.map&lt;/b&gt;
14cec4db98a0 f Stub:BinaryOpICWithAllocationSiteStub(ADD_CreateAllocationMementos:String*Generic-&amp;gt;String)
14cec4db9920 f Stub:BinaryOpICWithAllocationSiteStub(ADD_CreateAllocationMementos:String*String-&amp;gt;String)
14cec4db99a0 f Stub:BinaryOpICWithAllocationSiteStub(ADD_CreateAllocationMementos:String*Smi-&amp;gt;String)
14cec4db9a20 22c LazyCompile:~nextTick node.js:389
14cec4db9cc0 156 Stub:KeyedLoadElementStub
14cec4db9e80 22 KeyedLoadIC:
14cec4db9f20 22 KeyedLoadIC:
14cec4db9fc0 56 Stub:DoubleToIStub
14cec4dba080 10c Stub:KeyedStoreElementStub
&lt;/pre&gt;

&lt;p&gt;This text file is what perf_events reads.&lt;/p&gt;

&lt;h2&gt;4. node.js Flame Graphs&lt;/h2&gt;

&lt;p&gt;Now that we have node 0.11.13+ running with --perf-basic-prof, we can create a flame graph using:&lt;/p&gt;

&lt;pre&gt;
$ &lt;b&gt;sudo bash&lt;/b&gt;
# &lt;b&gt;perf record -F 99 -p `pgrep -n node` -g -- sleep 30&lt;/b&gt;
# &lt;b&gt;perf script &amp;gt; out.nodestacks01&lt;/b&gt;
# &lt;b&gt;git clone --depth 1 http://github.com/brendangregg/FlameGraph&lt;/b&gt;
# &lt;b&gt;cd FlameGraph&lt;/b&gt;
# &lt;b&gt;./stackcollapse-perf.pl  ../out.nodestacks01.svg&lt;/b&gt;
&lt;/pre&gt;

&lt;p&gt;You can also use &lt;a href=&quot;https://www.npmjs.org/package/stackvis&quot;&gt;stackvis&lt;/a&gt;, by Dave Pacheco, a node.js implementation which has extra features.&lt;/p&gt;

&lt;p&gt;Here&#39;s an example result:&lt;/p&gt;

&lt;p&gt;&lt;object data=&quot;/images/brendangregg.com/30123cd4d080cd350e66bf4366f40fc5.svg&quot; type=&quot;image/svg+xml&quot; width=&quot;720&quot; height=&quot;788&quot;&gt;
&lt;img src=&quot;/images/brendangregg.com/30123cd4d080cd350e66bf4366f40fc5.jpg&quot; width=&quot;720&quot;&gt;
&lt;/object&gt;&lt;/p&gt;

&lt;p&gt;Note the JavaScript symbols are now readable. Click the &lt;a href=&quot;/blog/images/2014/node.js_flamegraph_symbols_01113.svg&quot;&gt;SVG&lt;/a&gt; to zoom in. This actual flame graph isn&#39;t very interesting, as I&#39;m just testing a dummy app to test out --perf-basic-prof.&lt;/p&gt;

&lt;p&gt;Thanks to &lt;a href=&quot;https://twitter.com/trevnorris&quot;&gt;Trevor Norris&lt;/a&gt; for first posting the instructions for doing this in a short &lt;a href=&quot;https://gist.github.com/trevnorris/9616784&quot;&gt;gist&lt;/a&gt;, which you may find useful to read. He also provides a script to facilitate this.&lt;/p&gt;

&lt;h2&gt;WARNING: map file growth&lt;/h2&gt;

&lt;p&gt;We can currently only use --perf-basic-prof for short periods (hours), due to &lt;a href=&quot;https://code.google.com/p/v8/issues/detail?id=3453&quot;&gt;bug 3453&lt;/a&gt;: the perf.map file can grow endlessly, eating Gbytes in a few days. It looks like symbols are moving location (they are supposed to stay put with --perf-basic-prof), causing the map file to keep growing.&lt;/p&gt;

&lt;p&gt;So we can create flame graphs on Linux currently, but it will be ad hoc until that bug is fixed, and we can run with this option all the time.&lt;/p&gt;

&lt;p&gt;If this bug is a nuisance to you, too, and it isn&#39;t yet fixed, please upvote that bug! If it&#39;s too painful to wait for the fix, you could run on an OS with DTrace, where node.js stack profiling doesn&#39;t have this issue (or, at least, run a canary instance for performance analysis).&lt;/p&gt;

&lt;h2&gt;More&lt;/h2&gt;

&lt;p&gt;We&#39;re doing more at Netflix with node.js analysis. Stay tuned, and also see the &lt;a href=&quot;http://techblog.netflix.com/&quot;&gt;Netflix Tech Blog&lt;/a&gt;.&lt;/p&gt;


</description>
        <pubDate>Wed, 17 Sep 2014 00:00:00 +0800</pubDate>
        <link>http://iftti.com/posts/2014/2014-09-17-node-flame-graphs-on-linux.html-fd5090be6.html</link>
        <guid isPermaLink="true">http://iftti.com/posts/2014/2014-09-17-node-flame-graphs-on-linux.html-fd5090be6.html</guid>
        
        
        <category>brendangregg</category>
        
      </item>
    
      <item>
        <title>魔咒 Mojolicious 框架的结构图</title>
        <description>

							&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/php-oa.com/366cad86e05b8761b814b4f39ed81eb7.jpg&quot; style=&quot;height: 765px; width: 800px;&quot;&gt;&lt;br&gt;
本图来源: &lt;a href=&quot;http://blog2.jamadam.com/?p=808&quot;&gt;http://blog2.jamadam.com/?p=808&lt;/a&gt;&lt;/p&gt;
						

</description>
        <pubDate>Tue, 16 Sep 2014 00:00:00 +0800</pubDate>
        <link>http://iftti.com/posts/2014/2014-09-16-mojolicious-class-diagram.html-67b45904e.html</link>
        <guid isPermaLink="true">http://iftti.com/posts/2014/2014-09-16-mojolicious-class-diagram.html-67b45904e.html</guid>
        
        
        <category>php-oa</category>
        
      </item>
    
      <item>
        <title>The MSRs of EC2</title>
        <description>

&lt;p&gt;Is Intel Turbo Boost running for my AWS EC2 cloud instance (which is a Xen guest)?&lt;/p&gt;

&lt;pre&gt;
ec2-guest# &lt;b&gt;./showboost&lt;/b&gt;
CPU MHz     : 2500
Turbo MHz   : 2900 (10 active)
Turbo Ratio : 116% (10 active)
CPU 0 summary every 5 seconds...

TIME       C0_MCYC      C0_ACYC        UTIL  RATIO    MHz
06:11:35   6428553166   7457384521      51%   116%   &lt;b&gt;2900&lt;/b&gt;
06:11:40   6349881107   7365764152      50%   115%   &lt;b&gt;2899&lt;/b&gt;
06:11:45   6240610655   7239046277      49%   115%   &lt;b&gt;2899&lt;/b&gt;
06:11:50   6225704733   7221962116      49%   116%   &lt;b&gt;2900&lt;/b&gt;
[...]
&lt;/pre&gt;

&lt;p&gt;Yes! These 2500 MHz CPUs are currently running at 2900 MHz.&lt;/p&gt;

&lt;p&gt;I guess the CPUs are cold enough to boost. What&#39;s their temperature?&lt;/p&gt;

&lt;pre&gt;
ec2-guest# &lt;b&gt;./cputemp -l 1&lt;/b&gt;
CPU1 CPU2 CPU3 CPU4 CPU5 CPU6 CPU7 CPU8 CPU9 CPU10 CPU11 CPU12 CPU13 CPU14 CPU15 CPU16
70 68 68 65 63 63 61 60 68 64 64 63 62 61 70 68
70 68 69 65 63 63 61 61 68 65 63 63 61 61 70 69
70 69 69 65 63 63 61 60 69 65 64 63 61 61 69 69
69 69 69 66 64 64 61 61 68 65 64 64 61 61 70 69
[...]
&lt;/pre&gt;

&lt;p&gt;Relatively cool: between 60 and 70 degrees Celsius. This is another tool from my &lt;a href=&quot;https://github.com/brendangregg/msr-cloud-tools&quot;&gt;msr-cloud-tools&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;In this post I&#39;ll describe MSRs, how to read them, and why measuring turbo boost is important.&lt;/p&gt;

&lt;h2&gt;Model Specific Registers (MSRs)&lt;/h2&gt;

&lt;div style=&quot;float:right;padding-left:5px;padding-bottom:5px&quot;&gt;&lt;a href=&quot;/blog/images/2014/cputemps.png&quot;&gt;&lt;img src=&quot;/images/brendangregg.com/3fdcd8199d3b291f36be8145702e96c2.jpg&quot; width=&quot;455&quot;&gt;&lt;/a&gt;&lt;/div&gt;

&lt;p&gt;Aka &lt;em&gt;machine&lt;/em&gt; specific registers, these are described in &lt;a href=&quot;http://www.intel.com/content/dam/www/public/us/en/documents/manuals/64-ia-32-architectures-software-developer-vol-3c-part-3-manual.pdf&quot;&gt;Vol 3c&lt;/a&gt; of the Intel 64 and IA-32 Architectures Software Developer&#39;s Manual. They access low level CPU information, including turbo boost ratios and temperature readings. They are read and written using the RDMSR and WRMSR instructions.&lt;/p&gt;

&lt;p&gt;The image on the right shows how CPU temperatures, measured using MSRs on an EC2 instance, vary based on CPU utilization (in blue). The workload is synthetic: all CPUs driven to 100% utilization for 5 minutes, then to 0% for a while, repeat. What&#39;s interesting is that temperature rises initially with CPU load, then drops sharply. Did system fans kick in? (So far I haven&#39;t found fan RPM MSRs, to confirm.)&lt;/p&gt;

&lt;p&gt;I&#39;m usually focused on the performance monitoring counters (PMCs; aka performance instrumentation counters (PICs), CPU performance counters (CPCs), etc.). These are read by RDPMC, and described in &lt;a href=&quot;http://www.intel.com/content/dam/www/public/us/en/documents/manuals/64-ia-32-architectures-software-developer-vol-3b-part-3-manual.pdf&quot;&gt;Vol 3b&lt;/a&gt; of the same manual. These can measure data cache misses, stall cycles, and other useful performance events.&lt;/p&gt;

&lt;p&gt;On AWS EC2, within a cloud instance (Xen guest), I&#39;ve never seen the PMCs work, eg, via &quot;&lt;a href=&quot;http://www.brendangregg.com/perf.html#CPUstatistics&quot;&gt;perf stat&lt;/a&gt;&quot;. That doesn&#39;t mean they can&#39;t ever work, just that they (or their controlling MSRs) aren&#39;t currently available.&lt;/p&gt;

&lt;p&gt;But a small handful of MSRs &lt;em&gt;are&lt;/em&gt; available on EC2. Here are the more interesting ones I&#39;ve found:&lt;/p&gt;

&lt;div style=&quot;padding-left:20px&quot;&gt;
&lt;table border=&quot;1&quot;&gt;
&lt;tr&gt;
&lt;th&gt;Reg&lt;/th&gt;
&lt;th&gt;Name&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;0xe7&lt;/td&gt;
&lt;td&gt;IA32_MPERF&lt;/td&gt;
&lt;td&gt;Bits 63:0 is TSC Frequency Clock Counter C0_MCNT TSC relative&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;0xe8&lt;/td&gt;
&lt;td&gt;IA32_APERF&lt;/td&gt;
&lt;td&gt;Bits 63:0 is TSC Frequency Clock Counter C0_ACNT actual clocks&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;0x19c&lt;/td&gt;
&lt;td&gt;IA32_THERM_STATUS&lt;/td&gt;
&lt;td&gt;Bits 22:16 is the CPU therm status digital readout (DO)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;0x1a2&lt;/td&gt;
&lt;td&gt;MSR_TEMPERATURE_TARGET&lt;/td&gt;
&lt;td&gt;Bits 23:16 is temp target (TT)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;0x1ad&lt;/td&gt;
&lt;td&gt;MSR_TURBO_RATIO_LIMIT&lt;/td&gt;
&lt;td&gt;Bits 7:0 is the turbo boost ratio (x100 for MHz) for 1 core active&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;0x1ae&lt;/td&gt;
&lt;td&gt;MSR_TURBO_RATIO_LIMIT1&lt;/td&gt;
&lt;td&gt;Bits 15:8 (for example) is the turbo boost ratio for 10 cores active&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;center&gt;&lt;font size=&quot;-1&quot;&gt;&lt;i&gt;Table 1. MSRs for Intel(R) Xeon(R) CPU E5-2670 v2&lt;/i&gt;&lt;/font&gt;&lt;/center&gt;
&lt;/div&gt;

&lt;p&gt;These are used by various kernel routines, like the idle thread and cpufreq.&lt;/p&gt;

&lt;p&gt;Note that these are &lt;b&gt;model specific&lt;/b&gt;, which means they can vary between different processor models (micro-architectures). For example, Silvermont has a read/write target offset in MSR_TEMPERATURE_TARGET (bits 29:24), which can lower the throttle temperature (PROCHOT). Such differences make MSRs non-portable and tricky to use, which is why standards like &lt;a href=&quot;http://en.wikipedia.org/wiki/Performance_Application_Programming_Interface&quot;&gt;PAPI&lt;/a&gt; are important.&lt;/p&gt;

&lt;h2&gt;Reading MSRs&lt;/h2&gt;

&lt;p&gt;Here&#39;s how you can measure MSRs (assuming Intel):&lt;/p&gt;

&lt;h3&gt;1. Determine your CPU type and micro-architecture&lt;/h3&gt;

&lt;pre&gt;
# &lt;b&gt;head /proc/cpuinfo&lt;/b&gt;
processor       : 0
vendor_id       : GenuineIntel
&lt;b&gt;cpu family      : 6
model           : 62&lt;/b&gt;
model name      : Intel(R) Xeon(R) CPU E5-2670 v2 @ 2.50GHz
[...]
&lt;/pre&gt;

&lt;p&gt;The family and model numbers tell us that this is the Ivy Bridge micro-architecture (see the &lt;a href=&quot;https://software.intel.com/en-us/articles/intel-architecture-and-processor-identification-with-cpuid-model-and-family-numbers&quot;&gt;Intel decoder&lt;/a&gt;). You can also use the cpuid tool (from the cpuid package), which should report micro-architecture directly.&lt;/p&gt;

&lt;h3&gt;2. Look up MSRs for your processor type

These are in Vol 3c of the &lt;a href=&quot;http://www.intel.com/content/www/us/en/processors/architectures-software-developer-manuals.html&quot;&gt;Intel software developer manual&lt;/a&gt;. This is a 540 page volume, and if you&#39;re new to it, you&#39;ll get lost a few times before you get the hang of it.

&lt;h3&gt;3. Install and load msr-tools:&lt;/h3&gt;

&lt;pre&gt;
# &lt;b&gt;apt-get install msr-tools&lt;/b&gt;
# &lt;b&gt;modprobe msr&lt;/b&gt;
&lt;/pre&gt;

&lt;/h3&gt;
&lt;p&gt;(Assuming Ubuntu.) The msr-tools package adds the rdmsr and wrmsr tools, and the msr kernel module.&lt;/p&gt;

&lt;h3&gt;4. Use rdmsr&lt;/h3&gt;

&lt;p&gt;Based on the addresses from (2). Eg, to read the turbo boost ratio when 10 cores are active (Ivy Bridge):&lt;/p&gt;

&lt;pre&gt;
# &lt;b&gt;rdmsr 0x1ae -f 15:8 -d&lt;/b&gt;
29
&lt;/pre&gt;

&lt;p&gt;Multiply by 100 to get MHz. The way these work is explained earlier in the manual.&lt;/p&gt;

&lt;p&gt;I did share a couple of tools in a &lt;a href=&quot;https://github.com/brendangregg/msr-cloud-tools&quot;&gt;msr-cloud-tools&lt;/a&gt; collection, however, I&#39;ve only written them for the processor type I&#39;m currently analyzing. You may need to edit these to get them using the right MSRs.&lt;/p&gt;

&lt;h2&gt;Why Measure Turbo Boost&lt;/h2&gt;

&lt;p&gt;We live in an annoying age for computer performance analysts: the error margin for many measurements &lt;em&gt;is over 10%&lt;/em&gt;, thanks to turbo boost, an Intel processor technology that can dynamically over-clock CPUs. Ubuntu is 10% faster than CentOS? Could just be turbo boost. New software version regressed by 5%? Could just be turbo boost. Tunable made things 10% faster? Could just be ... You get the picture.&lt;/p&gt;

&lt;p&gt;Turbo boost can make a 2500 MHz processor run at 3300 Mhz, depending on factors including temperature, power consumption, and the C-state of the cores. Colder servers run faster. I once had two identical servers at the top and bottom of a rack, and the top server ran 5% faster, as it received more cold air from the air conditioners. That&#39;s both great and maddening: I&#39;ll take the better performance, but it can also mess up measurements when I&#39;m comparing systems or software.&lt;/p&gt;

&lt;p&gt;There are three ways I&#39;ve historically dealt with this:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Turn off turbo boost in the BIOS when doing performance comparisons.&lt;/li&gt;
&lt;li&gt;Measure actual CPU cycles using CPU performance counters to observe the turbo boost rate.&lt;/li&gt;
&lt;li&gt;Run a short experiment (benchmark) to measure the current cycle rate, eg, &lt;a href=&quot;http://www.brendangregg.com/blog/2014-04-26/the-noploop-cpu-benchmark.html&quot;&gt;noploop&lt;/a&gt;.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;If you run your own datacenter, you can do them all. But as a Xen guest on AWS EC2, you can&#39;t change the BIOS and do (1). You can do option (3), but that can be time-consuming and difficult (less reliable) on very busy systems. Until recently, I thought you couldn&#39;t do (2), either, and then I found the MSRs...&lt;/p&gt;

&lt;h2&gt;Discovering MSRs&lt;/h2&gt;

&lt;p&gt;I was working on a suspected turbo boost issue when a colleague at Netflix, Scott, mentioned that he liked using the i7z command to debug turbo boost. We didn&#39;t think it would work on EC2, but I tried anyway.&lt;/p&gt;

&lt;p&gt;Most of the output was clearly wrong, but a lone column of temperature readings showed that something was working. Using opensnoop from my ftrace &lt;a href=&quot;https://github.com/brendangregg/perf-tools&quot;&gt;perf-tools&lt;/a&gt; collection to see how:&lt;/p&gt;

&lt;pre&gt;
# &lt;b&gt;./opensnoop -n i7z&lt;/b&gt;
Tracing open()s issued by process name &quot;i7z&quot;. Ctrl-C to end.
COMM             PID      FD FILE
i7z              8427    0x3 /proc/cpuinfo
i7z              8427    0x3 /dev/cpu/0/msr
i7z              8427    0x3 /dev/cpu/0/msr
i7z              8427    0x3 /dev/cpu/0/msr
i7z              8427    0x3 /dev/cpu/0/msr
[...]
&lt;/pre&gt;

&lt;p&gt;This showed that i7z was reading /dev/cpu/0/msr, and led me to take a close look at the available MSRs.&lt;/p&gt;

&lt;p&gt;I&#39;d normally use CPU_CLK_Unhalted.Core, but that wasn&#39;t available. After some digging, I found I could use the ratio of IA32_APERF deltas to IA32_MPERF deltas, which shows how much faster the time stamp counter (TSC, which is cycle-based) is moving when the processor is in the C0 state.&lt;/p&gt;

&lt;p&gt;It was an enormous relief to find a way to directly measure real clock rates, and turbo boost. My error margins have vanished: &lt;strong&gt;I can measure performance again&lt;/strong&gt;.&lt;/p&gt;

&lt;h2&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;A handful of model-specific registers, MSRs, are available in Xen guests including on AWS EC2. These allow the real clock rate, and the degree of turbo boost, to be measured. This is important to know for any performance comparison, as variations in turbo boost can skew results by over 10%, based on how hot or cold servers were during the test.&lt;/p&gt;

&lt;p&gt;I&#39;ve written a couple of tools so far, in &lt;a href=&quot;https://github.com/brendangregg/msr-cloud-tools&quot;&gt;msr-cloud-tools&lt;/a&gt;, to measure CPU turbo boost and temperature. As is the nature of MSRs, they are specific to processor types, and these scripts only work (so far) on our Intel(R) Xeon(R) CPU E5-2670 v2s. If you want to use these tools or MSRs yourself, you may need to find out the right MSRs to use for your processor type. The good news is that the vendor documentation from Intel and AMD is very good, although it takes some time to dig through.&lt;/p&gt;


</description>
        <pubDate>Mon, 15 Sep 2014 00:00:00 +0800</pubDate>
        <link>http://iftti.com/posts/2014/2014-09-15-the-msrs-of-ec2.html-261b110e2.html</link>
        <guid isPermaLink="true">http://iftti.com/posts/2014/2014-09-15-the-msrs-of-ec2.html-261b110e2.html</guid>
        
        
        <category>brendangregg</category>
        
      </item>
    
      <item>
        <title>Chrome 控制台不完全指南</title>
        <description>

					
		
&lt;!-- div id=&quot;ad1&quot;&gt;
&lt;script type=&quot;text/javascript&quot;&gt;
google_ad_client = &quot;ca-pub-7056282119617872&quot;;
google_ad_slot = &quot;6645040531&quot;;
google_ad_width = 300;
google_ad_height = 250;
&lt;/script&gt;
&lt;script type=&quot;text/javascript&quot;
src=&quot;http://pagead2.googlesyndication.com/pagead/show_ads.js&quot;&gt;
&lt;/script&gt;
&lt;/div&gt;
&lt;br/ --&gt;

&lt;p&gt;Chrome的开发者工具已经强大到没朋友的地步了，特别是其功能丰富界面友好的console，使用得当可以有如下功效：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;更高「逼格」更快「开发调试」更强「进阶级的Frontender」&lt;/li&gt;
&lt;li&gt;Bug无处遁形「Console大法好」&lt;/li&gt;
&lt;/ul&gt;
&lt;p id=&quot;console.log&quot;&gt;&lt;strong&gt;console.log&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;大家都会用log，但鲜有人很好地利用&lt;code&gt;console.error&lt;/code&gt; , &lt;code&gt;console.warn&lt;/code&gt; 等将输出到控制台的信息进行分类整理。&lt;br&gt;
他们功能区别不大，意义在于将输出到控制台的信息进行归类，或者说让它们更语义化。&lt;br&gt;
各个所代表的语义如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;code&gt;console.log&lt;/code&gt;：普通信息&lt;/li&gt;
&lt;li&gt;
&lt;code&gt;console.info&lt;/code&gt;：提示类信息&lt;/li&gt;
&lt;li&gt;
&lt;code&gt;console.error&lt;/code&gt;：错误信息&lt;/li&gt;
&lt;li&gt;
&lt;code&gt;console.warn&lt;/code&gt;：警示信息&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;当合理使用上述log方法后，可以很方便地在控制台选择查看特定类型的信息。&lt;/p&gt;
&lt;pre class=&quot;brush: javascript; gutter: true&quot;&gt;console.log(&#39;一颗红心向太阳&#39;,&#39;吼吼~&#39;);
console.info(&#39;楼上药不能停！&#39;);
console.warn(&#39;楼上嘴太贱！&#39;);
console.error(&#39;楼上关你毛事？&#39;);&lt;/pre&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/d5e95bb37e0c71af0bdd999b972baaaa.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;如果再配合&lt;code&gt;console.group&lt;/code&gt; 与&lt;code&gt;console.groupEnd&lt;/code&gt;，可以将这种分类管理的思想发挥到极致。这适合于在开发一个规模很大模块很多很复杂的Web APP时，将各自的log信息分组到以各自命名空间为名称的组里面。&lt;/p&gt;
&lt;pre class=&quot;brush: javascript; gutter: true&quot;&gt;console.group(&quot;app.foo&quot;);
console.log(&quot;来自foo模块的信息 blah blah blah...&quot;);
console.groupEnd();
console.group(&quot;app.bar&quot;);
console.log(&quot;来自bar模块的信息 blah blah blah...&quot;);
console.groupEnd();&lt;/pre&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/914b4100ac5a5855232814d630b9d17a.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;而关于&lt;code&gt;console.log&lt;/code&gt;，早已被玩儿坏了。一切都源于Chrome提供了这么一个API：第一个参数可以包含一些格式化的指令比如&lt;code&gt;%c&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;比如给&lt;code&gt;hello world&lt;/code&gt; 做件漂亮的嫁衣再拉出来见人：&lt;/p&gt;
&lt;pre class=&quot;brush: javascript; gutter: true&quot;&gt;console.log(&#39;%chello world&#39;,&#39;font-size:25px;color:red;&#39;);&lt;/pre&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/0a91e566d84ea5f4b9574e12e526be28.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;如果你觉得不够过瘾，那就把你能写出来的最华丽的CSS样式都应用上吧，比如渐变。于是你可以得到如下华丽丽的效果：&lt;/p&gt;
&lt;pre class=&quot;brush: javascript; gutter: true&quot;&gt;console.log(&#39;%chello world&#39;, &#39;background-image:-webkit-gradient( linear, left top, right top, color-stop(0, #f22), color-stop(0.15, #f2f), color-stop(0.3, #22f), color-stop(0.45, #2ff), color-stop(0.6, #2f2),color-stop(0.75, #2f2), color-stop(0.9, #ff2), color-stop(1, #f22) );color:transparent;-webkit-background-clip: text;font-size:5em;&#39;);&lt;/pre&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/f56abd73148e969921282b14399b2bf1.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;各种招大招的节奏啊~&lt;/p&gt;
&lt;p&gt;看着上面密集的代码不用惊慌，上面&lt;code&gt;console.log()&lt;/code&gt;第二个参数全是纯CSS用来控制样式的，你不会陌生。而第一个参数里可以带用百分号开头的转义指令，如上面输出带样式的文字时使用的&lt;code&gt;%c&lt;/code&gt;指令。更详细的指令参见官方API文档的&lt;a href=&quot;https://developer.chrome.com/devtools/docs/console-api#consolelogobject-object&quot; target=&quot;_blank&quot;&gt;这个表格&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;如果还不够过瘾，那咱们来log一些图片吧，甚至。。。动图？&lt;br&gt;
对，你得先有图，我们拿&lt;a href=&quot;http://wayou.github.io/2014/09/10/chrome-console-tips-and-tricks/rabbit.gif&quot; target=&quot;_blank&quot;&gt;这张图&lt;/a&gt;为例。&lt;/p&gt;
&lt;pre class=&quot;brush: javascript; gutter: true&quot;&gt;console.log(&quot;%c&quot;, &quot;padding:50px 300px;line-height:120px;background:url(&#39;http://wayou.github.io/2014/09/10/chrome-console-tips-and-tricks/rabbit.gif&#39;) no-repeat;&quot;);&lt;/pre&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/db01480fbeeebb529c351bc2e5475602.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;看着上面摇摆的豆比兔是不是有种抽它一脸的冲动。&lt;/p&gt;
&lt;p&gt;除此，&lt;code&gt;console.table&lt;/code&gt; 更是直接以表格的形式将数据输出，不能赞得太多！&lt;br&gt;
借用之前写过的&lt;a href=&quot;http://www.cnblogs.com/Wayou/p/things_you_dont_know_about_frontend.html&quot; target=&quot;_blank&quot;&gt;一篇博文&lt;/a&gt;里的例子：&lt;/p&gt;
&lt;pre class=&quot;brush: javascript; gutter: true&quot;&gt;var data = [{&#39;品名&#39;: &#39;杜雷斯&#39;, &#39;数量&#39;: 4}, {&#39;品名&#39;: &#39;冈本&#39;, &#39;数量&#39;: 3}];
console.table(data);&lt;/pre&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/7833b24728025a3efdc85e63209ffc5d.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;另外，&lt;code&gt;console.log()&lt;/code&gt; 接收不定参数，参数间用逗号分隔，最终会输出会将它们以空白字符连接。&lt;/p&gt;
&lt;pre class=&quot;brush: javascript; gutter: true&quot;&gt;console.log(&#39;%c你好&#39;,&#39;color:red;&#39;,&#39;小明&#39;,&#39;你知道小红被妈妈打了么&#39;);&lt;/pre&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/cb402cd38f1ec0315f42aa9b73c4e9c4.jpg&quot;&gt;&lt;/p&gt;
&lt;p id=&quot;console.assert&quot;&gt;&lt;strong&gt;console.assert&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;当你想代码满足某些条件时才输出信息到控制台，那么你大可不必写&lt;code&gt;if&lt;/code&gt;或者三元表达式来达到目的，&lt;code&gt;cosole.assert&lt;/code&gt;便是这样场景下一种很好的工具，它会先对传入的表达式进行断言，只有表达式为假时才输出相应信息到控制台。&lt;/p&gt;
&lt;pre class=&quot;brush: javascript; gutter: true&quot;&gt;var isDebug=false;
console.assert(isDebug,&#39;开发中的log信息。。。&#39;);&lt;/pre&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/4cadc5180db78fb6c85d657ffed27680.jpg&quot;&gt;&lt;/p&gt;
&lt;p id=&quot;console.count&quot;&gt;&lt;strong&gt;console.count&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;除了条件输出的场景，还有常见的场景是计数。&lt;br&gt;
当你想统计某段代码执行了多少次时也大可不必自己去写相关逻辑，内置的&lt;code&gt;console.count&lt;/code&gt;可以很地胜任这样的任务。&lt;/p&gt;
&lt;pre class=&quot;brush: javascript; gutter: true&quot;&gt;function foo(){
	//其他函数逻辑blah blah。。。
	console.count(&#39;foo 被执行的次数：&#39;);
}
foo();
foo();
foo();&lt;/pre&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/a93f97fb513473387ce84829106a7cdc.jpg&quot;&gt;&lt;/p&gt;
&lt;p id=&quot;console.dir&quot;&gt;&lt;strong&gt;console.dir&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;将DOM结点以JavaScript对象的形式输出到控制台&lt;br&gt;
而&lt;code&gt;console.log&lt;/code&gt;是直接将该DOM结点以DOM树的结构进行输出，与在元素审查时看到的结构是一致的。不同的展现形式，同样的优雅，各种体位任君选择反正就是方便与体贴。&lt;/p&gt;
&lt;pre class=&quot;brush: javascript; gutter: true&quot;&gt;console.dir(document.body);
console.log(document.body);&lt;/pre&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/f28db380ab154d439975e8434a7f8f5c.jpg&quot;&gt;&lt;/p&gt;
&lt;p id=&quot;console.time-console.timeend&quot;&gt;&lt;strong&gt;console.time &amp;amp; console.timeEnd&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;输出一些调试信息是控制台最常用的功能，当然，它的功能远不止于此。当做一些性能测试时，同样可以在这里很方便地进行。&lt;br&gt;
比如需要考量一段代码执行的耗时情况时，可以用&lt;code&gt;console.time&lt;/code&gt;与 &lt;code&gt;console.timeEnd&lt;/code&gt;来做此事。&lt;/p&gt;
&lt;p&gt;这里借用官方文档的例子：&lt;/p&gt;
&lt;pre class=&quot;brush: javascript; gutter: true&quot;&gt;console.time(&quot;Array initialize&quot;);
var array= new Array(1000000);
for (var i = array.length - 1; i &amp;gt;= 0; i--) {
    array[i] = new Object();
};
console.timeEnd(&quot;Array initialize&quot;);&lt;/pre&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/8121c10ec764ef115b42c701c3c93c9c.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;当然，我们也可以选择自己写代码来计时：&lt;/p&gt;
&lt;pre class=&quot;brush: javascript; gutter: true&quot;&gt;var start=new Date().getTime();
var array= new Array(1000000);
for (var i = array.length - 1; i &amp;gt;= 0; i--) {
    array[i] = new Object();
};
console.log(new Date().getTime()-start);&lt;/pre&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/5d660a0afb15c9d3bd7aa2b4e9b53714.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;相信你也看到了，用内置的&lt;code&gt;console.time&lt;/code&gt;是多么地方便，省去了自己写代码来计算的工作量。另外值得一提的是，通过调用内置的&lt;code&gt;console.time&lt;/code&gt;得到的结果要比自己手动计算的时间差更精确可靠。&lt;/p&gt;
&lt;p id=&quot;console.profile-console.timelime&quot;&gt;&lt;strong&gt;console.profile &amp;amp; console.timeLime&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;当想要查看CPU使用相关的信息时，可以使用&lt;code&gt;console.profile&lt;/code&gt;配合 &lt;code&gt;console.profileEnd&lt;/code&gt;来完成这个需求。&lt;br&gt;
这一功能可以通过UI界面来完成，Chrome 开发者工具里面有个tab便是&lt;code&gt;Profile&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;与此类似的功能还有&lt;code&gt;console.timeLine&lt;/code&gt;配合 &lt;code&gt;console.timeLineEnd&lt;/code&gt;,它的作用是开始记录一段时间轴，同样可以通过Chrome开发者工具里的&lt;code&gt;Timeline&lt;/code&gt; 标签来进行相应操作。&lt;/p&gt;
&lt;p&gt;所以在我看来这两个方法有点鸡肋，因为都可以通过操作界面来完成。但至少他提供了一种命令行方式的交互，还是多了种姿势供选择吧。&lt;/p&gt;
&lt;p id=&quot;console.trace&quot;&gt;&lt;strong&gt;console.trace&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;堆栈跟踪相关的调试可以使用&lt;code&gt;console.trace&lt;/code&gt;。这个同样可以通过UI界面完成。当代码被打断点后，可以在&lt;code&gt;Call Stack&lt;/code&gt;面板中查看相关堆栈信息。&lt;/p&gt;
&lt;p&gt;上面介绍的都是挂在&lt;code&gt;window.console&lt;/code&gt;这个对象下面的方法，统称为&lt;a href=&quot;https://developer.chrome.com/devtools/docs/console-api&quot; target=&quot;_blank&quot;&gt;Console API&lt;/a&gt;，接下来的这些方法确切地说应该叫命令，是Chrome内置提供，在控制台中使用的，他们统称为&lt;a href=&quot;https://developer.chrome.com/devtools/docs/commandline-api&quot; target=&quot;_blank&quot;&gt;Command Line API&lt;/a&gt;。&lt;/p&gt;
&lt;h2 id=&quot;section&quot;&gt;$&lt;/h2&gt;
&lt;p&gt;似乎美刀总是被程序员及各种编程语言所青睐「你看看PHP代码就知道PHPer有多爱钱了」，在Chrome的控制台里，$用处还真是蛮多且方便的。&lt;br&gt;
&lt;code&gt;$_&lt;/code&gt;命令返回最近一次表达式执行的结果，功能跟按向上的方向键再回车是一样的，但它可以做为一个变量使用在你接下来的表达式中：&lt;/p&gt;
&lt;pre class=&quot;brush: javascript; gutter: true&quot;&gt;2+2//回车，再
$_+1//回车得5&lt;/pre&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/74aa2bd9c74b45ed3921afda86b9929e.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;上面的&lt;code&gt;$_&lt;/code&gt;需要领悟其奥义才能使用得当，而$0~$4则代表了最近5个你选择过的DOM节点。&lt;br&gt;
什么意思？在页面右击选择&lt;code&gt;审查元素&lt;/code&gt;，然后在弹出来的DOM结点树上面随便点选，这些被点过的节点会被记录下来，而&lt;code&gt;$0&lt;/code&gt;会返回最近一次点选的DOM结点，以此类推，$1返回的是上上次点选的DOM节点，最多保存了5个，如果不够5个，则返回&lt;code&gt;undefined&lt;/code&gt;。&lt;br&gt;
&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/b2ca526dd9d1de78e020c6f6f454fce6.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;另外值得一赞的是，Chrome 控制台中原生支持类jQuery的选择器，也就是说你可以用&lt;code&gt;$&lt;/code&gt;加上熟悉的css选择器来选择DOM节点，多么滴熟悉。&lt;/p&gt;
&lt;pre class=&quot;brush: javascript; gutter: true&quot;&gt;$(&#39;body&#39;)&lt;/pre&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/8f937d1e44ee23c353d497403bc8d03e.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;$(selector)返回的是满足选择条件的首个DOM元素。&lt;br&gt;
剥去她伪善的外衣，其实&lt;code&gt;$(selector)&lt;/code&gt;是原生JavaScript &lt;code&gt;document.querySelector()&lt;/code&gt; 的封装。&lt;br&gt;
同时另一个命令 &lt;code&gt;$ $(selector) &lt;/code&gt;返回的是所有满足选择条件的元素的一个集合，是对&lt;code&gt;document.querySelectorAll()&lt;/code&gt; 的封装。&lt;/p&gt;
&lt;pre class=&quot;brush: text; gutter: true&quot;&gt;$$(&#39;div&#39;)&lt;/pre&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/e198b36215ca4a9ee9edc0dd44df8737.jpg&quot;&gt;&lt;/p&gt;
&lt;p id=&quot;copy&quot;&gt;&lt;strong&gt;copy&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;通过此命令可以将在控制台获取到的内容复制到剪贴板。&lt;/p&gt;
&lt;pre class=&quot;brush: javascript; gutter: true&quot;&gt;copy(document.body)&lt;/pre&gt;
&lt;p&gt;然后你就可以到处粘了：&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/ff779f3da420d243ef6b5a3771addfe5.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;看完此条命令行，机智的你是不是跟脑洞全开的我一样，冒出了这样一个想法：那就是通过这个命令可以在JavaScript里进行复制操作从而不用依赖Flash插件了。&lt;br&gt;
But现实是残酷的，如之前所述的，这里的控制台命令只能在控制台中环境中执行，因为他不依附于任何全局变量比如&lt;code&gt;window&lt;/code&gt;，所以其实在JS代码里是访问不了这个&lt;code&gt;copy&lt;/code&gt;方法的，所以从代码层面来调用复制功能也就无从谈起。但愿有天浏览器会提供相应的JS实现吧~&lt;/p&gt;
&lt;p id=&quot;keys-values&quot;&gt;&lt;strong&gt;keys &amp;amp; values&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;这是一对基友。前者返回传入对象所有属性名组成的数据，后者返回所有属性值组成的数组。具体请看下面的例子：&lt;/p&gt;
&lt;pre class=&quot;brush: javascript; gutter: true&quot;&gt;var tboy={name:&#39;wayou&#39;,gender:&#39;unknown&#39;,hobby:&#39;opposite to the gender&#39;};
keys(tboy);
values(tboy);&lt;/pre&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/8aaa34355859977d6b3a9d7650d45e4e.jpg&quot;&gt;&lt;/p&gt;
&lt;p id=&quot;monitor-unmonitor&quot;&gt;&lt;strong&gt;monitor &amp;amp; unmonitor&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;monitor(function)，它接收一个函数名作为参数，比如&lt;code&gt;function a&lt;/code&gt;,每次&lt;code&gt;a&lt;/code&gt;被执行了，都会在控制台输出一条信息，里面包含了函数的名称&lt;code&gt;a&lt;/code&gt;及执行时所传入的参数。&lt;/p&gt;
&lt;p&gt;而unmonitor(function)便是用来停止这一监听。&lt;/p&gt;
&lt;pre class=&quot;brush: javascript; gutter: true&quot;&gt;function sayHello(name){
	alert(&#39;hello,&#39;+name);
}
monitor(sayHello);
sayHello(&#39;wayou&#39;);
unmonitor(sayHello);
sayHello(&#39;wayou&#39;);&lt;/pre&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/ee020e40525748c7e11632da5d96140e.jpg&quot;&gt;&lt;/p&gt;
&lt;p id=&quot;debug-undebug&quot;&gt;&lt;strong&gt;debug &amp;amp; undebug&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;debug同样也是接收一个函数名作为参数。当该函数执行时自动断下来以供调试，类似于在该函数的入口处打了个断点，可以通过debugger来做到，同时也可以通过在Chrome开发者工具里找到相应源码然后手动打断点。&lt;br&gt;
而&lt;code&gt;undebug&lt;/code&gt; 则是解除该断点。&lt;/p&gt;
&lt;p&gt;而其他还有好些命令则让人没有说的欲望，因为好些都可以通过Chrome开发者工具的UI界面来操作并且比用在控制台输入要方便。&lt;/p&gt;


&lt;!-- div id=&quot;ad1&quot;&gt;
&lt;/div --&gt;


	


	

</description>
        <pubDate>Mon, 15 Sep 2014 00:00:00 +0800</pubDate>
        <link>http://iftti.com/posts/2014/2014-09-15-76985-0aa560f7c.html</link>
        <guid isPermaLink="true">http://iftti.com/posts/2014/2014-09-15-76985-0aa560f7c.html</guid>
        
        
        <category>jobbole</category>
        
      </item>
    
      <item>
        <title>Git工作流指南</title>
        <description>

					
		
&lt;!-- div id=&quot;ad1&quot;&gt;
&lt;script type=&quot;text/javascript&quot;&gt;
google_ad_client = &quot;ca-pub-7056282119617872&quot;;
google_ad_slot = &quot;6645040531&quot;;
google_ad_width = 300;
google_ad_height = 250;
&lt;/script&gt;
&lt;script type=&quot;text/javascript&quot;
src=&quot;http://pagead2.googlesyndication.com/pagead/show_ads.js&quot;&gt;
&lt;/script&gt;
&lt;/div&gt;
&lt;br/ --&gt;

&lt;h3&gt;译序&lt;/h3&gt;
&lt;p&gt;工作流其实不是一个初级主题，背后的本质问题其实是有效的项目流程管理和高效的开发协同约定，不仅是&lt;code&gt;Git&lt;/code&gt;或&lt;code&gt;SVN&lt;/code&gt;等&lt;code&gt;SCM&lt;/code&gt;工具的使用。&lt;/p&gt;
&lt;p&gt;这篇指南以大家在&lt;code&gt;SVN&lt;/code&gt;中已经广为熟悉使用的集中式工作流作为起点，循序渐进地演进到其它高效的分布式工作流，还介绍了如何配合使用便利的&lt;code&gt;Pull Request&lt;/code&gt;功能，体系地讲解了各种工作流的应用。&lt;/p&gt;
&lt;p&gt;行文中实践原则和操作示例并重，对于&lt;code&gt;Git&lt;/code&gt;的资深玩家可以梳理思考提升，而新接触的同学，也可以跟着step-by-step操作来操练学习并在实际工作中上手使用。&lt;/p&gt;
&lt;p&gt;关于&lt;code&gt;Git&lt;/code&gt;工作流主题，网上体系的中文资料不多，主要是零散的操作说明，希望这篇文章能让你更深入理解并在工作中灵活有效地使用起来。&lt;/p&gt;
&lt;p&gt;PS：&lt;br&gt;
文中&lt;code&gt;Pull Request&lt;/code&gt;的介绍用的是&lt;code&gt;Bitbucket&lt;/code&gt;代码托管服务，由于和&lt;code&gt;GitHub&lt;/code&gt;基本一样，如果你用的是&lt;code&gt;GitHub&lt;/code&gt;（我自己也主要使用&lt;code&gt;GitHub&lt;/code&gt;托管代码），不影响理解和操作。&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://weibo.com/oldratlee&quot;&gt;自己&lt;/a&gt;理解粗浅，译文源码在&lt;a href=&quot;https://github.com/quickhack/translations/tree/master/git-workflows-and-tutorials&quot;&gt;&lt;code&gt;GitHub&lt;/code&gt;上&lt;/a&gt;，翻译中不足和不对之处，欢迎建议（&lt;a href=&quot;https://github.com/quickhack/translations/issues&quot;&gt;提交Issue&lt;/a&gt;）和指正（&lt;a href=&quot;https://github.com/quickhack/translations/fork&quot;&gt;Fork后提交代码&lt;/a&gt;）！&lt;/p&gt;
&lt;h1&gt;
&lt;code&gt;Git&lt;/code&gt;工作流指南&lt;/h1&gt;
&lt;p&gt;工作流有各式各样的用法，但也正因此使得在实际工作中如何上手使用变得很头大。这篇指南通过总览公司团队中最常用的几种&lt;code&gt;Git&lt;/code&gt;工作流让大家可以上手使用。&lt;/p&gt;
&lt;p&gt;在阅读的过程中请记住，本文中的几种工作流是作为方案指导而不是条例规定。在展示了各种工作流可能的用法后，你可以从不同的工作流中挑选或揉合出一个满足你自己需求的工作流。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/images/jobbole.com/999470a3a4116e34d5427dc6bdf0cbf0.jpg&quot; alt=&quot;Git Workflows&quot;&gt;&lt;/p&gt;
&lt;h2&gt;概述&lt;/h2&gt;
&lt;h3&gt;集中式工作流&lt;/h3&gt;
&lt;p&gt;如果你的开发团队成员已经很熟悉&lt;code&gt;Subversion&lt;/code&gt;，集中式工作流让你无需去适应一个全新流程就可以体验&lt;code&gt;Git&lt;/code&gt;带来的收益。这个工作流也可以作为向更&lt;code&gt;Git&lt;/code&gt;风格工作流迁移的友好过渡。&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://blog.jobbole.com/76847/&quot;&gt;了解更多 »&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/images/jobbole.com/13e406eae8561ee406f34639b631d0fd.jpg&quot; alt=&quot;Git Workflows: SVN-style&quot;&gt;&lt;/p&gt;
&lt;h3&gt;功能分支工作流&lt;/h3&gt;
&lt;p&gt;功能分支工作流以集中式工作流为基础，不同的是为各个新功能分配一个专门的分支来开发。这样可以在把新功能集成到正式项目前，用&lt;code&gt;Pull Requests&lt;/code&gt;的方式讨论变更。&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://blog.jobbole.com/76857/&quot;&gt;了解更多 »&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/images/jobbole.com/21c6e654d3850b1caa45473dcb550d6b.jpg&quot; alt=&quot;Git Workflows: Feature Branch&quot;&gt;&lt;/p&gt;
&lt;h3&gt;
&lt;code&gt;Gitflow&lt;/code&gt;工作流&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;Gitflow&lt;/code&gt;工作流通过为功能开发、发布准备和维护分配独立的分支，让发布迭代过程更流畅。严格的分支模型也为大型项目提供了一些非常必要的结构。&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://blog.jobbole.com/76867/&quot;&gt;了解更多 »&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/images/jobbole.com/11d5a90aefab6129bb07c97b9abb7378.jpg&quot; alt=&quot;Git Workflows: Gitflow Cycle&quot;&gt;&lt;/p&gt;
&lt;h3&gt;
&lt;code&gt;Forking&lt;/code&gt;工作流&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;Forking&lt;/code&gt;工作流是分布式工作流，充分利用了&lt;code&gt;Git&lt;/code&gt;在分支和克隆上的优势。可以安全可靠地管理大团队的开发者（&lt;code&gt;developer&lt;/code&gt;），并能接受不信任贡献者（&lt;code&gt;contributor&lt;/code&gt;）的提交。&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://blog.jobbole.com/76861/&quot;&gt;了解更多 »&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/images/jobbole.com/dea8475fb42b1b660538152beebdb833.jpg&quot; alt=&quot;Git Workflows: Forking&quot;&gt;&lt;/p&gt;
&lt;h3&gt;&lt;code&gt;Pull Requests&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;Pull requests&lt;/code&gt;是&lt;code&gt;Bitbucket&lt;/code&gt;提供的让开发者更方便地进行协作的功能，提供了友好的&lt;code&gt;Web&lt;/code&gt;界面可以在提议的修改合并到正式项目之前对修改进行讨论。&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://blog.jobbole.com/76854/&quot;&gt;了解更多 »&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/images/jobbole.com/bc5080dfa0f5a2e57f2d8b2551ca08fa.jpg&quot; alt=&quot;Workflows: Pull Requests&quot;&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;　　　　　　　　&lt;a href=&quot;http://blog.jobbole.com/76847/&quot;&gt;集中式工作流 »&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;译注&lt;/em&gt;&lt;/strong&gt;&lt;br&gt;
&lt;a href=&quot;http://weibo.com/oldratlee&quot;&gt;自己&lt;/a&gt;理解粗浅，译文源码在&lt;a href=&quot;https://github.com/quickhack/translations/tree/master/git-workflows-and-tutorials&quot;&gt;&lt;code&gt;GitHub&lt;/code&gt;上&lt;/a&gt;，翻译中不足和不对之处，欢迎建议（&lt;a href=&quot;https://github.com/quickhack/translations/issues&quot;&gt;提交Issue&lt;/a&gt;）和指正（&lt;a href=&quot;https://github.com/quickhack/translations/fork&quot;&gt;Fork后提交代码&lt;/a&gt;）！&lt;/p&gt;


&lt;!-- div id=&quot;ad1&quot;&gt;
&lt;/div --&gt;


	


	

</description>
        <pubDate>Sun, 14 Sep 2014 00:00:00 +0800</pubDate>
        <link>http://iftti.com/posts/2014/2014-09-14-76843-9b68e8736.html</link>
        <guid isPermaLink="true">http://iftti.com/posts/2014/2014-09-14-76843-9b68e8736.html</guid>
        
        
        <category>jobbole</category>
        
      </item>
    
      <item>
        <title>适合码农工作时玩的游戏：Scrum</title>
        <description>
&lt;p&gt;&lt;img src=&quot;/images/devtang.com/ac3545959ccea6d9abb3418a92b5b607.jpg&quot;&gt;&lt;/p&gt;

&lt;h1&gt;前言&lt;/h1&gt;

&lt;p&gt;昨天遇到一个来自微软的面试者，在面试的最后，我简单介绍了一下我们团队使用一周一次的Scrum来做项目管理。他回答说：”我在微软也用Scrum，不过我们一周两次，时间在周二和周四上午，每次15分钟“。我听了就笑了，我说：“同学，你说的这个应该是Scrum的站立会议，Scrum实际上有4个会议，站立会议只是其中一个。另外，标准的站立会议应该每天一次，不是每周两次。”接着我给他介绍了Scrum的4个会议，每个会议的意义是什么，他若有所思。&lt;/p&gt;

&lt;p&gt;今天和同事吃饭说起这件事情，同事pw说：在他所了解到的使用Scrum的公司里面，我们应该是执行Scrum做得最规范的，同时我们从Scrum实践中，收获了非常多。&lt;/p&gt;

&lt;p&gt;大约在3年前（当时我们团队还在网易），我们团队开始尝试用Scrum来进行软件开发的项目管理。在经过了3年的摸索和调整后，我们不但多次保证了项目的顺利上线，而且建立起了适合自己团队的工作方式。&lt;/p&gt;

&lt;p&gt;正如Scrum官方指南所说，“Scrum是易于理解，但难以精通的”，在此向大家分享我们实践的心得体会，希望更多的开发团队能够运用Scrum来流化自己的开发流程。&lt;/p&gt;

&lt;h1&gt;Scrum是游戏规则&lt;/h1&gt;

&lt;p&gt;在&lt;a href=&quot;https://www.scrum.org/Scrum-Guide&quot;&gt;Scrum官方网站&lt;/a&gt;上，提供了中文版本的&lt;a href=&quot;https://www.scrum.org/Portals/0/Documents/Scrum%20Guides/2013/Scrum-Guide-CN.pdf#zoom=100&quot;&gt;《Scrum指南》&lt;/a&gt;，这份只有14页的文档的封面上，写下了其最核心原则：游戏规则。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/devtang.com/f36e7670b3f9eeafc833e0ee003b78cc.jpg&quot;&gt;&lt;/p&gt;

&lt;p&gt;什么是游戏规则？游戏规则是玩游戏的人为了更好地娱乐而制定的规则。所以Scrum的规则是为了让大家更开心，更有效地工作，而不是约束大家。事实上由于Scrum只是一个框架，所以在实践Scrum时，更多的规则需要团队成员共同制定，这更加体现了游戏规则的思想——大家自己制定的规则，必定是得到大家一致同意的、能让大家舒服工作的规则。&lt;/p&gt;

&lt;h1&gt;Scrum是基于经验的&lt;/h1&gt;

&lt;p&gt;Scrum强调经验的重要性，但是经验又是需要不断调整的，所以Scrum通过迭代增量的开发方式，来每次调整整个团队的经验，从而来优化可预测性。&lt;/p&gt;

&lt;p&gt;例如，我们在开发猿题库时，在每轮Scrum的结束时，我们会开回顾会议，将大家每天处理待办事项的速度（我们称做日均Story Point）总结在Wiki中，如下图所示。这样当我们估计一个新一轮的迭代工作是否能够完成时，就可以参考前面几十次的经验，做出更加理性地判断。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/devtang.com/21266d4f76795f6e8c2a2f639c3a26c5.jpg&quot;&gt;&lt;/p&gt;

&lt;h1&gt;Scrum的三大支柱&lt;/h1&gt;

&lt;p&gt;&lt;img src=&quot;/images/devtang.com/a6460ae3f5ac875b4bb168b36e0c6e27.jpg&quot;&gt;&lt;/p&gt;

&lt;p&gt;透明性、检视、调整是Scrum的三大支柱。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;透明性是指：团队成员要达到对信息的完全共享，以便对观察到的信息有相同的理解。&lt;/li&gt;
&lt;li&gt;检视是指：团队成员要不停地检查自己的状态，类似汽车的定期检查一样，通过检视了解当前项目的状态。&lt;/li&gt;
&lt;li&gt;调整是指：团队成员发现出现了会影响项目进度的事件后，要及时寻找对策。&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;以上的说法有些学术化，我们可以这样理解：&lt;/p&gt;

&lt;p&gt;群体智商常常会出现低于个体智商的现象，这是因为个体之间的信息通常不一致，每个人的信息都是片面的，所以造成了观点的片面，而通常情况下团队领导由于接受到的信息更全面，所以他的决策考虑会更周到一些。&lt;/p&gt;

&lt;p&gt;但是Scrum又强调团队需要是“自组织”的，这就需要群体进行决策而不是领导。为了群体更好的决策，所以Scrum特别强调信息的透明，这样大家的信息都是充分共享的，而检视是一种保证信息透明的方法，即定期地查看自己和团队的状态，有了信息的透明，这样团队成员就能共同发现项目执行中的问题，进而一起寻找解决办法，从而达到“自组织”的团队。&lt;/p&gt;

&lt;h1&gt;Scrum的基础游戏规则&lt;/h1&gt;

&lt;p&gt;Scrum定义了基础的游戏规则，在基础的游戏规则之上，团队可以依据自己的经验，制定更细致的规则。但更细致的规则不应该违背基础的规则。这就像国家的宪法一样，其它法律不能与宪法违背。&lt;/p&gt;

&lt;p&gt;那我们来看看Scrum有哪些基础的游戏规则。&lt;/p&gt;

&lt;h2&gt;角色定义&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/images/devtang.com/787a539aca009a1d0db5fb98e5c085c4.jpg&quot;&gt;&lt;/p&gt;

&lt;p&gt;玩三国杀的同学都知道，玩之前大家会抽身份：主公、反賊、忠臣、内奸。而Scrum的游戏规则里面，有以下几种身份角色：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;产品负责人：产品负责人是管理产品待办列表的唯一责任人，也是产品最终的责任人。（稍后我们在介绍计划会议时，解释什么是产品待办列表。）简单来说，最终如果产品没做好，应该扣产品负责人的工资。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;开发团队：开发团队是负责将每轮Scrum迭代中计划的功能（可能是产品稿+美术稿的形式），交付成可发布的产品的各种专业人员。这里的各种专业人员包括：服务器端开发、Javascript前端开发、客户端开发、测试人员等。开发团队是真正在玩这个Scrum游戏的人，其他人（例如产品负责人都只是部分参与）。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Scrum Master：Scrum Master类似于杀人游戏中的法官，即游戏组织者。Scrum Master并不是团队的领导，他仅仅是做一些组织工作，而对于一个“自组织”的团队来说，其实真正需要组织的事情也不太多，所以他常常由开发团队中的某一个人兼任。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;h3&gt;没有子团队&lt;/h3&gt;

&lt;p&gt;在Scrum的官方文档中，这样说道：&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Scrum 不认可开发团队中的所谓“子团队”,无论是测试还是业务分析的成员都不能划分为“子团队”。此规则无一例外。&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;所以我们看到，Scrum在定义角色的时候，强调开发团队中一个整体，包含把产品发布出来的所有相关的专业技术人员，并且开发团队共同承担开发的责任，只有这样，大家才能形成利益共同体，共同努力把产品做好。&lt;/p&gt;

&lt;p&gt;这一点也解释了为什么很多大公司玩不好Scrum。拿百度举例，百度的一个项目就有很多“子团队”。在百度，前端开发人员属于前端组，移动端开发人员属于移动端组，测试有专门的QA组，PM也有专门的组。这样的划分，进而造成大家的绩效评估并不是完全由项目执行的好坏来决定，而PM也需要花很大精力去推动大家，这样的团队没有共同的利益，是很难做到“自组织”的。&lt;/p&gt;

&lt;h3&gt;强调平等&lt;/h3&gt;

&lt;p&gt;Scrum中仅定义了“开发团队”这个整体的角色，在“开发团队”内部，大家都是平等的。因为只有这样，大家才能更加自由的共享信息，共同决策，否则决策权仍然掌握在少部分人手里。在Scrum的官方文档中，是这样说的：&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Scrum 不认可开发团队成员的头衔，无论承担哪种工作他们都叫做开发人员。此规则无一例外。&lt;/p&gt;&lt;/blockquote&gt;

&lt;h3&gt;游戏人数规则&lt;/h3&gt;

&lt;p&gt;开发团队还有一个不能不说的特点，就是他的规模必须足够小，因为他强调信息的透明，如果人数过大，光沟通的成本就大到无法承受了，所以官方文档上推荐的人数是 10人以内（不包括产品负责人和Scrum Master，除非他们也参与开发）。&lt;/p&gt;

&lt;p&gt;但是在实际执行中，由于业务的增长，团队人数很容易就超过10人。比如我们猿题库在创业时只有不到10人，现在已经成长到几十人了。这个时候，比较好的做法是进行团队的切分，比如我们试过将猿题库的服务器端和客户端进行拆分，这样保证每个团队还是在10人以内。如果以后再增长，可能客户端会再进行拆分成iOS团队和Android团队。&lt;/p&gt;

&lt;h3&gt;游戏时间&lt;/h3&gt;

&lt;p&gt;Scrum对每一轮的迭代时间并没有严格的规定，但它要求是小于一个月。对于每一轮的迭代，Scrum把它称作Sprint（冲刺）。&lt;/p&gt;

&lt;p&gt;作为创业公司，我们在最近两年都实践着一周一次Sprint的方式来工作。一周一次Sprint能够保证调整足够快，Sprint执行中是不鼓励需求改动的。所以一周一次的Sprint能够做到，对于比较急迫的需求改动，在下次Sprint时（下周）就可以执行。&lt;/p&gt;

&lt;p&gt;一周一次的Sprint也有不少问题，由于偏离本文主题，所以就不展开介绍了。现在我们的猿题库直播课项目组也在尝试进行2周一次的Sprint。总之，Sprint多长是由开发团队根据项目的具体特点来决定的，只要不超过一个月即可。&lt;/p&gt;

&lt;h2&gt;游戏玩法&lt;/h2&gt;

&lt;p&gt;讲了半天，终于讲到核心了，到底怎么玩这个游戏啊！为了更好的理解，我们先看看杀人游戏的玩法，杀人游戏定义了如下几个事件：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;天黑请闭眼，这个时候大家都闭上眼睛&lt;/li&gt;
&lt;li&gt;杀手睁眼，杀手杀人，杀手闭眼&lt;/li&gt;
&lt;li&gt;警察睁眼，警察检查，警察闭眼&lt;/li&gt;
&lt;li&gt;天亮了，宣布谁死了，大家讨论并投票谁是杀手，投出的嫌疑人被杀死。如果警察或杀手死了，宣布游戏结束，否则跳到第1步。&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;刚好，Scrum也定义了4个事件，分别是：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;计划会议&lt;/li&gt;
&lt;li&gt;每日站立会议&lt;/li&gt;
&lt;li&gt;评审会议&lt;/li&gt;
&lt;li&gt;回顾会议&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;以下我们来详细介绍一下这4个会议到底要具体怎么做。&lt;/p&gt;

&lt;h3&gt;计划会议&lt;/h3&gt;

&lt;p&gt;计划会议主要通过讨论，完成两件事情：做什么、怎么做。&lt;/p&gt;

&lt;p&gt;关于“做什么”：产品负责人会给出一个产品待办列表，然后由团队成员来根据预计的工作量以及以往的表现，来挑选接下来的Sprint需要完成的待办项。这里的特点是：由开发团队成员自己来挑选待办项，而不是由传统意义上的Tech Leader或产品负责人来挑选。这样保证了开发任务是由团队成员自己决定的，他更有责任心把事情完成。同时作为产品负责人，有必要非常明确地告诉开发团队每一个待办项的意义和重要性，这样开发团队才能做出有利于产品的挑选工作。&lt;/p&gt;

&lt;p&gt;关于“怎么做”：开发团队从待办列表中挑选完需要完成的待办项之后，就需要对每个要做的待办项进行评估。评估的工作就是讨论具体怎么做，这包括技术架构、实现细节的讨论。只有讨论得非常清楚之后，这项工作的工作量才会比较清楚。&lt;/p&gt;

&lt;p&gt;在讨论怎么做之后，一些敏捷公司推荐使用“出牌”的方式来评估工作量，我们也采用了这种方式，我们还专门做了一套Scrum扑克，用于出牌。如下图所示：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/devtang.com/0a67592fc1aeaaee160b6e72166be585.jpg&quot;&gt;&lt;/p&gt;

&lt;p&gt;出牌的规则是每个人出一张牌，用牌上的数字表示当前工作的工作量。通常大家还会事先约定好数字2代表的工作量，以保证大家的标准相同。为了避免相互影响，大家先把要出的牌扣着，然后同时翻开。之后，出最高分的和出最低分的同学要表达意见，说明为什么自己估计成这样，大家讨论，这样的过程可以保证大家的信息都是透明的，即没有忽略掉的技术实现难度或细节，在信息充分共享的情况下，通常大家第二次出牌时就可以达成一致了。&lt;/p&gt;

&lt;h3&gt;每日站立会议&lt;/h3&gt;

&lt;p&gt;每日站立会议是进行检视的方法。通常选择固定时间（我们是每天早上10点10分开），以养成团队工作习惯来避免组织成本。站立会议要尽量的短，通常控制在15分钟以内，选择站着开会，也是让大家有更大的预期快速结束。&lt;/p&gt;

&lt;p&gt;站立会议主要是为了沟通，以及发现潜在可能的问题，在站立会议上，团队成员每个人要讲3句话：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;我昨天做了什么&lt;/li&gt;
&lt;li&gt;我今天打算做什么&lt;/li&gt;
&lt;li&gt;我遇到了什么问题&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;通过这3句话来达到高效沟通的目的，对于会上提到的问题，通常是下来相关人员自行解决。&lt;/p&gt;

&lt;p&gt;站立会议通常能够发现项目进展的状态是否顺利，从而尽早采取相应的措施。时间较长的Sprint可以配合燃尽图，更方便地审视项目进展速度。&lt;/p&gt;

&lt;h3&gt;评审会议&lt;/h3&gt;

&lt;p&gt;Sprint 评审会议在 Sprint 结束时举行，用于检查计划中的工作，哪些完成了，哪些没有完成。在我们的实践中，我们会让开发的同事演示自己所做的功能，然后PM会看这个功能是否达到了要求。&lt;/p&gt;

&lt;h3&gt;回顾会议&lt;/h3&gt;

&lt;p&gt;回顾会议是开发团队检视自己，发现团队运转中的问题，并且定制游戏规则的过程。通过对前一个Sprint中的人、关系、过程、工具进行检视，团队成员能够总结出做得好的，和做得不好的。进而制定一个改进的方案。&lt;/p&gt;

&lt;p&gt;回顾会议是Scrum创建“自组织”团队的关键，它将团队自我改进变成了一个例行的会议，在这个会议中，讨论的都是大家对该游戏的感受，包括好的和不好的，最终大家为了玩得更爽，就会发扬好的，努力避免不好的，成为一个能够自我进化的集体。&lt;/p&gt;

&lt;p&gt;需要注意的是，回顾会议不应该成为吐槽大会，大家应该本着发现问题，解决问题的态度来讨论。例如：如果在回顾会议仅仅是抱怨产品老是改需求，或者抱怨时间不够，而不提出解决方案的话，是非常不好的。&lt;/p&gt;

&lt;p&gt;提出问题是容易的，麻烦的是提出解决方案。我们的老大郭常圳提出了一个办法，即我们思考：“如果再来一次，我们能不能做得更好”？如果我们发现，如果再来一次，由于客观原则，我们可能仍然无法避免同样的问题，那么我们就选择坦然接受而不是抱怨。&lt;/p&gt;

&lt;p&gt;因为很多时候本来就没有完美的、没有任何问题的解决方案，这就像软件都有Bug一样，如果Bug不可避免，我们就选择发现的时候尽量修复而不是编码的时候避免。&lt;/p&gt;

&lt;h3&gt;框架图&lt;/h3&gt;

&lt;p&gt;下图介绍了Scrum的整个框架：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/devtang.com/cd095ee776cea67c25f10846160db4a2.jpg&quot;&gt;&lt;/p&gt;

&lt;h2&gt;一些问题&lt;/h2&gt;

&lt;h3&gt;有什么辅助Scrum的工具？&lt;/h3&gt;

&lt;p&gt;我们使用的是Redmine的Scrum插件来开相关的Scrum会议。我们Scrum的回顾会议总结放在内部的Wiki上。也有团队喜欢直接用白板+便签来完成Scrum的相关会议。像JIRA一类的专业项目管理软件，也都支持Scrum。&lt;/p&gt;

&lt;h3&gt;游戏超时怎么办？&lt;/h3&gt;

&lt;p&gt;游戏超时通常就意味着游戏结束。在Scrum这个游戏中，团队成员不接受Sprint延期。所以不管有没有完成所有任务，评审会议和回顾会议都需要按时开，没有完成的任务需要进行仔细讨论，分析其原因到底是什么，从而在下一轮Sprint中尽量避免出现同样的问题。&lt;/p&gt;

&lt;h3&gt;开发团队自己挑任务，会不会造成项目进度很慢？&lt;/h3&gt;

&lt;p&gt;通常情况下不会。如果我们真正把Scrum做好，大家能享受到Scrum带来的各种好处，例如团队每个人都能参与决策团队做事方式，每个人都能积极的追求效率，而一次次成功的Scrum，带给大家的成就感也是巨大的。&lt;/p&gt;

&lt;p&gt;好的Scrum执行还能保证团队不会随意加班，我们已经很久没有周末加班了，平时晚上大部分时间也都能做到按时下班，这对于互联网公司来说，几乎是不可想像的。&lt;/p&gt;

&lt;p&gt;不加班只是一个附属品，最重要的是按时发布产品，我们创业2年多来从来没有延期发布过产品。这样使得我们的运营推广计划能够非常有序地执行。&lt;/p&gt;

&lt;p&gt;需要强调的是，不加班并不是代表我们的工作轻松，通常情况下我们的Scrum安排还是比较紧张的，因为我们都想创业时跑得快一些。不加班也不是我们的原则，我们的原则是按时发布产品，所以当有一些特殊情况产生时，我们也会适当的加班。我们只是不把加班当作一个常态的工作方式，因为我们认为工作效率比工作时长更为重要。另一方面我们认为创业是长跑，保持良好的发布节奏已经非常好了，长期加班造成的身体懈怠可能会造成工作效率的损失。&lt;/p&gt;

&lt;h3&gt;Scrum适合所有团队吗？&lt;/h3&gt;

&lt;p&gt;首先Scrum是非常适合程序员的，因为程序员天生就不喜欢约束。Scrum的“自组织”团队的思想很容易让程序员感觉到自己是团队的主人。另外Scrum是非常反会议的，4个会议都严格地规定了时间长度，所以可以让程序员有充足的时间花在编码上。Scrum也是比较反需求临时变更的，由于Sprint周期短（我们才一周），所以变更可以根据重要程度放到下一个Sprint中。&lt;/p&gt;

&lt;p&gt;Scrum非常强调团队作为一个整体来做事情，所以并没有刻意地去评估每个人具体的工作量。这需要团队每个人都比较自觉。当然，由于强调透明和检视，所以团队内如果有人懈怠的话，团队里其他人是很容易发现的。&lt;/p&gt;

&lt;p&gt;所以，如果你的团队人数在10人左右，又能保证团队是一个整体为项目负责，那就有了尝试Scrum的基础。&lt;/p&gt;

&lt;h3&gt;为什么很多公司用不好Scrum？&lt;/h3&gt;

&lt;p&gt;Scrum指南里面也提到，Scrum是“易于学习，难于精通的”。所以Scrum本来就比较难做好。我感觉到几个比较容易出现的问题是：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;团队里面有人不信Scrum能比以前的软件开发方式更好。游戏规则使终是游戏规则，如果有人不想玩游戏的话，游戏玩起来就没有那么愉快了。真正想做好Scrum就得认真学习Scrum指南，然后努力遵守Scrum的规则。只有当大家都努力玩这个游戏时，才能享受游戏的乐趣。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;随意更改Scrum的规则。例如我以前在有道的团队就把Scrum的每日站会改成了每周二，周四开一个坐会，开会的方式也变成产品经理询问进度，各个技术人员汇报的方式，会议一次要开半个多小时。这一下子就把每日站会做得变味了。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;难以组建团队。之前说过像百度这类大公司，其公司文化不是一朝一夕形成的。Scrum的工作方式要求大家都为项目完全负责，而很多传统公司按职能来划分团队，例如PM团队、客户端团队、前端团队等，这会影响Scrum的执行。&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;


&lt;h3&gt;Scrum是终极大招吗&lt;/h3&gt;

&lt;p&gt;Scrum不是银弹，它并不能解决所有问题，实际上，很多时候它根本不提供解决问题的方法。Scrum本身只是一个框架，通过这个框架，我们更容易发现项目运行中的问题，通过定期的回顾会议来解决问题。&lt;/p&gt;

&lt;h1&gt;结束语&lt;/h1&gt;

&lt;p&gt;本文旨在通过介绍Scrum的核心思想和基本框架，吸引大家了解Scrum。要实践Scrum，还是需要进一步的学习才行。欢迎大家详细阅读&lt;a href=&quot;https://www.scrum.org/Portals/0/Documents/Scrum%20Guides/2013/Scrum-Guide-CN.pdf#zoom=100&quot;&gt;《Scrum指南》&lt;/a&gt;，然后尝试使用Scrum来让自己每天的工作变得轻松愉快。&lt;/p&gt;

&lt;p&gt;PS：我们的公司猿题库创业两年，做在线教育方向，不久前顺利拿到了1500万美元的C轮融资。我们现在很缺人，也欢迎大家加入我们，和我们一起玩Scrum游戏，感兴趣的可以看：&lt;a href=&quot;http://www.yuantiku.com/campus/&quot;&gt;职位介绍&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;愿大家玩得开心～&lt;/p&gt;

</description>
        <pubDate>Sat, 13 Sep 2014 00:00:00 +0800</pubDate>
        <link>http://iftti.com/posts/2014/2014-09-13-scrum-introduction-3e0aca621.html</link>
        <guid isPermaLink="true">http://iftti.com/posts/2014/2014-09-13-scrum-introduction-3e0aca621.html</guid>
        
        
        <category>devtang</category>
        
      </item>
    
      <item>
        <title>Git工作流指南：Gitflow工作流</title>
        <description>

					
		
&lt;!-- div id=&quot;ad1&quot;&gt;
&lt;script type=&quot;text/javascript&quot;&gt;
google_ad_client = &quot;ca-pub-7056282119617872&quot;;
google_ad_slot = &quot;6645040531&quot;;
google_ad_width = 300;
google_ad_height = 250;
&lt;/script&gt;
&lt;script type=&quot;text/javascript&quot;
src=&quot;http://pagead2.googlesyndication.com/pagead/show_ads.js&quot;&gt;
&lt;/script&gt;
&lt;/div&gt;
&lt;br/ --&gt;

&lt;p&gt;&lt;img alt=&quot;Git Workflows: Gitflow Cycle&quot; src=&quot;/images/jobbole.com/11d5a90aefab6129bb07c97b9abb7378.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;这节介绍的&lt;a href=&quot;http://nvie.com/posts/a-successful-git-branching-model/&quot;&gt;&lt;code&gt;Gitflow&lt;/code&gt;工作流&lt;/a&gt;借鉴自在&lt;a href=&quot;http://nvie.com/&quot;&gt;nvie&lt;/a&gt;的&lt;em&gt;Vincent Driessen&lt;/em&gt;。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Gitflow&lt;/code&gt;工作流定义了一个围绕项目发布的严格分支模型。虽然比&lt;a href=&quot;http://blog.jobbole.com/76857/&quot;&gt;功能分支工作流&lt;/a&gt;复杂几分，但提供了用于一个健壮的用于管理大型项目的框架。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Gitflow&lt;/code&gt;工作流没有用超出功能分支工作流的概念和命令，而是为不同的分支分配一个很明确的角色，并定义分支之间如何和什么时候进行交互。除了使用功能分支，在做准备、维护和记录发布也使用各自的分支。当然你可以用上功能分支工作流所有的好处：&lt;code&gt;Pull Requests&lt;/code&gt;、隔离实验性开发和更高效的协作。&lt;/p&gt;
&lt;h2&gt;工作方式&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;Gitflow&lt;/code&gt;工作流仍然用中央仓库作为所有开发者的交互中心。和其它的工作流一样，开发者在本地工作并&lt;code&gt;push&lt;/code&gt;分支到要中央仓库中。&lt;/p&gt;
&lt;h3&gt;历史分支&lt;/h3&gt;
&lt;p&gt;相对使用仅有的一个&lt;code&gt;master&lt;/code&gt;分支，&lt;code&gt;Gitflow&lt;/code&gt;工作流使用2个分支来记录项目的历史。&lt;code&gt;master&lt;/code&gt;分支存储了正式发布的历史，而&lt;code&gt;develop&lt;/code&gt;分支作为功能的集成分支。这样也方便&lt;code&gt;master&lt;/code&gt;分支上的所有提交分配一个版本号。&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/384db8bc89bb61260643861ee7d81a68.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;剩下要说明的问题围绕着这2个分支的区别展开。&lt;/p&gt;
&lt;h3&gt;功能分支&lt;/h3&gt;
&lt;p&gt;每个新功能位于一个自己的分支，这样可以&lt;a href=&quot;https://www.atlassian.com/git/tutorial/remote-repositories#!push&quot;&gt;&lt;code&gt;push&lt;/code&gt;到中央仓库以备份和协作&lt;/a&gt;。但功能分支不是从&lt;code&gt;master&lt;/code&gt;分支上拉出新分支，而是使用&lt;code&gt;develop&lt;/code&gt;分支作为父分支。当新功能完成时，&lt;a href=&quot;https://www.atlassian.com/git/tutorial/git-branches#!merge&quot;&gt;合并回&lt;code&gt;develop&lt;/code&gt;分支&lt;/a&gt;。新功能提交应该从不直接与&lt;code&gt;master&lt;/code&gt;分支交互。&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/6f6009c65d3b29ab4bd6dbe8daf98680.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;注意，从各种含义和目的上来看，功能分支加上&lt;code&gt;develop&lt;/code&gt;分支就是功能分支工作流的用法。但&lt;code&gt;Gitflow&lt;/code&gt;工作流没有在这里止步。&lt;/p&gt;
&lt;h3&gt;发布分支&lt;/h3&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/eea51bc17ceb40fcdad466ed6dcf9ff1.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;一旦&lt;code&gt;develop&lt;/code&gt;分支上有了做一次发布（或者说快到了既定的发布日）的足够功能，就从&lt;code&gt;develop&lt;/code&gt;分支上&lt;code&gt;fork&lt;/code&gt;一个发布分支。新建的分支用于开始发布循环，所以从这个时间点开始之后新的功能不能再加到这个分支上 —— 这个分支只应该做&lt;code&gt;Bug&lt;/code&gt;修复、文档生成和其它面向发布任务。一旦对外发布的工作都完成了，发布分支合并到&lt;code&gt;master&lt;/code&gt;分支并分配一个版本号打好&lt;code&gt;Tag&lt;/code&gt;。另外，这些从新建发布分支以来的做的修改要合并回&lt;code&gt;develop&lt;/code&gt;分支。&lt;/p&gt;
&lt;p&gt;使用一个用于发布准备的专门分支，使得一个团队可以在完善当前的发布版本的同时，另一个团队可以继续开发下个版本的功能。&lt;br&gt;
这也打造定义良好的开发阶段（比如，可以很轻松地说，『这周我们要做准备发布版本4.0』，并且在仓库的目录结构中可以实际看到）。&lt;/p&gt;
&lt;p&gt;常用的分支约定：&lt;/p&gt;
&lt;p&gt;&lt;code&gt;用于新建发布分支的分支: develop&lt;br&gt;
用于合并的分支: master&lt;br&gt;
分支命名: release-* 或 release/*&lt;/code&gt;&lt;/p&gt;
&lt;h3&gt;维护分支&lt;/h3&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/2b4add96df6d739a5d9715e28a91e3d7.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;维护分支或说是热修复（&lt;code&gt;hotfix&lt;/code&gt;）分支用于生成快速给产品发布版本（&lt;code&gt;production releases&lt;/code&gt;）打补丁，这是唯一可以直接从&lt;code&gt;master&lt;/code&gt;分支&lt;code&gt;fork&lt;/code&gt;出来的分支。修复完成，修改应该马上合并回&lt;code&gt;master&lt;/code&gt;分支和&lt;code&gt;develop&lt;/code&gt;分支（当前的发布分支），&lt;code&gt;master&lt;/code&gt;分支应该用新的版本号打好&lt;code&gt;Tag&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;为&lt;code&gt;Bug&lt;/code&gt;修复使用专门分支，让团队可以处理掉问题而不用打断其它工作或是等待下一个发布循环。你可以把维护分支想成是一个直接在&lt;code&gt;master&lt;/code&gt;分支上处理的临时发布。&lt;/p&gt;
&lt;h2&gt;示例&lt;/h2&gt;
&lt;p&gt;下面的示例演示本工作流如何用于管理单个发布循环。假设你已经创建了一个中央仓库。&lt;/p&gt;
&lt;h3&gt;创建开发分支&lt;/h3&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/2087d023b1fa4a8ef7cdb655be5b70b6.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;第一步为&lt;code&gt;master&lt;/code&gt;分支配套一个&lt;code&gt;develop&lt;/code&gt;分支。简单来做可以&lt;a href=&quot;https://www.atlassian.com/git/tutorial/git-branches#!branch&quot;&gt;本地创建一个空的&lt;code&gt;develop&lt;/code&gt;分支&lt;/a&gt;，&lt;code&gt;push&lt;/code&gt;到服务器上：&lt;/p&gt;
&lt;p&gt;&lt;code&gt;git branch develop&lt;br&gt;
git push -u origin develop&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;以后这个分支将会包含了项目的全部历史，而&lt;code&gt;master&lt;/code&gt;分支将只包含了部分历史。其它开发者这时应该&lt;a href=&quot;https://www.atlassian.com/git/tutorial/git-basics#!clone&quot;&gt;克隆中央仓库&lt;/a&gt;，建好&lt;code&gt;develop&lt;/code&gt;分支的跟踪分支：&lt;/p&gt;
&lt;p&gt;&lt;code&gt;git clone ssh://user@host/path/to/repo.git&lt;br&gt;
git checkout -b develop origin/develop&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;现在每个开发都有了这些历史分支的本地拷贝。&lt;/p&gt;
&lt;h3&gt;小红和小明开始开发新功能&lt;/h3&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/fbd7737bbd5776f2a1fd81e0b4212386.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;这个示例中，小红和小明开始各自的功能开发。他们需要为各自的功能创建相应的分支。新分支不是基于&lt;code&gt;master&lt;/code&gt;分支，而是应该&lt;a href=&quot;https://www.atlassian.com/git/tutorial/git-branches#!checkout&quot;&gt;基于&lt;code&gt;develop&lt;/code&gt;分支&lt;/a&gt;：&lt;/p&gt;
&lt;p&gt;&lt;code&gt;git checkout -b some-feature develop&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;他们用老套路添加提交到各自功能分支上：编辑、暂存、提交：&lt;br&gt;
&lt;code&gt;git status&lt;br&gt;
git add&lt;br&gt;
git commit&lt;/code&gt;&lt;/p&gt;
&lt;h3&gt;小红完成功能开发&lt;/h3&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/932f2e68b86d0efdfb7672efadd68daf.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;添加了提交后，小红觉得她的功能OK了。如果团队使用&lt;code&gt;Pull Requests&lt;/code&gt;，这时候可以发起一个用于合并到&lt;code&gt;develop&lt;/code&gt;分支。否则她可以直接合并到她本地的&lt;code&gt;develop&lt;/code&gt;分支后&lt;code&gt;push&lt;/code&gt;到中央仓库：&lt;/p&gt;
&lt;p&gt;&lt;code&gt;git pull origin develop&lt;br&gt;
git checkout develop&lt;br&gt;
git merge some-feature&lt;br&gt;
git push&lt;br&gt;
git branch -d some-feature&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;第一条命令在合并功能前确保&lt;code&gt;develop&lt;/code&gt;分支是最新的。注意，功能决不应该直接合并到&lt;code&gt;master&lt;/code&gt;分支。冲突解决方法和&lt;a href=&quot;http://blog.jobbole.com/76847/&quot;&gt;集中式工作流&lt;/a&gt;一样。&lt;/p&gt;
&lt;h3&gt;小红开始准备发布&lt;/h3&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/9f61d51173637803af25ff88af98da5a.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;这个时候小明正在实现他的功能，小红开始准备她的第一个项目正式发布。像功能开发一样，她用一个新的分支来做发布准备。这一步也确定了发布的版本号：&lt;/p&gt;
&lt;p&gt;&lt;code&gt;git checkout -b release-0.1 develop&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;这个分支是清理发布、执行所有测试、更新文档和其它为下个发布做准备操作的地方，像是一个专门用于改善发布的功能分支。&lt;/p&gt;
&lt;p&gt;只要小红创建这个分支并&lt;code&gt;push&lt;/code&gt;到中央仓库，这个发布就是功能冻结的。任何不在&lt;code&gt;develop&lt;/code&gt;分支中的新功能都推到下个发布循环中。&lt;/p&gt;
&lt;h3&gt;小红完成发布&lt;/h3&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/8b7653292f34d8d3176bfac1a80717d1.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;一旦准备好了对外发布，小红合并修改到&lt;code&gt;master&lt;/code&gt;分支和&lt;code&gt;develop&lt;/code&gt;分支上，删除发布分支。合并回&lt;code&gt;develop&lt;/code&gt;分支很重要，因为在发布分支中已经提交的更新需要在后面的新功能中也要是可用的。另外，如果小红的团队要求&lt;code&gt;Code Review&lt;/code&gt;，这是一个发起&lt;code&gt;Pull Request&lt;/code&gt;的理想时机。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;git checkout master&lt;br&gt;
git merge release-0.1&lt;br&gt;
git push&lt;br&gt;
git checkout develop&lt;br&gt;
git merge release-0.1&lt;br&gt;
git push&lt;br&gt;
git branch -d release-0.1&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;发布分支是作为功能开发（&lt;code&gt;develop&lt;/code&gt;分支）和对外发布（&lt;code&gt;master&lt;/code&gt;分支）间的缓冲。只要有合并到&lt;code&gt;master&lt;/code&gt;分支，就应该打好&lt;code&gt;Tag&lt;/code&gt;以方便跟踪。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;git tag -a 0.1 -m &quot;Initial public release&quot; master&lt;br&gt;
git push --tags&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Git&lt;/code&gt;有提供各种勾子（&lt;code&gt;hook&lt;/code&gt;），即仓库有事件发生时触发执行的脚本。可以配置一个勾子，在你&lt;code&gt;push&lt;/code&gt;中央仓库的&lt;code&gt;master&lt;/code&gt;分支时，自动构建好对外发布。&lt;/p&gt;
&lt;h3&gt;最终用户发现&lt;code&gt;Bug&lt;/code&gt;
&lt;/h3&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/acbdff5c1fb4a3e412fbd534a1e11a1c.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;对外发布后，小红回去和小明一起做下个发布的新功能开发，直到有最终用户开了一个&lt;code&gt;Ticket&lt;/code&gt;抱怨当前版本的一个&lt;code&gt;Bug&lt;/code&gt;。为了处理&lt;code&gt;Bug&lt;/code&gt;，小红（或小明）从&lt;code&gt;master&lt;/code&gt;分支上拉出了一个维护分支，提交修改以解决问题，然后直接合并回&lt;code&gt;master&lt;/code&gt;分支：&lt;/p&gt;
&lt;p&gt;“`&lt;br&gt;
git checkout -b issue-#001 master&lt;/p&gt;
&lt;h1&gt;Fix the bug&lt;/h1&gt;
&lt;p&gt;git checkout master&lt;br&gt;
git merge issue-#001&lt;br&gt;
git push&lt;br&gt;
“`&lt;/p&gt;
&lt;p&gt;就像发布分支，维护分支中新加这些重要修改需要包含到&lt;code&gt;develop&lt;/code&gt;分支中，所以小红要执行一个合并操作。然后就可以安全地&lt;a href=&quot;https://www.atlassian.com/git/tutorial/git-branches#!branch&quot;&gt;删除这个分支&lt;/a&gt;了：&lt;/p&gt;
&lt;p&gt;&lt;code&gt;git checkout develop&lt;br&gt;
git merge issue-#001&lt;br&gt;
git push&lt;br&gt;
git branch -d issue-#001&lt;/code&gt;&lt;/p&gt;
&lt;h2&gt;下一站&lt;/h2&gt;
&lt;p&gt;到了这里，但愿你对&lt;a href=&quot;http://blog.jobbole.com/76847/&quot;&gt;集中式工作流&lt;/a&gt;、&lt;a href=&quot;http://blog.jobbole.com/76857/&quot;&gt;功能分支工作流&lt;/a&gt;和&lt;code&gt;Gitflow&lt;/code&gt;工作流已经感觉很舒适了。你应该也牢固的掌握了本地仓库的潜能，&lt;code&gt;push&lt;/code&gt;/&lt;code&gt;pull&lt;/code&gt;模式和&lt;code&gt;Git&lt;/code&gt;健壮的分支和合并模型。&lt;/p&gt;
&lt;p&gt;记住，这里演示的工作流只是可能用法的例子，而不是在实际工作中使用&lt;code&gt;Git&lt;/code&gt;不可违逆的条例。所以不要畏惧按自己需要对工作流的用法做取舍。不变的目标就是让&lt;code&gt;Git&lt;/code&gt;为你所用。&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;a href=&quot;http://blog.jobbole.com/76857/&quot;&gt;« 功能分支工作流&lt;/a&gt;　　　　&lt;a href=&quot;http://blog.jobbole.com/76861/&quot;&gt;&lt;code&gt;Forking&lt;/code&gt;工作流 »&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;译注&lt;/em&gt;&lt;/strong&gt;&lt;br&gt;
&lt;a href=&quot;http://weibo.com/oldratlee&quot;&gt;自己&lt;/a&gt;理解粗浅，译文源码在&lt;a href=&quot;https://github.com/quickhack/translations/tree/master/git-workflows-and-tutorials&quot;&gt;&lt;code&gt;GitHub&lt;/code&gt;上&lt;/a&gt;，翻译中不足和不对之处，欢迎建议（&lt;a href=&quot;https://github.com/quickhack/translations/issues&quot;&gt;提交Issue&lt;/a&gt;）和指正（&lt;a href=&quot;https://github.com/quickhack/translations/fork&quot;&gt;Fork后提交代码&lt;/a&gt;）！&lt;/p&gt;


&lt;!-- div id=&quot;ad1&quot;&gt;
&lt;/div --&gt;


	


	

</description>
        <pubDate>Sat, 13 Sep 2014 00:00:00 +0800</pubDate>
        <link>http://iftti.com/posts/2014/2014-09-13-76867-05517d972.html</link>
        <guid isPermaLink="true">http://iftti.com/posts/2014/2014-09-13-76867-05517d972.html</guid>
        
        
        <category>jobbole</category>
        
      </item>
    
      <item>
        <title>Git工作流指南：Forking工作流</title>
        <description>

					
		
&lt;!-- div id=&quot;ad1&quot;&gt;
&lt;script type=&quot;text/javascript&quot;&gt;
google_ad_client = &quot;ca-pub-7056282119617872&quot;;
google_ad_slot = &quot;6645040531&quot;;
google_ad_width = 300;
google_ad_height = 250;
&lt;/script&gt;
&lt;script type=&quot;text/javascript&quot;
src=&quot;http://pagead2.googlesyndication.com/pagead/show_ads.js&quot;&gt;
&lt;/script&gt;
&lt;/div&gt;
&lt;br/ --&gt;

&lt;p&gt;&lt;code&gt;Forking&lt;/code&gt;工作流和前面讨论的几种工作流有根本的不同。这种工作流不是使用单个服务端仓库作为『中央』代码基线，而让各个开发者都有一个服务端仓库。这意味着各个代码贡献者有2个&lt;code&gt;Git&lt;/code&gt;仓库而不是1个：一个本地私有的，另一个服务端公开的。&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/916693eb04687b42b8581c20d664206c.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Forking&lt;/code&gt;工作流的一个主要优势是，贡献的代码可以被集成，而不需要所有人都能&lt;code&gt;push&lt;/code&gt;代码到仅有的中央仓库中。开发者&lt;code&gt;push&lt;/code&gt;到自己的服务端仓库，而只有项目维护者才能&lt;code&gt;push&lt;/code&gt;到正式仓库。这样项目维护者可以接受任何开发者的提交，但无需给他正式代码库的写权限。&lt;/p&gt;
&lt;p&gt;效果就是一个分布式的工作流，能为大型、自发性的团队（包括了不受信的第三方）提供灵活的方式来安全的协作。也让这个工作流成为开源项目的理想工作流。&lt;/p&gt;
&lt;h2&gt;工作方式&lt;/h2&gt;
&lt;p&gt;和其它的&lt;code&gt;Git&lt;/code&gt;工作流一样，&lt;code&gt;Forking&lt;/code&gt;工作流要先有一个公开的正式仓库存储在服务器上。但一个新的开发者想要在项目上工作时，不是直接从正式仓库克隆，而是&lt;code&gt;fork&lt;/code&gt;正式项目在服务器上创建一个拷贝。&lt;/p&gt;
&lt;p&gt;这个仓库拷贝作为他个人公开仓库 —— 其它开发者不允许&lt;code&gt;push&lt;/code&gt;到这个仓库，但可以&lt;code&gt;pull&lt;/code&gt;到修改（后面我们很快就会看这点很重要）。在创建了自己服务端拷贝之后，和之前的工作流一样，开发者执行&lt;a href=&quot;https://www.atlassian.com/git/tutorial/git-basics#!clone&quot;&gt;&lt;code&gt;git clone&lt;/code&gt;命令&lt;/a&gt;克隆仓库到本地机器上，作为私有的开发环境。&lt;/p&gt;
&lt;p&gt;要提交本地修改时，&lt;code&gt;push&lt;/code&gt;提交到自己公开仓库中 —— 而不是正式仓库中。然后，给正式仓库发起一个&lt;code&gt;pull request&lt;/code&gt;，让项目维护者知道有更新已经准备好可以集成了。对于贡献的代码，&lt;code&gt;pull request&lt;/code&gt;也可以很方便地作为一个讨论的地方。&lt;/p&gt;
&lt;p&gt;为了集成功能到正式代码库，维护者&lt;code&gt;pull&lt;/code&gt;贡献者的变更到自己的本地仓库中，检查变更以确保不会让项目出错，&lt;a href=&quot;https://www.atlassian.com/git/tutorial/git-branches#!merge&quot;&gt;合并变更到自己本地的&lt;code&gt;master&lt;/code&gt;分支&lt;/a&gt;，然后&lt;a href=&quot;https://www.atlassian.com/git/tutorial/remote-repositories#!push&quot;&gt;&lt;code&gt;push&lt;/code&gt;&lt;/a&gt;&lt;code&gt;master&lt;/code&gt;分支到服务器的正式仓库中。到此，贡献的提交成为了项目的一部分，其它的开发者应该执行&lt;code&gt;pull&lt;/code&gt;操作与正式仓库同步自己本地仓库。&lt;/p&gt;
&lt;h3&gt;正式仓库&lt;/h3&gt;
&lt;p&gt;在&lt;code&gt;Forking&lt;/code&gt;工作流中，『官方』仓库的叫法只是一个约定，理解这点很重要。从技术上来看，各个开发者仓库和正式仓库在&lt;code&gt;Git&lt;/code&gt;看来没有任何区别。事实上，让正式仓库之所以正式的唯一原因是它是项目维护者的公开仓库。&lt;/p&gt;
&lt;h3&gt;
&lt;code&gt;Forking&lt;/code&gt;工作流的分支使用方式&lt;/h3&gt;
&lt;p&gt;所有的个人公开仓库实际上只是为了方便和其它的开发者共享分支。各个开发者应该用分支隔离各个功能，就像在&lt;a href=&quot;http://blog.jobbole.com/76857/&quot;&gt;功能分支工作流&lt;/a&gt;和&lt;a href=&quot;http://blog.jobbole.com/76861/&quot;&gt;&lt;code&gt;Gitflow&lt;/code&gt;工作流&lt;/a&gt;一样。唯一的区别是这些分支被共享了。在&lt;code&gt;Forking&lt;/code&gt;工作流中这些分支会被&lt;code&gt;pull&lt;/code&gt;到另一个开发者的本地仓库中，而在功能分支工作流和&lt;code&gt;Gitflow&lt;/code&gt;工作流中是直接被&lt;code&gt;push&lt;/code&gt;到正式仓库中。&lt;/p&gt;
&lt;h2&gt;示例&lt;/h2&gt;
&lt;h3&gt;项目维护者初始化正式仓库&lt;/h3&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/b82733ae63c882c6b70b14c905bce278.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;和任何使用&lt;code&gt;Git&lt;/code&gt;项目一样，第一步是创建在服务器上一个正式仓库，让所有团队成员都可以访问到。通常这个仓库也会作为项目维护者的公开仓库。&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://www.atlassian.com/git/tutorial/git-basics#!init&quot;&gt;公开仓库应该是裸仓库&lt;/a&gt;，不管是不是正式代码库。所以项目维护者会运行像下面的命令来搭建正式仓库：&lt;/p&gt;
&lt;p&gt;&lt;code&gt;ssh user@host&lt;br&gt;
git init --bare /path/to/repo.git&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Bitbucket&lt;/code&gt;和&lt;code&gt;Stash&lt;/code&gt;提供了一个方便的&lt;code&gt;GUI&lt;/code&gt;客户端以完成上面命令行做的事。这个搭建中央仓库的过程和前面提到的工作流完全一样。如果有现存的代码库，维护者也要&lt;code&gt;push&lt;/code&gt;到这个仓库中。&lt;/p&gt;
&lt;h3&gt;开发者&lt;code&gt;fork&lt;/code&gt;正式仓库&lt;/h3&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/7c6801ba6a7afecb818271ada4831f11.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;其它所有的开发需要&lt;code&gt;fork&lt;/code&gt;正式仓库。可以用&lt;code&gt;git clone&lt;/code&gt;命令&lt;a href=&quot;https://confluence.atlassian.com/display/BITBUCKET/Set+up+SSH+for+Git&quot;&gt;用&lt;code&gt;SSH&lt;/code&gt;协议连通到服务器&lt;/a&gt;，拷贝仓库到服务器另一个位置 —— 是的，&lt;code&gt;fork&lt;/code&gt;操作基本上就只是一个服务端的克隆。&lt;code&gt;Bitbucket&lt;/code&gt;和&lt;code&gt;Stash&lt;/code&gt;上可以点一下按钮就让开发者完成仓库的&lt;code&gt;fork&lt;/code&gt;操作。&lt;/p&gt;
&lt;p&gt;这一步完成后，每个开发都在服务端有一个自己的仓库。和正式仓库一样，这些仓库应该是裸仓库。&lt;/p&gt;
&lt;h3&gt;开发者克隆自己&lt;code&gt;fork&lt;/code&gt;出来的仓库&lt;/h3&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/089f2d34fe90855c9f4041290bd466f0.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;下一步，各个开发者要克隆自己的公开仓库，用熟悉的&lt;code&gt;git clone&lt;/code&gt;命令。&lt;/p&gt;
&lt;p&gt;在这个示例中，假定用&lt;code&gt;Bitbucket&lt;/code&gt;托管了仓库。记住，如果这样的话各个开发者需要有各自的&lt;code&gt;Bitbucket&lt;/code&gt;账号，使用下面命令克隆服务端自己的仓库：&lt;/p&gt;
&lt;p&gt;&lt;code&gt;git clone https://user@bitbucket.org/user/repo.git&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;相比前面介绍的工作流只用了一个&lt;code&gt;origin&lt;/code&gt;远程别名指向中央仓库，&lt;code&gt;Forking&lt;/code&gt;工作流需要2个远程别名 —— 一个指向正式仓库，另一个指向开发者自己的服务端仓库。别名的名字可以任意命名，常见的约定是使用&lt;code&gt;origin&lt;/code&gt;作为远程克隆的仓库的别名（这个别名会在运行&lt;code&gt;git clone&lt;/code&gt;自动创建），&lt;code&gt;upstream&lt;/code&gt;（上游）作为正式仓库的别名。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;git remote add upstream https://bitbucket.org/maintainer/repo&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;需要自己用上面的命令创建&lt;code&gt;upstream&lt;/code&gt;别名。这样可以简单地保持本地仓库和正式仓库的同步更新。注意，如果上游仓库需要认证（比如不是开源的），你需要提供用户：&lt;/p&gt;
&lt;p&gt;&lt;code&gt;git remote add upstream https://user@bitbucket.org/maintainer/repo.git&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;这时在克隆和&lt;code&gt;pull&lt;/code&gt;正式仓库时，需要提供用户的密码。&lt;/p&gt;
&lt;h3&gt;开发者开发自己的功能&lt;/h3&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/987063d75bdea4e149f32a435873dfc2.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;在刚克隆的本地仓库中，开发者可以像其它工作流一样的编辑代码、&lt;a href=&quot;https://www.atlassian.com/git/tutorial/git-basics#!commit&quot;&gt;提交修改&lt;/a&gt;和&lt;a href=&quot;https://www.atlassian.com/git/tutorial/git-branches#!branch&quot;&gt;新建分支&lt;/a&gt;：&lt;/p&gt;
&lt;p&gt;&lt;code&gt;git checkout -b some-feature&lt;br&gt;
// Edit some code&lt;br&gt;
git commit -a -m &quot;Add first draft of some feature&quot;&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;所有的修改都是私有的直到&lt;code&gt;push&lt;/code&gt;到自己公开仓库中。如果正式项目已经往前走了，可以用&lt;a href=&quot;https://www.atlassian.com/git/tutorial/remote-repositories#!pull&quot;&gt;&lt;code&gt;git pull&lt;/code&gt;命令&lt;/a&gt;获得新的提交：&lt;/p&gt;
&lt;p&gt;&lt;code&gt;git pull upstream master&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;由于开发者应该都在专门的功能分支上工作，&lt;code&gt;pull&lt;/code&gt;操作结果会都是&lt;a href=&quot;https://www.atlassian.com/git/tutorial/git-branches#!merge&quot;&gt;快进合并&lt;/a&gt;。&lt;/p&gt;
&lt;h3&gt;开发者发布自己的功能&lt;/h3&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/b4a640a60cc06a66fe561a4065c68b45.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;一旦开发者准备好了分享新功能，需要做二件事。首先，通过&lt;code&gt;push&lt;/code&gt;他的贡献代码到自己的公开仓库中，让其它的开发者都可以访问到。他的&lt;code&gt;origin&lt;/code&gt;远程别名应该已经有了，所以要做的就是：&lt;/p&gt;
&lt;p&gt;&lt;code&gt;git push origin feature-branch&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;这里和之前的工作流的差异是，&lt;code&gt;origin&lt;/code&gt;远程别名指向开发者自己的服务端仓库，而不是正式仓库。&lt;/p&gt;
&lt;p&gt;第二件事，开发者要通知项目维护者，想要合并他的新功能到正式库中。&lt;code&gt;Bitbucket&lt;/code&gt;和&lt;code&gt;Stash&lt;/code&gt;提供了&lt;a href=&quot;https://confluence.atlassian.com/display/STASH/Using+pull+requests+in+Stash&quot;&gt;&lt;code&gt;Pull Request&lt;/code&gt;&lt;/a&gt;按钮，弹出表单让你指定哪个分支要合并到正式仓库。一般你会想集成你的功能分支到上游远程仓库的&lt;code&gt;master&lt;/code&gt;分支中。&lt;/p&gt;
&lt;h3&gt;项目维护者集成开发者的功能&lt;/h3&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/ca354b957ccbbf2c7be5109f8e3823b4.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;当项目维护者收到&lt;code&gt;pull request&lt;/code&gt;，他要做的是决定是否集成它到正式代码库中。有二种方式来做：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;直接在&lt;code&gt;pull request&lt;/code&gt;中查看代码&lt;/li&gt;
&lt;li&gt;
&lt;code&gt;pull&lt;/code&gt;代码到他自己的本地仓库，再手动合并&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;第一种做法更简单，维护者可以在&lt;code&gt;GUI&lt;/code&gt;中查看变更的差异，做评注和执行合并。但如果出现了合并冲突，需要第二种做法来解决。这种情况下，维护者需要从开发者的服务端仓库中&lt;a href=&quot;https://www.atlassian.com/git/tutorial/remote-repositories#!fetch&quot;&gt;&lt;code&gt;fetch&lt;/code&gt;&lt;/a&gt;功能分支，合并到他本地的&lt;code&gt;master&lt;/code&gt;分支，解决冲突：&lt;/p&gt;
&lt;p&gt;&lt;code&gt;git fetch https://bitbucket.org/user/repo feature-branch&lt;br&gt;
// 查看变更&lt;br&gt;
git checkout master&lt;br&gt;
git merge FETCH_HEAD&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;变更集成到本地的&lt;code&gt;master&lt;/code&gt;分支后，维护者要&lt;code&gt;push&lt;/code&gt;变更到服务器上的正式仓库，这样其它的开发者都能访问到：&lt;/p&gt;
&lt;p&gt;&lt;code&gt;git push origin master&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;注意，维护者的&lt;code&gt;origin&lt;/code&gt;是指向他自己公开仓库的，即是项目的正式代码库。到此，开发者的贡献完全集成到了项目中。&lt;/p&gt;
&lt;h3&gt;开发者和正式仓库做同步&lt;/h3&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/3d978acc5444789b040c49092aade99f.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;由于正式代码库往前走了，其它的开发需要和正式仓库做同步：&lt;/p&gt;
&lt;p&gt;&lt;code&gt;git pull upstream master&lt;/code&gt;&lt;/p&gt;
&lt;h2&gt;下一站&lt;/h2&gt;
&lt;p&gt;如果你之前是使用&lt;code&gt;SVN&lt;/code&gt;，&lt;code&gt;Forking&lt;/code&gt;工作流可能看起来像是一个激进的范式切换（paradigm shift）。但不要害怕，这个工作流实际上就是在&lt;a href=&quot;http://blog.jobbole.com/76857/&quot;&gt;功能分支工作流&lt;/a&gt;之上引入另一个抽象层。不是直接通过单个中央仓库来分享分支，而是把贡献代码发布到开发者自己的服务端仓库中。&lt;/p&gt;
&lt;p&gt;示例中解释了，一个贡献如何从一个开发者流到正式的&lt;code&gt;master&lt;/code&gt;分支中，但同样的方法可以把贡献集成到任一个仓库中。比如，如果团队的几个人协作实现一个功能，可以在开发之间用相同的方法分享变更，完全不涉及正式仓库。&lt;/p&gt;
&lt;p&gt;这使得&lt;code&gt;Forking&lt;/code&gt;工作流对于松散组织的团队来说是个非常强大的工具。任一开发者可以方便地和另一开发者分享变更，任何分支都能有效地合并到正式代码库中。&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;a href=&quot;http://blog.jobbole.com/76867/&quot;&gt;« &lt;code&gt;Gitflow&lt;/code&gt;工作流&lt;/a&gt;　　　　&lt;a href=&quot;http://blog.jobbole.com/76854/&quot;&gt;&lt;code&gt;Pull Requests&lt;/code&gt; »&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;译注&lt;/em&gt;&lt;/strong&gt;&lt;br&gt;
&lt;a href=&quot;http://weibo.com/oldratlee&quot;&gt;自己&lt;/a&gt;理解粗浅，译文源码在&lt;a href=&quot;https://github.com/quickhack/translations/tree/master/git-workflows-and-tutorials&quot;&gt;&lt;code&gt;GitHub&lt;/code&gt;上&lt;/a&gt;，翻译中不足和不对之处，欢迎建议（&lt;a href=&quot;https://github.com/quickhack/translations/issues&quot;&gt;提交Issue&lt;/a&gt;）和指正（&lt;a href=&quot;https://github.com/quickhack/translations/fork&quot;&gt;Fork后提交代码&lt;/a&gt;）！&lt;/p&gt;


&lt;!-- div id=&quot;ad1&quot;&gt;
&lt;/div --&gt;


	


	

</description>
        <pubDate>Sat, 13 Sep 2014 00:00:00 +0800</pubDate>
        <link>http://iftti.com/posts/2014/2014-09-13-76861-7ea1d0906.html</link>
        <guid isPermaLink="true">http://iftti.com/posts/2014/2014-09-13-76861-7ea1d0906.html</guid>
        
        
        <category>jobbole</category>
        
      </item>
    
      <item>
        <title>Git工作流指南：功能分支工作流</title>
        <description>

					
		
&lt;!-- div id=&quot;ad1&quot;&gt;
&lt;script type=&quot;text/javascript&quot;&gt;
google_ad_client = &quot;ca-pub-7056282119617872&quot;;
google_ad_slot = &quot;6645040531&quot;;
google_ad_width = 300;
google_ad_height = 250;
&lt;/script&gt;
&lt;script type=&quot;text/javascript&quot;
src=&quot;http://pagead2.googlesyndication.com/pagead/show_ads.js&quot;&gt;
&lt;/script&gt;
&lt;/div&gt;
&lt;br/ --&gt;

&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/d5bda9a0d553f06781a4a82728c6351c.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;一旦你玩转了&lt;a href=&quot;http://blog.jobbole.com/76847/&quot; target=&quot;_blank&quot;&gt;集中式工作流&lt;/a&gt;，在开发过程中可以很简单地加上功能分支，用来鼓励开发者之间协作和简化交流。&lt;/p&gt;
&lt;p&gt;功能分支工作流背后的核心思路是所有的功能开发应该在一个专门的分支，而不是在&lt;code&gt;master&lt;/code&gt;分支上。这个隔离可以方便多个开发者在各自的功能上开发而不会弄乱主干代码。另外，也保证了&lt;code&gt;master&lt;/code&gt;分支的代码一定不会是有问题的，极大有利于集成环境。&lt;/p&gt;
&lt;p&gt;功能开发隔离也让&lt;a href=&quot;http://blog.jobbole.com/76854/&quot;&gt;&lt;code&gt;pull requests&lt;/code&gt;工作流&lt;/a&gt;成功可能，&lt;code&gt;pull requests&lt;/code&gt;工作流能为每个分支发起一个讨论，在分支合入正式项目之前，给其它开发者有表示赞同的机会。另外，如果你在功能开发中有问题卡住了，可以开一个&lt;code&gt;pull requests&lt;/code&gt;来向同学们征求建议。这些做法的重点就是，&lt;code&gt;pull requests&lt;/code&gt;让团队成员之间互相评论工作变成非常方便！&lt;/p&gt;
&lt;h2&gt;工作方式&lt;/h2&gt;
&lt;p&gt;功能分支工作流仍然用中央仓库，并且&lt;code&gt;master&lt;/code&gt;分支还是代表了正式项目的历史。但不是直接提交本地历史到各自的本地&lt;code&gt;master&lt;/code&gt;分支，开发者每次在开始新功能前先创建一个新分支。功能分支应该有个有描述性的名字，比如&lt;code&gt;animated-menu-items&lt;/code&gt;或&lt;code&gt;issue-#1061&lt;/code&gt;，这样可以让分支有个清楚且高聚焦的用途。&lt;/p&gt;
&lt;p&gt;在&lt;code&gt;master&lt;/code&gt;分支和功能分支之间，&lt;code&gt;Git&lt;/code&gt;是没有技术上的区别，所以开发者可以用和集中式工作流中完全一样的方式编辑、暂存和提交修改到功能分支上。&lt;/p&gt;
&lt;p&gt;另外，功能分支也可以（且应该）&lt;code&gt;push&lt;/code&gt;到中央仓库中。这样不修改正式代码就可以和其它开发者分享提交的功能。由于&lt;code&gt;master&lt;/code&gt;仅有的一个『特殊』分支，在中央仓库上存多个功能分支不会有任何问题。当然，这样做也可以很方便地备份各自的本地提交。&lt;/p&gt;
&lt;h3&gt;&lt;code&gt;Pull Requests&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;功能分支除了可以隔离功能的开发，也使得通过&lt;a href=&quot;http://blog.jobbole.com/76854/&quot;&gt;&lt;code&gt;Pull Requests&lt;/code&gt;&lt;/a&gt;讨论变更成为可能。一旦某个开发完成一个功能，不是立即合并到&lt;code&gt;master&lt;/code&gt;，而是&lt;code&gt;push&lt;/code&gt;到中央仓库的功能分支上并发起一个&lt;code&gt;Pull Request&lt;/code&gt;请求去合并修改到&lt;code&gt;master&lt;/code&gt;。在修改成为主干代码前，这让其它的开发者有机会先去&lt;code&gt;Review&lt;/code&gt;变更。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Code Review&lt;/code&gt;是&lt;code&gt;Pull Requests&lt;/code&gt;的一个重要的收益，但&lt;code&gt;Pull Requests&lt;/code&gt;目的是讨论代码一个通用方式。你可以把&lt;code&gt;Pull Requests&lt;/code&gt;作为专门给某个分支的讨论。这意味着可以在更早的开发过程中就可以进行&lt;code&gt;Code Review&lt;/code&gt;。比如，一个开发者开发功能需要帮助时，要做的就是发起一个&lt;code&gt;Pull Request&lt;/code&gt;，相关的人就会自动收到通知，在相关的提交旁边能看到需要帮助解决的问题。&lt;/p&gt;
&lt;p&gt;一旦&lt;code&gt;Pull Request&lt;/code&gt;被接受了，发布功能要做的就和集中式工作流就很像了。首先，确定本地的&lt;code&gt;master&lt;/code&gt;分支和上游的&lt;code&gt;master&lt;/code&gt;分支是同步的。然后合并功能分支到本地&lt;code&gt;master&lt;/code&gt;分支并&lt;code&gt;push&lt;/code&gt;已经更新的本地&lt;code&gt;master&lt;/code&gt;分支到中央仓库。&lt;/p&gt;
&lt;p&gt;仓库管理的产品解决方案像&lt;a href=&quot;http://bitbucket.org/&quot;&gt;&lt;code&gt;Bitbucket&lt;/code&gt;&lt;/a&gt;或&lt;a href=&quot;http://www.atlassian.com/stash&quot;&gt;&lt;code&gt;Stash&lt;/code&gt;&lt;/a&gt;，可以良好地支持&lt;code&gt;Pull Requests&lt;/code&gt;。可以看看&lt;code&gt;Stash&lt;/code&gt;的&lt;a href=&quot;https://confluence.atlassian.com/display/STASH/Using+pull+requests+in+Stash&quot;&gt;&lt;code&gt;Pull Requests&lt;/code&gt;文档&lt;/a&gt;。&lt;/p&gt;
&lt;h2&gt;示例&lt;/h2&gt;
&lt;p&gt;下面的示例演示了如何把&lt;code&gt;Pull Requests&lt;/code&gt;作为&lt;code&gt;Code Review&lt;/code&gt;的方式，但注意&lt;code&gt;Pull Requests&lt;/code&gt;可以用于很多其它的目的。&lt;/p&gt;
&lt;h3&gt;小红开始开发一个新功能&lt;/h3&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/6a0124fd0bb9ead1ec33e1a8e41e90b2.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;在开始开发功能前，小红需要一个独立的分支。使用下面的命令&lt;a href=&quot;https://www.atlassian.com/git/tutorial/git-branches#!checkout&quot;&gt;新建一个分支&lt;/a&gt;：&lt;/p&gt;
&lt;p&gt;&lt;code&gt;git checkout -b marys-feature master&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;这个命令检出一个基于&lt;code&gt;master&lt;/code&gt;名为&lt;code&gt;marys-feature&lt;/code&gt;的分支，&lt;code&gt;Git&lt;/code&gt;的&lt;code&gt;-b&lt;/code&gt;选项表示如果分支还不存在则新建分支。这个新分支上，小红按老套路编辑、暂存和提交修改，按需要提交以实现功能：&lt;/p&gt;
&lt;p&gt;&lt;code&gt;git status&lt;br&gt;
git add&lt;br&gt;
git commit&lt;/code&gt;&lt;/p&gt;
&lt;h3&gt;小红要去吃个午饭&lt;/h3&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/79fdde599a10a04ceb52f3eb3025a34f.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;早上小红为新功能添加一些提交。去吃午饭前，&lt;code&gt;push&lt;/code&gt;功能分支到中央仓库是很好的做法，这样可以方便地备份，如果和其它开发协作，也让他们可以看到小红的提交。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;git push -u origin marys-feature&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;这条命令&lt;code&gt;push&lt;/code&gt; &lt;code&gt;marys-feature&lt;/code&gt;分支到中央仓库（&lt;code&gt;origin&lt;/code&gt;），&lt;code&gt;-u&lt;/code&gt;选项设置本地分支去跟踪远程对应的分支。设置好跟踪的分支后，小红就可以使用&lt;code&gt;git push&lt;/code&gt;命令省去指定推送分支的参数。&lt;/p&gt;
&lt;h3&gt;小红完成功能开发&lt;/h3&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/2fd409cc78827d3c7e3be74f7cf6808f.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;小红吃完午饭回来，完成整个功能的开发。&lt;a href=&quot;https://www.atlassian.com/git/tutorial/git-branches#!merge&quot;&gt;在合并到&lt;code&gt;master&lt;/code&gt;之前&lt;/a&gt;，她发起一个&lt;code&gt;Pull Request&lt;/code&gt;让团队的其它人知道功能已经完成。但首先，她要确认中央仓库中已经有她最近的提交：&lt;/p&gt;
&lt;p&gt;&lt;code&gt;git push&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;然后，在她的&lt;code&gt;Git&lt;/code&gt; &lt;code&gt;GUI&lt;/code&gt;客户端中发起&lt;code&gt;Pull Request&lt;/code&gt;，请求合并&lt;code&gt;marys-feature&lt;/code&gt;到&lt;code&gt;master&lt;/code&gt;，团队成员会自动收到通知。&lt;code&gt;Pull Request&lt;/code&gt;很酷的是可以在相关的提交旁边显示评注，所以你可以很对某个变更集提问。&lt;/p&gt;
&lt;h3&gt;小黑收到&lt;code&gt;Pull Request&lt;/code&gt;
&lt;/h3&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/1361eddb171308c4ea79f0eff3e7c4aa.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;小黑收到了&lt;code&gt;Pull Request&lt;/code&gt;后会查看&lt;code&gt;marys-feature&lt;/code&gt;的修改。决定在合并到正式项目前是否要做些修改，且通过&lt;code&gt;Pull Request&lt;/code&gt;和小红来回地讨论。&lt;/p&gt;
&lt;h3&gt;小红再做修改&lt;/h3&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/a1554c004d66beaf396c45b7a49fd295.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;要再做修改，小红用和功能第一个迭代完全一样的过程。编辑、暂存、提交并&lt;code&gt;push&lt;/code&gt;更新到中央仓库。小红这些活动都会显示在&lt;code&gt;Pull Request&lt;/code&gt;上，小黑可以断续做评注。&lt;/p&gt;
&lt;p&gt;如果小黑有需要，也可以把&lt;code&gt;marys-feature&lt;/code&gt;分支拉到本地，自己来修改，他加的提交也会一样显示在&lt;code&gt;Pull Request&lt;/code&gt;上。&lt;/p&gt;
&lt;h3&gt;小红发布她的功能&lt;/h3&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/8556146c63b1228b896d49314a5f5785.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;一旦小黑可以的接受&lt;code&gt;Pull Request&lt;/code&gt;，就可以合并功能到稳定项目代码中（可以由小黑或是小红来做这个操作）：&lt;/p&gt;
&lt;p&gt;&lt;code&gt;git checkout master&lt;br&gt;
git pull&lt;br&gt;
git pull origin marys-feature&lt;br&gt;
git push&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;无论谁来做合并，首先要检出&lt;code&gt;master&lt;/code&gt;分支并确认是它是最新的。然后执行&lt;code&gt;git pull origin marys-feature&lt;/code&gt;合并&lt;code&gt;marys-feature&lt;/code&gt;分支到和已经和远程一致的本地&lt;code&gt;master&lt;/code&gt;分支。你可以使用简单&lt;code&gt;git merge marys-feature&lt;/code&gt;命令，但前面的命令可以保证总是最新的新功能分支。最后更新的&lt;code&gt;master&lt;/code&gt;分支要重新&lt;code&gt;push&lt;/code&gt;回到&lt;code&gt;origin&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;这个过程常常会生成一个合并提交。有些开发者喜欢有合并提交，因为它像一个新功能和原来代码基线的连通符。但如果你偏爱线性的提交历史，可以在执行合并时&lt;code&gt;rebase&lt;/code&gt;新功能到&lt;code&gt;master&lt;/code&gt;分支的顶部，这样生成一个快进（&lt;code&gt;fast-forward&lt;/code&gt;）的合并。&lt;/p&gt;
&lt;p&gt;一些&lt;code&gt;GUI&lt;/code&gt;客户端可以只要点一下『接受』按钮执行好上面的命令来自动化&lt;code&gt;Pull Request&lt;/code&gt;接受过程。如果你的不能这样，至少在功能合并到&lt;code&gt;master&lt;/code&gt;分支后能自动关闭&lt;code&gt;Pull Request&lt;/code&gt;。&lt;/p&gt;
&lt;h3&gt;与此同时，小明在做和小红一样的事&lt;/h3&gt;
&lt;p&gt;当小红和小黑在&lt;code&gt;marys-feature&lt;/code&gt;上工作并讨论她的&lt;code&gt;Pull Request&lt;/code&gt;的时候，小明在自己的功能分支上做完全一样的事。&lt;/p&gt;
&lt;p&gt;通过隔离功能到独立的分支上，每个人都可以自主的工作，当然必要的时候在开发者之间分享变更还是比较繁琐的。&lt;/p&gt;
&lt;h2&gt;下一站&lt;/h2&gt;
&lt;p&gt;到了这里，但愿你发现了功能分支可以很直接地在&lt;a href=&quot;http://blog.jobbole.com/76847/&quot;&gt;集中式工作流&lt;/a&gt;的仅有的&lt;code&gt;master&lt;/code&gt;分支上完成多功能的开发。另外，功能分支还使用了&lt;code&gt;Pull Request&lt;/code&gt;，使得可以在你的版本控制&lt;code&gt;GUI&lt;/code&gt;客户端中讨论某个提交。&lt;/p&gt;
&lt;p&gt;功能分支工作流是开发项目异常灵活的方式。问题是，有时候太灵活了。对于大型团队，常常需要给不同分支分配一个更具体的角色。&lt;code&gt;Gitflow&lt;/code&gt;工作流是管理功能开发、发布准备和维护的常用模式。&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;a href=&quot;http://blog.jobbole.com/76847/&quot;&gt;« 集中式工作流&lt;/a&gt;　　　　&lt;a href=&quot;http://blog.jobbole.com/76867/&quot;&gt;&lt;code&gt;Gitflow&lt;/code&gt;工作流 »&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;译注&lt;/em&gt;&lt;/strong&gt;&lt;br&gt;
&lt;a href=&quot;http://weibo.com/oldratlee&quot;&gt;自己&lt;/a&gt;理解粗浅，译文源码在&lt;a href=&quot;https://github.com/quickhack/translations/tree/master/git-workflows-and-tutorials&quot;&gt;&lt;code&gt;GitHub&lt;/code&gt;上&lt;/a&gt;，翻译中不足和不对之处，欢迎建议（&lt;a href=&quot;https://github.com/quickhack/translations/issues&quot;&gt;提交Issue&lt;/a&gt;）和指正（&lt;a href=&quot;https://github.com/quickhack/translations/fork&quot;&gt;Fork后提交代码&lt;/a&gt;）！&lt;/p&gt;


&lt;!-- div id=&quot;ad1&quot;&gt;
&lt;/div --&gt;


	


	

</description>
        <pubDate>Sat, 13 Sep 2014 00:00:00 +0800</pubDate>
        <link>http://iftti.com/posts/2014/2014-09-13-76857-70092e8dc.html</link>
        <guid isPermaLink="true">http://iftti.com/posts/2014/2014-09-13-76857-70092e8dc.html</guid>
        
        
        <category>jobbole</category>
        
      </item>
    
  </channel>
</rss>
