<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>IT技术干货</title>
    <description>[IT技术干货iftti.com] @KernelHacks</description>
    <link>http://iftti.com/</link>
    <atom:link href="http://iftti.com/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Mon, 23 Feb 2015 23:46:49 +0800</pubDate>
    <lastBuildDate>Mon, 23 Feb 2015 23:46:49 +0800</lastBuildDate>
    <generator>Jekyll v2.2.0</generator>
    
      <item>
        <title>spark streaming 的 transform 操作示例</title>
        <description>

  
  &lt;div style=&quot;background-color: #FFF;&quot;&gt;
    &lt;p&gt;前两篇，一篇说在 spark 里用 SQL 方便，一篇说 updatestateByKey 可以保留状态做推算。那么怎么综合起来呢？目前看到的 spark streaming 和 spark SQL 的示例全都是在 output 阶段的 &lt;code&gt;foreachRDD&lt;/code&gt; 里才调用 SQL。实际在 output 之前，也是可以对 DStream 里的 RDD 做复杂的转换操作的，这就是 &lt;code&gt;transform&lt;/code&gt; 方法。&lt;/p&gt;
&lt;p&gt;通过 &lt;code&gt;transform&lt;/code&gt; 方法，可以做到 SQL 请求的结果依然是 DStream 数据，这样就可以使用 &lt;code&gt;updateStateByKey&lt;/code&gt; 方法了。下面是示例：&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;org.apache.spark.SparkConf&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;org.apache.spark.sql._&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;org.apache.spark.sql.SQLContext&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;org.apache.spark.SparkContext&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;org.apache.spark.SparkContext._&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;org.apache.spark.streaming.&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Seconds&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;StreamingContext&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;org.apache.spark.streaming.StreamingContext._&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;object&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;LogStash&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;case&lt;/span&gt; &lt;span class=&quot;kd&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;Status&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;avg:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Double&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nl&quot;&gt;count:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Int&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;var&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;countTrend&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.0&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;var&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;avgTrend&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.0&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%(&lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;prev:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Status&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;):&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Status&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
      &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;prev&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;count&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;this&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;countTrend&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;this&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;count&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;prev&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;count&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;toDouble&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;prev&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;count&lt;/span&gt;
      &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
      &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;prev&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;avg&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;this&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;avgTrend&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;this&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;avg&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;prev&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;avg&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;prev&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;avg&lt;/span&gt;
      &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
      &lt;span class=&quot;k&quot;&gt;this&lt;/span&gt;
    &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;override&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;toString&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Trend($avg, $count, $avgTrend, $countTrend)&quot;&lt;/span&gt;
    &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
  &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;updatestatefunc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;newValue:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Seq&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Status&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;nl&quot;&gt;oldValue:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Option&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Status&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;]):&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Option&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Status&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;prev&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;oldValue&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;getOrElse&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Status&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;())&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;current&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;newValue&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;newValue&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;last&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;prev&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;Status&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;Some&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;current&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;main&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;args:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Array&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;String&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;])&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sparkConf&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;SparkConf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;setMaster&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;local[2]&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;setAppName&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;LogStash&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sc&lt;/span&gt;  &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;SparkContext&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sparkConf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ssc&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;StreamingContext&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Seconds&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;ssc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;checkpoint&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;/tmp/spark-streaming-logstash&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sqc&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;SQLContext&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sqc._&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lines&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ssc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;socketTextStream&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;localhost&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;8888&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;lines&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;transform&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rdd&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
      &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rdd&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;count&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;sqc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;jsonRDD&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rdd&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;registerTempTable&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;logstash&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sqlreport&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sqc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;sql&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;SELECT message, COUNT(message) AS host_c, AVG(lineno) AS line_a FROM logstash WHERE path = &#39;/var/log/system.log&#39; AND lineno &amp;gt; 70 GROUP BY message ORDER BY host_c DESC LIMIT 100&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;sqlreport&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;map&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;r&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;toString&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Status&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;toString&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;toDouble&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;toString&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;toInt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)))&lt;/span&gt;
      &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;rdd&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;map&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Status&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;()))&lt;/span&gt;
      &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
    &lt;span class=&quot;o&quot;&gt;}).&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;updateStateByKey&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;updatestatefunc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;ssc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;start&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;ssc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;awaitTermination&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;()&lt;/span&gt;
  &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;这里有一点需要注意，也是耽误我时间最多的地方：&lt;code&gt;transform&lt;/code&gt; 方法的参数和返回，代码里的定义是 &lt;code&gt;RDD[T]&lt;/code&gt; 和 &lt;code&gt;RDD[U]&lt;/code&gt;。我不懂 Java/Scala，以为是只要是 RDD 对象即可。实践证明，其实要任意场合下返回的 RDD 里的数据类型也保持一致。&lt;/p&gt;
&lt;p&gt;在上例中，就是 if 条件下返回的是 &lt;code&gt;RDD[(String, Status)]&lt;/code&gt;，那么 else 条件下，也必须返回一个 &lt;code&gt;RDD[(String, Status)]&lt;/code&gt;，如果直接返回原始的 rdd(也就是 &lt;code&gt;RDD[String]&lt;/code&gt;)，就会报错。&lt;/p&gt;
    &lt;hr&gt;
    
    &lt;hr&gt;
  &lt;!-- JiaThis Button BEGIN --&gt;
&lt;div class=&quot;jiathis_style&quot;&gt;
&lt;span class=&quot;jiathis_txt&quot;&gt;分享到：&lt;/span&gt;
&lt;a class=&quot;jiathis_button_tsina&quot;&gt;新浪微博&lt;/a&gt;
&lt;a class=&quot;jiathis_button_weixin&quot;&gt;微信&lt;/a&gt;
&lt;a class=&quot;jiathis_button_renren&quot;&gt;人人网&lt;/a&gt;
&lt;a class=&quot;jiathis_button_ydnote&quot;&gt;有道云笔记&lt;/a&gt;
&lt;a class=&quot;jiathis_button_gmail&quot;&gt;Gmail邮箱&lt;/a&gt;
&lt;a class=&quot;jiathis_button_twitter&quot;&gt;Twitter&lt;/a&gt;
&lt;a class=&quot;jiathis_button_googleplus&quot;&gt;Google+&lt;/a&gt;
&lt;a class=&quot;jiathis_button_hi&quot;&gt;百度空间&lt;/a&gt;
&lt;a class=&quot;jiathis_button_fb&quot;&gt;Facebook&lt;/a&gt;
&lt;a class=&quot;jiathis_button_douban&quot;&gt;豆瓣&lt;/a&gt;
&lt;a href=&quot;http://www.jiathis.com/share?uid=1589850&quot; class=&quot;jiathis jiathis_txt jiathis_separator jtico jtico_jiathis&quot; target=&quot;_blank&quot;&gt;更多&lt;/a&gt;
&lt;a class=&quot;jiathis_counter_style&quot;&gt;&lt;/a&gt;
&lt;/div&gt;
&lt;script type=&quot;text/javascript&quot;&gt;
var jiathis_config={
	data_track_clickback:true,
	summary:&quot;&quot;,
	ralateuid:{
		&quot;tsina&quot;:&quot;1035836154&quot;
	},
	shortUrl:false,
	hideMore:false
}
&lt;/script&gt;
&lt;script type=&quot;text/javascript&quot; src=&quot;http://v3.jiathis.com/code/jia.js?uid=1589850&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt;
&lt;!-- JiaThis Button END --&gt;
&lt;!-- UY BEGIN --&gt;


&lt;!-- UY END --&gt;
  &lt;/div&gt;

</description>
        <pubDate>Sat, 14 Feb 2015 00:00:00 +0800</pubDate>
        <link>http://iftti.com/posts/2015/2015-02-14-spark-streaming-transform-bb8b7f26e.html</link>
        <guid isPermaLink="true">http://iftti.com/posts/2015/2015-02-14-spark-streaming-transform-bb8b7f26e.html</guid>
        
        
        <category>chenlinux</category>
        
      </item>
    
      <item>
        <title>spark streaming 的 state 操作示例</title>
        <description>

  
  &lt;div style=&quot;background-color: #FFF;&quot;&gt;
    &lt;p&gt;前一篇学习演示了 spark streaming 的基础运用。下一步进入稍微难一点的，利用 checkpoint 来保留上一个窗口的状态，这样可以做到移动窗口的更新统计。&lt;/p&gt;
&lt;p&gt;首先还是先演示一下 spark 里传回调函数的用法，上一篇里用 DStream 处理模拟了 &lt;code&gt;SUM()&lt;/code&gt;，这个纯加法是最简单的了，那么如果 &lt;code&gt;AVG()&lt;/code&gt; 怎么做呢？&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;&lt;span class=&quot;n&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;r&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;logs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;filter&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;equals&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;/var/log/system.log&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)).&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;filter&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;lineno&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;70&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;map&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;message&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;lineno&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)).&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;reduceByKey&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
      &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;_1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;_1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;_2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;_2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;o&quot;&gt;}).&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;map&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;AlertMsg&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;_1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;_2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;_2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;_2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;_1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;_2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;_2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)).&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;()&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;这段跟之前做 SUM 的那段的区别：&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;DStream 处理成 PairDStream 的时候，Value 不是单纯的 1，而是一个 Seq[Double, Int]。避免了上一个示例里分开两个 DStream 然后再 join 起来的操作；&lt;/li&gt;
  &lt;li&gt;给 &lt;code&gt;reduceByKey&lt;/code&gt; 传了一个稍微复杂的匿名函数。在这一个函数里计算了 SUM 和 COUNT，后面 map 只需要做一下除法就是 AVG 了。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;不过这里还用不上上一次窗口的状态。真正需要上一次窗口状态的，是 &lt;code&gt;reduceByKeyAndWindow&lt;/code&gt; 和 &lt;code&gt;updateStateByKey&lt;/code&gt;。&lt;code&gt;reduceByKeyAndWindow&lt;/code&gt; 和 &lt;code&gt;reduceByKey&lt;/code&gt; 的区别，就是除了计算新数据的函数，还要传递一个处理过期数据的函数。&lt;/p&gt;
&lt;p&gt;下面用 &lt;code&gt;updateStateByKey&lt;/code&gt; ，演示一下如何计算每个窗口的平均值，跟上一个窗口的平均值的涨跌幅度，如果波动超过 10%，则输出：&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;org.apache.spark.SparkConf&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;org.apache.spark.SparkContext&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;org.apache.spark.SparkContext._&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;org.apache.spark.streaming.&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Seconds&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;StreamingContext&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;org.apache.spark.streaming.StreamingContext._&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;scala.util.parsing.json.JSON&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;object&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;LogStash&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;case&lt;/span&gt; &lt;span class=&quot;kd&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;LogStashV1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;message:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;String&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nl&quot;&gt;path:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;String&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nl&quot;&gt;host:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;String&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nl&quot;&gt;lineno:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Double&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nl&quot;&gt;timestamp:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;String&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;case&lt;/span&gt; &lt;span class=&quot;kd&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;Status&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;sum:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Double&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nl&quot;&gt;count:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Int&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;avg&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sum&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;scala&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;math&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;max&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;count&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;var&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;countTrend&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.0&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;var&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;avgTrend&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.0&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+(&lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;sum:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Double&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nl&quot;&gt;count:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Int&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;):&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Status&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;newStatus&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Status&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;count&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;this&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;count&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;newStatus&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;countTrend&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;count&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;this&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;count&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;toDouble&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;this&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;count&lt;/span&gt;
      &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
      &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;this&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;avg&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;newStatus&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;avgTrend&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;newStatus&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;avg&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;this&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;avg&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;this&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;avg&lt;/span&gt;
      &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;newStatus&lt;/span&gt;
    &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;override&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;toString&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Trend($count, $sum, $avg, $countTrend, $avgTrend)&quot;&lt;/span&gt;
    &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
  &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;updatestatefunc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;newValue:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Seq&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;[(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Double&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Int&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)],&lt;/span&gt; &lt;span class=&quot;nl&quot;&gt;oldValue:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Option&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Status&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;]):&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Option&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Status&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;prev&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;oldValue&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;getOrElse&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Status&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;())&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;var&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;current&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;prev&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;newValue&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;map&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;_1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;newValue&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;map&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;_2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;sum&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;Some&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;current&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;main&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;args:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Array&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;String&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;])&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sparkConf&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;SparkConf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;setMaster&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;local[2]&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;setAppName&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;LogStash&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sc&lt;/span&gt;  &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;SparkContext&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sparkConf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ssc&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;StreamingContext&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Seconds&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lines&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ssc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;socketTextStream&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;localhost&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;8888&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;jsonf&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lines&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;map&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;JSON&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;parseFull&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)).&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;map&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;get&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;asInstanceOf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scala&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;collection&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;immutable&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;Map&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;String&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Any&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;]])&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;logs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;jsonf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;map&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;LogStashV1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;message&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;toString&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;path&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;toString&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;host&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;toString&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;lineno&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;toString&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;toDouble&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;@timestamp&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;toString&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;r&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;logs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;filter&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;equals&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;/var/log/system.log&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)).&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;filter&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;lineno&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;70&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;map&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;message&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;lineno&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)).&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;reduceByKey&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
      &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;_1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;_1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;_2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;_2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;o&quot;&gt;}).&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;updateStateByKey&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;updatestatefunc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;filter&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;_2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;avgTrend&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;abs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;ssc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;start&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;ssc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;awaitTermination&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;()&lt;/span&gt;
  &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;这里因为流数据只有 sum 和 count，但是又想留存两个 trend 数据，所以使用了一个新的 cast class，把 trend 数据作为 class 的 value member。对于 state 来说，看到的就是一整个 class 了。&lt;/p&gt;
&lt;p&gt;依然有参考资料：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://blog.cloudera.com/blog/2014/11/how-to-do-near-real-time-sessionization-with-spark-streaming-and-apache-hadoop/&quot;&gt;http://blog.cloudera.com/blog/2014/11/how-to-do-near-real-time-sessionization-with-spark-streaming-and-apache-hadoop/&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.scottlogic.com/blog/2013/07/29/spark-stream-analysis.html&quot;&gt;http://www.scottlogic.com/blog/2013/07/29/spark-stream-analysis.html&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/rshepherd/spark-streaming-average/blob/master/src/main/scala/StreamingAverage.scala&quot;&gt;https://github.com/rshepherd/spark-streaming-average/blob/master/src/main/scala/StreamingAverage.scala&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
    &lt;hr&gt;
    
    &lt;hr&gt;
  &lt;!-- JiaThis Button BEGIN --&gt;
&lt;div class=&quot;jiathis_style&quot;&gt;
&lt;span class=&quot;jiathis_txt&quot;&gt;分享到：&lt;/span&gt;
&lt;a class=&quot;jiathis_button_tsina&quot;&gt;新浪微博&lt;/a&gt;
&lt;a class=&quot;jiathis_button_weixin&quot;&gt;微信&lt;/a&gt;
&lt;a class=&quot;jiathis_button_renren&quot;&gt;人人网&lt;/a&gt;
&lt;a class=&quot;jiathis_button_ydnote&quot;&gt;有道云笔记&lt;/a&gt;
&lt;a class=&quot;jiathis_button_gmail&quot;&gt;Gmail邮箱&lt;/a&gt;
&lt;a class=&quot;jiathis_button_twitter&quot;&gt;Twitter&lt;/a&gt;
&lt;a class=&quot;jiathis_button_googleplus&quot;&gt;Google+&lt;/a&gt;
&lt;a class=&quot;jiathis_button_hi&quot;&gt;百度空间&lt;/a&gt;
&lt;a class=&quot;jiathis_button_fb&quot;&gt;Facebook&lt;/a&gt;
&lt;a class=&quot;jiathis_button_douban&quot;&gt;豆瓣&lt;/a&gt;
&lt;a href=&quot;http://www.jiathis.com/share?uid=1589850&quot; class=&quot;jiathis jiathis_txt jiathis_separator jtico jtico_jiathis&quot; target=&quot;_blank&quot;&gt;更多&lt;/a&gt;
&lt;a class=&quot;jiathis_counter_style&quot;&gt;&lt;/a&gt;
&lt;/div&gt;
&lt;script type=&quot;text/javascript&quot;&gt;
var jiathis_config={
	data_track_clickback:true,
	summary:&quot;&quot;,
	ralateuid:{
		&quot;tsina&quot;:&quot;1035836154&quot;
	},
	shortUrl:false,
	hideMore:false
}
&lt;/script&gt;
&lt;script type=&quot;text/javascript&quot; src=&quot;http://v3.jiathis.com/code/jia.js?uid=1589850&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt;
&lt;!-- JiaThis Button END --&gt;
&lt;!-- UY BEGIN --&gt;


&lt;!-- UY END --&gt;
  &lt;/div&gt;

</description>
        <pubDate>Sat, 14 Feb 2015 00:00:00 +0800</pubDate>
        <link>http://iftti.com/posts/2015/2015-02-14-spark-streaming-state-4e57fc126.html</link>
        <guid isPermaLink="true">http://iftti.com/posts/2015/2015-02-14-spark-streaming-state-4e57fc126.html</guid>
        
        
        <category>chenlinux</category>
        
      </item>
    
      <item>
        <title>spark streaming 和 spark sql 结合示例</title>
        <description>

  
  &lt;div style=&quot;background-color: #FFF;&quot;&gt;
    &lt;p&gt;之前在博客上演示过如果在 spark 里读取 elasticsearch 中的数据。自然往下一步想，是不是可以把一些原先需要定期请求 elasticsearch 的监控内容挪到 spark 里完成？这次就是探讨一下 spark streaming 环境上如何快速统计各维度的数据。期望目标是，可以实现对流数据的异常模式过滤。平常只需要简单调整模式即可。&lt;/p&gt;
&lt;h2 id=&quot;spark-&quot;&gt;spark 基础预备&lt;/h2&gt;
&lt;p&gt;之前作为示例，都是直接在 spark-shell 交互式命令行里完成的。这次说说在正式的情况下怎么做。&lt;/p&gt;
&lt;p&gt;spark 是用 scala 写的，scala 的打包工具叫 sbt。首先通过 &lt;code&gt;sudo port install sbt&lt;/code&gt; 安装好。然后创建目录：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;mkdir -p ./logstash/src/main/scala/
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;sbt 打包的配置文件则放在 &lt;code&gt;./logstash/logstash.sbt&lt;/code&gt; 位置。内容如下(注意之间的空行是必须的)：&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;:=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;LogStash Project&quot;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;version&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;:=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;1.0&quot;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;scalaVersion&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;:=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;2.10.4&quot;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;libraryDependencies&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;org.apache.spark&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%%&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;spark-core&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;1.2.0&quot;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;libraryDependencies&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;org.apache.spark&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%%&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;spark-streaming&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;1.2.0&quot;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;libraryDependencies&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;org.apache.spark&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%%&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;spark-sql&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;1.2.0&quot;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;然后是程序主文件 &lt;code&gt;./logstash/src/main/scala/LogStash.scala&lt;/code&gt;，先来一个最简单的，从 logstash/output/tcp 收数据并解析出来。注意，因为 spark 只能用 pull 方式获取数据，所以 logstash/output/tcp 必须以 &lt;code&gt;mode =&amp;gt; &#39;server&#39;&lt;/code&gt; 方式运行。 &lt;/p&gt;
&lt;pre&gt;&lt;code&gt;output {
    tcp {
        codec =&amp;gt; json_lines
        mode  =&amp;gt; &#39;server&#39;
        port  =&amp;gt; 8888
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&quot;spark-streaming-&quot;&gt;spark streaming 基础示例&lt;/h2&gt;
&lt;p&gt;编辑主文件如下：&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;org.apache.spark.SparkConf&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;org.apache.spark.SparkContext&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;org.apache.spark.SparkContext._&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;org.apache.spark.streaming.&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Seconds&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;StreamingContext&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;org.apache.spark.streaming.StreamingContext._&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;scala.util.parsing.json.JSON&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;object&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;LogStash&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;main&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;args:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Array&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;String&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;])&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sparkConf&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;SparkConf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;setMaster&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;local[2]&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;setAppName&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;LogStash&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sc&lt;/span&gt;  &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;SparkContext&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sparkConf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ssc&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;StreamingContext&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Seconds&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lines&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ssc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;socketTextStream&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;localhost&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;8888&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;jsonf&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lines&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;map&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;JSON&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;parseFull&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)).&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;map&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;get&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;asInstanceOf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scala&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;collection&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;immutable&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;Map&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;String&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Any&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;]])&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;jsonf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;filter&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;lineno&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)==&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;75&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;window&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Seconds&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;30&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)).&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;foreachRDD&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rdd&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;rdd&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;foreach&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;r&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;println&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;path&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;))&lt;/span&gt;
      &lt;span class=&quot;o&quot;&gt;})&lt;/span&gt;
    &lt;span class=&quot;o&quot;&gt;})&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;ssc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;start&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;ssc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;awaitTermination&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;()&lt;/span&gt;
  &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;非常一目了然，每 10 秒挪动一次 window，window 宽度是 30 秒，把 JSON 数据解析出来以后，做过滤和循环输出。这里需要提示一下的是 &lt;code&gt;.foreachRDD&lt;/code&gt; 方法。这是一个 output 方法。spark streaming 里对 input 收到的 DStream 一定要有 output 处理，那么最常见的就是用 foreachRDD 把 DStream 里的 RDDs 循环一遍，做 save 啊，print 啊等等后续。&lt;/p&gt;
&lt;p&gt;然后用 sbt 工具编译后就可以运行了：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sbt package &amp;amp;&amp;amp; ./spark-1.2.0-bin-hadoop2.4/bin/spark-submit --class &quot;LogStash&quot; --master local[2] target/scala-2.10/logstash-project_2.10-1.0.jar
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&quot;sql-&quot;&gt;进阶：数据映射和 SQL 处理&lt;/h2&gt;
&lt;p&gt;下面看如何在 spark streaming 上使用 spark SQL。前面通过解析 JSON，得到的是 Map 类型的数据，这个无法直接被 SQL 使用。通常的做法是，通过预定的 scala 里的 &lt;code&gt;cast class&lt;/code&gt;，来转换成 spark SQL 支持的表类型。主文件改成这样：&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;org.apache.spark.SparkConf&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;org.apache.spark.SparkContext&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;org.apache.spark.SparkContext._&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;org.apache.spark.streaming.&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Seconds&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;StreamingContext&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;org.apache.spark.streaming.StreamingContext._&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;org.apache.spark.sql.SQLContext&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;org.apache.spark.sql._&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;scala.util.parsing.json.JSON&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;object&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;LogStash&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;case&lt;/span&gt; &lt;span class=&quot;kd&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;LogStashV1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;message:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;String&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nl&quot;&gt;path:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;String&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nl&quot;&gt;host:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;String&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nl&quot;&gt;lineno:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Double&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nl&quot;&gt;timestamp:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;String&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;case&lt;/span&gt; &lt;span class=&quot;kd&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;AlertMsg&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;host:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;String&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nl&quot;&gt;count:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Int&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nl&quot;&gt;value:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Double&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;main&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;args:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Array&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;String&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;])&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sparkConf&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;SparkConf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;setMaster&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;local[2]&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;setAppName&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;LogStash&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sc&lt;/span&gt;  &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;SparkContext&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sparkConf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ssc&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;StreamingContext&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Seconds&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sqc&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;SQLContext&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sqc._&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lines&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ssc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;socketTextStream&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;localhost&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;8888&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;jsonf&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lines&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;map&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;JSON&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;parseFull&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)).&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;map&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;get&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;asInstanceOf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scala&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;collection&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;immutable&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;Map&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;String&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Any&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;]])&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;logs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;jsonf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;map&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;LogStashV1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;message&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;toString&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;path&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;toString&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;host&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;toString&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;lineno&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;toString&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;toDouble&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;@timestamp&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;toString&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;logs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;foreachRDD&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rdd&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;rdd&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;registerAsTable&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;logstash&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sqlreport&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sqc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;sql&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;SELECT message, COUNT(message) AS host_c, SUM(lineno) AS line_a FROM logstash WHERE path = &#39;/var/log/system.log&#39; AND lineno &amp;gt; 70 GROUP BY message ORDER BY host_c DESC LIMIT 100&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;sqlreport&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;map&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;AlertMsg&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;toString&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;toString&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;toInt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;toString&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;toDouble&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)).&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;collect&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;foreach&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;println&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;o&quot;&gt;})&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;ssc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;start&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;ssc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;awaitTermination&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;()&lt;/span&gt;
  &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;通过加载 SQLContext，就可以把 RDD 转换成 table，然后通过 SQL 方式写请求了。这里有一个地方需要注意的是，因为最开始转换 JSON 的时候，键值对的 value 类型是 Any(因为要兼容复杂结构)，所以后面赋值的时候需要具体转换成合适的类型。于是悲催的就有了 &lt;code&gt;.toString.toInt&lt;/code&gt; 这样的写法。。。&lt;/p&gt;
&lt;h2 id=&quot;sql--1&quot;&gt;同样效果的非 SQL 实现&lt;/h2&gt;
&lt;p&gt;不用 spark SQL 当然也能做到，而且如果需要复杂处理的时候，还少不了自己写。如果把上例中那段 foreachRDD 替换成下面这样，效果是完全一样的：&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;&lt;span class=&quot;n&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;r&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;logs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;filter&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;equals&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;/var/log/system.log&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)).&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;filter&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;lineno&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;70&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;host_c&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;map&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;message&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;reduceByKey&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;groupByKey&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;map&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;message&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;lineno&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;reduceByKey&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;groupByKey&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;join&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;host_c&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;foreachRDD&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rdd&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;rdd&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;map&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;AlertMsg&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;_1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;_2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;_2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;head&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;_2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;_1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;head&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)).&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;collect&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;foreach&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;println&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;o&quot;&gt;})&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;这里面用到的 &lt;code&gt;.groupByKey&lt;/code&gt; 和 &lt;code&gt;.reduceByKey&lt;/code&gt; 方法，都是专门针对 PairsDStream 对象的，所以前面必须通过 &lt;code&gt;.map&lt;/code&gt; 方法把普通 DStream 转换一下。&lt;/p&gt;
&lt;p&gt;这里还有一个很厉害的方法，叫 &lt;code&gt;.updatestateByKey&lt;/code&gt; 。可以有一个 checkpoint 存上一个 window 的数据，具体示例稍后更新。&lt;/p&gt;
&lt;h2 id=&quot;jsonrdd-&quot;&gt;更简洁的 jsonRDD 方法&lt;/h2&gt;
&lt;p&gt;在简单需求的时候，可能还是觉得能用 SQL 就用 SQL 比较好。但是提前定义 cast class 真的比较麻烦。其实对于 JSON 数据，spark SQL 是有提供更简洁的处理接口的。可以直接写成这样：&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;org.apache.spark.SparkConf&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;org.apache.spark.SparkContext&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;org.apache.spark.SparkContext._&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;org.apache.spark.streaming.&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Seconds&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;StreamingContext&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;org.apache.spark.streaming.StreamingContext._&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;org.apache.spark.sql.SQLContext&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;org.apache.spark.sql._&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;object&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;LogStash&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;case&lt;/span&gt; &lt;span class=&quot;kd&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;AlertMsg&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;host:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;String&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nl&quot;&gt;count:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;String&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nl&quot;&gt;value:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;String&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;main&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;args:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Array&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;String&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;])&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sparkConf&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;SparkConf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;setMaster&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;local[2]&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;setAppName&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;LogStash&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sc&lt;/span&gt;  &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;SparkContext&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sparkConf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ssc&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;StreamingContext&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Seconds&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sqc&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;SQLContext&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sqc._&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lines&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ssc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;socketTextStream&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;localhost&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;8888&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;lines&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;foreachRDD&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rdd&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
      &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rdd&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;count&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sqc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;jsonRDD&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rdd&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;//        t.printSchema()&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;registerTempTable&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;logstash&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sqlreport&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sqc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;sql&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;SELECT host, COUNT(host) AS host_c, AVG(lineno) AS line_a FROM logstash WHERE path = &#39;/var/log/system.log&#39; AND lineno &amp;gt; 70 GROUP BY host ORDER BY host_c DESC LIMIT 100&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;sqlreport&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;map&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;AlertMsg&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;toString&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;toString&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;toString&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)).&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;collect&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;foreach&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;println&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
    &lt;span class=&quot;o&quot;&gt;})&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;ssc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;start&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;ssc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;awaitTermination&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;()&lt;/span&gt;
  &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;这样，不用自己解析 JSON，直接加载到 SQLContext 里。可以通过 &lt;code&gt;.printSchema&lt;/code&gt; 方法查看到 JSON 被转换成了什么样的表结构。&lt;/p&gt;
&lt;h2 id=&quot;todo&quot;&gt;TODO&lt;/h2&gt;
&lt;p&gt;SQL 的方式可以很方便的做到对实时数据的阈值监控处理，但是 SQL 是建立在 RDD 上的如何利用 DStream 的上一个 window 的 state 状态实现比如环比变化处理，移动均线处理，还没找到途径。&lt;/p&gt;
&lt;h2 id=&quot;see-also&quot;&gt;See Also&lt;/h2&gt;
&lt;p&gt;spark 目前文档不多，尤其是 streaming 和 SQL 方面的。感谢下面两个网址，对我上手帮助颇多：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://apache-spark-user-list.1001560.n3.nabble.com/Spark-Streaming-Json-file-groupby-function-td9618.html&quot;&gt;http://apache-spark-user-list.1001560.n3.nabble.com/Spark-Streaming-Json-file-groupby-function-td9618.html&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://databricks.gitbooks.io/databricks-spark-reference-applications/content/logs_analyzer/chapter1/windows.html&quot;&gt;http://databricks.gitbooks.io/databricks-spark-reference-applications/content/logs_analyzer/chapter1/windows.html&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
    &lt;hr&gt;
    
    &lt;hr&gt;
  &lt;!-- JiaThis Button BEGIN --&gt;
&lt;div class=&quot;jiathis_style&quot;&gt;
&lt;span class=&quot;jiathis_txt&quot;&gt;分享到：&lt;/span&gt;
&lt;a class=&quot;jiathis_button_tsina&quot;&gt;新浪微博&lt;/a&gt;
&lt;a class=&quot;jiathis_button_weixin&quot;&gt;微信&lt;/a&gt;
&lt;a class=&quot;jiathis_button_renren&quot;&gt;人人网&lt;/a&gt;
&lt;a class=&quot;jiathis_button_ydnote&quot;&gt;有道云笔记&lt;/a&gt;
&lt;a class=&quot;jiathis_button_gmail&quot;&gt;Gmail邮箱&lt;/a&gt;
&lt;a class=&quot;jiathis_button_twitter&quot;&gt;Twitter&lt;/a&gt;
&lt;a class=&quot;jiathis_button_googleplus&quot;&gt;Google+&lt;/a&gt;
&lt;a class=&quot;jiathis_button_hi&quot;&gt;百度空间&lt;/a&gt;
&lt;a class=&quot;jiathis_button_fb&quot;&gt;Facebook&lt;/a&gt;
&lt;a class=&quot;jiathis_button_douban&quot;&gt;豆瓣&lt;/a&gt;
&lt;a href=&quot;http://www.jiathis.com/share?uid=1589850&quot; class=&quot;jiathis jiathis_txt jiathis_separator jtico jtico_jiathis&quot; target=&quot;_blank&quot;&gt;更多&lt;/a&gt;
&lt;a class=&quot;jiathis_counter_style&quot;&gt;&lt;/a&gt;
&lt;/div&gt;
&lt;script type=&quot;text/javascript&quot;&gt;
var jiathis_config={
	data_track_clickback:true,
	summary:&quot;&quot;,
	ralateuid:{
		&quot;tsina&quot;:&quot;1035836154&quot;
	},
	shortUrl:false,
	hideMore:false
}
&lt;/script&gt;
&lt;script type=&quot;text/javascript&quot; src=&quot;http://v3.jiathis.com/code/jia.js?uid=1589850&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt;
&lt;!-- JiaThis Button END --&gt;
&lt;!-- UY BEGIN --&gt;


&lt;!-- UY END --&gt;
  &lt;/div&gt;

</description>
        <pubDate>Fri, 13 Feb 2015 00:00:00 +0800</pubDate>
        <link>http://iftti.com/posts/2015/2015-02-13-spark-streaming-sql-d9299745e.html</link>
        <guid isPermaLink="true">http://iftti.com/posts/2015/2015-02-13-spark-streaming-sql-d9299745e.html</guid>
        
        
        <category>chenlinux</category>
        
      </item>
    
      <item>
        <title>rsyslog 的 TCP 转发性能测试</title>
        <description>

  
  &lt;div style=&quot;background-color: #FFF;&quot;&gt;
    &lt;p&gt;做一个日志手机系统，一般有两个思路。一个是提供一个多语言 SDK 包，然后开发者只需要找到对应的 SDK 加载即可；一个是采用最通用的日志传输协议，让开发者采用现成的协议实现。在通用协议里，最常见的，就是 syslog 协议。不过 syslog 过去采用 UDP 的印象太过深入人心，rsyslog 虽然宣称在测试用达到了每秒上百万的性能，也没多少人相信。那么，到底用 syslog 协议做跨网络传输，靠不靠谱？自己用压测，来证明一下！&lt;/p&gt;
&lt;h2 id=&quot;section&quot;&gt;测试环境&lt;/h2&gt;
&lt;p&gt;两台测试机。其中：&lt;/p&gt;
&lt;p&gt;A 配置为 imtcp/514，omfwd 到 B 的 514。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;module( load=&quot;imtcp&quot; )
input( type=&quot;imtcp&quot; port=&quot;514&quot; ruleset=&quot;forwardruleset&quot; )
Ruleset( name=&quot;forwardruleset&quot; )
{
    action (
        type=&quot;omfwd&quot;
        Target=&quot;$b-server-ip&quot;
        Port=&quot;514&quot;
        Protocol=&quot;tcp&quot;
        RebindInterval=&quot;5000&quot;
        name=&quot;action_fwd&quot;
        queue.filename=&quot;action_fwd&quot;
        queue.size=&quot;50000&quot;
        queue.dequeuebatchsize=&quot;1000&quot;
        queue.maxdiskspace=&quot;5G&quot;
        queue.discardseverity=&quot;3&quot;
        queue.checkpointinterval=&quot;10&quot;
        queue.type=&quot;linkedlist&quot;
        queue.workerthreads=&quot;1&quot;
        queue.timeoutshutdown=&quot;10&quot;
        queue.timeoutactioncompletion=&quot;10&quot;
        queue.timeoutenqueue=&quot;20&quot;
        queue.timeoutworkerthreadshutdown=&quot;10&quot;
        queue.workerthreadminimummessages=&quot;5000&quot;
        queue.maxfilesize=&quot;500M&quot;
        queue.saveonshutdown=&quot;on&quot;
    )
    stop
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;B 配置为 imtcp/514，omfile 到本机。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;module( load=&quot;imtcp&quot; )
input( type=&quot;imtcp&quot; port=&quot;514&quot; ruleset=&quot;recordruleset&quot; )
Ruleset( name=&quot;recordruleset&quot; )
{
    action( type=&quot;omfile&quot; file=&quot;/data1/debug.log&quot; template=&quot;defaultLogFormat&quot; asyncWriting=&quot;on&quot; flushOnTXEnd=&quot;off&quot; ioBufferSize=&quot;81920k&quot; flushInterval=&quot;5&quot;)
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&quot;section-1&quot;&gt;测试工具&lt;/h2&gt;
&lt;p&gt;为了控制测试的速度，放弃之前压测 logstash 时候用的 logger 命令，采用 syslog-ng 项目自带的 loggen 命令。本来准备编译一下 syslog-ng，不过报错太多，实在复杂，看了一下 loggen.c 本身没啥依赖，所以决定采用最简单的办法获取 loggen 命令——下载 syslog-ng.rpm，然后直接解压压缩包！&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;wget http://mirrors.zju.edu.cn/epel/5/x86_64/syslog-ng-2.1.4-9.el5.x86_64.rpm
rpm2cpio syslog-ng-2.1.4-9.el5.x86_64.rpm  | cpio -div
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;我这不能直接通过 yum install 安装，因为 syslog-ng 跟系统里已有的 rsyslog 是冲突的。&lt;/em&gt;&lt;/p&gt;
&lt;h2 id=&quot;section-2&quot;&gt;测试命令&lt;/h2&gt;
&lt;p&gt;rpm 获取的 loggen 命令还不支持 &lt;code&gt;--read-data&lt;/code&gt; 参数，只能自己模拟填充数据。所以测试命令如下：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;./usr/bin/loggen -r 10000 -i -s 500 -I 600 $a-server-ip 514
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;意即单条长度 500 字节，每秒 10000 条的频率，持续发送 600 秒。&lt;/p&gt;
&lt;h2 id=&quot;section-3&quot;&gt;验证方式&lt;/h2&gt;
&lt;p&gt;rsyslog 有专门的 impstats 模块，输出本身运行情况的统计，可以通过如下配置开启：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;module( load=&quot;impstats&quot; interval=&quot;60&quot; severity=&quot;6&quot; log.syslog=&quot;on&quot; format=&quot;json&quot; resetCounters=&quot;on&quot;)
template( name=&quot;dynaFileRsyslog&quot; type=&quot;string&quot; string=&quot;/data1/rsyslog/impstats/%$year%/%$month%/%$day%_impstats.log&quot; )
if ( $syslogfacility-text == &#39;syslog&#39; ) then
{
    action  ( type=&quot;omfile&quot;  DynaFile=&quot;dynaFileRsyslog&quot; FileCreateMode=&quot;0600&quot; )
    stop
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;每 60 秒会输出 JSON 格式的统计数据，类似这样：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;2015-02-11T20:00:43.176325+08:00 localhost rsyslogd-pstats: {&quot;name&quot;:&quot;action_fwd queue&quot;,&quot;size&quot;:0,&quot;enqueued&quot;:0,&quot;full&quot;:0,&quot;discarded.full&quot;:0,&quot;discarded.nf&quot;:0,&quot;maxqsize&quot;:0}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;其中，enqueued 表示进入队列的条目数，size 表示暂存在内存中的条目数，discarded.full 表示队列满丢弃的条目数，discarded.nf 表示队列将满丢弃的条目数。&lt;/p&gt;
&lt;p&gt;如果内存队列都不够用，那么 rsyslog 会记录到磁盘队列上，这时候看到类似上面的统计数据的另一条记录，区别是 &lt;code&gt;&quot;name&quot;:&quot;action_fwd queue[DA]&quot;&lt;/code&gt;，这个 DA 就是磁盘队列的意思。&lt;/p&gt;
&lt;h2 id=&quot;section-4&quot;&gt;测试结果&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;每秒 5 万条的发送，可以做到毫无 size 的全部即时转发。&lt;/li&gt;
  &lt;li&gt;加大 &lt;code&gt;queue.size&lt;/code&gt; 到 10 倍，即时转发能力提高到 12 万条。&lt;/li&gt;
  &lt;li&gt;再加大 &lt;code&gt;queue.workerthreads&lt;/code&gt; 到 10，即时转发能力提高到 15 万条。&lt;/li&gt;
  &lt;li&gt;单独加大 &lt;code&gt;queue.dequeuebatchsize&lt;/code&gt; 到 10 倍，即时转发能力提高到 17 万条。&lt;/li&gt;
  &lt;li&gt;同时加大 &lt;code&gt;queue.size&lt;/code&gt; 和 &lt;code&gt;queue.dequeuebatchsize&lt;/code&gt; 到 10 倍 ，即时转发能力提高到 18 万条。&lt;/li&gt;
  &lt;li&gt;加大频率到 24 万，进入磁盘队列，因为这时候已经到千兆网卡瓶颈。&lt;/li&gt;
  &lt;li&gt;加大模拟长度到 5000 字节，即时转发能力下降到 1 万。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;最后，尽可能删除掉各种配置，以默认方式运行，发现转发能力也能达到 5 万条。查了一下源码，默认的 &lt;code&gt;queue.size&lt;/code&gt; 是 1000，&lt;code&gt;queue.dequeuebatchsize&lt;/code&gt; 是 16。说明在这段大小(初始测试值是默认值的 50 多倍)内，性能变化不大。&lt;/p&gt;
&lt;h2 id=&quot;section-5&quot;&gt;长期运行&lt;/h2&gt;
&lt;p&gt;测试每次只运行几分钟，还需要长期运行的考验。运行两三天的观察，同时加大到 10 倍的配置(即短期测试可以跑满网卡的配置)，在长期稳定每秒 5 万条的测试中，也会出现内存队列的 size 数。还需继续观察 size 是否累积，以及更大量的情况是否会出现磁盘队列。&lt;/p&gt;
    &lt;hr&gt;
    
    &lt;hr&gt;
  &lt;!-- JiaThis Button BEGIN --&gt;
&lt;div class=&quot;jiathis_style&quot;&gt;
&lt;span class=&quot;jiathis_txt&quot;&gt;分享到：&lt;/span&gt;
&lt;a class=&quot;jiathis_button_tsina&quot;&gt;新浪微博&lt;/a&gt;
&lt;a class=&quot;jiathis_button_weixin&quot;&gt;微信&lt;/a&gt;
&lt;a class=&quot;jiathis_button_renren&quot;&gt;人人网&lt;/a&gt;
&lt;a class=&quot;jiathis_button_ydnote&quot;&gt;有道云笔记&lt;/a&gt;
&lt;a class=&quot;jiathis_button_gmail&quot;&gt;Gmail邮箱&lt;/a&gt;
&lt;a class=&quot;jiathis_button_twitter&quot;&gt;Twitter&lt;/a&gt;
&lt;a class=&quot;jiathis_button_googleplus&quot;&gt;Google+&lt;/a&gt;
&lt;a class=&quot;jiathis_button_hi&quot;&gt;百度空间&lt;/a&gt;
&lt;a class=&quot;jiathis_button_fb&quot;&gt;Facebook&lt;/a&gt;
&lt;a class=&quot;jiathis_button_douban&quot;&gt;豆瓣&lt;/a&gt;
&lt;a href=&quot;http://www.jiathis.com/share?uid=1589850&quot; class=&quot;jiathis jiathis_txt jiathis_separator jtico jtico_jiathis&quot; target=&quot;_blank&quot;&gt;更多&lt;/a&gt;
&lt;a class=&quot;jiathis_counter_style&quot;&gt;&lt;/a&gt;
&lt;/div&gt;
&lt;script type=&quot;text/javascript&quot;&gt;
var jiathis_config={
	data_track_clickback:true,
	summary:&quot;&quot;,
	ralateuid:{
		&quot;tsina&quot;:&quot;1035836154&quot;
	},
	shortUrl:false,
	hideMore:false
}
&lt;/script&gt;
&lt;script type=&quot;text/javascript&quot; src=&quot;http://v3.jiathis.com/code/jia.js?uid=1589850&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt;
&lt;!-- JiaThis Button END --&gt;
&lt;!-- UY BEGIN --&gt;


&lt;!-- UY END --&gt;
  &lt;/div&gt;

</description>
        <pubDate>Thu, 12 Feb 2015 00:00:00 +0800</pubDate>
        <link>http://iftti.com/posts/2015/2015-02-12-rsyslog-forwarder-testing-537b33082.html</link>
        <guid isPermaLink="true">http://iftti.com/posts/2015/2015-02-12-rsyslog-forwarder-testing-537b33082.html</guid>
        
        
        <category>chenlinux</category>
        
      </item>
    
      <item>
        <title>Python 批量写入 Elasticsearch 脚本</title>
        <description>

  
  &lt;div style=&quot;background-color: #FFF;&quot;&gt;
    &lt;p&gt;Elasticsearch 官方和社区提供了各种各样的客户端库，在之前的博客中，我陆陆续续提到和演示过 Perl 的，Javascript 的，Ruby 的。上周写了一版 Python 的，考虑到好像很难找到现成的示例，如何用 python 批量写数据进 Elasticsearch，今天一并贴上来。&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;c&quot;&gt;#!/usr/bin/env pypy&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#coding:utf-8&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;re&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sys&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;time&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;datetime&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;logging&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;elasticsearch&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Elasticsearch&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;elasticsearch&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;helpers&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;elasticsearch&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ConnectionTimeout&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;es&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Elasticsearch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;192.168.0.2&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&#39;192.168.0.3&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sniff_on_start&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sniff_on_connection_fail&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;max_retries&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;retry_on_timeout&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;logging&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;basicConfig&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;logging&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;getLogger&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;elasticsearch&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setLevel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;logging&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;WARN&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;logging&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;getLogger&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;urllib3&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setLevel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;logging&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;WARN&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;parse_www&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;logline&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;try&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;time_local&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;request&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;http_user_agent&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;staTus&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;remote_addr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;http_referer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;request_time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;body_bytes_sent&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;http_x_forwarded_proto&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;http_x_forwarded_for&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;http_host&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;http_cookie&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;upstream_response_time&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;logline&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;`&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
		&lt;span class=&quot;k&quot;&gt;try&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
			&lt;span class=&quot;n&quot;&gt;upstream_response_time&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;upstream_response_time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
		&lt;span class=&quot;k&quot;&gt;except&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
			&lt;span class=&quot;n&quot;&gt;upstream_response_time&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;method&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;uri&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;verb&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;request&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39; &#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;arg&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{}&lt;/span&gt;
		&lt;span class=&quot;k&quot;&gt;try&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
			&lt;span class=&quot;n&quot;&gt;url_path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;url_args&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;uri&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;?&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
			&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;args&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;url_args&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;&amp;amp;&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
				&lt;span class=&quot;n&quot;&gt;k&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;v&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;args&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;=&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
				&lt;span class=&quot;n&quot;&gt;arg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;v&lt;/span&gt;
		&lt;span class=&quot;k&quot;&gt;except&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
			&lt;span class=&quot;n&quot;&gt;url_path&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;uri&lt;/span&gt;
		&lt;span class=&quot;c&quot;&gt;# Why %z do not implement?&lt;/span&gt;
	        &lt;span class=&quot;n&quot;&gt;date&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;datetime&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;datetime&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;strptime&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;time_local&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&#39;[&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%d&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;/%b/%Y:%H:%M:%S +0800]&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;ret&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
			&lt;span class=&quot;s&quot;&gt;&quot;@timestamp&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;date&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;strftime&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%F&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;T%T+0800&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
			&lt;span class=&quot;s&quot;&gt;&quot;host&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;127.0.0.1&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
			&lt;span class=&quot;s&quot;&gt;&quot;method&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;method&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lstrip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;&quot;&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
			&lt;span class=&quot;s&quot;&gt;&quot;url_path&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;url_path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
			&lt;span class=&quot;s&quot;&gt;&quot;url_args&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;arg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
			&lt;span class=&quot;s&quot;&gt;&quot;verb&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;verb&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rstrip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;&quot;&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
			&lt;span class=&quot;s&quot;&gt;&quot;http_user_agent&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;http_user_agent&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
			&lt;span class=&quot;s&quot;&gt;&quot;status&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;staTus&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
			&lt;span class=&quot;s&quot;&gt;&quot;remote_addr&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;remote_addr&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;strip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;[]&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
			&lt;span class=&quot;s&quot;&gt;&quot;http_referer&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;http_referer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
			&lt;span class=&quot;s&quot;&gt;&quot;request_time&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;request_time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
			&lt;span class=&quot;s&quot;&gt;&quot;body_bytes_sent&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;body_bytes_sent&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
			&lt;span class=&quot;s&quot;&gt;&quot;http_x_forwarded_proto&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;http_x_forwarded_proto&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
			&lt;span class=&quot;s&quot;&gt;&quot;http_x_forwarded_for&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;http_x_forwarded_for&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
			&lt;span class=&quot;s&quot;&gt;&quot;http_host&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;http_host&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
			&lt;span class=&quot;s&quot;&gt;&quot;http_cookie&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;http_cookie&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
			&lt;span class=&quot;s&quot;&gt;&quot;upstream_response_time&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;upstream_response_time&lt;/span&gt;
		&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
		&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;_index&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;logstash-mweibo-www-&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;date&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;strftime&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;%Y.%m.&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%d&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;_type&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;nginx&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;_source&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ret&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;except&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
		&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;_index&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;logstash-mweibo-www-&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;datetime&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;datetime&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;now&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;strftime&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;%Y.%m.&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%d&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;_type&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;nginx&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;_source&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:{&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;message&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;logline&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}}&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;get_log&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;start_time&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;log_buffer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;while&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;try&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;line&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sys&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stdin&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;readline&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;except&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;break&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;line&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;helpers&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bulk&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;es&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;log_buffer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;del&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;log_buffer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;log_buffer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;break&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;line&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;ret&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;parse_www&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;line&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rstrip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;log_buffer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ret&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;while&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;log_buffer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2000&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;and&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;log_buffer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2000&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
                &lt;span class=&quot;k&quot;&gt;try&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
                    &lt;span class=&quot;n&quot;&gt;helpers&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bulk&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;es&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;log_buffer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
                &lt;span class=&quot;k&quot;&gt;except&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ConnectionTimeout&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
                    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;try again&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
                    &lt;span class=&quot;k&quot;&gt;continue&lt;/span&gt;
                &lt;span class=&quot;k&quot;&gt;del&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;log_buffer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;log_buffer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;
                &lt;span class=&quot;k&quot;&gt;break&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;startime&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;timeout&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;helpers&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bulk&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;es&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;log_buffer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;start_time&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
                &lt;span class=&quot;k&quot;&gt;del&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;log_buffer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;log_buffer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sleep&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;__name__&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&#39;__main__&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;get_log&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;和 Perl、Ruby 的客户端不同，Python 的客户端只支持两种 transport 方式，urllib3 或者 thrift。也就是说，木有像事件驱动啊之类的办法。&lt;/p&gt;
&lt;p&gt;测试一下，这个脚本如果不发送数据，一秒处理日志条数在15k，发送数据，一秒只有2k。确实比较让人失望，于是决定换成 pypy 试试——我司不少日志处理脚本都是用 pypy 运行的。&lt;/p&gt;
&lt;p&gt;服务器上使用 pypy ，是通过 EPEL 安装的，之前都只用核心模块，这次需要安装 elasticsearch 模块。所以需要先给 pypy 加上 pip：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;wget https://raw.github.com/pypa/pip/master/contrib/get-pip.py
pypy get-pip.py
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;网上大多说之前还要下载一个叫 distribute_setup.py 的脚本来运行，实测不需要，而且这个脚本的下载链接也失效了。&lt;/p&gt;
&lt;p&gt;然后通过 pip 安装 elasticsearch 包即可：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;/usr/lib64/pypy-2.0.2/bin/pip install elasticsearch
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;测试，pypy 比 python 处理日志速度快一倍，写 ES 速度快一半。不过 3300eps 依然很慢就是了。&lt;/p&gt;
&lt;h2 id=&quot;section&quot;&gt;测试中碰到的其他问题&lt;/h2&gt;
&lt;p&gt;可以看到脚本里已经设置了多次重试和超时重连，不过依然会收到写入超时和失败的返回，原来 Elasticsearch 默认对每个 node 做 segment merge 的时候，有磁盘保护措施，速度上限限制在 20MB/s。这在压测的时候就容易触发。&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;[2015-01-10 09:41:51,273][INFO ][index.engine.internal ] [node1][logstash-2015.01.10][2] now throttling indexing: numMergesInFlight=6,maxNumMerges=5&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;修改配置重启即可：&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-yaml&quot; data-lang=&quot;yaml&quot;&gt;&lt;span class=&quot;l-Scalar-Plain&quot;&gt;indices.store.throttle.type：merge&lt;/span&gt;
&lt;span class=&quot;l-Scalar-Plain&quot;&gt;indices.store.throttle.max_bytes_per_sec：500mb&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;关于这个问题，ES 也有讨论：&lt;a href=&quot;https://github.com/elasticsearch/elasticsearch/issues/6081&quot;&gt;Should we lower the default merge IO throttle rate?&lt;/a&gt;。或许未来会有更灵活的策略。&lt;/p&gt;
&lt;p&gt;更多 ES 性能测试和优化建议，参考：&lt;a href=&quot;http://www.elasticsearch.org/guide/en/elasticsearch/guide/current/indexing-performance.html&quot;&gt;http://www.elasticsearch.org/guide/en/elasticsearch/guide/current/indexing-performance.html&lt;/a&gt;&lt;/p&gt;
    &lt;hr&gt;
    
    &lt;hr&gt;
  &lt;!-- JiaThis Button BEGIN --&gt;
&lt;div class=&quot;jiathis_style&quot;&gt;
&lt;span class=&quot;jiathis_txt&quot;&gt;分享到：&lt;/span&gt;
&lt;a class=&quot;jiathis_button_tsina&quot;&gt;新浪微博&lt;/a&gt;
&lt;a class=&quot;jiathis_button_weixin&quot;&gt;微信&lt;/a&gt;
&lt;a class=&quot;jiathis_button_renren&quot;&gt;人人网&lt;/a&gt;
&lt;a class=&quot;jiathis_button_ydnote&quot;&gt;有道云笔记&lt;/a&gt;
&lt;a class=&quot;jiathis_button_gmail&quot;&gt;Gmail邮箱&lt;/a&gt;
&lt;a class=&quot;jiathis_button_twitter&quot;&gt;Twitter&lt;/a&gt;
&lt;a class=&quot;jiathis_button_googleplus&quot;&gt;Google+&lt;/a&gt;
&lt;a class=&quot;jiathis_button_hi&quot;&gt;百度空间&lt;/a&gt;
&lt;a class=&quot;jiathis_button_fb&quot;&gt;Facebook&lt;/a&gt;
&lt;a class=&quot;jiathis_button_douban&quot;&gt;豆瓣&lt;/a&gt;
&lt;a href=&quot;http://www.jiathis.com/share?uid=1589850&quot; class=&quot;jiathis jiathis_txt jiathis_separator jtico jtico_jiathis&quot; target=&quot;_blank&quot;&gt;更多&lt;/a&gt;
&lt;a class=&quot;jiathis_counter_style&quot;&gt;&lt;/a&gt;
&lt;/div&gt;
&lt;script type=&quot;text/javascript&quot;&gt;
var jiathis_config={
	data_track_clickback:true,
	summary:&quot;&quot;,
	ralateuid:{
		&quot;tsina&quot;:&quot;1035836154&quot;
	},
	shortUrl:false,
	hideMore:false
}
&lt;/script&gt;
&lt;script type=&quot;text/javascript&quot; src=&quot;http://v3.jiathis.com/code/jia.js?uid=1589850&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt;
&lt;!-- JiaThis Button END --&gt;
&lt;!-- UY BEGIN --&gt;


&lt;!-- UY END --&gt;
  &lt;/div&gt;

</description>
        <pubDate>Wed, 11 Feb 2015 00:00:00 +0800</pubDate>
        <link>http://iftti.com/posts/2015/2015-02-11-python-elasticsearch-bulk-41b467215.html</link>
        <guid isPermaLink="true">http://iftti.com/posts/2015/2015-02-11-python-elasticsearch-bulk-41b467215.html</guid>
        
        
        <category>chenlinux</category>
        
      </item>
    
      <item>
        <title>LogStash::Outputs::ElaticSearch 使用 http 协议时的内存泄露问题</title>
        <description>

  
  &lt;div style=&quot;background-color: #FFF;&quot;&gt;
    &lt;p&gt;Logstash 早年有三种不同的插件写数据到 Elasticsearch 中，分别采用 node，http 和 river 方式。从 1.4 版本以后，在重构的 LogStash::Outputs::ElasticSearch 插件中，通过 &lt;code&gt;protocol&lt;/code&gt; 参数，完成了对多种方式的整合。其中，node 和 transport 方式，都是调用 Java 库的 API，而 http 方式，则调用的 REST API。&lt;/p&gt;
&lt;p&gt;在 Elasticsearch 集群和 Logstash 集群不在一个网段的时候，一般都只能采用 REST API 写数据。而且根据测试情况，采用 http 方式的写入性能，也要稍微高过 node 方式，所以，我一直都推荐采用这种方式。不过随着系统的长期运行，却发现日志流转总是不太顺畅，实际写入 Elasticsearch 的数据慢慢的就会越来越少。因为 Logstash 本身内部并无缓存机制，所以比较难判断到底是哪步出了问题——甚至可能就是 Elasticsearch 在高负载情况下就写不动？&lt;/p&gt;
&lt;p&gt;和 childe 聊了一下携程采用 transport 方式运行的情况，发现他们的 Elasticsearch 集群没有出现过类似越写越少的情况。把 logstash 的配置改成写文件，也一直没有再出现堵塞消息队列的情况。问题就此锁定在 logstash 写数据的 http 过程中。&lt;/p&gt;
&lt;p&gt;进到源码目录里阅读&lt;a href=&quot;https://github.com/elasticsearch/logstash/blob/1.4/lib/logstash/outputs/elasticsearch/protocol.rb#L67&quot;&gt;相关代码&lt;/a&gt;，发现在 &lt;code&gt;build_client&lt;/code&gt; 方法里有很有趣的一段注释：&lt;/p&gt;
&lt;blockquote&gt;
  &lt;h1 id=&quot;use-ftw-to-do-indexing-requests-for-now-until-we&quot;&gt;Use FTW to do indexing requests, for now, until we&lt;/h1&gt;
  &lt;p&gt;# can identify and resolve performance problems of elasticsearch-ruby&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;这个好玩了。因为我在两年前用过官方出的 elasticsearch 的 Perl 客户端库，性能是非常不错的。怎么 Ruby 库会这么被嫌弃？&lt;/p&gt;
&lt;p&gt;于是又切换到当前最新的 1.5.0beta1 版本看看这块是&lt;a href=&quot;https://github.com/logstash-plugins/logstash-output-elasticsearch/blob/master/lib/logstash/outputs/elasticsearch/protocol.rb#L70&quot;&gt;怎么处理的&lt;/a&gt;。最新版已经放弃了作者自己的 FTW 库，用上了官方的 Ruby 库，具体传输层用的是 JRuby 专有的 Manticore 库。&lt;/p&gt;
&lt;p&gt;然后又发现 github 上几个相关的 issue：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/elasticsearch/logstash/issues/1604&quot;&gt;File descriptors are leaked when using HTTP&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/elasticsearch/logstash/pull/1777&quot;&gt;Add HTTP Auth and SSL to the ES output plugin&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/cheald/manticore/wiki/Performance&quot;&gt;manticore wiki/Performance&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;所以问题很明确了，logstash-1.4.2 依赖的 ftw-0.0.39，有内存泄露问题。logstash 开发者在去年十一月升级了 ftw-0.0.40 解决这个问题，但是 logstash-1.4 那时候已经没有 release 计划了…… 差不多同时间，LogStash::Outputs::ElasticSearch 更换了底层 HTTP 依赖库为性能跟 FTW 相近的 Manticore，并且在前些天随 1.5.0beta1 版本发布。&lt;/p&gt;
&lt;p&gt;升级成 1.5.0beta1 后，测试运行几天，Elasticsearch 的写入数据量一直没有下降。可以认定问题解决。&lt;/p&gt;
&lt;p&gt;Logstash-1.5 和 Logstash-1.4 在 plugin API 方面没有什么变化，有写自己 plugin 的童鞋不用太过担心，可以放心测试然后升级使用。我目前发现的唯一一个变化就是：Logstash-1.5 改用 jackson 库替代原生 json 库了。所以原先可以直接：&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-ruby&quot; data-lang=&quot;ruby&quot;&gt;&lt;span class=&quot;n&quot;&gt;parsed&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;JSON&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;parse&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;msg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;现在应该通过 logstash 内部方式调用：&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-ruby&quot; data-lang=&quot;ruby&quot;&gt;&lt;span class=&quot;nb&quot;&gt;require&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&#39;logstash/json&#39;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;parsed&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;LogStash&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;no&quot;&gt;Json&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;load&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;msg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
    &lt;hr&gt;
    
    &lt;hr&gt;
  &lt;!-- JiaThis Button BEGIN --&gt;
&lt;div class=&quot;jiathis_style&quot;&gt;
&lt;span class=&quot;jiathis_txt&quot;&gt;分享到：&lt;/span&gt;
&lt;a class=&quot;jiathis_button_tsina&quot;&gt;新浪微博&lt;/a&gt;
&lt;a class=&quot;jiathis_button_weixin&quot;&gt;微信&lt;/a&gt;
&lt;a class=&quot;jiathis_button_renren&quot;&gt;人人网&lt;/a&gt;
&lt;a class=&quot;jiathis_button_ydnote&quot;&gt;有道云笔记&lt;/a&gt;
&lt;a class=&quot;jiathis_button_gmail&quot;&gt;Gmail邮箱&lt;/a&gt;
&lt;a class=&quot;jiathis_button_twitter&quot;&gt;Twitter&lt;/a&gt;
&lt;a class=&quot;jiathis_button_googleplus&quot;&gt;Google+&lt;/a&gt;
&lt;a class=&quot;jiathis_button_hi&quot;&gt;百度空间&lt;/a&gt;
&lt;a class=&quot;jiathis_button_fb&quot;&gt;Facebook&lt;/a&gt;
&lt;a class=&quot;jiathis_button_douban&quot;&gt;豆瓣&lt;/a&gt;
&lt;a href=&quot;http://www.jiathis.com/share?uid=1589850&quot; class=&quot;jiathis jiathis_txt jiathis_separator jtico jtico_jiathis&quot; target=&quot;_blank&quot;&gt;更多&lt;/a&gt;
&lt;a class=&quot;jiathis_counter_style&quot;&gt;&lt;/a&gt;
&lt;/div&gt;
&lt;script type=&quot;text/javascript&quot;&gt;
var jiathis_config={
	data_track_clickback:true,
	summary:&quot;&quot;,
	ralateuid:{
		&quot;tsina&quot;:&quot;1035836154&quot;
	},
	shortUrl:false,
	hideMore:false
}
&lt;/script&gt;
&lt;script type=&quot;text/javascript&quot; src=&quot;http://v3.jiathis.com/code/jia.js?uid=1589850&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt;
&lt;!-- JiaThis Button END --&gt;
&lt;!-- UY BEGIN --&gt;


&lt;!-- UY END --&gt;
  &lt;/div&gt;

</description>
        <pubDate>Tue, 10 Feb 2015 00:00:00 +0800</pubDate>
        <link>http://iftti.com/posts/2015/2015-02-10-logstash-outputs-elasticsearch-http-memory-leak-9b836f773.html</link>
        <guid isPermaLink="true">http://iftti.com/posts/2015/2015-02-10-logstash-outputs-elasticsearch-http-memory-leak-9b836f773.html</guid>
        
        
        <category>chenlinux</category>
        
      </item>
    
      <item>
        <title>小米运维—互联网企业级监控系统实践</title>
        <description>

                &lt;h2&gt;# Introduction&lt;/h2&gt;
&lt;p&gt;监控系统是整个运维环节，乃至整个产品生命周期中最重要的一环，事前及时预警发现故障，事后提供翔实的数据用于追查定位问题。监控系统作为一个成熟的运维产品，业界有很多开源的实现可供选择。当公司刚刚起步，业务规模较小，运维团队也刚刚建立的初期，选择一款开源的监控系统，是一个省时省力，效率最高的方案。之后，随着业务规模的持续快速增长，监控的对象也越来越多，越来越复杂，监控系统的使用对象也从最初少数的几个SRE，扩大为更多的DEVS，SRE。这时候，监控系统的容量和用户的“使用效率”成了最为突出的问题。&lt;/p&gt;
&lt;p&gt;监控系统业界有很多杰出的开源监控系统。我们在早期，一直在用zabbix，不过随着业务的快速发展，以及互联网公司特有的一些需求，现有的开源的监控系统在性能、扩展性、和用户的使用效率方面，已经无法支撑了。&lt;/p&gt;
&lt;p&gt;因此，我们在过去的一年里，从互联网公司的一些需求出发，从各位SRE、SA、DEVS的使用经验和反馈出发，结合业界的一些大的互联网公司做监控，用监控的一些思考出发，设计开发了小米的监控系统：open-falcon。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;open-falcon的目标是做最开放、最好用的互联网企业级监控产品。&lt;/strong&gt;&lt;/p&gt;
&lt;h2&gt;# Highlights and features&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;strong&gt;强大灵活的数据采集&lt;/strong&gt;：自动发现，支持falcon-agent、snmp、支持用户主动push、用户自定义插件支持、opentsdb data model like（timestamp、endpoint、metric、key-value tags）&lt;/li&gt;
&lt;li&gt;
&lt;strong&gt;水平扩展能力&lt;/strong&gt;：支持每个周期上亿次的数据采集、告警判定、历史数据存储和查询&lt;/li&gt;
&lt;li&gt;
&lt;strong&gt;高效率的告警策略管理&lt;/strong&gt;：高效的portal、支持策略模板、模板继承和覆盖、多种告警方式、支持callback调用&lt;/li&gt;
&lt;li&gt;
&lt;strong&gt;人性化的告警设置&lt;/strong&gt;：最大告警次数、告警级别、告警恢复通知、告警暂停、不同时段不同阈值、支持维护周期&lt;/li&gt;
&lt;li&gt;
&lt;strong&gt;高效率的graph组件&lt;/strong&gt;：单机支撑200万metric的上报、归档、存储（周期为1分钟）&lt;/li&gt;
&lt;li&gt;
&lt;strong&gt;高效的历史数据query组件&lt;/strong&gt;：采用rrdtool的数据归档策略，秒级返回上百个metric一年的历史数据&lt;/li&gt;
&lt;li&gt;
&lt;strong&gt;dashboard&lt;/strong&gt;：多维度的数据展示，用户自定义Screen&lt;/li&gt;
&lt;li&gt;
&lt;strong&gt;高可用&lt;/strong&gt;：整个系统无核心单点，易运维，易部署，可水平扩展&lt;/li&gt;
&lt;li&gt;
&lt;strong&gt;开发语言&lt;/strong&gt;： 整个系统的后端，全部golang编写，portal和dashboard使用python编写。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;# Architecture&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;/images/noops.me/e91c0418abdf19f5f12ced0652c143c2.jpg&quot; alt=&quot;open-falcon architecture&quot;&gt;&lt;br&gt;
备注：虚线所在的aggregator组件还在设计开发阶段。&lt;/p&gt;
&lt;p&gt;每台服务器，都有安装falcon-agent，falcon-agent是一个golang开发的daemon程序，用于自发现的采集单机的各种数据和指标，这些指标包括不限于以下几个方面，共计400多项指标。&lt;br&gt;
   – CPU相关&lt;br&gt;
   – 磁盘相关&lt;br&gt;
   – IO&lt;br&gt;
   – Load&lt;br&gt;
   – 内存相关&lt;br&gt;
   – 网络相关&lt;br&gt;
   – 端口存活、进程存活&lt;br&gt;
   – ntp offset（插件）&lt;br&gt;
   – 某个进程资源消耗（插件）&lt;br&gt;
   – netstat、ss 等相关统计项采集&lt;br&gt;
   – 机器内核配置参数&lt;/p&gt;
&lt;p&gt;只要安装了falcon-agent的机器，就会自动开始采集各项指标，主动上报，不需要用户在server做任何配置（这和zabbix有很大的不同），这样做的好处，就是用户维护方便，覆盖率高。当然这样做也会server端造成较大的压力，不过open-falcon的服务端组件单机性能足够高，同时都可以水平扩展，所以自动多采集足够多的数据，反而是一件好事情，对于SRE和DEV来讲，事后追查问题，不再是难题。&lt;/p&gt;
&lt;p&gt;另外，falcon-agent提供了一个proxy-gateway，用户可以方便的通过http接口，push数据到本机的gateway，gateway会帮忙高效率的转发到server端。&lt;/p&gt;
&lt;p&gt;falcon-agent，可以在我们的github上找到 （https://github.com/open-falcon/agent）&lt;/p&gt;
&lt;h2&gt;# Data model&lt;/h2&gt;
&lt;p&gt;Data Model是否强大，是否灵活，对于监控系统用户的“使用效率”至关重要。比如以zabbix为例，上报的数据为hostname（或者ip）、metric，那么用户添加告警策略、管理告警策略的时候，就只能以这两个维度进行。举一个最常见的场景：&lt;/p&gt;
&lt;p&gt;hostA的磁盘空间，小于5%，就告警。一般的服务器上，都会有两个主要的分区，根分区和home分区，在zabbix里面，就得加两条规则；如果是hadoop的机器，一般还会有十几块的数据盘，还得再加10多条规则，这样就会痛苦，不幸福，不利于自动化。&lt;/p&gt;
&lt;p&gt;open-falcon，采用和opentsdb相同的数据格式：metric加多组key value tags，举两个例子：&lt;br&gt;
&lt;code&gt;{&lt;br&gt;
    metric: load.1min,&lt;br&gt;
    endpoint: open-falcon-host,&lt;br&gt;
    tags: srv=falcon,idc=aws-sgp,group=az1,&lt;br&gt;
    value: 1.5,&lt;br&gt;
    timestamp: `date +%s`,&lt;br&gt;
    counterType: GAUGE,&lt;br&gt;
    step: 60&lt;br&gt;
}&lt;br&gt;
{&lt;br&gt;
    metric: net.port.listen,&lt;br&gt;
    endpoint: open-falcon-host,&lt;br&gt;
    tags: port=3306,&lt;br&gt;
    value: 1,&lt;br&gt;
    timestamp: `date +%s`,&lt;br&gt;
    counterType: GAUGE,&lt;br&gt;
    step: 60&lt;br&gt;
}&lt;/code&gt;&lt;br&gt;
通过这样的数据结构，我们就可以从多个维度来配置告警，配置dashboard等等。&lt;br&gt;
备注：endpoint是一个特殊的tag。&lt;/p&gt;
&lt;h2&gt;# Data collection&lt;/h2&gt;
&lt;p&gt;transfer，接收客户端发送的数据，做一些数据规整，检查之后，转发到多个后端系统去处理。在转发到每个后端业务系统的时候，transfer会根据一致性hash算法，进行数据分片，来达到后端业务系统的水平扩展。&lt;/p&gt;
&lt;p&gt;transfer 提供jsonRpc接口和telnet接口两种方式，transfer自身是无状态的，挂掉一台或者多台不会有任何影响，同时transfer性能很高，每分钟可以转发超过500万条数据。&lt;/p&gt;
&lt;p&gt;transfer目前支持的业务后端，有三种，judge、graph、opentsdb。judge是我们开发的高性能告警判定组件，graph是我们开发的高性能数据存储、归档、查询组件，opentsdb是开源的时间序列数据存储服务。可以通过transfer的配置文件来开启。&lt;/p&gt;
&lt;p&gt;transfer的数据来源，一般有三种：&lt;br&gt;
1. falcon-agent采集的基础监控数据&lt;br&gt;
2. falcon-agent执行用户自定义的插件返回的数据&lt;br&gt;
3. client library：线上的业务系统，都嵌入使用了统一的perfcounter.jar，对于业务系统中每个RPC接口的qps、latency都会主动采集并上报&lt;/p&gt;
&lt;p&gt;说明：上面这三种数据，都会先发送给本机的proxy-gateway，再由gateway转发给transfer。&lt;/p&gt;
&lt;h2&gt;# Alerting&lt;/h2&gt;
&lt;p&gt;报警判定，是由judge组件来完成。用户在web portal来配置相关的报警策略，存储在MySQL中。heartbeat server 会定期加载MySQL中的内容。judge也会定期和heartbeat server保持沟通，来获取相关的报警策略。&lt;/p&gt;
&lt;p&gt;heartbeat sever不仅仅是单纯的加载MySQL中的内容，根据模板继承、模板项覆盖、报警动作覆盖、模板和hostGroup绑定，计算出最终关联到每个endpoint的告警策略，提供给judge组件来使用。&lt;/p&gt;
&lt;p&gt;transfer转发到judge的每条数据，都会触发相关策略的判定，来决定是否满足报警条件，如果满足条件，则会发送给alarm，alarm再以邮件、短信、米聊等形式通知相关用户，也可以执行用户预先配置好的callback地址。&lt;/p&gt;
&lt;p&gt;用户可以很灵活的来配置告警判定策略，比如连续n次都满足条件、连续n次的最大值满足条件、不同的时间段不同的阈值、如果处于维护周期内则忽略 等等。&lt;/p&gt;
&lt;h2&gt;# Query&lt;/h2&gt;
&lt;p&gt;到这里，数据已经成功的存储在了graph里。如何快速的读出来呢，读过去1小时的，过去1天的，过去一月的，过去一年的，都需要在1秒之内返回。&lt;/p&gt;
&lt;p&gt;这些都是靠graph和query组件来实现的，transfer会将数据往graph组件转发一份，graph收到数据以后，会以rrdtool的数据归档方式来存储，同时提供查询RPC接口。&lt;/p&gt;
&lt;p&gt;query面向终端用户，收到查询请求后，会去多个graph里面，查询不同metric的数据，汇总后统一返回给用户。&lt;/p&gt;
&lt;h2&gt;# Dashboard&lt;/h2&gt;
&lt;p&gt;dashboard首页，用户可以以多个维度来搜索endpoint列表，即可以根据上报的tags来搜索关联的endpoint。&lt;br&gt;
&lt;img src=&quot;/images/noops.me/5c6604b3080db006cd9ccaf07b061133.jpg&quot; alt=&quot;open-falcon  dashboard homepage&quot;&gt;&lt;/p&gt;
&lt;p&gt;用户可以自定义多个metric，添加到某个screen中，这样每天早上只需要打开screen看一眼，服务的运行情况便尽在掌握了。&lt;br&gt;
&lt;img src=&quot;/images/noops.me/d1008cd7cb1bbabf15e65559d406292f.jpg&quot; alt=&quot;open-falcon dashboard screen&quot;&gt;&lt;/p&gt;
&lt;p&gt;当然，也可以查看清晰大图，横坐标上zoom in/out，快速筛选反选。总之用户的“使用效率”是第一要务。&lt;br&gt;
&lt;img src=&quot;/images/noops.me/2cac60f1cb29e1fb65d6ba028020de0d.jpg&quot; alt=&quot;open-falcon big graph&quot;&gt;&lt;/p&gt;
&lt;h2&gt;# Web portal&lt;/h2&gt;
&lt;p&gt;一个高效的portal，对于提升用户的“使用效率”，加成很大，平时大家都这么忙，能给各位SRE、Devs减轻一些负担，那是再好不过了。&lt;/p&gt;
&lt;p&gt;这是host group的管理页面，可以和服务树结合，机器进出服务树节点，相关的模板会自动关联或者解除。这样服务上下线，都不需要手动来变更监控，大大提高效率，降低遗漏和误报警。&lt;br&gt;
&lt;img src=&quot;/images/noops.me/dd4a9bda274eb51aa28c89ff4bc429e6.jpg&quot; alt=&quot;open-falcon portal HostGroup&quot;&gt;&lt;/p&gt;
&lt;p&gt;一个最简单的模板的例子，模板支持继承和策略覆盖，模板和host group绑定后，host group下的机器会自动应用该模板的所有策略。&lt;br&gt;
&lt;img src=&quot;/images/noops.me/7fba6afa172f442b636f7b6e73820035.jpg&quot; alt=&quot;open-falcon template&quot;&gt;&lt;/p&gt;
&lt;p&gt;当然，也可以写一个简单的表达式，就能达到监控的目的，这对于那些endpoint不是机器名的场景非常方便。&lt;br&gt;
&lt;img src=&quot;/images/noops.me/a7bf857c5daae27e47c10ec8f67261b8.jpg&quot; alt=&quot;open-falcon expression&quot;&gt;&lt;/p&gt;
&lt;p&gt;添加一个表达式也是很简单的。&lt;br&gt;
&lt;img src=&quot;/images/noops.me/6d28f75a8e343858e13c85c9beb69613.jpg&quot; alt=&quot;open-falcon add an expression&quot;&gt;&lt;/p&gt;
&lt;h2&gt;# Storage&lt;/h2&gt;
&lt;p&gt;对于监控系统来讲，历史数据的存储和高效率查询，永远是个很难的问题！&lt;br&gt;
1. 数据量大：目前我们的监控系统，每个周期，大概有2000万次数据上报（上报周期为1分钟和5分钟两种，各占50%），一天24小时里，从来不会有业务低峰，不管是白天和黑夜，每个周期，总会有那么多的数据要更新。&lt;br&gt;
2. 写操作多：一般的业务系统，通常都是读多写少，可以方便的使用各种缓存技术，再者各类数据库，对于查询操作的处理效率远远高于写操作。而监控系统恰恰相反，写操作远远高于读。每个周期几千万次的更新操作，对于常用数据库（MySQL、postgresql、mongodb）都是无法完成的。&lt;br&gt;
3. 高效率的查：我们说监控系统读操作少，是说相对写入来讲。监控系统本身对于读的要求很高，用户经常会有查询上百个meitric，在过去一天、一周、一月、一年的数据。如何在1秒内返回给用户并绘图，这是一个不小的挑战。&lt;/p&gt;
&lt;p&gt;open-falcon在这块，投入了较大的精力。我们把数据按照用途分成两类，一类是用来绘图的，一类是用户做数据挖掘的。&lt;/p&gt;
&lt;p&gt;对于绘图的数据来讲，查询要快是关键，同时不能丢失信息量。对于用户要查询100个metric，在过去一年里的数据时，数据量本身就在那里了，很难1秒之类能返回，另外就算返回了，前端也无法渲染这么多的数据，还得采样，造成很多无谓的消耗和浪费。我们参考rrdtool的理念，在数据每次存入的时候，会自动进行采样、归档。我们的归档策略如下，历史数据保存5年。同时为了不丢失信息量，数据归档的时候，会按照平均值采样、最大值采样、最小值采样存三份。&lt;br&gt;
“`&lt;br&gt;
// 1分钟一个点存 12小时&lt;br&gt;
c.RRA(“AVERAGE”, 0.5, 1, 720)&lt;/p&gt;
&lt;p&gt;// 5m一个点存2d&lt;br&gt;
c.RRA(“AVERAGE”, 0.5, 5, 576)&lt;br&gt;
c.RRA(“MAX”, 0.5, 5, 576)&lt;br&gt;
c.RRA(“MIN”, 0.5, 5, 576)&lt;/p&gt;
&lt;p&gt;// 20m一个点存7d&lt;br&gt;
c.RRA(“AVERAGE”, 0.5, 20, 504)&lt;br&gt;
c.RRA(“MAX”, 0.5, 20, 504)&lt;br&gt;
c.RRA(“MIN”, 0.5, 20, 504)&lt;/p&gt;
&lt;p&gt;// 3小时一个点存3个月&lt;br&gt;
c.RRA(“AVERAGE”, 0.5, 180, 766)&lt;br&gt;
c.RRA(“MAX”, 0.5, 180, 766)&lt;br&gt;
c.RRA(“MIN”, 0.5, 180, 766)&lt;/p&gt;
&lt;p&gt;// 1天一个点存5year&lt;br&gt;
c.RRA(“AVERAGE”, 0.5, 720, 730)&lt;br&gt;
c.RRA(“MAX”, 0.5, 720, 730)&lt;br&gt;
c.RRA(“MIN”, 0.5, 720, 730)&lt;br&gt;
“`&lt;/p&gt;
&lt;p&gt;对于原始数据，transfer会打一份到hbase，也可以直接使用opentsdb，transfer支持往opentsdb写入数据。&lt;/p&gt;
&lt;h2&gt;# Committers&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;laiwei:  https://github.com/laiwei     来炜没睡醒@微博 / hellolaiwei@微信&lt;/li&gt;
&lt;li&gt;秦晓辉: https://github.com/ulricqin  Ulricqin@微博 cnperl@微信&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;# Contributors&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;近期我们会把绝大数的组件整理到 http://github.com/open-falcon ， 期待大家一起贡献，推动，做最开放、最好用的企业级监控系统。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;# TODO&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;metric的聚合&lt;/li&gt;
&lt;li&gt;环比、同比报警判定&lt;/li&gt;
&lt;li&gt;流量的突升突降判定&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;# License&lt;/h2&gt;
&lt;p&gt;Copyright 2014-2015 Xiaomi, Inc.&lt;br&gt;
Licensed under the Apache License,&lt;br&gt;
Version 2.0:&lt;/p&gt;
&lt;p&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/p&gt;
            

</description>
        <pubDate>Mon, 09 Feb 2015 00:00:00 +0800</pubDate>
        <link>http://iftti.com/posts/2015/2015-02-09--p=1798-373e6bccc.html</link>
        <guid isPermaLink="true">http://iftti.com/posts/2015/2015-02-09--p=1798-373e6bccc.html</guid>
        
        
        <category>noops</category>
        
      </item>
    
      <item>
        <title>谈谈 React Native</title>
        <description>
&lt;p&gt;&lt;img src=&quot;/images/devtang.com/d7adf63159a97b2defa884a93e735ed5.jpg&quot;&gt;&lt;/p&gt;

&lt;h2&gt;前言&lt;/h2&gt;

&lt;p&gt;几天前，Facebook 在 React.js Conf 2015 大会上推出了 React Native（&lt;a href=&quot;https://www.youtube.com/watch?v=7rDsRXj9-cU&quot;&gt;视频链接&lt;/a&gt;）。我发了一条微博(&lt;a href=&quot;http://www.weibo.com/1708947107/C1WHHwqZv?from=page_1005051708947107_profile&amp;amp;wvr=6&amp;amp;mod=weibotime&amp;amp;type=comment#_rnd1422782358309&quot;&gt;地址&lt;/a&gt;），结果引来了 100 多次转发。为什么 React Native 会引来如此多的关注呢？我在这里谈谈我对 React Native 的理解。&lt;/p&gt;

&lt;p&gt;一个新框架的出现总是为了解决现有的一些问题，那么对于现在的移动开发者来说，到底有哪些问题 React Native 能涉及呢？&lt;/p&gt;

&lt;h3&gt;人才稀缺的问题&lt;/h3&gt;

&lt;p&gt;首先的问题是：移动开发人才的稀缺。看看那些培训班出来的人吧，经过 3 个月的培训就可以拿到 8K 甚至上万的工作。在北京稍微有点工作经验的 iOS 开发，就要求 2 万一个月的工资。这说明当前移动互联网和创业的火热，已经让业界没有足够的开发人才了，所以大家都用涨工资来抢人才。而由于跨平台的框架（例如 PhoneGap，RubyMotion）都还是不太靠谱，所以对于稍微大一些的公司，都会选择针对 iOS 和 Android 平台分别做不同的定制开发。而 JavaScript 显然是一个群众基础更广的语言，这将使得相关人才更容易获得，同时由于后面提到的代码复用问题得到解决，也能节省一部分开发人员。&lt;/p&gt;

&lt;h3&gt;代码复用的问题&lt;/h3&gt;

&lt;p&gt;React Native 虽然强调自己不是 “Write once, run anywhere” 的框架，但是它至少能像 Google 的 &lt;a href=&quot;https://github.com/google/j2objc&quot;&gt;j2objc&lt;/a&gt; 那样，在 Model 层实现复用。那些底层的、与界面无关的逻辑，相信 React Native 也可以实现复用。这样，虽然 UI 层的工作还是需要做 iOS 和 Android 两个平台，但如果抽象得好，Logic 和 Model 层的复用不但可以让代码复用，更可能实现底层的逻辑的单元测试。这样移动端的代码质量将更加可靠。&lt;/p&gt;

&lt;p&gt;其实 React Native 宣传的 “Learning once, write anywhere” 本身也是一种复用的思想。大家厌烦了各种各样的编程语言，如果有一种语言真的能够统一移动开发领域，对于所有人都是好事。&lt;/p&gt;

&lt;h3&gt;UI 排版的问题&lt;/h3&gt;

&lt;p&gt;我自己一直不喜欢苹果新推出的 AutoLayout 那套解决方案，其实 HTML 和 CSS 在界面布局和呈现上深耕多年，Android 也是借鉴的 HTML 的那套方案，苹果完全可以也走这套方案的。但是苹果选择发明了一个 Constraint 的东西来实现排版。在企业的开发中，其实大家很少使用 Xib 的，而手写 Constraint 其实是非常痛苦的。所以出现了 &lt;a href=&quot;https://github.com/Masonry/Masonry&quot;&gt;Masonry&lt;/a&gt; 一类的开源框架来解决这类同行的痛苦。&lt;/p&gt;

&lt;p&gt;我一直在寻找使用类似 HTML + CSS 的排版，但是使用原生控件渲染的框架。其实之前 &lt;a href=&quot;https://github.com/gavinkwoe/BeeFramework&quot;&gt;BeeFramework&lt;/a&gt; 就做了这方面的事情。所以我还专门代表 InfoQ 对他进行过采访。BeeFramework 虽然开源多年，而且有 2000 多的 star 数，但是受限于它自身的影响力以及框架的复杂性，一直没有很大的成功。至少我不知道有什么大的公司采用。&lt;/p&gt;

&lt;p&gt;这次 Facebook 的 React Native 做的事情相比 &lt;a href=&quot;https://github.com/gavinkwoe/BeeFramework&quot;&gt;BeeFramework&lt;/a&gt; 更加激进。它不但采用了类似 HTML + CSS 的排版，还把语言也换成了 JavaScript，这下子改变可以称作巨大了。但是 Facebook 有它作为全球互联网企业的光环，相信会有不少开发者跟进采用 React Native。&lt;/p&gt;

&lt;p&gt;不过也说回来，Facebook 开源的也不一定都好，比如 &lt;a href=&quot;https://github.com/facebookarchive/three20&quot;&gt;three20&lt;/a&gt; 就被 Facebook 放弃了，但是不可否认 &lt;a href=&quot;https://github.com/facebookarchive/three20&quot;&gt;three20&lt;/a&gt; 作为一个框架，在那个时期的特定价值。所以 React Native 即使没有成功，它也将人们关注的焦点放在了移动开发的效率上了。很可能会有越来越多相关的框架因此涌现出来。&lt;/p&gt;

&lt;h3&gt;MVVM&lt;/h3&gt;

&lt;p&gt;MVVM 在 Web 开发领域相当火热，而 iOS 领域的 &lt;a href=&quot;https://github.com/ReactiveCocoa/ReactiveCocoa&quot;&gt;ReactiveCocoa&lt;/a&gt; 虽然很火，但是还是非常小众。纠其原因，一方面是 ReactiveCocoa 带来的编程习惯上的改变实在太大，ReactiveCocoa 和 MVVM 的学习成本还是很高。另一方面是 ReactiveCocoa 在代码可读性、可维护性和协作上不太友好。&lt;/p&gt;

&lt;p&gt;而 Web 开发领域对 MVVM 编程模式的接受程度就大不相同了，在 Web 开发中有相当多的被广泛使用的 MVVM 的框架，例如 &lt;a href=&quot;http://en.wikipedia.org/wiki/AngularJS&quot;&gt;AngularJS&lt;/a&gt;。相信 React Native 会推动 MVVM 应用在移动端的开发。&lt;/p&gt;

&lt;h3&gt;动态更新&lt;/h3&gt;

&lt;p&gt;终于说到最 “鸡冻人心” 的部分了。你受够了每次发新版本都要审核一个星期吗？苹果的审核团队在效率上的低下，使得我们这一群狠不得每天迭代更新一版的敏捷开发团队被迫每 2 周或 1 个月更新一次版本。很多团队上一个版本还没审核结束，下一个版本就做好了。&lt;/p&gt;

&lt;p&gt;React Native 的语言是基于 JavaScript，这必然会使得代码可以从服务器端动态更新成为可能。到时候，每天更新不再是梦想。当然，代码的安全性将更一步受到挑战，如何有效保护核心代码的安全将是一个难题。&lt;/p&gt;

&lt;h2&gt;总结&lt;/h2&gt;

&lt;p&gt;不管怎么样，这确确实实是一个移动互联网的时代，我相信随着几年的发展，移动互联网的开发生态也会积累出越来越多宝贵的框架，以支撑出更加伟大的 App 出现。作为一个移动开发者，我很高兴能够成为这个时代的主角，用移动开发技术改变人们的生活。&lt;/p&gt;

&lt;p&gt;愿大家珍惜这样的机会，玩得开心～&lt;/p&gt;

</description>
        <pubDate>Sun, 01 Feb 2015 00:00:00 +0800</pubDate>
        <link>http://iftti.com/posts/2015/2015-02-01-talk-about-react-native-e773c25db.html</link>
        <guid isPermaLink="true">http://iftti.com/posts/2015/2015-02-01-talk-about-react-native-e773c25db.html</guid>
        
        
        <category>devtang</category>
        
      </item>
    
      <item>
        <title>PRML读书会第十章  Approximate Inference</title>
        <description>

						&lt;p style=&quot;text-align: center&quot;&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 15pt&quot;&gt;&lt;strong&gt;PRML读书会第十章 Approximate Inference&lt;br&gt;
&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p style=&quot;text-align: center&quot;&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 15pt&quot;&gt;&lt;strong&gt;主讲人 戴玮&lt;br&gt;
&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p style=&quot;text-align: center&quot;&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;&lt;strong&gt;（新浪微博: &lt;a href=&quot;http://weibo.com/u/1433881802&quot;&gt;@戴玮_CASIA&lt;/a&gt;）&lt;br&gt;
&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;Wilbur_中博(1954123) 20:02:04&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt; 我们在前面看到，概率推断的核心任务就是计算某分布下的某个函数的期望、或者计算边缘概率分布、条件概率分布等等。 比如前面在第九章尼采兄讲EM时，我们就计算了对数似然函数在隐变量后验分布下的期望。这些任务往往需要积分或求和操作。 但在很多情况下，计算这些东西往往不那么容易。因为首先，我们积分中涉及的分布可能有很复杂的形式，这样就无法直接得到解析解，而我们当然希望分布是类似指数族分布这样具有共轭分布、容易得到解析解的分布形式；其次，我们要积分的变量空间可能有很高的维度，这样就把我们做数值积分的路都给堵死了。因为这两个原因，我们进行精确计算往往是不可行的。&lt;br&gt;
为了解决这一问题，我们需要引入一些近似计算方法。&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt; 近似计算有随机和确定两条路子。随机方法也就是MCMC之类的采样法，我们会在讲第十一章的时候专门讲到，而确定近似法就是我们这一章讲的变分。变分法的优点主要是：有解析解、计算开销较小、易于在大规模问题中应用。但它的缺点是推导出想要的形式比较困难。也就是说，人琢磨的部分比较复杂，而机器算的部分比较简单。这和第十一章的采样法的优缺点恰好有互补性。所以我们可以在不同的场合应用变分法或采样法。这里我的一个问题是：是否可以结合二者的优点，使得人也不用考虑太多、机器算起来也比较简单？&lt;br&gt;
变分法相当于把微积分从变量推广到函数上。我们都知道，微积分是用来分析变量变化、也就是函数性质的，这里函数定义为f: x -&amp;gt; f(x)，而导数则是df/dx；与之相对，变分用到了泛函的概念：F: f -&amp;gt; F(f)，也就是把函数映射为某个值，而相应地，也有导数dF/df，衡量函数是如何变化的。比如我们熟悉的信息论中的熵，就是把概率分布这个函数映射到熵这个值上。和微积分一样，我们也可以通过导数为0的条件求解无约束极值问题，以及引入拉格朗日乘子来求解有约束极值问题。比如说，我们可以通过概率分布积分为1的约束，求解最大熵的变分问题。PRML的附录D和E有比较详细的解释，我们后面也还会看到，这里就不多说了。&lt;br&gt;
变分法这名字听起来比较可怕，但它的核心思想，就是从某个函数空间中找到满足某些条件或约束的函数。我们在统计推断当中用到的变分法，实际上就是用形式简单的分布，去近似形式复杂、不易计算的分布，这样再做积分运算就会容易很多。 比如，我们可以在所有高斯分布当中，选一个和目标分布最相似的分布，这样后面做进一步计算时就容易获得解析解。此外，我们还可以假设多元分布的各变量之间独立，这样积分的时候就可以把它们变成多个一元积分，从而解决高维问题。这也是最简单的两种近似。&lt;br&gt;
&lt;/span&gt;&lt;span id=&quot;more-7763&quot;&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt; 概率推断中的变分近似方法，最根本的思想，就是想用形式简单的分布去近似形式复杂、不易计算的分布。比如，我们可以在指数族函数空间当中，选一个和目标分布最相像的分布，这样计算起来就方便多了。&lt;br&gt;
显然，我们这里需要一个衡量分布之间相似性或差异性的度量，然后我们才能针对这个度量进行最优化，求相似性最大或差异性最小的分布。一般情况下，我们会选用KL散度：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/afe24c856b2bc43a55ab237b58994092.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
或者&lt;img src=&quot;/images/52nlp.cn/bf06b8a75cee1c3f30276c4ac4c9e999.jpg&quot; alt=&quot;&quot;&gt;，当然离散分布就不是积分而是在离散状态上求和。这个值是非负的，而且只在两分布完全相同的情况下取0，所以可以看成两分布之间的距离。但这种度量是不对称的，也就是&lt;img src=&quot;/images/52nlp.cn/00a5c3912b3a22fb0cb1ceab3223c43f.jpg&quot; alt=&quot;&quot;&gt;，而我们在优化的时候，这两种度量实际上都可以使用。这样一来，我们后面也会看到，会造成一些有趣且奇怪的现象。有了这个度量，我们就可以对某个给定的概率分布，求一个在某些条件下和它最相似或距离最小的分布。这里我们看几个例子，直观地认识一下KL散度的不对称性、以及产生这种不对称性的原因。这是两个方差不同的一元高斯分布，其中方差较小的是q（红色曲线），方差较大的是p（蓝色曲线）：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/a488fd1e953fce1ecca283101ccb88d5.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;根据KL散度的公式&lt;img src=&quot;/images/52nlp.cn/e8c2a972b661486470fb3ec8ba23bf1b.jpg&quot; alt=&quot;&quot;&gt;，我们能否估计一下，是KL(q||p)较大，还是KL(p||q)较大？我们可以看到，在曲线的中间部分，q(x) &amp;gt; p(x)，因此，如果光考虑这部分，显然KL(q||p)会比较大。但是，考虑两边 q(x) &amp;lt; p(x) 的部分，我们可以看到，q(x) 很快趋近于0，此时 p(x)/q(x) 会变得很大，比中间部分要大得多（打个比方，0.8/0.4 和 0.01/0.001）。尽管还要考虑 log 前面的 q(x)，但当 q(x) 不等于0时，分母趋近于0造成的影响还是压倒性的。所以综合考虑，KL(q||p)要小于KL(p||q)。它们的精确值分别为0.32和0.81。另一个例子是，如果两个高斯分布方差相等，则KL散度也会相等：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/9f5f993ed5cd47460045be15d46a2051.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
这一点很容易理解。再来看一个复杂一点的例子。在这个例子中，q是单峰高斯分布，p是双峰高斯分布：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/368ca23aafb46a110ffdf118dc782b45.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
这三种情况中，p的两个峰没有分开，有一定粘连，而q则分别拟合了p的左峰、右峰（见PRML 4.4节的拉普拉斯近似，上次读书会也简单介绍过，可参看上次读书会的总结），以及拟合p的均值和方差（即单峰高斯分布的两个参数）。三种拟合情况对应左、中、右三图。对于这三种情况，KL(q||p)分别为1.17、0.09、0.07，KL(p||q)分别为23.2、0.12、0.07。可以看到，无论是哪一种KL散度，在p的双峰没有完全分开的情况下，用单峰高斯q去近似双峰高斯p得到的最优解，都相当于拟合p的均值和方差。如果p的两个峰分开的话，情况会如何呢？&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/c9449cbcb42cf309f567a1929f068b26.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
和前一个例子一样，我们分别拟合p的左峰、右峰，以及均值和方差。显然，这里由于p中间有一段概率密度为0的区域，所以可以想见，KL(q||p)可能会比较大。实际情况也是如此：KL(q||p)分别为0.69、0.69、3.45，KL(p||q)分别为43.9、15.4、0.97。可以看到，如果用KL(p||q)做最优化，结果和双峰粘连时一样，仍然是拟合p的均值和方差，也就是所谓的moment-matching；而用KL(q||p)做最优化，结果则会有所变化：会拟合双峰的其中一峰，也就是所谓的mode-seeking。&lt;br&gt;
我们从前面这几个例子中，可以总结一个规律：用KL(q||p)做最优化，是希望p(x)为0的地方q(x)也要为0，否则q(x)/p(x)就会很大，刚才例子的右图在中间部分（5附近）就违背了这一点；反之，如果用KL(p||q)做最优化，就要尽量避免p(x)不为0而q(x)用0去拟合的情况，或者说p(x)不为0的地方q(x)也不要为0，刚才例子的左、中两图也违反了这一点。&lt;br&gt;
所以，KL(q||p)得到的近似分布q(x)会比较窄，因为它希望q(x)为0的地方可能比较多；而KL(p||q)得到的近似分布q(x)会比较宽，因为它希望q(x)不为0的地方比较多。&lt;br&gt;
最后看一个多元高斯分布的例子，书上的图10.3： &lt;img src=&quot;/images/52nlp.cn/e9c0ff564835d562b6bb9f9f9a2ededc.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;即有了前面的讲解，我们可以猜一下，哪些图是KL(q||p)得到的最优解，哪些图是KL(p||q)得到的最优解。&lt;br&gt;
由于KL(q||p)至少可以拟合到其中的一个峰上，而KL(p||q)拟合的结果，其概率密度最大的地方可能没什么意义，所以很多情况下，KL(q||p)得到的结果更符合我们的需要。到这里有什么问题吗。。理解理解。。KL散度这东西。&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;color: #333333;font-size: 12pt&quot;&gt;&lt;span style=&quot;font-family: Arial&quot;&gt;============================&lt;/span&gt;&lt;span style=&quot;font-family: 宋体&quot;&gt;讨论&lt;/span&gt;&lt;span style=&quot;font-family: Arial&quot;&gt;=================================&lt;br&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;飞羽(346723494) 20:24:23&lt;br&gt;
KL(q||p) 就是相当于用q去拟合p？&lt;br&gt;
Yuli(764794071) 20:25:31&lt;br&gt;
KL就是KL Divergence（相对熵）吧 用信息论来解释的话 是用来衡量两个正函数是否相似&lt;br&gt;
飞羽(346723494) 20:25:57&lt;br&gt;
对， 就是相对熵&lt;br&gt;
Wilbur_中博(1954123) 20:27:06&lt;br&gt;
嗯，我们现在有一个分布p，很多时候是后验分布，但它形式复杂，所以想用形式比较简单的q去近似p。其实也可以直接用后验分布的统计量，比如mode或mean去代替整个分布，进行进一步计算，比如最大后验什么的。但现在如果用近似分布去做预测的话，性能会好得多。&lt;br&gt;
linbo-phd-bayesian(99878724) 20:27:15&lt;br&gt;
请问为何KL(q||p)》=0，为何没有《0啊，有知道的吗？&lt;br&gt;
飞羽(346723494) 20:28:06&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/828a75a7912690fd1c64da1b2b37e932.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;Wilbur_中博(1954123) 20:29:21&lt;br&gt;
那个不太难证，利用ln凹函数性质可以证出来。。不过细节我忘记了，呵呵。查一查吧。。应该很多地方都有的。&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;逸风(421723497) 20:30:44&lt;br&gt;
PRML P56&lt;br&gt;
Wilbur_中博(1954123) 20:31:50&lt;br&gt;
总之就是利用KL作为目标函数，去做最优化。。找到和已知复杂分布最相近的一个近似分布。这一章的基本思路就是这样。具体动机最开始的时候已经提到过了。&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;逸风(421723497) 20:35:31&lt;br&gt;
为什么要用KL散度这样一个不具备对称性的”距离”，而不采用对称性的测度呢？有什么好处?&lt;br&gt;
Wilbur_中博(1954123) 20:37:15&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;似乎没有特别好的对称的度量？PRML的公式(10.20)提过一种叫Hellinger distance的度量，是对称的，但后来也没有用这个。不知道为什么。不容易优化？有没有了解原因的朋友。。比如说，为啥不用 (p(x) – q(x) ^ 2 做积分作为度量？ 或者其他什么的。&lt;br&gt;
WayneZhang(824976094) 20:41:52&lt;br&gt;
我感觉是优化求解过程中近似时自然而然导出了KL这个度量。&lt;br&gt;
karnon(447457116) 20:42:24&lt;br&gt;
KL算的是熵的增益，所以一定是那种形式 ，这取决于你怎么定义”近似”, 认为信息增益最少就是”近似”也是一种合理的定义&lt;br&gt;
Wilbur_中博(1954123) 20:43:07&lt;br&gt;
这里目的是为了找近似分布&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;color: #333333;font-size: 12pt&quot;&gt;&lt;span style=&quot;font-family: Arial&quot;&gt;========================&lt;/span&gt;&lt;span style=&quot;font-family: 宋体&quot;&gt;讨论结束&lt;/span&gt;&lt;span style=&quot;font-family: Arial&quot;&gt;=================================&lt;br&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;我们在PRML这本书的4.4节，其实看到过一种简单的近似方法，或者可以说是最简单的近似方法之一：拉普拉斯近似。它是用高斯分布去近似目标分布的极值点也就是mode。这里并没有涉及到变分的概念。&lt;br&gt;
它只是要求高斯分布的mode和目标分布的mode位置相同，方法就是把目标分布在mode处做泰勒级数展开到第二阶，然后用对应的高斯分布去代替，就是把未知系数给凑出来：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/5414097800184dbee322576141b51507.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
这是目标分布在\theta^*（mode）的二阶泰勒展开：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/523918b6319b238d426006c730105700.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
一比较就知道高斯分布的两个参数应该取：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/11bcb31e5b94e501ae7a83fec2513711.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
也就是PRML图10.1的红线：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/da54e25cfee8083263dca584bf485cd5.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
棕色部分是目标分布，绿线是我们用变分近似，在高斯分布中选一个和目标函数的KL散度最小的分布。&lt;br&gt;
反正就均值和方差两个未知参数，优化起来应该不难。&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;&lt;strong&gt;下面开始讲10.1.1 可分解分布，这一节非常重要，可以说是本章的基础和最重点的部分。&lt;/strong&gt;基本思想就是，我们把近似分布限制在可分解分布的范围内，也就是满足(10.5)式：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/ec5dd522109cb571d9d4df9ae8a6fc40.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
可以说，这个分布的各组变量Z_i互相之间是独立的。这样一来，我们计算这个分布的积分时，就可以变成多个较低维度的积分，就算用数值积分什么的也会简单很多。 在统计物理当中，这种可分解形式的变分近似方法称作平均场（mean field）方法，这个名字实际上是很直观的，和它最后得到的解的形式有关，我们马上会看到。不过现在不仅在统计物理领域，机器学习很多时候也就管它叫mean field了。现在很火的RBM什么的，求参数时经常能看到这个术语。&lt;br&gt;
上一章曾经讲过，最小化KL距离，和最大化下界L(q)是一回事，也就是(10.2)到(10.4)这三个式子：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/6773adf7c953556cac59c2cff585a7e4.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/80a29d3889ef8e4a0c6abe27f00bd0dc.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/4a8471566c0879a387641d6fbdfe3d68.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
这和9.4节当中(9.70)到(9.72)实际上是一样的，区别在于Z不仅是隐变量还把参数吸收了进来。等式左边那项和我们想求的Z无关，所以可以看成常数，而右边的p(Z|X)是我们想去近似的，不知道具体形式，所以可以间接通过最大化右边第一项来达到最小化右边第二项也就是KL散度的目的。&lt;br&gt;
根据上面的(10.5)式会得到公式(10.6)：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/d9586762c0aab850c0a4084e61f37bf5.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
我们这里也可以看MLAPP的(21.28)到(21.31)：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/3496cd9cba013bd4e7ed2eec836e37d4.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
推导得要详细得多。。所以多备几本参考书是必要的。&lt;br&gt;
MLAPP是Machine Learning – A Probabilistic Perspective的缩写。。群共享里应该有吧。很不错的机器学习书。&lt;br&gt;
huajh7(284696304) 21:02:10&lt;br&gt;
插一句。这里优化的目标其实是最大化low bound L(q)  （log P(D)是对数证据，常数，KL(Q||P)=0时，L(Q)最大）。也就是找到一个最合适的q分布，而不是优化参数。 优化过程中，求导，拉格朗日什么，是针对q分布的，也就是泛函。 这是为什么叫变分法：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/dc256e5afc9f30fbf48d9cae96189d5e.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
Wilbur_中博(1954123) 21:03:04&lt;br&gt;
好，谢谢。我看了你的博客http://www.blog.huajh7.com/variational-bayes/，文章写得很好。你好像毕业论文就是专门做这个的吧？ 也许你下次可以再专门讲一讲你对变分近似的心得体会，呵呵。&lt;br&gt;
简单说，这里的推导就是每一步只看q_j相关的那些项，和q_j无关的项全都归到常数里去。比如(21.30)的这部分：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/c6435044fa231cc530817d7324c43230.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
实际上就全扔到常数里去了。哦。。还少了个(21.32)：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/90840aeafec86143868efc68c799c06b.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
这里我们是在除了x_j之外的其他x_i上求期望，也就是这个东西：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/32d2fcc142d7ec4c4a47ad00faad9aa9.jpg&quot; alt=&quot;&quot;&gt;，它是关于x_j的函数。&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;下面讲一下10.1.1的可分解分布，也就是刚才说过的，假设多元分布可分解为多个一元分布的乘积，即用&lt;img src=&quot;/images/52nlp.cn/b449af457cb53ca76b95769d54821442.jpg&quot; alt=&quot;&quot;&gt;去近似p(x)。由于各个变量之间是解耦的，所以我们可以每次只关注单个变量的最优化，也就是用所谓坐标下降（coordinate descent）的方式来做最优化。具体做法，就是把最小化KL散度转化为最大化L(q)（参见公式(10.2)到(10.4)），然后把公式(10.5)代入(10.3)，每次把L(q)其中一个q_j当做变量，而把其他q_i当做常数，对L(q)进行最优化：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/24fd9835171a07c8a07b4181b26605af.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;这里：&lt;img src=&quot;/images/52nlp.cn/620749a2821a2773dde187ff01479361.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
前面讲过，KL散度也可以写成：&lt;img src=&quot;/images/52nlp.cn/e0bacf58a0dc56d32275cde64f0081f0.jpg&quot; alt=&quot;&quot;&gt;，可以看到，(10.6)最后得到的这个&lt;img src=&quot;/images/52nlp.cn/2e3a4d49fb235dd2e5ff8fcfabead53b.jpg&quot; alt=&quot;&quot;&gt;，恰好是负KL散度的形式。我们知道，KL散度为0也就是最小的时候，两分布恰好相同，因此每一步的最优化结果可得到：&lt;img src=&quot;/images/52nlp.cn/5d8bdfaa95dfabe71e25e12fb04b9b23.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
也就是每一步更新的结果，可得到分解出来的变量的分布为：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/c27bd825b65a7d0b454917ef5579c948.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
就是两边都取exp然后归一化。由于是以其他变量的均值作为当前变量分布的形式，所以这种方法也称作mean-field。这部分内容也可以参见MLAPP的21.3.1，那一节讲得感觉比PRML清楚一些。 那个公式是比较头疼。。不过只要记住只有一个q_j是变量，其他都当成常数，推一推应该也ok。&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;重新回顾下前面的内容：&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;变分推断的核心思想是：用形式比较简单、做积分运算比较容易的分布，去替代原先形式复杂、不易求积分的分布。因此，这里的主要问题就是：如何找到和原分布近似程度较高的简单分布。前面我们讲了一些变分推断的背景知识和KL divergence（KLD）的相关知识，还稍微讲了讲假设分布可分解时是如何推导出mean field形式的。KLD是衡量两个分布差异大小的方式之一，KLD越大则差异越大，反之则两分布越相似。因此，我们可以将KL(q||p)作为目标函数，并限定q为较简单的分布形式，找到这类分布中最接近原分布p的那个分布。我们这里主要关注的近似对象是后验分布。因为我们前面一直在讲如何求后验分布，但后验分布求出来的形式往往不那么好用，所以需要用简单分布去近似。然而，计算p(Z|X)需要计算归一化因子p(X)。p(X)是边缘分布，需要对p(Z,X)做积分，而p(Z,X)又不那么容易积分。因此，我们可以直接用未归一化的p(Z,X)作为近似计算的目标，也就是下面这个关系：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/224bb29ee712163f9a94768cd2218cf0.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
其中：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/941157d77289fec6652518fa47163f4b.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;这里ln(p(X))只是个常数，所以极小化KLD和极大化L得到的结果是一样的，但对L做最优化可直接用联合概率分布去做、而不用归一化。:我们想要得到的简单分布具有什么样的形式？我们喜欢的一种简单分布是可分解分布，就是说，我们可以假设各个隐变量Z_i之间是独立的，因此可拆成各隐变量分布的乘积：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/cd36bbe4a6cec1e0fdd487f1c59b4a2c.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
那么，各个隐变量的L可写为：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/734e0bc5ed498fd8829bd9ee6679c4e1.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
其中&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/f24e28434aea94aef41cfab3e9dff907.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;这里&lt;img src=&quot;/images/52nlp.cn/f9169a5f5132fa9168ccbf0f147f1d67.jpg&quot; alt=&quot;&quot;&gt;表示：&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/54990acd5cecaca2ef97a7641e98aa7b.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;这是对Z_j之外的其他所有随机变量求期望，也叫做mean field。极大化L相当于极小化&lt;img src=&quot;/images/52nlp.cn/2be1aefc5310a5283eaabf826758b342.jpg&quot; alt=&quot;&quot;&gt;，显然&lt;img src=&quot;/images/52nlp.cn/73b812dbcf75507751c5ed12c68e6dee.jpg&quot; alt=&quot;&quot;&gt;取和&lt;img src=&quot;/images/52nlp.cn/78437a7e55a5652358f7bfe385ba9f0b.jpg&quot; alt=&quot;&quot;&gt;完全相同的形式时，KLD极小，同时L极大。所以我们有最优解：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/3e2780d9fa66df02cbb1a8b8bf5439cc.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
这里p是已知的，所以可对它做积分。对除Z_j以外的随机变量求期望得到的分布，就是分解出来的q_j的分布。我们每一步迭代都对每一个分解分布q_j进行求解。这种方法也称做coordinate descent。&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;color: #333333;font-size: 12pt&quot;&gt;&lt;span style=&quot;font-family: Arial&quot;&gt;============================&lt;/span&gt;&lt;span style=&quot;font-family: 宋体&quot;&gt;讨论&lt;/span&gt;&lt;span style=&quot;font-family: Arial&quot;&gt;=================================&lt;br&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;一夏 吕(992463596) 21:15:06&lt;br&gt;
10.6后面有，10.7的const没有必要吧？我当时好像看懂了 做的笔记 现在一下看不懂了。。 后面那个很简单 是因为  其他的Zi积分为1。我记起来了 ，把后面的lnqi的和拆开，只把j的那一项留着，其他的都可以积分积掉，划到const里，这里主要是吧j的那一项拿出来表示，其他的 不相干的都不管。&lt;br&gt;
huajh7(284696304) 21:25:23&lt;br&gt;
有必要吧。否则不相等了，这里const表示归一化常量。实际上需要特别注意const，尤其自己推导的时候，const更多是表示与z_j无关的量，而不是指一个常量。在概率图中就是不在z_j的马尔科夫毯上的量。阿邦(1549614810) 21:26:08&lt;br&gt;
mean filed看koller的最清楚&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;一夏 吕(992463596) 21:26:57&lt;br&gt;
注意Z是大写，所以j的那个积分里 其他的i都积分为1了。&lt;br&gt;
huajh7(284696304) 21:27:48&lt;br&gt;
const 有必要。exp(E_{i~=j}[..]) 是没有归一化的。&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;一夏 吕(992463596) 21:28:23&lt;br&gt;
哪里有exp&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;huajh7(284696304) 21:28:32&lt;br&gt;
ln .&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;软件所-张巍&amp;lt;zh3f@qq.com&amp;gt; 21:26:42&lt;br&gt;
问个问题：用分解的分布去近似原始分布，精度怎么保证，有没有直观点的解释。&lt;br&gt;
Wilbur_中博(1954123) 21:28:32&lt;br&gt;
@软件所-张巍 的问题是好问题啊。。一般来说，似乎是把变分近似看作在MAP和贝叶斯推断（用整个分布）之间的一种trade-off？&lt;br&gt;
huajh7(284696304) 21:29:54&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt; 给个图 ：&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/images/52nlp.cn/c228eec01f2676af5eec68ac75255e11.jpg&quot; alt=&quot;&quot;&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;&lt;br&gt;
Wilbur_中博(1954123) 21:30:01&lt;br&gt;
因为一个是用后验分布的点估计，一个是用整个分布，不错，这是哪里的图？&lt;br&gt;
huajh7(284696304) 21:32:23&lt;br&gt;
variational bayeian 可以说是分布式distributional approximation，也就是wilbur说的，用的是整个分布。 The Variational Bayes Method in Signal Processing这本书的 第9页。&lt;br&gt;
李笑一(94755934) 21:32:46&lt;br&gt;
@张巍，我记得变分法能保证收敛到local minimum。一般情况下，最大似然是non convex的，但是变分下界却是convex，下界的minimum就是下一步要前进的方向。&lt;br&gt;
一夏 吕(992463596) 21:33:23&lt;br&gt;
但是变分法的前提是把dependence去掉了，这样才能把总概率拆开成各自概率的积。即使是convex的 ，也只是逼近原先intractable 的形式。10.7的那个我还是觉得const没必要。&lt;br&gt;
Wilbur_中博(1954123) 21:36:04&lt;br&gt;
其实就是没归一化的，所以要加个const，(10.9)那个也是这样&lt;br&gt;
一夏 吕(992463596) 21:36:17&lt;br&gt;
后面那个是求期望，就是上面那个花括号里的&lt;br&gt;
李笑一(94755934) 21:36:30&lt;br&gt;
@huajh7，图上看来，EM更好使？？？&lt;br&gt;
一夏 吕(992463596) 21:37:16&lt;br&gt;
EM是可解的时候用的，只是有隐变量&lt;br&gt;
秦淮/sun人家(76961223) 21:37:31&lt;br&gt;
EM是可以求得精确地后验&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;Happy(467594602) 21:38:30&lt;br&gt;
直观解释 请参照jordan写的introduction&lt;br&gt;
huajh7(284696304) 21:38:33&lt;br&gt;
10.6-10.9 就是利用KL（q||p）=0&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/8cceb175a4546ced787478b2e620c220.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
分母就是const，VB也可以看成是EM。&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;一夏 吕(992463596) 21:41:00&lt;br&gt;
@Happy  jordan的introduction 是他的那本书吗？&lt;br&gt;
Happy(467594602) 21:41:20&lt;br&gt;
introduction to variational methods in graphical model&lt;br&gt;
用简单分布的族 把复杂分布包裹起来 ，然后复杂分布的每一点都有一个简单分布的参数来近似&lt;br&gt;
一夏 吕(992463596) 21:42:47&lt;br&gt;
thanks 他还有一本书 是Graphical Models, Exponential Families, and Variational Inference&lt;br&gt;
huajh7(284696304) 21:43:25&lt;br&gt;
Neal,HintonA view of the EM algorithm that justifies incremental, sparse,and other variants.pdf 这篇文章 说EM，其实就是变分贝叶斯。&lt;br&gt;
Happy(467594602) 21:43:25&lt;br&gt;
后一本太难了。。&lt;br&gt;
李笑一(94755934) 21:43:26&lt;br&gt;
@huajh，弱弱的问一下，分母将Z marginalize掉这步只是在推导中出现是吧，编程的时候不会出现实际的过程？&lt;br&gt;
huajh7(284696304) 21:44:06&lt;br&gt;
写程序的时候，还是还归一化的。比如，GMM中的隐变量，全部算出来之后，然后再归一化。&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;一夏 吕(992463596) 21:45:37&lt;br&gt;
如果隐变量很多不是exponential个组合了&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;huajh7(284696304) 21:46:01&lt;br&gt;
就转化为exponential&lt;br&gt;
Wilbur_中博(1954123) 21:46:01&lt;br&gt;
mean field的过程中呢？每个Z_j的分布也都要归一化么？@huajh7&lt;br&gt;
Happy(467594602) 21:46:16&lt;br&gt;
我咋记得不用归一化。。mean-field&lt;br&gt;
一夏 吕(992463596) 21:46:18&lt;br&gt;
那就很费时间&lt;br&gt;
huajh7(284696304) 21:46:45&lt;br&gt;
后来会知道。算的是充分统计量。&lt;br&gt;
一夏 吕(992463596) 21:46:46&lt;br&gt;
如果有64个，每个01分布就是2^64次方&lt;br&gt;
李笑一(94755934) 21:47:33&lt;br&gt;
恩，partition function永远是问题&lt;br&gt;
Happy(467594602) 21:47:35&lt;br&gt;
程序里面没有归一化步骤吧，推导中体现了&lt;br&gt;
李笑一(94755934) 21:48:12&lt;br&gt;
啥叫充分统计量？&lt;br&gt;
huajh7(284696304) 21:48:16&lt;br&gt;
概率才归一化啊。&lt;br&gt;
Happy(467594602) 21:48:30&lt;br&gt;
指数族里面有&lt;br&gt;
huajh7(284696304) 21:48:35&lt;br&gt;
充分统计量能完全表示一个分布。对&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;一夏 吕(992463596) 21:49:29&lt;br&gt;
不用归一化吧，看10.9 10.10中间那个公式下面那段话&lt;br&gt;
huajh7(284696304) 21:49:42&lt;br&gt;
为什么是指数族。。。一个最主要的原因就是其充分统计量是可计算的&lt;br&gt;
Happy(467594602) 21:50:03&lt;br&gt;
这个jordan后面那个书有深入介绍。。&lt;br&gt;
一夏 吕(992463596) 21:51:09&lt;br&gt;
通常不需要求出分布，而是得到分布的类型和参数&lt;br&gt;
huajh7(284696304) 21:51:33&lt;br&gt;
@一夏 吕 可能理解不一样。归一化不是指计算那个积分(partition function)..&lt;br&gt;
一夏 吕(992463596) 21:51:41&lt;br&gt;
通常就是指数族，自然服从积分为1&lt;br&gt;
Happy(467594602) 21:52:25&lt;br&gt;
没有自然哈 也有归一化系数&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;一夏 吕(992463596) 21:53:00&lt;br&gt;
恩 但是那个是和分布本身有关的，知道了参数就可以推，比如高斯的方差&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;李笑一(94755934) 21:56:38&lt;br&gt;
这部分有没有类似的书写的不错的。直接讲替代教材得了&lt;br&gt;
Happy(467594602) 21:57:29&lt;br&gt;
jordan那个不错，不过主要是针对graphical model的&lt;br&gt;
Wilbur_中博(1954123) 21:58:14&lt;br&gt;
我除了PRML和MLAPP，还看了一下Bayesian Reasoning and Machine Learning的最后一章&lt;br&gt;
一夏 吕(992463596) 21:58:16&lt;br&gt;
有看多lda的吗 那个里面的variational inference和这个方法完全不同&lt;br&gt;
Happy(467594602) 21:58:28&lt;br&gt;
肿么不同。。&lt;br&gt;
秦淮/sun人家(76961223) 21:58:33&lt;br&gt;
@一夏 吕 其实是一样的&lt;br&gt;
huajh7(284696304) 21:58:33&lt;br&gt;
bishop不喜欢详细推导的。讲清楚就行。这里有篇:&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;A Tutorial on Variational Bayesian Inference （http://staffwww.dcs.shef.ac.uk/people/c.fox/fox_vbtut.pdf）  还是很清楚的 。LDA 。其实是一样的。。建立的图模型上，比较直观。&lt;br&gt;
秦淮/sun人家(76961223) 21:59:37&lt;br&gt;
LDA那篇文章就是使用的mean field&lt;br&gt;
一夏 吕(992463596) 21:59:38&lt;br&gt;
blei用拉格朗日乘子法做的。。&lt;br&gt;
Happy(467594602) 21:59:46&lt;br&gt;
一样的啊。。&lt;br&gt;
秦淮/sun人家(76961223) 22:00:02&lt;br&gt;
不同的优化方法而已……&lt;br&gt;
huajh7(284696304) 22:00:03&lt;br&gt;
嗯。其实是一样的。&lt;br&gt;
秦淮/sun人家(76961223) 22:00:12&lt;br&gt;
本质是一样的&lt;br&gt;
Happy(467594602) 22:00:21&lt;br&gt;
直觉一致&lt;br&gt;
秦淮/sun人家(76961223) 22:00:26&lt;br&gt;
不一样的是Expectation propagation 那篇&lt;br&gt;
huajh7(284696304) 22:00:37&lt;br&gt;
对。那个感觉有些难。&lt;br&gt;
一夏 吕(992463596) 22:00:40&lt;br&gt;
恩 也是搞kl距离  方法各不相同&lt;br&gt;
Happy(467594602) 22:01:52&lt;br&gt;
对这些有兴趣就看jordan的大作吧，这些全部都归到架构里去了&lt;br&gt;
一夏 吕(992463596) 22:05:45&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;http://www.cs.princeton.edu/courses/archive/fall11/cos597C/lectures/variational-inference-i.pdf&lt;/p&gt;
&lt;p&gt;推荐这个 blei的讲义&lt;br&gt;
&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;一夏 吕(992463596) 22:09:18&lt;br&gt;
variational inference 是不是只是对指数族的才有用?&lt;br&gt;
Happy(467594602) 22:09:26&lt;br&gt;
一样的 统计模型下&lt;br&gt;
一夏 吕(992463596) 22:10:04&lt;br&gt;
我一般只在贝叶斯学派的文章里见到，一般都用Gibbs sampling&lt;br&gt;
Happy(467594602) 22:10:16&lt;br&gt;
也不一定。。&lt;br&gt;
一夏 吕(992463596) 22:11:00&lt;br&gt;
比如rbm就不能用variational inference&lt;br&gt;
Happy(467594602) 22:11:21&lt;br&gt;
可以啊，mean-field必须可以用&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;天际一线(1002621044) 22:19:40&lt;br&gt;
lda 那个话题模型 谁有完整的算法啊&lt;br&gt;
Happy(467594602) 22:23:48&lt;br&gt;
lda老模型了吧。。程序应该多如牛毛&lt;br&gt;
秦淮/sun人家(76961223) 22:24:22&lt;br&gt;
对啊，mean field ，expectation propagation ，gibbs sampling，distributed ，online的都有&lt;br&gt;
一堆&lt;br&gt;
_Matrix_(690119781) 22:28:18&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;https://github.com/sudar/Yahoo_LDA 这个可能满足你的要求&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;陪你听风(407365525) 22:31:18&lt;br&gt;
在效果上，variational inference，gibbs sampling两个谁更好呢&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;秦淮/sun人家(76961223) 22:38:35&lt;br&gt;
sampling近似效果好，慢，不好分布式计算&lt;br&gt;
陪你听风(407365525) 22:39:08&lt;br&gt;
vb比较容易分布式吗&lt;br&gt;
huajh7(284696304) 22:40:24&lt;br&gt;
噗。VB是可以很自然地分布式的。。&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;李笑一(94755934) 22:42:16&lt;br&gt;
弱问。。VB为啥自然可以用分布式&lt;br&gt;
huajh7(284696304) 22:45:22&lt;br&gt;
利用variational message passing 框架下即可。。。节点之间传递充分统计量。充分统计量(一阶矩，二阶矩）的consensus或diffusion是有较多paper的。图模型中的BP或loopy BP算一种分布式嘛？&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;李笑一(94755934) 22:48:28&lt;br&gt;
有个问题，不同问题的vb是否需要自己推导出来？不能随意套用别人的推导呢？&lt;br&gt;
huajh7(284696304) 22:49:05&lt;br&gt;
推导框架。如出一辙。。但自己推并不容易的。&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;李笑一(94755934) 23:04:01&lt;br&gt;
karnon，一篇jmlr的文章，在一个问题上证了vb 的全局解&lt;br&gt;
Global Analytic Solution of Fully-observed Variational Bayesian Matrix Factorization&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;看明白了给讲讲。。。&lt;br&gt;
huajh7(284696304) 23:10:47&lt;br&gt;
2,3年前就出来了。。这篇估计是combined and extended。&lt;br&gt;
light.(513617306) 23:15:13&lt;br&gt;
这个是证明了在矩阵分解这个问题上的全局最有，不证明在其他模型上也是这样。》？&lt;br&gt;
karnon(447457116) 23:15:34&lt;br&gt;
这就已经很牛了&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-size: 12pt&quot;&gt;&lt;span style=&quot;font-family: 微软雅黑&quot;&gt;李笑一(94755934) 23:17:43&lt;br&gt;
vb对于不同问题有不同的解，我觉得除非熟到一定程度了，否则不可能拿来一个问题就能用vb的&lt;br&gt;
karnon(447457116) 23:21:29&lt;br&gt;
我看看，我知道最近有些文章研究全局收敛的矩阵分解问题，粗翻了一下，好像说的是把vb转成一个等价的svd问题？&lt;br&gt;
&lt;/span&gt;&lt;span style=&quot;color: #333333&quot;&gt;&lt;span style=&quot;font-family: Arial&quot;&gt;========================&lt;/span&gt;&lt;span style=&quot;font-family: 宋体&quot;&gt;讨论结束&lt;/span&gt;&lt;span style=&quot;font-family: Arial&quot;&gt;=================================&lt;br&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt; 接着主要讲几个变分推断的例子，试图阐述清楚变分推断到底是如何应用的。首先是二元高斯分布的近似。我们假设二元高斯分布是可分解的，也就是两变量之间独立。&lt;br&gt;
二元高斯分布&lt;img src=&quot;/images/52nlp.cn/0bd1a644cddb18b2fbfdaf42e87b3b20.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
其中&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/b27410eba6e99626e3ca5bb724004879.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
可分解形式为：&lt;img src=&quot;/images/52nlp.cn/729963e34ab8b84c1fbb42c7e1efa98a.jpg&quot; alt=&quot;&quot;&gt;&lt;img src=&quot;/images/52nlp.cn/13c80aca194f42ccb325c1a2efecda87.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
我们想用q(z)去近似p(z)，用前面推导出来的(10.9)：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/8846983febdaf2440902296c2f6c343a.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;因为是求z1的分布，所以按(10.9)，我们在z2上求期望，得到(10.11)。然后，我们就可以祭出第二章修炼的法宝——配方法，从(10.11)得到高斯分布：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/b46bc51768fde2db3aa96f024b0a6503.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
其中&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/213eb6094d2c0515a36f9132362853bc.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
同样，z2的分布也可如法炮制：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/baf8954f5e33394573f9466cf1c5f0ea.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
其中&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/5c3a6d57b78ceb43f9e1670b093bb89c.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
它们是完全对称的。因为m1里有z2的期望，而m2里又有z1的期望，所以我们可以设一个初始值，然后迭代求解。但实际上这两个式子恰好有解析解：&lt;img src=&quot;/images/52nlp.cn/04a3c1053593171fe0af9656a362c065.jpg&quot; alt=&quot;&quot;&gt;和&lt;img src=&quot;/images/52nlp.cn/c2d22acaa3357bdf5557adad548c7699.jpg&quot; alt=&quot;&quot;&gt;，我们可把它们代入(10.13)和(10.15)验证一下。&lt;br&gt;
下面我们重点看一下参数推断问题，但其核心思想实际上和前面讲的例子区别不大。同样还是先看一下高斯分布：&lt;br&gt;
我们想推断后验高斯分布的均值&lt;img src=&quot;/images/52nlp.cn/df05892c456e1abc917586cd6956c88c.jpg&quot; alt=&quot;&quot;&gt;和精度&lt;img src=&quot;/images/52nlp.cn/21a57cd60eb849ddb6ec8169c03e3074.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
假如我们观察到N个数据&lt;img src=&quot;/images/52nlp.cn/a29f552d9647a9042b3351ea024df4bd.jpg&quot; alt=&quot;&quot;&gt;，那么似然函数就是：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/b6b7d11f93cd3fc2b425dd7bcf791b7e.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;另外引入先验分布，均值服从高斯分布、精度服从Gamma分布：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/ef21c3d02f0e820d6eaa9f7f69f960cb.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
其实这个问题我们前面第二章就讲过，不用变分推断也能直接求出来，但这里用变分推断实际上增加了更多的灵活性，因为如果先验和似然的形式不是高斯-Gamma的形式，而是更加复杂，那么我们也可以利用变分推断来算参数，这是非常方便的。我们这里只是用我们熟悉的高斯分布来举例子，把这个弄明白，以后再推广到其他例子上就容易多了。&lt;br&gt;
利用mean field形式(10.9)，我们可计算出&lt;img src=&quot;/images/52nlp.cn/6cbfb9b50b625ce5d2db619b3a70b7c2.jpg&quot; alt=&quot;&quot;&gt;的分布：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/e0a38452dfb3a1dbb6bc228289a35d55.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;可以看到，&lt;img src=&quot;/images/52nlp.cn/bb70ca051116367139128a6a8430955d.jpg&quot; alt=&quot;&quot;&gt;服从高斯分布形式&lt;img src=&quot;/images/52nlp.cn/35079a87c61b4054d8d2dc79793116a3.jpg&quot; alt=&quot;&quot;&gt;，且通过配方，可得到该分布参数为：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/a4ae341b251df8e41ac191b10a62a4b3.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
注意到，样本越多也就是N越大时，均值会趋向于样本均值&lt;img src=&quot;/images/52nlp.cn/70df279cbc90f389578938ccd23f912b.jpg&quot; alt=&quot;&quot;&gt;，同时精度趋向于无穷大。同样可用(10.9)计算&lt;img src=&quot;/images/52nlp.cn/1385e151b13c25d4a16408142b1428e6.jpg&quot; alt=&quot;&quot;&gt;的分布，得到：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/e7d6325e97492891e7582d41bd273426.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;它服从Gamma分布形式&lt;img src=&quot;/images/52nlp.cn/eda37a4cf4ef8dd47bd2bebb32ae616a.jpg&quot; alt=&quot;&quot;&gt;，可以看到，(10.27)和(10.30)里，仍然有和另一分布相关的期望需要计算，所以我们可以设定初始值，然后迭代计算。迭代过程和收敛后的结果图书上10.4所示：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/89bef34456b36f327f41e16334d35312.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;再看一个例子，是用变分推断计算线性回归的参数。线性回归的参数w，有似然和先验如下：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/305262cd74d7ca4a3da8b58417a0dd2d.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
2.3.6讲过，&lt;img src=&quot;/images/52nlp.cn/6342a159616614d337b8c009b7a00034.jpg&quot; alt=&quot;&quot;&gt;的共轭先验是Gamma分布：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/aa409b988035220472732963ca9be7fd.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
这样联合分布就是：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/fc7374262b88d92757f0b21665cc1d65.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
其概率图模型为图10.8：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/6340840ca1af9ee43993d032e770c9de.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
利用变分推断来计算w和&lt;img src=&quot;/images/52nlp.cn/5f38c67a398b17f063e3b101d74de258.jpg&quot; alt=&quot;&quot;&gt;，同样是假设它们有可分解形式：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/965a3e96a265e14958e6f74d21fe569a.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
再用(10.9)（这个绝对是看家法宝）来搞，得到：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/41619174f338af464cee1bce415de7f2.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;可看到它服从Gamma分布：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/8fe7376d7d0bd0983ce5e673add24ac8.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
其中&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/88b3cac3a73ed54d82798c8c6c45fbc6.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
以及：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/0e17740dfd0eead82fc7df36bc413079.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;可看到它服从高斯分布：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/ba7515610c3026bb4d6db6aef4f86e6b.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
其中&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/484b8ff35fe7f53586ea36545c7f8769.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
(10.95)和(10.97)里还有奇怪的东西&lt;img src=&quot;/images/52nlp.cn/cc059b5965652da13215a62cc34a74c9.jpg&quot; alt=&quot;&quot;&gt;和&lt;img src=&quot;/images/52nlp.cn/0b031c6aa596eec1ba3b79dfd1f306be.jpg&quot; alt=&quot;&quot;&gt;，从附录B可知，它们分别是：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/b937ab547f2267444d5138022db6bd17.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;所以我们仍然可以迭代计算：给初始值，每一步都算出a_N、b_N和m_N、S_N，代入求解。&lt;br&gt;
掌握了上面的三个例子，我想推广到其他情况也都没有太大难度了。其实书中还有一个例子也非常重要，就是10.2所讲的用变分推断计算高斯混合模型的参数。不过我想尼采兄讲第九章时已经打下了很好的基础，再加上刚才讲的这一章的例子，看懂这部分应该不难。&lt;br&gt;
后面还有一些有趣的内容，比如Expectation Propagation，是说对&lt;img src=&quot;/images/52nlp.cn/607dae661d4a83292803896fb562de40.jpg&quot; alt=&quot;&quot;&gt;做极小化，而不是&lt;img src=&quot;/images/52nlp.cn/86e0c8ad15f0121dc2dd3ed3d33c97fb.jpg&quot; alt=&quot;&quot;&gt;。因为积分里前面那项变成了p(Z)而不是q(Z)，而p(Z)又是复杂分布，所以这里处理方式有所不同。感兴趣的朋友可以看看10.7节是如何做的。&lt;br&gt;
我讲的内容就到这里。我个人的一点心得体会就是：高斯分布以及其他常用分布的形式、还有第二章讲到的配方法一定要掌握好，这是识别分布和直接计算分布参数的最大利器。然后就是这一章的(10.9)，也就是用可分解分布去做近似得到的mean field，这也是比较常用的。其实群里有不少对变分推断很了解的高手，比如@huajh7 ，大家对这一块有什么问题也可以找他们交流讨论。&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;color: #333333;font-size: 12pt&quot;&gt;&lt;span style=&quot;font-family: Arial&quot;&gt;============================&lt;/span&gt;&lt;span style=&quot;font-family: 宋体&quot;&gt;讨论&lt;/span&gt;&lt;span style=&quot;font-family: Arial&quot;&gt;=================================&lt;br&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;数据挖掘(983272906) 21:44:16&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/d7086388eabf0fe367da66672aae5656.jpg&quot; alt=&quot;&quot;&gt;这种分解有没有什么限制条件&lt;br&gt;
Wilbur_中博(1954123) 21:45:20&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt; 这不是分解，是从先验和似然算联合分布。可分解的简单分布形式是(10.91)。&lt;br&gt;
Y(414474527) 21:47:49&lt;br&gt;
变分推断怎么应用到实际问题中呢&lt;br&gt;
tzk&amp;lt;tangzk2011@163.com&amp;gt; 21:48:29&lt;br&gt;
LDA的原始论文用的也是变分呢。。&lt;br&gt;
&amp;lt;(523723864) 21:48:43&lt;br&gt;
10.9式一定是tractable的吗？&lt;br&gt;
zeno(117127143) 21:52:58&lt;br&gt;
平均场假设可以有效减少参数。&lt;br&gt;
Wilbur_中博(1954123) 21:53:54&lt;br&gt;
@Y 实际问题吗？我觉得就是作为一种工具，求解模型参数的时候会比较简单吧。之前在稀疏编码里看到过一些，我觉得这篇文章不错：http://ipg.epfl.ch/~seeger/lapmalmainweb/papers/seeger_wipf_spm10.pdf 。另外RBM似乎也有用这个的。&lt;br&gt;
zeno(117127143) 21:54:19&lt;br&gt;
变分把推断变为求极值问题，怎么求是另外一门课&lt;br&gt;
Wilbur_中博(1954123) 21:54:43&lt;br&gt;
@&amp;lt; 我觉得不一定。。还得看p(X,Z)是什么样的。&lt;br&gt;
@zeno 嗯&lt;img src=&quot;/images/52nlp.cn/60dc5d0cf7ce28d42586a88af194df26.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
&amp;lt;(523723864) 21:55:01&lt;br&gt;
按照10.9主要是推式子咯，事先不知道qj的分布&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;Wilbur_中博(1954123) 21:55:57&lt;br&gt;
嗯，应该是。。但是一般来说都可以想办法搞出来吧，(10.9)的积分。&lt;br&gt;
karnon(447457116) 21:59:27&lt;br&gt;
为什么一开始又要用复杂分布呢，建模时用那些复杂的模型，最后到求解时都退化成naive模型，所以事实上，和naive模型一样&lt;br&gt;
Wilbur_中博(1954123) 22:02:31&lt;br&gt;
可能一开始就用简单分布的话，推出后验分布有连锁效应，就会越来越差吧。现在搞出后验分布再用简单分布去近似，我觉得道理上还是能说得通。&lt;br&gt;
zeno(117127143) 22:03:27&lt;br&gt;
那为啥有泰勒展开，展开把高次舍弃，不都不是原来函数了吗&lt;br&gt;
&amp;lt;(523723864) 22:04:21&lt;br&gt;
关键是每次迭代的时候lower bound会不会上去&lt;br&gt;
karnon(447457116) 22:04:22&lt;br&gt;
如果你要用 taylor展开来近似，那就得证明近似后你的解的性质不变 ，所以不是任何问题都能随便近似&lt;br&gt;
Wilbur_中博(1954123) 22:04:43&lt;br&gt;
@&amp;lt; 是，我觉得这个蛮关键的&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;karnon(447457116) 22:06:49&lt;br&gt;
就是你的解为什么好，它好在哪，近似之后，这些好处是不是还保留着，这在变分法中，完全没有讨论&lt;br&gt;
zeno(117127143) 22:10:58&lt;br&gt;
要是有kl跟概率差异定量关系就没问题了，平均场本来就是假设，变分推断是合理的，kl嘛，不好说，反正不像熟悉的欧式度量，pgm不只变分一种推断方法，所以也不能建成简单模型。说实在如果能解决一类小问题效果不错就已经很好了，mrf，hmm，crf，都能算到pgm中。pgm解决不少问题。&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;阿邦(1549614810) 23:41:20&lt;br&gt;
推断方法不坑，主要还是模型的问题&lt;br&gt;
karnon(447457116) 0:02:10&lt;br&gt;
我总感觉，一定有基于非概率模型的方法&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑&quot;&gt;&lt;span style=&quot;font-size: 12pt&quot;&gt;弹指一瞬间(337595903) 6:31:34&lt;br&gt;
昨晚大家讨论的好热闹啊。@karnon：我觉得近似推理对原模型的好处还是保留着的。虽然求解的时候是在简单模型上做，但是简单模型的求解目标是去近似原模型的最优而不是简单模型的最优。这个和一上来就做简单模型假设是不大一样的。近似推理可以理解为在最优解附近找一个次优解，但总体目标还是原模型最优解的方向。而简单模型求解可能目标就不一样了。相比之下，还是用近似推理来解原问题比较好。（个人理解不一定对，欢迎跟帖&lt;img src=&quot;/images/52nlp.cn/5374610cb635ee0b1c8445978c8c8dd6.jpg&quot; alt=&quot;&quot;&gt;）&lt;br&gt;
zeno(117127143) 6:52:44&lt;br&gt;
我喜欢概率模型，概率既能对不确定性建模更能对未知建模。做单选题25%表达的是学生对答案的未知，同样的题对老师就是已知的。同样问题用非概率解你需要知道的更多。同样四道单选题三道不会，其他三道分别选a，b，c。第四道用概率方法根据一定先验会尽量选d。不用概率方法根本做不了这种问题。&lt;br&gt;
同样如果知道了答案，肯定不会用概率方法，概率比通常非概率方法麻烦。&lt;br&gt;
karnon(447457116) 7:33:16&lt;br&gt;
这只是理想的情况，概率模型的缺点，在于它需要精确地刻划细节。&lt;/span&gt;&lt;span style=&quot;font-size: 9pt&quot;&gt;&lt;span style=&quot;color: black&quot;&gt;&lt;br&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;注：PRML读书会系列文章由 &lt;a href=&quot;http://weibo.com/dmalgorithms&quot;&gt;@Nietzsche_复杂网络机器学习&lt;/a&gt; 同学授权发布，转载请注明原作者和相关的主讲人，谢谢。&lt;/p&gt;
&lt;p&gt;PRML读书会讲稿PDF版本以及更多资源下载地址：&lt;a href=&quot;http://vdisk.weibo.com/u/1841149974&quot;&gt;http://vdisk.weibo.com/u/1841149974&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;本文链接地址：&lt;a href=&quot;http://www.52nlp.cn/prml%E8%AF%BB%E4%B9%A6%E4%BC%9A%E7%AC%AC%E5%8D%81%E7%AB%A0-approximate-inference&quot;&gt;http://www.52nlp.cn/prml读书会第十章-approximate-inference&lt;/a&gt;&lt;/p&gt;

											

</description>
        <pubDate>Sat, 31 Jan 2015 00:00:00 +0800</pubDate>
        <link>http://iftti.com/posts/2015/2015-01-31-prml%25e8%25af%25bb%25e4%25b9%25a6%25e4%25bc%259a%25e7%25ac%25ac%25e5%258d%2581%25e7%25ab%25a0-approximate-inference-c3b1e2e4b.html</link>
        <guid isPermaLink="true">http://iftti.com/posts/2015/2015-01-31-prml%25e8%25af%25bb%25e4%25b9%25a6%25e4%25bc%259a%25e7%25ac%25ac%25e5%258d%2581%25e7%25ab%25a0-approximate-inference-c3b1e2e4b.html</guid>
        
        
        <category>52nlp</category>
        
      </item>
    
      <item>
        <title>PRML读书会第十四章 Combining Models</title>
        <description>

						&lt;p style=&quot;text-align: center&quot;&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 15pt&quot;&gt;&lt;strong&gt;PRML读书会第十四章 Combining Models&lt;br&gt;
&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p style=&quot;text-align: center&quot;&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 15pt&quot;&gt;&lt;strong&gt;主讲人 网神&lt;br&gt;
&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p style=&quot;text-align: center&quot;&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;&lt;strong&gt;（新浪微博: &lt;a href=&quot;http://weibo.com/ghtimaq&quot;&gt;@豆角茄子麻酱凉面&lt;/a&gt;）&lt;br&gt;
&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;网神(66707180) 18:57:18&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;大家好，今天我们讲一下第14章combining models，这一章是联合模型，通过将多个模型以某种形式结合起来，可以获得比单个模型更好的预测效果。包括这几部分：&lt;br&gt;
committees, 训练多个不同的模型，取其平均值作为最终预测值。&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;boosting: 是committees的特殊形式，顺序训练L个模型，每个模型的训练依赖前一个模型的训练结果。&lt;br&gt;
决策树：不同模型负责输入变量的不同区间的预测，每个样本选择一个模型来预测，选择过程就像在树结构中从顶到叶子的遍历。&lt;br&gt;
conditional mixture model条件混合模型：引入概率机制来选择不同模型对某个样本做预测，相比决策树的硬性选择，要有很多优势。&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;本章主要介绍了这几种混合模型。讲之前，先明确一下混合模型与Bayesian model averaging的区别，贝叶斯模型平均是这样的：假设有H个不同模型h，每个模型的先验概率是p(h)，一个数据集的分布是： &lt;img src=&quot;/images/52nlp.cn/34049e89a2b52fec00e21028537c8a95.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
整个数据集X是由一个模型生成的，关于h的概率仅仅表示是由哪个模型来生成的 这件事的不确定性。而本章要讲的混合模型是数据集中，不同的数据点可能由不同模型生成。看后面讲到的内容就明白了。&lt;/span&gt;&lt;span id=&quot;more-8143&quot;&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt; 首先看committes，committes是一大类，包括boosting，首先将最简单的形式，就是讲多个模型的预测的平均值作为最后的预测。主要讲这么做的合理性，为什么这么做会提高预测性能。从频率角度的概念，bias-variance trade-off可以解释，这个理论在3.5节讲过，我们把这个经典的图copy过来：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/d7fd27da2d2a4078b55b51a962f362b5.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
这个图大家都记得吧，左边一列是对多组数据分别训练得到一个模型，对应一条sin曲线，看左下角这个图，正则参数lamda取得比较小，得到一个bias很小，variance很大的一个模型 。每条线的variance都很大，这样模型预测的错误就比较大，但是把这么多条曲线取一个平均值，得到右下角图上的红色线，红色线跟真实sin曲线也就是蓝色线 基本拟合。所以用平均之后模型来预测，variance准确率就提高了很多，这是直观上来看，接下里从数学公式推导看下：&lt;br&gt;
有一个数据集，用bootstrap方法构造M个不同的训练集bootstrap方法就是从数据集中随机选N个放到训练集中，做M次，就得到M个训练集，M个训练集训练的到M个模型，用&lt;img src=&quot;/images/52nlp.cn/260f62f05c7137d87363d42d0b4e6b71.jpg&quot; alt=&quot;&quot;&gt;表示，那么用committees方法，对于某个x，最终预测值是:&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/d026fc480331d6ecd46b8d5c71d5fa5f.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
我们来看这个预测值是如何比单个&lt;img src=&quot;/images/52nlp.cn/207b73339c50de814a5445bfb85e5e4e.jpg&quot; alt=&quot;&quot;&gt;预测值准确的，假设准确的预测模型是h(x)，那么训练得到的y(x)跟h(x)的关系是：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/8d881215254c90eb202f718e704e257b.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
后面那一项是模型的error&lt;br&gt;
ZealotMaster(850458544) 19:24:34&lt;br&gt;
能使error趋近于0嘛？&lt;br&gt;
网神(66707180) 19:25:13&lt;br&gt;
模型越好越趋近于0，但很难等于0，这里committes方法就比单个模型更趋近于0&lt;br&gt;
ZealotMaster(850458544) 19:25:28&lt;br&gt;
求证明&lt;br&gt;
网神(66707180) 19:25:39&lt;br&gt;
正在证明，平均平方和错误如下：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/9feca5e8a98966062b8a00a2a5ae3a78.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
也就是单个模型的期望error是：&lt;br&gt;
&lt;/span&gt;&lt;!--more--&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/images/52nlp.cn/edfc885341d5e2aff7cb8e9ce2d8cc9e.jpg&quot; alt=&quot;&quot;&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;&lt;br&gt;
如果用M个模型分别做预测，其平均错误是:&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/images/52nlp.cn/706886a24a82934640f0ed3cb2c77de5.jpg&quot; alt=&quot;&quot;&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;&lt;br&gt;
而如果用committes的结果来做预测，其期望错误是：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/31942c7f7dfe2621817f0c9865e4a9c2.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
这个&lt;img src=&quot;/images/52nlp.cn/1a89b3a12117c64cd4ce4ec4e94a73e8.jpg&quot; alt=&quot;&quot;&gt;跑到了平方的里面，如果假设不同模型的error都是0均值，并且互不相关，也就是：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/c4125acd5323dde10033a3fc4bba789b.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
就可以得到：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/9ce6f5f44232139ae5cea112cb38a2e6.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;在不同模型error互不相关的假设的下，committes错误是单个模型error的1/M，但实际上，不同模型的error通常是相关的，因此error不会减少这么多，但肯定是小于单个模型的error，接下来讲boosting，可以不考虑那个假设，取得实质的提高.boosting应该是有不同的变种，其中最出名的就是AdaBoost, adaptive boosting. 它可以将多个弱分类器结合，取得很好的预测效果，所谓弱分类器就是，只要比随即预测强一点，大概就是只要准确率超50%就行吧，这是我的理解。&lt;br&gt;
boosting的思想是依次预测每个分类器，每个分类器训练时，对每个数据点加一个权重。训练完一个分类器模型，该模型分错的，再下一个模型训练时，增大权重；分对的，减少权重，具体的算法如下，我把整个算法帖出来，再逐步解释：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/74327fcd9031e4d559da041b2c53566c.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/295f07ae732d0fffd28e8c3cc0c29568.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;大家看下面这个图比较形象：&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/images/52nlp.cn/e590871e4c1161436b676afe1f724a66.jpg&quot; alt=&quot;&quot;&gt;&lt;span style=&quot;font-family: 宋体;font-size: 12pt&quot;&gt;&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;第一步，初始化每个数据点的权重为1/N.接下来依次训练M个分类器，每个分类器训练时，最小化加权的错误函数(14.15)，错误函数看上面贴的算法，从这个错误函数可以看出，权重相同时，尽量让更多的x分类正确，权重不同时，优先让权重大的x分类正确，训练完一个模型后，式(14.16)计算&lt;img src=&quot;/images/52nlp.cn/807e0108a531b7e3541ef0f0f4f3ece1.jpg&quot; alt=&quot;&quot;&gt;，既分类错误的样本的加权比例. 然后式(14.17)计算:&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/images/52nlp.cn/2db3e0932ccb3ce7187014eed4a66bb5.jpg&quot; alt=&quot;&quot;&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;&lt;br&gt;
只要分类器准确率大于50%，&lt;img src=&quot;/images/52nlp.cn/41cc5a635ecdc66bca26bc7f3701f079.jpg&quot; alt=&quot;&quot;&gt;就小于0.5, &lt;img src=&quot;/images/52nlp.cn/74f10425ee28391e2e2f2bdb5fe0c54e.jpg&quot; alt=&quot;&quot;&gt;就大于0。而且&lt;img src=&quot;/images/52nlp.cn/a19d93aa1bb729ab897d177389b86d75.jpg&quot; alt=&quot;&quot;&gt;越小(既对应的分类器准确率越高)，&lt;img src=&quot;/images/52nlp.cn/2fe09f26a79caaead8801af51e4ebe94.jpg&quot; alt=&quot;&quot;&gt;就越大，然后用&lt;img src=&quot;/images/52nlp.cn/ead1ea951290b8e893a4cba92085cd28.jpg&quot; alt=&quot;&quot;&gt;更新每个数据点的权重，即式(14.18)：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/da9febec80ce6450d40a1e28f71df3c3.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
可以看出，对于分类错误的数据点，&lt;img src=&quot;/images/52nlp.cn/41b20a6189947ac9baa3ff337c56e9c8.jpg&quot; alt=&quot;&quot;&gt;大于0，所以exp(a)就大于1，所以权重变大。但是从这个式子看不出，对于分类正确的样本，权重变小。这个式子表明，分类正确的样本，其权重不变 ，因为exp(0)=1.这是个疑问。如此循环，训练完所有模型，最后用式(14.19)做预测:&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/images/52nlp.cn/3c2a3346f113a9b81d3e5fe279875d95.jpg&quot; alt=&quot;&quot;&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;&lt;br&gt;
从上面过程可以看出，如果训练集合中某个样本在逐步训练每个模型时，一直被分错，他的权重就会一直变大，最后对应的&lt;img src=&quot;/images/52nlp.cn/bb9d57b80044785da4fed52b76c63f11.jpg&quot; alt=&quot;&quot;&gt;也越来越大，下面看一个图例：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/01c27eaff2ed90a28ea7acc4a30630da.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
图中有蓝红两类样本 ，分类器是单个的平行于轴线的阈值 ，第一个分类器(m=1)把大部分点分对了，但有2个蓝点，3个红点不对，m=2时，这几个错的就变大了，圈越大，对应其权重越大 ，后面的分类器似乎是专门为了这个几个错分点而在努力工作，经过150个分类器，右下角那个图的分割线已经很乱了，看不出到底对不对 ，应该是都已经分对了吧。&lt;br&gt;
网神(66707180) 19:59:59&lt;br&gt;
@ZealotMaster 不知道是否明白点了，大家有啥问题？&lt;br&gt;
ZealotMaster(850458544) 20:00:14&lt;br&gt;
嗯，清晰一些了，这个也涉及over fitting嘛？感觉m=150好乱……&lt;br&gt;
苦瓜炒鸡蛋(852383636) 20:02:23&lt;br&gt;
是过拟合&lt;br&gt;
网神(66707180) 20:02:25&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;不同的分割线，也就是不同的模型，是主要针对不同的点的，针对哪些点，由模型组合时的系数&lt;img src=&quot;/images/52nlp.cn/aafecb351a1a31b0f825770387701584.jpg&quot; alt=&quot;&quot;&gt;来影响。&lt;br&gt;
苦瓜炒鸡蛋(852383636) 20:04:50&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;这是韩家炜 那个数据挖掘书的那一段：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/f86f32740fb812fc6e23eb32b195e041.jpg&quot; alt=&quot;&quot;&gt;网神(66707180) 20:04:56&lt;br&gt;
嗯，这章后面也讲到了boosting对某些错分的样本反应太大，一些异常点会对模型造成很大的影响。&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;color: #333333;font-family: Arial;font-size: 12pt&quot;&gt;================================================================&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;接下来讲boosting的错误函数，我们仔细看下对boosting错误函数的分析，boosting最初用统计学习理论来分析器泛化错误的边界bound，但后来发现这个bound太松，没有意义。实际性能比这个理论边界好得多，后来用指数错误函数来表示。从优化指数损失函数来解释adaboost比较直观，每次固定其他分类器和系数将常量分出来，能推出单分类器的损失函数及系数，再根据常量的形式能得出下一步数据权重的更新方式。指数错误函数如下：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/a832ff02b251a31145ec2978d68eff2c.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;其中N是N个样本点，&lt;img src=&quot;/images/52nlp.cn/b8c3a8bf62dc4c9e13218d8403823502.jpg&quot; alt=&quot;&quot;&gt;是多个线性分类器的线性组合:&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/images/52nlp.cn/d41bb2c0761dab497048dc45bfd4ffcb.jpg&quot; alt=&quot;&quot;&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/b8687ae74c4e3b9fed6b093363871e50.jpg&quot; alt=&quot;&quot;&gt;是分类的目标值。我们的目标是训练系数&lt;img src=&quot;/images/52nlp.cn/eb49075d7a0a1f14bf88060f24fb4f29.jpg&quot; alt=&quot;&quot;&gt;和分类器&lt;img src=&quot;/images/52nlp.cn/3eb90f63a61969bf3d35ef14ae7bdc84.jpg&quot; alt=&quot;&quot;&gt;的参数，使E最小。&lt;br&gt;
最小化E的方法，是先只针对一个分类器进行最小化，而固定其他分类器的参数，例如我们固定&lt;img src=&quot;/images/52nlp.cn/811390986a918a4328a5279ae2e05ceb.jpg&quot; alt=&quot;&quot;&gt;和其系数&lt;img src=&quot;/images/52nlp.cn/894dde0986ff93853f09a0c9dba2b9da.jpg&quot; alt=&quot;&quot;&gt;，只针对&lt;img src=&quot;/images/52nlp.cn/f999ce74aa7649e48e050a5a3f981326.jpg&quot; alt=&quot;&quot;&gt;做优化，这样错误函数E可以改写为：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/373a495db2fdbb519fbc8cb765c45dfa.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
也就是把固定的分类器的部分都当做一个常量：&lt;img src=&quot;/images/52nlp.cn/cde9e250563b43d4d752d702013f1f15.jpg&quot; alt=&quot;&quot;&gt;，只保留&lt;img src=&quot;/images/52nlp.cn/cebba926cd9548ae16514f83ac833c22.jpg&quot; alt=&quot;&quot;&gt;相关的部分。我们用&lt;img src=&quot;/images/52nlp.cn/a7ca764158500d7e35c390dfea231508.jpg&quot; alt=&quot;&quot;&gt;表示&lt;img src=&quot;/images/52nlp.cn/53ad4e58c492c5ef02d8b5dc4d2c43ef.jpg&quot; alt=&quot;&quot;&gt;分对的数据集，&lt;img src=&quot;/images/52nlp.cn/02b058b549de0c8290c43ce15af9fa44.jpg&quot; alt=&quot;&quot;&gt;表示分错的数据集，E又可以写成如下形式：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/d319b0ebe90b8f42d323cd8ca7058933.jpg&quot; alt=&quot;&quot;&gt;上式中，因为将数据分成两部分，也就确定了&lt;img src=&quot;/images/52nlp.cn/669955e9a11c6efd6ee26a2cf76d36c4.jpg&quot; alt=&quot;&quot;&gt;个&lt;img src=&quot;/images/52nlp.cn/7221debdbb3f59bbe487e43acfe790a5.jpg&quot; alt=&quot;&quot;&gt;是否相等，所以消去了这两个变量&lt;br&gt;
看起来清爽点了：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/fec2e051f7afb0652921e4402e50ae60.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;这里面后一项是常量，前一项就跟前面boosting的算法里所用的错误函数：&lt;img src=&quot;/images/52nlp.cn/ea919b926a43c4de2201f4f3816060dc.jpg&quot; alt=&quot;&quot;&gt;形式一样了。&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;上面是将：&lt;img src=&quot;/images/52nlp.cn/f149927c432b8f2592555df3a13249b6.jpg&quot; alt=&quot;&quot;&gt;对&lt;img src=&quot;/images/52nlp.cn/c86fb57e89d291fce7a7634fc521d00a.jpg&quot; alt=&quot;&quot;&gt;做最小化得出的结论,即指数错误函数就是boosting里求单个模型时所用的错误函数.类似，也可以得到指数错误函数里的&lt;img src=&quot;/images/52nlp.cn/1e2c14d1dae0e07b7c3659f94461b258.jpg&quot; alt=&quot;&quot;&gt;就是boosting里的&lt;img src=&quot;/images/52nlp.cn/33f552c6b7bcfa63dd0ef3dd6ce33808.jpg&quot; alt=&quot;&quot;&gt;，确定了&lt;img src=&quot;/images/52nlp.cn/36805bbb7eb128a7e8f8a39a8242ebfc.jpg&quot; alt=&quot;&quot;&gt;根据&lt;img src=&quot;/images/52nlp.cn/68ecdd3a5231a2fb61c36b4e30f5b5a7.jpg&quot; alt=&quot;&quot;&gt;以及&lt;img src=&quot;/images/52nlp.cn/2e56f39601ac51a6e978eae5248de331.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;可以得到，更新w的方法：&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/images/52nlp.cn/99f557fc742c9d3238f5c52d4ac0d210.jpg&quot; alt=&quot;&quot;&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;&lt;br&gt;
又因为&lt;img src=&quot;/images/52nlp.cn/0900b5f4e8d8c739a1803155b25afba4.jpg&quot; alt=&quot;&quot;&gt;有：&lt;img src=&quot;/images/52nlp.cn/9f388e495a3cfacb5d0132ae5219df07.jpg&quot; alt=&quot;&quot;&gt;这又跟boosting里更新数据点权重的方法以一致。&lt;br&gt;
总之，就是想说明，用指数错误函数可以描述boosting的error分析,接下来看看指数错误函数的性质，再贴一下指数错误函数的形式：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/67d2eb762baaf54f62c009489901ccc6.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/images/52nlp.cn/9e2bd681a4be0fb754048de7ee7c7d77.jpg&quot; alt=&quot;&quot;&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;&lt;br&gt;
其期望error是：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/4d1dedcacdfdcfc2d0decf2358bfecd3.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;然后最所有的y(x)做variational minimization,得到：&lt;img src=&quot;/images/52nlp.cn/bf904034bcd7eb6996e190b594af6fa2.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
这是half the log-odds ，看下指数错误函数的图形：&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/images/52nlp.cn/f8aa82ad988b68e84aa440c0e41d476a.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
绿色的线是指数错误函数&lt;img src=&quot;/images/52nlp.cn/dd5d35e6b757c24a8b026353c11de82a.jpg&quot; alt=&quot;&quot;&gt;可以看到，对于分错的情况，既z&amp;lt;0时，绿色的线呈指数趋势上升，所以异常点会对训练结果的影响很大。图中红色的线是corss-entropy 错误，是逻辑分类中用的错误函数，对分错的情况，随错误变大，其函数值基本是线性增加的，蓝色线是svm用的错误函数，叫hinge 函数。&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;color: #333333;font-family: Arial;font-size: 12pt&quot;&gt;================================================================&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;大家有没有要讨论的？公式比较多，需要推导一下才能理解。接下来讲决策树和条件混合模型。决策树概念比较简单，容易想象是什么样子的，可以认为决策树是多个模型，每个模型负责x的一个区间的预测，通过树形结构来寻找某个x属于哪个区间，从而得到预测值。&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p style=&quot;background: white&quot;&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;决策树有多个算法比较出名，ID3, C4.5, CART，书上以CART为例讲的。&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p style=&quot;background: white&quot;&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;CART叫classification and regression trees先看一个图示：&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p style=&quot;background: white&quot;&gt;&lt;img src=&quot;/images/52nlp.cn/b80d17c180021e8e8dbbc88e621bf8bf.jpg&quot; alt=&quot;&quot;&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p style=&quot;background: white&quot;&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;这个二维输入空间，被划分成5个区间，每个区间的类别分别是A-E，它的决策树如下，很简单明了：&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p style=&quot;background: white&quot;&gt;&lt;img src=&quot;/images/52nlp.cn/2dfa58b5ce59e7101c723d7e39d024c5.jpg&quot; alt=&quot;&quot;&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p style=&quot;background: white&quot;&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;决策树在一些领域应用比较多，最主要的原因是，他有很好的可解释性。那如何训练得到一个合适的决策树呢？也就是如何决定需要哪些节点，节点上选择什么变量作为判断依据，以及判断的阈值是多大。&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p style=&quot;background: white&quot;&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;首先错误函数是残差的平方和，而树的结构则用贪心策略来演化。&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p style=&quot;background: white&quot;&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;开始只有一个节点，也就是根节点，对应整个输入空间，然后将空间划分为2，在多个划分选择之中，选择使残差最小的那个划分，书上提到这个区间划分以及划分点阈值的选择，是用穷举的方法。然后对不同的叶子节点再分别划分子区间。这样树不停长大，何时停止增加节点呢？简单的方法是当残差小于某个值的时候。但经验发现经常再往多做一些划分后，残差又可以大幅降低&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p style=&quot;background: white&quot;&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;所以通常的做法是，先构造一个足够大的树，使叶子节点多到某个数量，然后再剪枝，合并的原则是使叶子尽量减少，同时残差又不要增大。&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p style=&quot;background: white&quot;&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;也就是最小化这个值：&lt;img src=&quot;/images/52nlp.cn/4da5f86c0a1ab4fb6fca505a12f20349.jpg&quot; alt=&quot;&quot;&gt;其中：&lt;img src=&quot;/images/52nlp.cn/5d56a4269bfddba2754944fcb2b3feaf.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p style=&quot;background: white&quot;&gt;&lt;img src=&quot;/images/52nlp.cn/144bd69bdc28eab53dd7395630a3d811.jpg&quot; alt=&quot;&quot;&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;是某个叶子负责的那个区间里的所有数据点的预测值的平均值：&lt;img src=&quot;/images/52nlp.cn/74f2951dbc2d33edd22645b390fce6f9.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p style=&quot;background: white&quot;&gt;&lt;img src=&quot;/images/52nlp.cn/c177d700b2e20742dedf4bd0b5a8e9ad.jpg&quot; alt=&quot;&quot;&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;是某个数据点的预测值，&lt;img src=&quot;/images/52nlp.cn/f18645cad1e4c4abcbedc62fd6d792b2.jpg&quot; alt=&quot;&quot;&gt;是叶子的总数，&lt;img src=&quot;/images/52nlp.cn/60fc4069a865e8f4b2222850b22ec6b6.jpg&quot; alt=&quot;&quot;&gt;控制残差和叶子数的trade-off，这是剪枝的依据。&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p style=&quot;background: white&quot;&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;以上是针对回归说的，对于分类，剪枝依据仍然是：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/99b2f1337e51650c3c0ea1533d8216ef.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p style=&quot;background: white&quot;&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;只是Q(T)不再是残差的平方和，而是cross-entropy或Gini index&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p style=&quot;background: white&quot;&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;交叉熵的计算是:&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p style=&quot;background: white&quot;&gt;&lt;img src=&quot;/images/52nlp.cn/4b34d974a272161fe86ce3e4f3116a1c.jpg&quot; alt=&quot;&quot;&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p style=&quot;background: white&quot;&gt;&lt;img src=&quot;/images/52nlp.cn/82157131bc27d6d7bd242d728bbc78a5.jpg&quot; alt=&quot;&quot;&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;是第&lt;img src=&quot;/images/52nlp.cn/a0f805dbf1793bb77d02538c702f707f.jpg&quot; alt=&quot;&quot;&gt;个叶子，也就是第&lt;img src=&quot;/images/52nlp.cn/4c246d3eef0898fc02c7a1da7fc84b92.jpg&quot; alt=&quot;&quot;&gt;个区间里的数据点，被赋予类别k的比例。&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p style=&quot;background: white&quot;&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;Gini index计算是：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/d939cde3d6bf1441594d67ec4167915e.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p style=&quot;background: white&quot;&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;决策树的优点是直观，可解释性好。缺点是每个节点对输入空间的划分都是根据某个维度来划分的，在坐标空间里看，就是平行于某个轴来划分的，其次，决策树的划分是硬性的，在回归问题里，硬性划分更明显。&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;color: #333333;font-family: Arial;font-size: 12pt&quot;&gt;================================================================&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p style=&quot;background: white&quot;&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;决策树就讲这么多，接下来是conditional mixture models条件混合模型。条件混合模型，我的理解是，将不同的模型依概率来结合，这部分讲了线性回归的条件混合，逻辑回归的条件混合，和更一般性的混合方法mixture of experts，首先看线性回归的混合。&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p style=&quot;background: white&quot;&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;第9章讲过高斯混合模型，其模型是这样的：&lt;img src=&quot;/images/52nlp.cn/becba5426d95be96d0a89031b3205dfc.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p style=&quot;background: white;margin-left: 78pt&quot;&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;这是用多个高斯密度分布来拟合输入空间，就像这种x的分布：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/277734c4a3303676c03408ca3690a285.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p style=&quot;background: white&quot;&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;线性回归混合的实现，可以把这个高斯混合分布扩展成条件高斯分布，模型如下：&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p style=&quot;background: white&quot;&gt;&lt;img src=&quot;/images/52nlp.cn/d028e0621c86b9a6ed597c8b26937419.jpg&quot; alt=&quot;&quot;&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p style=&quot;background: white&quot;&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;这里面，有K个线性回归&lt;img src=&quot;/images/52nlp.cn/4ce9bc7a20415f31a3b7b4a62f84a6f2.jpg&quot; alt=&quot;&quot;&gt;，其权重参数是&lt;img src=&quot;/images/52nlp.cn/564f48cd6f45a53cfff3370e8d314050.jpg&quot; alt=&quot;&quot;&gt;，将多个线性回归的预测值确定的概率联合起来，得到最终预测值的概率分布。模型中的参数&lt;img src=&quot;/images/52nlp.cn/c27406d7a7204d3deca0d739867e6f41.jpg&quot; alt=&quot;&quot;&gt;包括&lt;img src=&quot;/images/52nlp.cn/0828741d5bf8791b66f8387c3ce47481.jpg&quot; alt=&quot;&quot;&gt;&lt;img src=&quot;/images/52nlp.cn/613c6d88ba9211eeb571d2d53c1fa3a3.jpg&quot; alt=&quot;&quot;&gt;三部分，接下来看如何训练得到这些参数，总体思路是用第9章介绍的EM算法，引入一个隐藏变量&lt;img src=&quot;/images/52nlp.cn/7f7ebb4a6fc07f14aebb8b0e3c33978d.jpg&quot; alt=&quot;&quot;&gt;，每个训练样本点对应一个&lt;img src=&quot;/images/52nlp.cn/9667b64c7930238e14af4800ceab34b6.jpg&quot; alt=&quot;&quot;&gt;，&lt;img src=&quot;/images/52nlp.cn/3ad9215828c6978c210396112dc30d3c.jpg&quot; alt=&quot;&quot;&gt;是一个K维的二元向量，&lt;img src=&quot;/images/52nlp.cn/0ee4cbc7cef75aaab2f65c9121fcdae6.jpg&quot; alt=&quot;&quot;&gt;,如果第k个模型负责生成第n个数据点，则&lt;img src=&quot;/images/52nlp.cn/983d4586b3b69b02c1a6f3885d81f173.jpg&quot; alt=&quot;&quot;&gt;等于1，否则等于0，这样，我们可以写出log似然函数：&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p style=&quot;background: white&quot;&gt;&lt;img src=&quot;/images/52nlp.cn/7a312f3cfd352e39ac7dd82417cacc09.jpg&quot; alt=&quot;&quot;&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p style=&quot;background: white&quot;&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;然后用EM算法结合最大似然估计来求各个参数，EM算法首先选择所有参数的初始值，假设是&lt;img src=&quot;/images/52nlp.cn/cb4d067adcafafbe8a58e55a9f3ace4a.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p style=&quot;background: white&quot;&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;在E步根据这些参数值，可以得到模型k相对于数据点n的后验概率：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/468b966639ba4d611e15abeca48f24ef.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p style=&quot;background: white&quot;&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;书上提高一个词，这个后验概率又叫做responsibilities，大概是这个数据点n由模型k 负责生成的概率吧，有了这个responsibilities，就可以计算似然函数的期望值了，如下：&lt;img src=&quot;/images/52nlp.cn/15500fa0f8a9ca32c40084f27e8dc32b.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p style=&quot;background: white&quot;&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;在EM的M步，我们令&lt;img src=&quot;/images/52nlp.cn/4b83391f2c00113c5b147e5d2c1c13b5.jpg&quot; alt=&quot;&quot;&gt;为固定值，最大化这个期望值&lt;img src=&quot;/images/52nlp.cn/e5a0b6b32795686f6e529d3f0ac08020.jpg&quot; alt=&quot;&quot;&gt;,从而求得新的参数&lt;img src=&quot;/images/52nlp.cn/a5488cec20c3b3aab46088fbceaa3c46.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p style=&quot;background: white&quot;&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;首先看&lt;img src=&quot;/images/52nlp.cn/18a362abba3c845d778039c12d9d2b34.jpg&quot; alt=&quot;&quot;&gt;，&lt;img src=&quot;/images/52nlp.cn/dcdce906dabd83bd34ad6c8b41b07af1.jpg&quot; alt=&quot;&quot;&gt;是各个模型的混合权重系数，满足&lt;img src=&quot;/images/52nlp.cn/201664070d7ab0feca4e3ba72556da29.jpg&quot; alt=&quot;&quot;&gt;，用拉格朗日乘子法，可以求得:&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/5133c5765ef06bca503423ea6b86b5c2.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p style=&quot;background: white&quot;&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;接下来求线性回归的参数&lt;img src=&quot;/images/52nlp.cn/e62058614509d71fd623da7cb916d24d.jpg&quot; alt=&quot;&quot;&gt;，将似然函数期望值的式子里的高斯分布展开，可以得到如下式子：&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p style=&quot;background: white&quot;&gt;&lt;img src=&quot;/images/52nlp.cn/a2d2f03dd14d2af8b5ce33874907703f.jpg&quot; alt=&quot;&quot;&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p style=&quot;background: white&quot;&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;要求第k个模型的&lt;img src=&quot;/images/52nlp.cn/8b3aeba8b9e7d59f95b893b43791a381.jpg&quot; alt=&quot;&quot;&gt;，其他模型的W都对其没有影响，可以统统归做后面的const，这是因为log似然函数，每个模型之间是相加的关系，一求导数，其他模型的项就都消去了。上面的式子是一个加权的最小平方损失函数，每个数据点对应一个权重&lt;img src=&quot;/images/52nlp.cn/b60c2bcad7a820a58080ed40059a936d.jpg&quot; alt=&quot;&quot;&gt;，这个权重可以看做是数据点的有效精度，将这个式子对&lt;img src=&quot;/images/52nlp.cn/f9cdbf22b6a12d269e824e87f536327c.jpg&quot; alt=&quot;&quot;&gt;求导，可以得到：&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p style=&quot;background: white&quot;&gt;&lt;img src=&quot;/images/52nlp.cn/4d55ead19dc7202918743f57a0703c1d.jpg&quot; alt=&quot;&quot;&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p style=&quot;background: white&quot;&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;最后求得&lt;img src=&quot;/images/52nlp.cn/029b89bf4181821bf9b311f5870cca43.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p style=&quot;background: white&quot;&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;其中&lt;img src=&quot;/images/52nlp.cn/7407d67188a8ef4a59f1781280d69460.jpg&quot; alt=&quot;&quot;&gt;，同样，对&lt;img src=&quot;/images/52nlp.cn/ebc3e3dcd784d2f0ed9c2fb87aa56da8.jpg&quot; alt=&quot;&quot;&gt;求导，得到：&lt;img src=&quot;/images/52nlp.cn/c344b994c842620d11f1f1c540097684.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p style=&quot;background: white&quot;&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;这样的到了所有的参数的新值，再重复E步和M步，迭代求得满意的最终的参数值。&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p style=&quot;background: white&quot;&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;接下来看一个线性回归混合模型EM求参数的图示，两条直线对应两个线性回归模型用EM方法迭代求参数，两条直线最终拟合两部分数据点，中间和右边分别是经过了30轮和50轮迭代，下面那一排，表示每一轮里，每个数据点对应的responsibilities，也就是类k对于数据点n的后验概率：&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p style=&quot;background: white&quot;&gt;&lt;img src=&quot;/images/52nlp.cn/fab335c33f3f473d37d68f4cae5d8f1b.jpg&quot; alt=&quot;&quot;&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p style=&quot;background: white&quot;&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;最终求得的混合模型图示：&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p style=&quot;background: white&quot;&gt;&lt;img src=&quot;/images/52nlp.cn/ca096490972c57c8819c7f3a419424a8.jpg&quot; alt=&quot;&quot;&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p style=&quot;background: white&quot;&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;接下来讲逻辑回归混合模型，逻辑回归模型本身就定义了一个目标值的概率，所以可以直接用逻辑回归本身作为概率混合模型的组件，其模型如下：&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p style=&quot;background: white&quot;&gt;&lt;img src=&quot;/images/52nlp.cn/b6b7473fb2928f3833f4d43cf500f9d1.jpg&quot; alt=&quot;&quot;&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p style=&quot;background: white&quot;&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;其中&lt;img src=&quot;/images/52nlp.cn/7cd4693032b90dff67fecf2ca98880bd.jpg&quot; alt=&quot;&quot;&gt;，这里面的参数&lt;img src=&quot;/images/52nlp.cn/134c3e48dd1cfc67d178d694613a5150.jpg&quot; alt=&quot;&quot;&gt;包括&lt;img src=&quot;/images/52nlp.cn/48ffddd8b724787806eb9c01fd909824.jpg&quot; alt=&quot;&quot;&gt;两部分。求参数的方法也是用EM，这里就不细讲了，要提一下的是在M步，似然函数不是一个closed-form的形式，不能直接求导来的出参数，需要用IRLS(iterative reweighted least squares)等迭代方法来求，下图是一个逻辑回归混合模型的训练的图示，左图是两个类别的数据，蓝色和红色表示实际的分布，中间图是用一个逻辑回归来拟合的模型，右图是用两个逻辑回归混合模型拟合的图形：&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p style=&quot;background: white&quot;&gt;&lt;img src=&quot;/images/52nlp.cn/dea49da5c750abf5102dc0e7b0d602f8.jpg&quot; alt=&quot;&quot;&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p style=&quot;background: white&quot;&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;接下来讲mixtures of experts，mixture of experts是更一般化的混合模型，前面讲的两个混合模型，其混合系数是固定值，我们可以让混合系数是输入x的函数即：&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p style=&quot;background: white&quot;&gt;&lt;img src=&quot;/images/52nlp.cn/24abfd5e8b55dbf7c29a3dcf0d74e48b.jpg&quot; alt=&quot;&quot;&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p style=&quot;background: white&quot;&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;系数&lt;img src=&quot;/images/52nlp.cn/0aac3223f1bd024d03fd61562af7a911.jpg&quot; alt=&quot;&quot;&gt;叫做gating函数，组成模型&lt;img src=&quot;/images/52nlp.cn/1e67281c9e866be02aa564af464190c5.jpg&quot; alt=&quot;&quot;&gt;叫做experts，,每个experts可以对输入空间的不同区域建模，对不同区域的输入进行预测，而gating函数则决定一个experts该负责哪个区域。&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p style=&quot;background: white&quot;&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;gating函数满足&lt;img src=&quot;/images/52nlp.cn/3d5acf4bcb4eb78019b74187c82a616e.jpg&quot; alt=&quot;&quot;&gt;，因此可以用softmax函数来作为gating函数，如果expert函数也是线性函数，则整个模型就可以用EM算法来确定。在M步，可能需要用IRLS，来迭代求解，这个模型仍然有局限性，更一般化的模型是hierarchical mixture of experts&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;也就是混合模型的每个组件又可以是一个混合模型。好了就讲这么多吧，书上第14章的内容都讲完了。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;注：PRML读书会系列文章由 &lt;a href=&quot;http://weibo.com/dmalgorithms&quot;&gt;@Nietzsche_复杂网络机器学习&lt;/a&gt; 同学授权发布，转载请注明原作者和相关的主讲人，谢谢。&lt;/p&gt;
&lt;p&gt;PRML读书会讲稿PDF版本以及更多资源下载地址：&lt;a href=&quot;http://vdisk.weibo.com/u/1841149974&quot;&gt;http://vdisk.weibo.com/u/1841149974&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;本文链接地址：&lt;a href=&quot;http://www.52nlp.cn/prml%E8%AF%BB%E4%B9%A6%E4%BC%9A%E7%AC%AC%E5%8D%81%E5%9B%9B%E7%AB%A0-combining-models&quot;&gt;http://www.52nlp.cn/prml读书会第十四章-combining-models&lt;/a&gt;&lt;/p&gt;

											

</description>
        <pubDate>Sat, 31 Jan 2015 00:00:00 +0800</pubDate>
        <link>http://iftti.com/posts/2015/2015-01-31-prml%25e8%25af%25bb%25e4%25b9%25a6%25e4%25bc%259a%25e7%25ac%25ac%25e5%258d%2581%25e5%259b%259b%25e7%25ab%25a0-combining-models-5048ec172.html</link>
        <guid isPermaLink="true">http://iftti.com/posts/2015/2015-01-31-prml%25e8%25af%25bb%25e4%25b9%25a6%25e4%25bc%259a%25e7%25ac%25ac%25e5%258d%2581%25e5%259b%259b%25e7%25ab%25a0-combining-models-5048ec172.html</guid>
        
        
        <category>52nlp</category>
        
      </item>
    
      <item>
        <title>PRML读书会第十二章 Continuous Latent Variables</title>
        <description>

						&lt;p style=&quot;text-align: center&quot;&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 15pt&quot;&gt;&lt;strong&gt;PRML读书会第十二章 Continuous Latent Variables&lt;br&gt;
&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p style=&quot;text-align: center&quot;&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 15pt&quot;&gt;&lt;strong&gt;主讲人 戴玮&lt;br&gt;
&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p style=&quot;text-align: center&quot;&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;&lt;strong&gt;（新浪微博: &lt;a href=&quot;http://weibo.com/u/1433881802&quot;&gt;@戴玮_CASIA&lt;/a&gt;）&lt;br&gt;
&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;Wilbur_中博(1954123) 20:00:49&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt; 我今天讲PRML的第十二章，连续隐变量。既然有连续隐变量，一定也有离散隐变量，那么离散隐变量是什么？我们可能还记得之前尼采兄讲过的9.2节的高斯混合模型。它有一个K维二值隐变量z，不仅只能取0-1两个值，而且K维中只能有1维为1、其他维必须为0，表示我们观察到的x属于K类中的哪一类。显然，这里的隐变量z就是个离散隐变量。不过我们容易想到，隐变量未必像kmeans或GMM这种聚类算法那样，非此即彼、非白即黑，我们当然也可能在各个聚类或组成成分之间连续变化。而且很多情况下，连续变化都是更合理、更容易推广的。所以，我们这一章引入了连续隐变量。&lt;br&gt;
书中举了一个例子：从某张特定的手写数字图像，通过平移和旋转变换生成多张图像。虽然我们观察到的是整个图像像素的一个高维数据空间中的样本，但实际上只是由平移和旋转这三个隐变量产生的，这里的平移和旋转就是连续隐变量。还举了个石油流量的例子，是从两个隐变量经过测量得到12个观察变量，那里的两个隐变量也是连续的。 一般来说，样本不会精确处在由隐变量表示的低维流形上，而是可能稍有偏差，这种偏差可视作噪声。噪声的来源各种各样，不是我们能把握的，一般只能统一把它们看成单一的噪声项来处理。&lt;br&gt;
最简单的情况下，我们可以把隐变量和观察变量都假设为高斯分布，并且利用2.3.1讲过的条件分布与边缘分布之间的线性高斯关系，来建立观察变量与隐变量之间的线性模型。这样，我们就可以建立主成分分析（PCA）以及与之相关的因子分析（FA）的概率模型。不过在此之前，我们还是看看传统视角是如何处理主成分分析的：&lt;/span&gt;&lt;span id=&quot;more-7900&quot;&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt; PCA也叫Karhunen-Loève transform（KL变换）或Hotelling transform（霍特林变换）。&lt;strong&gt;它有两种可产生相同算法的等价视角：最大方差和最小重构误差。&lt;/strong&gt;两种视角都希望找到一组正交投影，把原数据投影到低维的线性子空间上。但最大方差视角是说，我们希望数据投影之后在投影方向上有最大方差；而最小重构误差视角是说，我们希望投影后的数据和原数据之间的均方差最小。前者由Hotelling于1933年提出，后者由Pearson于1901年提出。&lt;br&gt;
先来看最大方差视角。首先定义样本均值和样本协方差：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/b99934fff635cda2e353b2191e21409a.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/831a6ed7b557047382859de034a4b3ad.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
然后，我们可以得到某个投影方向u_1上的方差：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/0892535a7ef546610904b28e7498e2e5.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
不失一般性，我们令&lt;img src=&quot;/images/52nlp.cn/3ad823fdbfcb0915da8d39a1ea2fe0f0.jpg&quot; alt=&quot;&quot;&gt;，这样我们就可以将&lt;img src=&quot;/images/52nlp.cn/dac0adc4705a1962fb0b0a087c150aca.jpg&quot; alt=&quot;&quot;&gt;作为约束，将方差最大作为目标函数，把这个问题看作有约束最优化问题，因此可用拉格朗日乘子法求解：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/53437eb0f20a867cfb173d42cb268542.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
令其导数为0，可得到：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/7c020691545605b2ad66f642427c61ba.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
这是我们熟悉的线性代数中的特征值分解问题，lambda_1和u_1分别是S的特征值和特征向量。而且可以看到，这里求出的u_1方向的最大方差就是：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/0e54682e4a995cefaa577d24efe4dcd7.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
在余下的方向中依次选择最大方差方向，就是S由大到小给出的各个特征值以及对应的特征向量，这也容易从S是实对称矩阵、因此得到的特征向量之间是正交的这一点看出来。&lt;br&gt;
再来看最小重构误差视角，由投影方向之间的标准正交关系，我们可以得到样本在D个投影方向下的表示：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/4f8ea4b9e0c354ea06067628c0357b9c.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
但我们不想用D个投影方向，而是想用M&amp;lt;D个方向来表示样本，并且希望这样表示尽可能接近原样本。那么原样本与M个方向重构得到的样本之间的误差，用均方差来衡量就是：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/a306848d512988d6489b38325c827723.jpg&quot; alt=&quot;&quot;&gt;&lt;img src=&quot;/images/52nlp.cn/dfe988d8290a27f14b1aa58fd50c11a2.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
上面的公式12.14展开之后就是：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/5c071ccb21483c753f793c53a8e450cb.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
我们想最小化这个重构误差项。因为投影方向之间正交，所以也可以逐一求解，也就是目标函数：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/890829eabd568415b6170bafa65d8b95.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
约束条件是：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/a340768f14f200dc201183b961a4f698.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
同样可以由拉格朗日乘子法得到：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/a9f9e57a8aa7b20fd324bba59f8caf37.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
这和最大方差视角一样，也是特征值问题。只不过这里是去掉较小特征值对应的方向，因为那些方向对应着较小的重构误差，而先前是保留较大特征值对应的方向。但得到的结果是完全一样的。&lt;br&gt;
在D个特征向量中选择前M个作为投影方向，得到的重构误差就是：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/369eb493ebcf535e0ac6fcdba5e7fae5.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
下面简单谈谈PCA的复杂度问题。我们知道，S是DxD维矩阵，对S做特征值分解需要O(D^3)的复杂度。如果仅需要前M个最大的特征值以及特征向量，那么有一些算法可达到O(M*D^2)复杂度，比如power method(Golub and Van Loan, 1996)。然而，即使是O(M*D^2)，在D较大的时候也是难以接受的。比如我们可能会用PCA做人脸识别的一些处理，人脸图像一般是几万维的，那么O(M*D^2)就是至少好几亿的复杂度，这显然是无法接受的。一会我们要讲的概率角度下用EM算法求解PPCA可以进一步降低复杂度。但仅从矩阵分解角度，实际上有一个非常巧妙的方法，也是PCA实现中常用的一种技巧：&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;我们知道，对S做特征值分解的公式是：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/5916645bc24974b2e70cb517fd0ee416.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
我们可以把左右都左乘X，得到：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/33ffeea704716c6e4e982d2b6fc0a1bf.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
令&lt;img src=&quot;/images/52nlp.cn/90e1ebb8a6228fa0841c42c84df2e24f.jpg&quot; alt=&quot;&quot;&gt;得到：&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/images/52nlp.cn/74ac84c4b66207ddf565cd7fd87027ab.jpg&quot; alt=&quot;&quot;&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;&lt;br&gt;
这里可以看到，对&lt;img src=&quot;/images/52nlp.cn/cfb024ff5bd6af47b5df75186e1eb69a.jpg&quot; alt=&quot;&quot;&gt;做特征值分解，与对&lt;img src=&quot;/images/52nlp.cn/8a32135daed7c7fe5cf2e94dd7234623.jpg&quot; alt=&quot;&quot;&gt;做特征值分解，它们有着相同的非零特征值，但特征向量是不一样的。&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;不过，对(12.28)左右同时左乘X^T之后，我们可以得到：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/752cc7da04b307b1473fe9c70dd20ee5.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
则&lt;img src=&quot;/images/52nlp.cn/2dc7c17b1c7d6ef920d18728e66ca345.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
因为我们要求投影方向是标准正交的，所以归一化之后就是&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/d49afb9f2488864af9a928ceb1b4211e.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
因为XX^T是NxN矩阵，和样本数量相关而和特征维度无关，所以如果是特征空间维度很高，但样本数量相对来说不那么大，那么我们就可以对XX^T做特征值分解，而不是X^TX。这样算法求解起来会快很多。但是，如果样本数量也很大、特征维度也很高，这样做也不合适了。这时就需要借助概率模型来求解。&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;color: #333333;font-size: 12pt&quot;&gt;&lt;span style=&quot;font-family: Arial&quot;&gt;============================&lt;/span&gt;&lt;span style=&quot;font-family: 宋体&quot;&gt;讨论&lt;/span&gt;&lt;span style=&quot;font-family: Arial&quot;&gt;=================================&lt;br&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;Wilbur_中博(1954123) 20:30:56&lt;br&gt;
好了，有问题现在可以提问。&lt;br&gt;
dxhml(601765336) 20:32:36&lt;br&gt;
好像还没看出和连续隐变量的关系，是不是数据在主成分方向的投影是隐变量，因为是连续的，所以。。。&lt;br&gt;
Wilbur_中博(1954123) 20:33:35&lt;br&gt;
嗯，还没讲到连续隐变量。。前面说了，这是传统视角的PCA，而不是一会要讲的概率视角。 概率视角才有隐变量观察变量那套东西。&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;我觉得这部分应该都蛮熟悉的，不会有什么问题吧。&lt;br&gt;
Phinx(411584794) 20:34:46&lt;br&gt;
为什么维度太高了，就不合适啊？&lt;br&gt;
coli&amp;lt;fwforever@126.com&amp;gt; 20:35:19&lt;br&gt;
现在应该是理论阶段吧&lt;br&gt;
布曲(15673189) 20:35:22&lt;br&gt;
矩阵运算太复杂了&lt;br&gt;
Wilbur_中博(1954123) 20:36:11&lt;br&gt;
因为O(D^3)里的D就是维度，太高了矩阵分解很慢，就算是O(D^2)也够慢的。前面说了，上万维的图像，一平方就好几亿了。&lt;br&gt;
也不是理论阶段了，其实那个把DxD变为NxN的技巧就很实用的。没什么问题就继续了哈&lt;br&gt;
布曲(15673189) 20:37:54&lt;br&gt;
是的 大家可以去网上下pca人脸识别的matlab小代码&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-size: 12pt&quot;&gt;&lt;span style=&quot;color: #333333&quot;&gt;&lt;span style=&quot;font-family: Arial&quot;&gt;========================&lt;/span&gt;&lt;span style=&quot;font-family: 宋体&quot;&gt;讨论结束&lt;/span&gt;&lt;span style=&quot;font-family: Arial&quot;&gt;=================================&lt;/span&gt;&lt;/span&gt;&lt;span style=&quot;font-family: 微软雅黑&quot;&gt;&lt;br&gt;
Wilbur_中博(1954123) 20:40:45&lt;br&gt;
前面讲的传统视角，是把原数据空间往较低维子空间上做线性投影，找这个投影方向。下面，我们从概率角度、用隐变量来建模。PCA也可以从这种角度导出。PPCA相较于传统PCA，有以下优点：&lt;br&gt;
1. 既可以捕捉数据中的线性相关关系，又可以对高斯分布中的自由参数加以限制；&lt;br&gt;
2. 在少数几个特征向量的特征值较大、其他都较小时，用EM算法求解PCA是很高效的，而且它不用计算协方差阵作为中间步骤；&lt;br&gt;
3. 概率模型和EM的结合，使我们可以解决数据集中的missing values；&lt;br&gt;
4. PPCA的混合模型也可用EM求解；&lt;br&gt;
5. 贝叶斯方法可自动确定数据的主子空间维度；&lt;br&gt;
6. 它的似然函数可以有概率解释，因此可与其他概率密度模型做比较；&lt;br&gt;
7. PPCA可作为类条件概率用到分类器中；&lt;br&gt;
8. 可从中采样得到新样本。&lt;br&gt;
PPCA是前面2.3.1讲过的线性高斯模型的一个实例。线性高斯模型就是说，边缘分布是高斯的，条件分布也是高斯的，而且条件分布的均值与边缘分布之间是线性关系。这里我们引入隐变量z及其分布p(z)，并且观察变量x是由z产生的，因此有条件分布p(x|z)。&lt;br&gt;
不失一般性，设z服从0均值单位方差的高斯分布，则有： &lt;img src=&quot;/images/52nlp.cn/44feaff1fbae18453cb44aa55b43012d.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
以及&lt;br&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/images/52nlp.cn/56e09683252e68480a8193e09bd3773f.jpg&quot; alt=&quot;&quot;&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;&lt;br&gt;
这里的参数有三个：从隐变量空间到观察变量空间的线性变换W、均值mu和方差sigma^2。从产生式的观点来看，我们先从p(z)采样得到隐变量z的值，然后用这个值经过线性变换，并加上均值项（可看做一个常数偏移）和高斯噪声项，就可以得到x：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/a03d318b8514a340c5aa7fe1a47d9310.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
这里z是M维高斯隐变量，epsilon是D维零均值高斯分布噪声，且有方差sigma^2*I，也就是说，我们假定各维噪声之间是独立的、且方差相等。从这个概率框架可以看出，我们先是用W建立了隐变量与观察变量之间的关系，再用mu和epsilon描述了观察变量的概率分布是什么样的。&lt;br&gt;
我觉得PPCA比起传统PCA，有一点优势就是利用了概率分布。因为我们的数据即使在某个低维子空间上，也不可能分布在整个子空间，而是只处在其中一个小区域。概率模型就很好地利用了这一点。当然，除了生成数据之外，概率模型更大的优势还是通过观察变量，也就是手里的数据，去推断参数也就是W、mu、sigma^2是什么。这就要利用一些统计推断方法，比如最大似然法。但想用最大似然，必须先知道似然函数是什么。由2.3.1, 2.3.2, 2.3.3这几节的锤炼，我们应该对高斯分布的边缘分布、条件分布等形式的推导已经很熟练了。所以我们从前面给出的p(z)和p(x|z)，容易得到边缘分布p(x)：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/79c6098ef66b225cdbd610878b23feee.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;其中&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/5bab34dfb56074d6160ebcd82ff55653.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
实际上把(12.31)和(12.32)的协方差，代入到2.3.3最后给出的边缘分布公式中，可以得到这里给出的边缘分布p(x)。当然也可以像书上讲的那样，从(12.33)来推导。&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/445e36f4386106000d14374d4d7e9122.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
因此我们得到了似然函数的具体形式：它也是一个高斯分布，以及它的期望和协方差是什么。高斯分布的指数里有协方差的逆，这里协方差矩阵C是DxD维的：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/9e2bece2f4b69fa1aadeb0f41a5067c5.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
所以直接求逆的话复杂度也很高。我们也可以利用一些技巧，把直接求逆的O(D^3)复杂度降到O(M^3)&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/bda51c67e682cfe87fcc98e7536a2a16.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/614e49570058b7286611682f4a5116c6.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
有了似然函数，我们就可以用最大似然法来从观察变量求解未知参数了。首先，为了方便求解，我们对似然函数&lt;img src=&quot;/images/52nlp.cn/66fd2c0aa6af3bc2dc996fbea400e20b.jpg&quot; alt=&quot;&quot;&gt;取对数，这是我们都很熟悉的做法了，得到：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/92e67b01b0d631000ba81b50e1a3c34e.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;前面说过，我们的未知参数有三个：W, mu, sigma^2，似然函数逐一对它们求导，令导数为0，就可以得到各自的参数估计。&lt;br&gt;
mu的形式最简单&lt;img src=&quot;/images/52nlp.cn/8cc7a2feaa5e4af8a43635063d05bb35.jpg&quot; alt=&quot;&quot;&gt;代回(12.43)得到：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/1f06d904390c468630ce7b9857be393e.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
这里利用了sum(x_i^T*S*x_i) = Tr(X^T*S*X)，其中X的第i列是x_i，这是因为X’SX的第(i,j)个元素等于X’的第i行乘以S乘以X的第j列。因此，(12.43)的最后一项可变为(12.44)的最后一项。还要利用trace的性质：Tr(ABC) = Tr(BCA) = Tr(CAB)。&lt;br&gt;
但对剩下的两个参数求导以及求解的过程，这里就比较含糊其辞了，只是说求解比较复杂，但仍然可以得到解析解。 直接给出书上写的解析解形式：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/775a9a5b2b8efb02f9dc69b1367dfe50.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
这里的U_M由S（样本协方差阵）的最大的M个特征值对应的特征向量组成时，可以取到最大值，如果不是最大的而是任意特征向量，那么只是鞍点，这是由Tipping and Bishop (1999b)给出的证明。L_M是相应特征值的对角阵，而R是任意的MxM维旋转矩阵。 因为有R的存在，所以可以认为，从概率角度看，这些隐变量张成的子空间以及从该子空间如何生成样本比较重要，但取这个子空间的哪些方向则不是那么重要。只要子空间一致，我们可以任意旋转张成子空间的这些方向。我觉得这也是PPCA和传统PCA的区别之一。&lt;br&gt;
sigma^2的最大似然解是：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/2f834fb7211f036c00366af6f5812062.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
直观来看，它的物理意义是，被我们丢弃的那些维度的方差的平均值。从(12.32)和(12.33)看到，sigma^2实际上代表了噪声的方差，那么这个方差当然是越小越好，说明W已经尽可能保留了分布信息。它也可以看成某种平均重构误差。&lt;br&gt;
你们可以先讨论讨论。。看看有什么问题&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;dxhml(601765336) 21:36:52&lt;br&gt;
这样的模型的表达能力有限，X如果不是高斯分布呢？&lt;br&gt;
Wilbur_中博(1954123) 21:40:43&lt;br&gt;
@dxhml 是啊，所以这是最简单的模型，肯定还是要由浅入深么。后面还会讲到高斯分布之外的假设。&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;继续，既然给出了解析形式，那么我们仍然可以用特征值分解的方法来求解，因为W和sigma^2的最大似然估计都是和特征值以及特征向量相关的。但因为有似然函数，所以借助各种最优化工具，比如共轭梯度法，或者一会要讲到的EM，求解起来会更快。这一小节最后还讲到了PPCA因为建立了隐变量与观察变量之间的关系，所以自由度上远比直接估计高斯分布要小，这样估计起来所需要的数据也就小不少，速度也会快很多。&lt;br&gt;
下面讲PPCA的EM求解，EM法有这样几个好处：高维空间下比解析解速度快，可扩展到其他没有解析解的模型比如因子分析（FA）上，可处理missing data的情况。EM法尼采兄前面已经讲得很清楚了，就是在E、M两步间来回迭代。这里因为我们在E步求出z_n和z_n*z_n^T的后验期望，然后在M步把这两个期望代回去，就可以做对数似然的最大化了，然后可以得到新的W和sigma^2。&lt;br&gt;
前面我们看到，mu的形式非常简单，所以mu就直接用样本均值代替了，迭代求的只有W和sigma^2。也就是：&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/409ffcb0a30f2f29f34c5b9287b08f31.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
其中E步： &lt;img src=&quot;/images/52nlp.cn/3724cf507d9a4580decac871f2bb630f.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
M步：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/4df2c766a78e89b8b8ce75ca30fa45d4.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;EM法并不需要显式构造协方差阵，它的主要开销在于W和sigma^2的更新中对整个数据集做求和操作，所以复杂度是O(NDM)，可以看到，因为高维情况下D比较大，而我们降维后的维度M较小，所以比特征值分解最好的O(ND^2)要好不少。而且EM的online版本可以进一步提高效率。missing data就不讲了。。自己看看吧。&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;color: #333333;font-size: 12pt&quot;&gt;&lt;span style=&quot;font-family: Arial&quot;&gt;==========================&lt;/span&gt;&lt;span style=&quot;font-family: 宋体&quot;&gt;讨论&lt;/span&gt;&lt;span style=&quot;font-family: Arial&quot;&gt;=================================&lt;br&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-size: 12pt&quot;&gt;&lt;span style=&quot;font-family: 微软雅黑&quot;&gt;布曲(15673189) 22:07:20&lt;br&gt;
ppac优于传统的pca主要是因为运算速度吗，因为引入隐变量 可以用em更快的求解?&lt;br&gt;
Wilbur_中博(1954123) 22:10:20&lt;br&gt;
嗯，我觉得运算速度上是有优势的，因为特征值分解比较耗时。不过传统角度应该也可以用一些最优化方法和online方法求解那个代价函数吧。没有具体总结过。感觉更大的优势还是在概率角度解释数据比较好。前面我说过一个：我觉得PPCA比起传统PCA，有一点优势就是利用了概率分布。因为我们的数据即使在某个低维子空间上，也不可能分布在整个子空间，而是只处在其中一个小区域。概率模型就很好地利用了这一点。&lt;br&gt;
布曲(15673189) 22:12:12&lt;br&gt;
嗯 同意&lt;br&gt;
Wilbur_中博(1954123) 22:12:14&lt;br&gt;
我觉得这可能是比较有优势的。传统PCA投影角度感觉解释得有点粗糙了。是吧。&lt;br&gt;
&lt;/span&gt;&lt;span style=&quot;color: #333333&quot;&gt;&lt;span style=&quot;font-family: Arial&quot;&gt;========================&lt;/span&gt;&lt;span style=&quot;font-family: 宋体&quot;&gt;讨论结束&lt;/span&gt;&lt;span style=&quot;font-family: Arial&quot;&gt;=================================&lt;/span&gt;&lt;/span&gt;&lt;span style=&quot;font-family: 微软雅黑&quot;&gt;&lt;br&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;Wilbur_中博(1954123) 20:57:53&lt;br&gt;
从12.2.3继续讲，上次我们看到了怎么从贝叶斯角度解释PCA，也就是PPCA。简单说，就是假设有个隐变量z服从标准高斯分布，它通过某个线性变换W生成x，这个变换过程还可能有某些未知的随机因素比如噪声等，可将其假设为0均值同方差随机变量。所以，我们要做的就是通过手里拿到的数据X去推断未知参数：线性变换W、偏移量同时也是x的均值mu、未知随机量的方差sigma^2*I。这里W的各列张成了我们感兴趣的主成分空间。我们讲到，虽然通过最大似然解得到的W与原先传统PCA相比，求出来的主方向差不多，但用EM法求解复杂度降低了不少，而且从贝叶斯角度也可以对PCA的模型假设和生成过程有更深刻的认识。但有一个问题我们没有谈到，就是如何选择W中的哪些主方向是最有用的，或者说如何更有效地降维。&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;我们上次讲过，从(12.45)：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/ce1aca0f56496717453baa261ac0ba50.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;可以看到，它的最大似然解仍然与最大的那些特征值以及对应的特征向量相关，所以我们也可以像传统PCA那样，把特征值从大至小排列出来，看看是不是在哪个值上突然下降，把特征值分成较大和较小的两部分，那么我们就可以选择较大的那部分特征值以及与其对应的特征向量。&lt;br&gt;
但是，一方面，特征值未必存在这种突然下降的趋势，可能是比较平滑地逐渐下降；另一方面，我们既然用了贝叶斯方法，那就应该将贝叶斯进行到底。&lt;br&gt;
我们在第7章也讲过，用某些先验可以将特征的线性表示稀疏化，进而起到特征选择的效果，那么这里，我们也可以利用这一思想做降维。既然用贝叶斯，那就肯定要对W设一个先验分布。这里介绍了一种比较简单的方法：设W的先验分布为各维独立的高斯分布，且各维方差由precision超参数alpha_i控制，前面讲过，precision就是方差的倒数。那么，先验分布可以写作：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/34373ed4b7e5ad319000e1dd13a45ded.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
稍后可以看到，alpha_i在迭代计算的过程中，有些会趋向于无穷大，那么相应地，那个维度的w_i就会趋向于0，说明那个维度的特征没什么用。这样我们有用的特征就是alpha有限的那些维度，而且alpha越小的维度越有用。这是比较常见的稀疏化方法，更具体的内容可参见PRML的7.2节。求解方法就是一般贝叶斯求解的EM法，首先把W积掉，得到似然函数：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/f9c6a826e72e334f06ee948dad1044d9.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
这个边缘分布的积分是不易求的，我们可以借助前面讲过的4.4节的Laplace approximation，对alpha_i求边缘分布的极值，得到：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/3a6920eab28e085098c0a08501e14cd7.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
这是EM当中M步的估计alpha_i的计算公式。&lt;br&gt;
M步估计sigma^2的公式和先前PPCA的EM法讲过的(12.57)一样，而W的公式涉及到先验，所以稍有改变：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/260af3edf4838cb68f087437c287eb90.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;E步的公式没有变化。&lt;br&gt;
这样经过多次迭代之后，就可能有某些alpha_i会趋向于无穷大，我们就可以把那些方向去掉。当然，我们也可以进一步去掉那些较大的alpha_i，只留下较小的。不过一般来说去掉无穷大的那些已经足够了。下面是分别使用前面讲过的EM法和这里讲的带有稀疏化效果的EM法的图示，可以看到，降维效果是十分明显的：&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/e106db348f54966cdbe5a199b357e9c9.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;下面看一下12.2.4的因子分析FA，FA和之前PPCA的不同在于，PPCA的条件分布是：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/75982b300a5b77b91806f1b6fa03a526.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
而FA是：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/e90bad8dea6d785c8f43783f7651510f.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
求解方法什么的也都差不多。这里就不多讲了。说实话我对FA不理解得并不深，而且这里讲的FA和我之前理解的FA似乎也不太一样。我原先以为FA就是类似于PCA，各方向可任意旋转，不一定是特征值从大到小排列的那种，但各方向之间仍保持正交。不知道大家是怎么看FA的？&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;&lt;br&gt;
下面重点讲一下12.3 核PCA（KPCA）：&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;第六章讲过，我们可以方便地把内积扩展到非线性核，从而可以利用它来隐式求解非线性情况。这里，我们也利用核方法来求解PCA。因为要利用内积的非线性核，所以我们必须把传统PCA以内积方式表示出来。前面讲过，传统PCA是求样本协方差阵的特征值与特征向量：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/25e94111d3ea7b9100435b7fe6d6920a.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
样本协方差阵是DxD维的：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/01d7a36d90e41c8bf320c55f3d8875cc.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
并且特征向量有约束&lt;img src=&quot;/images/52nlp.cn/c6f16b01913746db9ddfa6b67894cff0.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
现在我们利用非线性变换phi把样本变换为&lt;img src=&quot;/images/52nlp.cn/4b7d2d21a2ca128e6908c1889460cdc9.jpg&quot; alt=&quot;&quot;&gt;，暂时为了方便假定变换后的向量有0均值&lt;img src=&quot;/images/52nlp.cn/e37234216d21ea1f8782314cdb554a2b.jpg&quot; alt=&quot;&quot;&gt;，之后再看不是0均值的情况。这样，变换后样本的协方差阵就变为：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/e98717cf2627c85b3196f47bfab3c61c.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
协方差阵的特征值分解变为：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/548c02b4379425c349765c6757b1836d.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
现在的形式是phi(x_n)*phi(x_n)^T，我们想把它变为包含phi(x_n)^T*phi(x_n)的形式，以便利用核而不是直接用变换后的样本phi(x_n)来求解。&lt;br&gt;
把刚才给出的(12.73)代入(12.74)，我们可以得到：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/470be3084fef902c5e8bff6d1a386fcc.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
这说明v_i可以表示为phi(x_n)的线性组合：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/8684b8d6955d25300fa25b1368df845d.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
但我们不知道这个系数a是什么，因为我们并不显式地知道非线性映射phi(x)。所以，这个a是我们之后要去求解的对象。&lt;br&gt;
将12.76代回到12.75，可以得到：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/a7f544c59b5bb8fc3aeab718064a0c1f.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
两边都左乘phi(x_l)^T，核这个神秘角色就出现在我们面前了：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/65c8ac4cdc0c2a7b8756ac14fefc3307.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
写成矩阵形式，就是：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/a575daa4830feb2e433435236cfe711a.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
两边都消去一个K（这样做不影响非0特征值）：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/422516c2f4be881999b97b20ec0adeeb.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
和传统PCA一样，有归一化约束：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/f5c38886da021094d75ac0ea82b69a66.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;这样，这个问题同样变成了特征值分解问题，我们可以(12.80)和(12.81)求出a_i和lambda_i。我们经过变换的样本phi(x)再经过特征向量投影后的值，也就可以表示为：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/a7906f9915f05ee40ecf930a8c855011.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;注意这里是用x和x_n的核的线性组合来表示的。由于主方向并不是线性的，所以我们无法求出它的形式是什么，而是只能求出如何通过核函数来得到投影到主方向后的降维形式如何用核函数表示出来。&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;我们先前为了简化计算而假定非线性映射phi(x_n)有0均值，现在去掉这个假设，将其扩展到一般形式。我们要减去均值以及计算减去均值之后的核矩阵：&lt;img src=&quot;/images/52nlp.cn/93acadac704fa63b505487af0575db59.jpg&quot; alt=&quot;&quot;&gt;&lt;img src=&quot;/images/52nlp.cn/b348e05864eccffb5b4e50d1a95fe739.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;用K弯的矩阵形式：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/2e53c70a1356360d500a4aa426f98e19.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
就可以像先前一样表示样本经过非线性变换在主方向下的投影。&lt;br&gt;
KPCA有一个缺点，就是K是NxN维的，而传统PCA的样本协方差阵是DxD维的，那么在数据规模相对于特征维度来说很大时，计算效率就比较低。&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;这么顺着讲公式是有点枯燥。。可能还没上次讲点基本的东西比较有趣。特别是一些细节。大家自己也要多看看，PRML这种书真是每看一遍都有新发现。&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;我现在讲第12章最后的一小部分：12.4的非线性隐变量模型。&lt;br&gt;
先来看看独立成分分析ICA。前面讲到，PCA假设隐变量服从高斯分布，这样想得到各维之间独立的隐变量，只需旋转隐变量使其协方差阵为对角阵即可，即隐变量各维之间线性无关。对于高斯分布来说，线性无关就是独立，因此其联合分布可分解为各维分布的乘积。&lt;br&gt;
如果不要求隐变量一定是高斯分布，但仍保持观察变量和隐变量之间具有线性关系，那就是这一小节讲到ICA。这样的话，协方差阵为对角阵就不够了，因为对于任意分布来说，线性无关只是独立的必要条件而非充分条件。&lt;br&gt;
得到这个独立解可以有很多方法，比如用高阶统计量而不仅仅是二阶的协方差来度量独立性，或者用互信息等方式来度量。书中介绍了一种描述隐变量的重尾分布：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/8a368d2f7d25c0bf8ef03aa771aabfe2.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
但也可以用其他分布。实际上这类分布可以分为超高斯和次高斯两种。具体细节可以看那本ICA的专著，可以在这里下载：http://ishare.iask.sina.com.cn/f/16979796.html&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;http://ufldl.stanford.edu/tutorial/index.php/ICA 和 http://ufldl.stanford.edu/tutorial/index.php/RICA 也有一些ICA的介绍，而且提供了一个不错的代码框架。我们如想实现那里介绍的方法，只需填入一小部分代码即可。&lt;/p&gt;
&lt;p&gt;下面看一下autoassociative neural networks：&lt;br&gt;
这一小节讲到的autoassociative neural networks，实际上就是最近deep learning中研究比较多的autoencoder。它的基本思想，就是让输入经过多层线性或非线性变换后，得到的输出尽量接近输入，然后取中间一层神经元作为隐变量。&lt;br&gt;
最简单的两层情况是：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/e77b64c8bcfeb1955319d6d065a5ab74.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/e55e015f4a2744eadc93be560b36e094.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
多层的话就是：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/1b21cfa88f60900f585061f77e8899d8.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/3ef4f0bc118b53772500c08d3b8b67d0.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
这里提到一点：中间隐变量的数量M需要小于输入变量的维度D。不过，现在deep learning中一般会对隐变量添加稀疏约束，这样就可以让M大于D且具有稀疏性，可以更好地描述数据。&lt;br&gt;
本小节最后也提到，由于包含了非线性变换，因此目标函数不再是简单的二次函数，会变成非凸问题，优化起来容易陷入局部极值。deep learning解决的问题之一，就是利用逐层优化的pretraining方式，使得在最后整体优化之前，找到一个比较好的初始解，这样即使陷入局部极值，找到的也会是比较好的局部极值。优化方式和传统神经网络相似，也是反向传播（BP）算法。具体步骤可参见：http://ufldl.stanford.edu/tutorial/index.php/Autoencoders ，和ICA一样，也有一个不错的代码框架，实现起来非常简单。&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;&lt;br&gt;
最后看一下12.4.3的非线性流形思想。我们可用多种方法来构造非线性流形：&lt;br&gt;
首先，我们可以用分段线性的方法，去捕捉非线性流形的信息。比如，可以先用kmeans等聚类算法，把数据分成多个聚类，然后在每个聚类上应用PCA等线性降维方法。&lt;br&gt;
其次，我们可以参数化曲线，并像PCA那样找到合适的参数，以便用曲线形式表示数据的主成分：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/386e22cdf5205587ba749d6f426013dc.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
这种方法称作主曲线分析，后来也有主曲面分析等推广。可参见：http://www.iro.umontreal.ca/~kegl/research/pcurves/&lt;br&gt;
最后看一下MDS、ISOMAP和LLE。MDS是从点对距离出发、希望降维后样本之间的点对距离能够尽量保持之前的点对距离。目标函数和求解方法都比较简单，这里就不赘述了。值得注意的是，这里我们并不需要每个样本的具体坐标，而是只需要知道样本之间的相对关系即可。而且，在欧氏距离下，MDS得到的坐标和PCA投影到最大主方向上的坐标是完全等价的。当然，MDS中使用的点对距离可以是任意的距离度量，因此得到的结果也可能和PCA投影坐标相去甚远。&lt;br&gt;
ISOMAP和LLE是点燃流形学习这把大火的两篇论文，都发表在science上。ISOMAP利用了MDS，只不过使用的距离度量是测地线距离，也就是通过样本相连的最短距离。比如从A经过B、C到D，而且这条路径是最短的A与D之间的路径，那么这条最短路径的长度就是A与D之间的测地线距离。LLE的思想同样很简单：它用样本点x的局部邻域内的样本去线性表示x，然后希望映射到低维空间后，其局部线性关系尽量保持不变，也就是说，局部的线性表示系数和邻域样本都尽量保持不变。LLE具体可参见：http://www.cs.nyu.edu/~roweis/lle/ ，有很详细的算法介绍和写得非常漂亮的matlab代码。可惜作者sam roweis前几年不幸去世，是机器学习界的一大损失。&lt;br&gt;
可以看到，前面介绍的以ISOMAP和LLE为代表的流形学习，其思想都是在低维空间中保持原先在高维空间中的某种样本之间的关系。这正是流形学习的核心思想：通过流形可能具有的先验信息，以保持样本中蕴含的关系为约束，来找到高维样本嵌入到低维流形的坐标。&lt;br&gt;
后面还介绍了GTM和SOM两种方法，提到前者可看作后者的概率方法，但我并不太了解它们，所以这里就不多说了。谢谢大家。这章就是这样：）&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;color: #333333;font-size: 12pt&quot;&gt;&lt;span style=&quot;font-family: Arial&quot;&gt;==========================&lt;/span&gt;&lt;span style=&quot;font-family: 宋体&quot;&gt;讨论&lt;/span&gt;&lt;span style=&quot;font-family: Arial&quot;&gt;==================================&lt;br&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;快乐的孩纸(328509423) 22:05:41&lt;br&gt;
autoencoder 实际上就是intput x，output x，来学习隐层，然后用隐层去做预测y？&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;Wilbur_中博(1954123) 22:07:31&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;是的，我做过一些实验，还挺好用的。有一些变种，比如对输入加一些噪声，但输出仍然是不带噪声的x，这样就是denoising autoencoder了。&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;快乐的孩纸(328509423) 22:08:47&lt;br&gt;
那如果是两层的autoencoder只有一个隐层，要是多个隐层呢？比如中间有3个隐层，得到这3个隐层，怎么用呢？这意味着intput x这个vector被表达成了，三个有层次的vectors了。&lt;br&gt;
Wilbur_中博(1954123) 22:11:11&lt;br&gt;
不是，就用最中间的那个隐层。你可以看成输入经过多次映射到达中间那个隐层，然后又经过多次映射恢复回先前的输入到达输出。&lt;br&gt;
快乐的孩纸(328509423) 22:12:09&lt;br&gt;
那就是autoencoder每次都是奇数层，然后得到中间的隐层，抽出来后用作regression来预测y？&lt;br&gt;
你是谁？(271182928) 22:12:42&lt;br&gt;
对，就保留隐含层。没想到这样对x重新表达后，再用regression来预测y会效果很好。那隐含层的x’向量，一般就假设是相互独立了吧。也就是开始的x里的correlation被抹去了。&lt;br&gt;
Wilbur_中博(1954123) 22:15:30&lt;br&gt;
对。比如说从100维输入到80维，再到60维，然后恢复的时候也是先到80维，再到100维。但其实我想有没有可能不必对称？比如从100到80到60，恢复的时候直接重构回100？我没有试过。我觉得可以试试。现在autoencoder的发展也蛮快的。但理论方面其实还差得很多，讲不出什么道理。效果确实还可以。&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;淘沙(271937125) 22:11:47&lt;br&gt;
能推荐 流形学习 的资料吗？&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;Wilbur_中博(1954123) 22:15:30&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;流形学习的资料蛮多的。里面思想和分支也很多。dodo的这篇文章非常好：http://blog.sina.com.cn/s/blog_4d92192101008end.html&lt;br&gt;
我觉得还是要抓流形学习的主要思想吧。而且流形学习比较好的一点就是，一般都会提供代码，我们可以从代码中学到很多。浙大的何晓飞和蔡登在这方面也很厉害，你可以去他们的主页看看。&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;初学者(75553395) 22:16:42&lt;br&gt;
弱问 这样的意思是之后的输入 只用隐含层就行了？&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;Wilbur_中博(1954123) 22:17:50&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;对，新来样本也用那个神经网络的权值映射到隐含层，然后用隐含层的输出作为分类器等传统方法的输入。&lt;br&gt;
不过其实还有各种技巧吧。。需要有监督的fine-tuning之类的。&lt;br&gt;
快乐的孩纸(328509423) 22:19:49&lt;br&gt;
autoencoder我不熟悉，我还有一个问题。 现在感觉和pca挺像的，就是learning的过程中只依赖原始intput：x，利用x内部的correlation和interaction来重新表达隐含层。 但是从直觉上看，在learning过程中，不仅考虑x的correlation，以及x与y的correlation，来最后得到x的新表达，理论上更利于y的预测精确度。比如PLS.&lt;br&gt;
Wilbur_中博(1954123) 22:24:35&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;你说的对。其实要考虑的东西有很多，比如x本身的结构，x和y的关系，如果有多任务的话那么y也是矩阵形式的，也可以研究y之间的结构和关系。还有我们从x提取出来的特征，特征之间其实也还有关系，因为提取特征也只是某一方面的假定而已。现在很火的CNN，如果在特征之间做一些工作，效果肯定还能更好。所以机器学习要研究的问题可以说无穷无尽。&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;快乐的孩纸(328509423) 22:26:03&lt;br&gt;
如果是多任务的话，那是multitask learning了。多个y之间也有correlation。 Jieping Ye那个组经常搞这个&lt;br&gt;
XXX&amp;lt;liyitan2144@163.com&amp;gt; 22:26:04&lt;br&gt;
@Wilbur 流型学习，近两年哪些方面比较火？&lt;br&gt;
Wilbur_中博(1954123) 22:26:21&lt;br&gt;
@快乐的孩纸 没错&lt;br&gt;
XXX&amp;lt;liyitan2144@163.com&amp;gt; 22:26:27&lt;br&gt;
Jieping Ye都快把multitask learning玩坏了&lt;br&gt;
快乐的孩纸(328509423) 22:26:42&lt;br&gt;
那公式推的一把一把的。@Wilbur 有没有autoencoder的好的源码framework啊？给我一个链接，我能try一下么？&lt;br&gt;
Wilbur_中博(1954123) 22:27:55&lt;br&gt;
@XXX 流形学习这两年基本沉寂了，火的时候已经过去了。不过还是非常实用的。&lt;br&gt;
XXX&amp;lt;liyitan2144@163.com&amp;gt; 22:30:04&lt;br&gt;
问一个肤浅的问题&lt;img src=&quot;/images/52nlp.cn/37d380945e3804c968093432f96eeae8.jpg&quot; alt=&quot;&quot;&gt;，现在火啥？（除了Deep learning）@Wilbur&lt;br&gt;
Wilbur_中博(1954123) 22:30:51&lt;br&gt;
@快乐的孩纸 你可以照着http://deeplearning.stanford.edu/wiki/index.php/Exercise:Sparse_Autoencoder 实现一下&lt;br&gt;
@XXX 我觉得hashing trick也挺火的，值得关注。另外组合优化相关的近似算法和快速算法也可以关注关注。&lt;br&gt;
XXX&amp;lt;liyitan2144@163.com&amp;gt; 22:33:04&lt;br&gt;
hashing trick 是指 learning to hash？组合优化相关的近似算法？这个哪里有？&lt;br&gt;
Wilbur_中博(1954123) 22:35:23&lt;br&gt;
倒不一定是learning to hash。简单的hash code也可以有不错的效果。比如google搞的WTA hashing和今年CVPR的best paper就是不错的hashing trick的文章。我个人不是太喜欢learning to hash。。所有以前有的连续情况的算法都搞成二元约束算法，有点恶心。&lt;br&gt;
组合优化方面的比如submodular optimization了，还有：&lt;br&gt;
Towards Efficient and Exact MAP-Inference for Large Scale Discrete Computer Vision Problems via Combinatorial Optimization&lt;br&gt;
我觉得做CV的人也值得关注。还有挺多的。。这是个大方向。&lt;br&gt;
XXX&amp;lt;liyitan2144@163.com&amp;gt; 22:39:33&lt;br&gt;
那 快速算法 又是指？难道是搞个收敛速率高的算法，再证个bound？&lt;br&gt;
Wilbur_中博(1954123) 22:45:26&lt;br&gt;
嗯，组合优化的技巧很多。。松弛到较大的解集、限制到较小的解集，看起来背道而驰但还都能用，就是分析起来思路不太一样。还有其他各种各样的思路。不过我其实也不是特别了解，就不妄谈了。。但机器学习或计算机视觉里的组合优化，肯定还是会有很大发展空间的。&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;注：PRML读书会系列文章由 &lt;a href=&quot;http://weibo.com/dmalgorithms&quot;&gt;@Nietzsche_复杂网络机器学习&lt;/a&gt; 同学授权发布，转载请注明原作者和相关的主讲人，谢谢。&lt;/p&gt;
&lt;p&gt;PRML读书会讲稿PDF版本以及更多资源下载地址：&lt;a href=&quot;http://vdisk.weibo.com/u/1841149974&quot;&gt;http://vdisk.weibo.com/u/1841149974&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;本文链接地址：&lt;a href=&quot;http://www.52nlp.cn/prml%E8%AF%BB%E4%B9%A6%E4%BC%9A%E7%AC%AC%E5%8D%81%E4%BA%8C%E7%AB%A0-continuous-latent-variables&quot;&gt;http://www.52nlp.cn/prml读书会第十二章-continuous-latent-variables&lt;/a&gt;&lt;/p&gt;

											

</description>
        <pubDate>Sat, 31 Jan 2015 00:00:00 +0800</pubDate>
        <link>http://iftti.com/posts/2015/2015-01-31-prml%25e8%25af%25bb%25e4%25b9%25a6%25e4%25bc%259a%25e7%25ac%25ac%25e5%258d%2581%25e4%25ba%258c%25e7%25ab%25a0-continuous-latent-variables-cad03f970.html</link>
        <guid isPermaLink="true">http://iftti.com/posts/2015/2015-01-31-prml%25e8%25af%25bb%25e4%25b9%25a6%25e4%25bc%259a%25e7%25ac%25ac%25e5%258d%2581%25e4%25ba%258c%25e7%25ab%25a0-continuous-latent-variables-cad03f970.html</guid>
        
        
        <category>52nlp</category>
        
      </item>
    
      <item>
        <title>PRML读书会第十三章 Sequential Data</title>
        <description>

						&lt;p style=&quot;text-align: center&quot;&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 15pt&quot;&gt;&lt;strong&gt;PRML读书会第十三章 Sequential Data&lt;br&gt;
&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p style=&quot;text-align: center&quot;&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 15pt&quot;&gt;&lt;strong&gt;主讲人 张巍&lt;br&gt;
&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p style=&quot;text-align: center&quot;&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;&lt;strong&gt;（新浪微博: &lt;a href=&quot;http://weibo.com/zh3f&quot;&gt;@张巍_ISCAS&lt;/a&gt;）&lt;br&gt;
&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;软件所-张巍&amp;lt;zh3f@qq.com&amp;gt; 19:01:27&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/b2d833e8575fcbb5b4f1ec58406c77b6.jpg&quot; alt=&quot;&quot;&gt;我们开始吧，十三章是关于序列数据，现实中很多数据是有前后关系的，例如语音或者DNA序列，例子就不多举了，对于这类数据我们很自然会想到用马尔科夫链来建模：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/2e5c659d66f02ae49b31b82411631439.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;例如直接假设观测数据之间服从一阶马尔科夫链，这个假设显然太简单了，因为很多数据时明显有高阶相关性的，一个解决方法是用高阶马尔科夫链建模：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/bd5dfce990fa9c0635b437cc6fdea541.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;但这样并不能完全解决问题 ：1、高阶马尔科夫模型参数太多；2、数据间的相关性仍然受阶数限制。一个好的解决方法，是引入一层隐变量，建立如下的模型：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/bfafcf38a00ed52d7690969f5d4c040a.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
&lt;/span&gt;&lt;span id=&quot;more-8012&quot;&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;这里我们假设隐变量之间服从一阶马尔科夫链，观测变量由其对应的隐变量生成。从上图可以看出，隐变量是一阶的，但是观测变量之间是全相关的，今天我们主要讨论的就是上图中的模型。如果隐变量是离散的，我们称之为Hidden Markov Models；如果是连续的，我们称之为: Linear Dynamical Systems。现在我们先来看一下HMM ，从图中可以看出，要完成建模，我们需要指定一下几个分布：&lt;br&gt;
1、转移概率：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/724c6c884c2aa14cbb92c0b680bcbc82.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
2、马尔科夫链的初始概率：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/2e57507f0cba5e24f26a8f49110e4e29.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
3、生成观测变量的概率(emission probabilities)：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/cad416f3063209bb94d0537f9da27f2b.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
对于HMM， 这里1和2我们已经假设成了离散分布，由隐变量Zn生成观测数据可以用混合高斯模型或者神经网络，书上的Zn是一个k维的布尔变量，由此再看隐变量转移概率公式、观测数据的生成公式就容易理解了。模型建好了，我们接下来主要讨论下面三个问题：&lt;br&gt;
1、学习问题：就是学习模型中的参数；&lt;br&gt;
2、预测问题：即&lt;img src=&quot;/images/52nlp.cn/bd3a3423f0fadb3cf704b61875848e91.jpg&quot; alt=&quot;&quot;&gt;,给定当前序列预测下一个观测变量；&lt;br&gt;
3、解码问题：即p(Z|X)，给定观测变量求隐变量，例如语音识别；&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;游侠(419504839) 19:24:21&lt;br&gt;
什么是解码问题？&lt;br&gt;
软件所-张巍&amp;lt;zh3f@qq.com&amp;gt; 19:25:18&lt;br&gt;
例如观测到了一段语音，要求识别其对应的句子。@游侠 我前面没怎么举例子，不知道这样说清楚没？&lt;br&gt;
游侠(419504839) 19:27:20&lt;br&gt;
这个和一般说的”推断”一样不&lt;br&gt;
软件所-张巍&amp;lt;zh3f@qq.com&amp;gt; 19:28:27&lt;br&gt;
这个也可以叫推断，只是推断是个比较一般的词汇。&lt;br&gt;
我们来看一下HMM有多少参数要学，对应于刚才说到的三个分布，我们也有三组参数要学。&lt;br&gt;
球猫(250992259) 19:30:46&lt;br&gt;
其实就是假设东西是一个马尔科夫模型生成的。。然后把参数用某种方法弄出来，最后根据模型的输出来给答案……是这样吧？&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;软件所-张巍&amp;lt;zh3f@qq.com&amp;gt; 19:32:45&lt;br&gt;
@球猫 对，都是这个思路，先把参数学出来，然后就可以做任何想要的推断了，在这里所谓的解码问题只是大家比较关心。&lt;br&gt;
软件所-张巍&amp;lt;zh3f@qq.com&amp;gt; 19:32:51&lt;br&gt;
好，继续，我们先来看1、学习问题。这里我们用EM算法来学习HMM的参数：&lt;br&gt;
1、是转移概率对应的转移矩阵；&lt;br&gt;
2、初始概率对应的离散分布参数；&lt;br&gt;
3、观测变量对应的分布参数（这里暂不指定）。&lt;br&gt;
用EM我们要做的就是：&lt;br&gt;
E步里根据当前参数估计隐变量的后验：&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/images/52nlp.cn/736a2e2a12b64dc79340bae7a36e93b2.jpg&quot; alt=&quot;&quot;&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;&lt;br&gt;
M步里最大化下面的期望：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/edc13aa9704330a6b35cee1c45f8dfac.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
先来看M步，这里相对简单一点，整个模型的全概率展开为：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/49a48650a8c699e258264e3e1fc9f41f.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
把13.10代入13.12,我们会发现计算时需要下面两个式子：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/2cc983dfd30f3a84fd820ef3cc01e2ac.jpg&quot; alt=&quot;&quot;&gt;和&lt;img src=&quot;/images/52nlp.cn/655a0b47e54ec5ba75b34946b2a52566.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
为了方便，我们就定义：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/2ffb982cf3a17d0f6bb3a0c45d7eadbe.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
这样我们在E步就主要求出这两个式子就行了，当然这也就意味着求出了整个后验&lt;img src=&quot;/images/52nlp.cn/fa5dff7cd3c680b2ba3955977f2eb213.jpg&quot; alt=&quot;&quot;&gt;，利用这两个式子，13.12可以化为：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/30e09c02fac5b09c9474893e0e923f4f.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
这个时候就可以用一些通用方法，例如Lagrange来求解了，结果也很简单：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/25d0a9dd4c0635c6729c565a2c1ede6a.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
对于观测变量的分布参数，与其具体分布形式相关，如果是高斯：&lt;img src=&quot;/images/52nlp.cn/2e3bcd1fff2e331832a6a8ff2025cdb9.jpg&quot; alt=&quot;&quot;&gt;，对应的最优解为：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/d70bf206853be75c5a78a7ecfd950521.jpg&quot; alt=&quot;&quot;&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;如果是离散：&lt;img src=&quot;/images/52nlp.cn/abc79c3994b7f946fc5cdd08971202eb.jpg&quot; alt=&quot;&quot;&gt;对应的最优解为：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/0688d4a65e4c1980384f99fb062fd6da.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
好，M步就这样， 现在来看E步，也是HMM比较核心的地方。刚才我们看到，E步需要求的是：&lt;img src=&quot;/images/52nlp.cn/98b7d691808cb81f5d285f813cf737f9.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
由马尔科夫的性质，我们可以推出：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/861783585412686f6dd662e1623f6fd2.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;其中：&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/images/52nlp.cn/2d4f6a26ff07fbf58b31f1030a0a93f7.jpg&quot; alt=&quot;&quot;&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;&lt;br&gt;
接下来我们就建立&lt;img src=&quot;/images/52nlp.cn/2f6b06a9c1f4c46ffc7927fc97bed61f.jpg&quot; alt=&quot;&quot;&gt;和&lt;img src=&quot;/images/52nlp.cn/a6665b323030957ae42dd9831458c5a6.jpg&quot; alt=&quot;&quot;&gt;的递推公式&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/745f4de9d42f2d7a7614952d11d0abe3.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
其中：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/87c6ad87d323a62678deb0dcde410129.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
这样我们从&lt;img src=&quot;/images/52nlp.cn/48480bd6d6f051edcadeac0e151ee3fd.jpg&quot; alt=&quot;&quot;&gt;开始，可以递推出所有的&lt;img src=&quot;/images/52nlp.cn/c9e0ebb93282120bdfe5c3621cbfabf7.jpg&quot; alt=&quot;&quot;&gt;，对于&lt;img src=&quot;/images/52nlp.cn/72582463aa548d1997b0c374ff19b0ca.jpg&quot; alt=&quot;&quot;&gt;，也进行类似的推导：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/f5cf1a95b03270463b50d95197da6576.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/images/52nlp.cn/d45a4ae8080f65a83944c0916cfcc763.jpg&quot; alt=&quot;&quot;&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;&lt;br&gt;
从上式可以看到&lt;img src=&quot;/images/52nlp.cn/928a4ff5354d175201f0199c8d1a8ad6.jpg&quot; alt=&quot;&quot;&gt;是一个逆推过程，所以我们需要初始值&lt;img src=&quot;/images/52nlp.cn/c822f0a47b1d789a0ad6f9c91d85681a.jpg&quot; alt=&quot;&quot;&gt;，定义13.35并没有明确&lt;img src=&quot;/images/52nlp.cn/c246542550e79656ad3e3126a2b0ae73.jpg&quot; alt=&quot;&quot;&gt;的定义：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/38658037963f1aa9adcef132498ba581.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
因为z_N后没有观测数据，不过我们可以从&lt;img src=&quot;/images/52nlp.cn/d57ab1a808c48c51dd22321d8dcfd55e.jpg&quot; alt=&quot;&quot;&gt;，得出:&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/58f44a00f48d59b728995fda22b9fcd6.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
这样&lt;img src=&quot;/images/52nlp.cn/c75f6ae6ac230447236788b2309bd3f5.jpg&quot; alt=&quot;&quot;&gt;就等于1，现在我们可以方便的求出所有的&lt;img src=&quot;/images/52nlp.cn/6a4a993e1090f69f4e3ceed436971b2a.jpg&quot; alt=&quot;&quot;&gt;和&lt;img src=&quot;/images/52nlp.cn/4de686ea532dc33aa70d2783f9978db8.jpg&quot; alt=&quot;&quot;&gt;了，利用13.13也就可以求出所有的&lt;img src=&quot;/images/52nlp.cn/f6a0e3e535c29cf0b07f88793d188a62.jpg&quot; alt=&quot;&quot;&gt;。类似的，我们可以求出&lt;img src=&quot;/images/52nlp.cn/8b9c67e0d48e355537f650bd7a80ec11.jpg&quot; alt=&quot;&quot;&gt;：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/6cff3d49ac9484aeb5eeef2bd4be2e67.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
这样在M步里求解所需要的分布就都求出来了，也就可以用EM来学习HMM的参数了，这里式子比较多，大家自己推一下会比较好理解。&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;第一个学习问题就这样了，接下来是预测问题，预测问题可以直接推导：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/eca6ce061f900bfec03e84261f76aaa7.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
现在就剩最后一个解码问题，也就是argmax_Z{p(Z|X)}，刚才我们在E步已经求出了：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/cf9adba69ca23e34bf5cac068fa17a22.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
但是现在的问题要复杂一点，因为我们要求概率最大的隐变量序列，用13.13可以求出单个隐变量，但是他们连在一起形成的整个序列可能概率很小，这个问题可以归结为一个动态规划：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/06e01ca9d2702eeb138a498396f80666.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;我们把HMM化成如上图的样子，最大化后验等价于最大化全概率，对于上图中的边，我们赋值为：&lt;br&gt;
log（&lt;img src=&quot;/images/52nlp.cn/de3fdc919b228a823017f3f142235fe1.jpg&quot; alt=&quot;&quot;&gt;）&lt;br&gt;
初始节点赋值为：&lt;br&gt;
log(&lt;img src=&quot;/images/52nlp.cn/e5271c8f4eab4d4457c02faad2afe4ef.jpg&quot; alt=&quot;&quot;&gt;*p(x_1|z_1)）&lt;br&gt;
其余节点赋值为：&lt;br&gt;
log(&lt;img src=&quot;/images/52nlp.cn/2e47bc456f485948b0ad7a1ea2944c80.jpg&quot; alt=&quot;&quot;&gt;)&lt;br&gt;
这样任何一个序列Z，其全概率等于exp（Z对应路径上节点和边的值求和），这样，解码问题就转化为一个最长路径问题，用动态规划可以直接求解。大家看这里有没有问题，HMM的主要内容就是这些&lt;img src=&quot;/images/52nlp.cn/c13ee5623f49d3491b6f040561f82f15.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;接下来的Linear Dynamical Systems 其实和HMM大同小异，只是把离散分布换成了高斯，然后就是第二章公式的反复应用，都是细节问题，就不在这里讲了，大家看看有问题我们可以讨论。这一章还是式子主导的，略过了不少式子，大家推的时候有问题我们可以随时讨论。&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;天涯游(872352128) 21:09:21&lt;br&gt;
我对hmm 的理解，觉得这麻烦的是概率的理解的了，概率分解才是hmm的核心，当然了还有动态规划了。概率分解其实是实验事件的分解，如前向 和后向了，还有就是EM算法了。&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;注：PRML读书会系列文章由 &lt;a href=&quot;http://weibo.com/dmalgorithms&quot;&gt;@Nietzsche_复杂网络机器学习&lt;/a&gt; 同学授权发布，转载请注明原作者和相关的主讲人，谢谢。&lt;/p&gt;
&lt;p&gt;PRML读书会讲稿PDF版本以及更多资源下载地址：&lt;a href=&quot;http://vdisk.weibo.com/u/1841149974&quot;&gt;http://vdisk.weibo.com/u/1841149974&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;本文链接地址：&lt;a href=&quot;prml%E8%AF%BB%E4%B9%A6%E4%BC%9A%E7%AC%AC%E5%8D%81%E4%B8%89%E7%AB%A0sequential-data&quot;&gt;http://www.52nlp.cn/prml读书会第十三章sequential-data&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;

											

</description>
        <pubDate>Sat, 31 Jan 2015 00:00:00 +0800</pubDate>
        <link>http://iftti.com/posts/2015/2015-01-31-prml%25e8%25af%25bb%25e4%25b9%25a6%25e4%25bc%259a%25e7%25ac%25ac%25e5%258d%2581%25e4%25b8%2589%25e7%25ab%25a0sequential-data-c6348f8fd.html</link>
        <guid isPermaLink="true">http://iftti.com/posts/2015/2015-01-31-prml%25e8%25af%25bb%25e4%25b9%25a6%25e4%25bc%259a%25e7%25ac%25ac%25e5%258d%2581%25e4%25b8%2589%25e7%25ab%25a0sequential-data-c6348f8fd.html</guid>
        
        
        <category>52nlp</category>
        
      </item>
    
      <item>
        <title>PRML读书会第十一章  Sampling Methods</title>
        <description>

						&lt;p style=&quot;text-align: center&quot;&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 15pt&quot;&gt;&lt;strong&gt;PRML读书会第十一章 Sampling Methods&lt;br&gt;
&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p style=&quot;text-align: center&quot;&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 15pt&quot;&gt;&lt;strong&gt;主讲人 网络上的尼采&lt;br&gt;
&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p style=&quot;text-align: center&quot;&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;&lt;strong&gt;（新浪微博: &lt;a href=&quot;http://weibo.com/dmalgorithms&quot;&gt;@Nietzsche_复杂网络机器学习&lt;/a&gt;）&lt;br&gt;
&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;网络上的尼采(813394698) 9:05:00&lt;br&gt;
今天的主要内容：Markov Chain Monte Carlo，Metropolis-Hastings，Gibbs Sampling，Slice Sampling，Hybrid Monte Carlo。&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;上一章讲到的平均场是统计物理学中常用的一种思想，将无法处理的复杂多体问题分解成可以处理的单体问题来近似，变分推断便是在平均场的假设约束下求泛函L(Q)极值的最优化问题，好处在于求解过程中可以推出精致的解析解。变分是从最优化的角度通过坐标上升法收敛到局部最优，这一章我们将通过计算从动力学角度见证Markov Chain Monte Carlo收敛到平稳分布。&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-size: 12pt&quot;&gt;&lt;span style=&quot;font-family: 微软雅黑&quot;&gt;先说sampling的原因，因为统计学中经常会遇到对复杂的分布做加和与积分，这往往是intractable的。MCMC方法出现后贝叶斯方法才得以发展，因为在那之前对不可观测变量（包括隐变量和参数）后验分布积分非常困难，对于这个问题上一章变分用的解决办法是通过最优化方法寻找一个和不可观测变量后验分布p(Z|X)近似的分布，这一章我们看下sampling的解决方法，举个简单的例子：比如我们遇到这种形式&lt;img src=&quot;/images/52nlp.cn/403848d79299f16681f756c183f294e8.jpg&quot; alt=&quot;&quot;&gt;，z是个连续随机变量，p(z)是它的分布，我们求f(z)的期望。如果我们从p(z)中sampling一个数据集z&lt;sup&gt;(l)&lt;/sup&gt;，然后再求个平均&lt;img src=&quot;/images/52nlp.cn/40bc5237d7d56eb35313e29f1918be21.jpg&quot; alt=&quot;&quot;&gt;来近似f(z)的期望&lt;/span&gt;&lt;span style=&quot;font-family: 宋体&quot;&gt;，&lt;/span&gt;&lt;span style=&quot;font-family: 微软雅黑&quot;&gt;so,问题就解决了，关键是如何从p(z)中做无偏的sampling。&lt;br&gt;
为了说明sampling的作用，我们先举个EM的例子，最大似然计算中求分布的积分问题，我们在第九章提到了，完整数据的log似然函数是对隐变量Z的积分：&lt;br&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span id=&quot;more-7819&quot;&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/images/52nlp.cn/2befd35915d9d7a1851998c8f0d678f1.jpg&quot; alt=&quot;&quot;&gt;&lt;span style=&quot;font-family: 宋体;font-size: 12pt&quot;&gt;&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;如果Z是比较复杂的分布，我们就需要对Z进行采样，从而得到：&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/images/52nlp.cn/aab3f5484f4a783a42609d85beaa65da.jpg&quot; alt=&quot;&quot;&gt;&lt;span style=&quot;font-family: 宋体;font-size: 12pt&quot;&gt;&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-size: 12pt&quot;&gt;&lt;span style=&quot;font-family: 微软雅黑&quot;&gt;具体就是从Z的后验分布&lt;img src=&quot;/images/52nlp.cn/b0d8daa13a9ec6dd954900ecbab087f0.jpg&quot; alt=&quot;&quot;&gt;中进行采样。&lt;/span&gt;&lt;span style=&quot;font-family: 宋体&quot;&gt;&lt;br&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-size: 12pt&quot;&gt;&lt;span style=&quot;font-family: 微软雅黑&quot;&gt;如果我们从贝叶斯的观点，把EM参数theta也当成一个分布的话，有下面一个IP算法:&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/5bb6a4be5a345d2c28bce43d999b282a.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
I步，我们无法直接对P(Z|X)取样，我们可以先对P(theta|X)取样&lt;img src=&quot;/images/52nlp.cn/862e08e601f43c7d6dc5e98249b1d214.jpg&quot; alt=&quot;&quot;&gt;，然后再对Z的后验分布进行取样：&lt;img src=&quot;/images/52nlp.cn/e2d7356be52b9da625381a8771583b23.jpg&quot; alt=&quot;&quot;&gt;&lt;/span&gt;&lt;span style=&quot;font-family: 宋体&quot;&gt;&lt;br&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-size: 12pt&quot;&gt;&lt;span style=&quot;font-family: 微软雅黑&quot;&gt;P步，利用上一步对P(Z|X)的取样，来确定新的参数分布：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/1127c38c07bef5ee109bb58cb1afbc96.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
然后按这个I步和P步的方式迭代。&lt;/span&gt;&lt;span style=&quot;font-family: 宋体&quot;&gt;&lt;br&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;接下来我们讲sampling methods，为了节省时间，两个基本的rejection sampling和Importance sampling不讲了，这两种方法在高维下都会失效，我们直奔主题MCMC（Markov Chain Monte Carlo ）。蒙特卡洛，是个地中海海滨城市，气候宜人，欧洲富人们的聚集地，更重要的是它是世界三大赌城之一，用这个命名就知道这种方法是基于随机的，过会我们会讲到。马尔科夫大神就不用多说了，他当时用自己的名字命名马尔科夫链是预见到这个模型巨大作用。他还有一个师弟叫李雅普洛夫，控制论里面的李雅普洛夫函数说的就是这位，他们的老师叫契比雪夫，都是圣彼得堡学派的。俄国数学家对人类的贡献是无价的orz&lt;br&gt;
最早的MCMC方法是美国科学家在研制原子弹时算积分发明的。我们先介绍一个最基本的Metropolis方法，这种方法的接受率是：&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/images/52nlp.cn/19827aa245ca32df88df427f17ef2a8a.jpg&quot; alt=&quot;&quot;&gt;&lt;span style=&quot;font-family: 宋体;font-size: 12pt&quot;&gt;&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-size: 12pt&quot;&gt;&lt;span style=&quot;font-family: 微软雅黑&quot;&gt;但有个要求，就是proposal distribution满足&lt;img src=&quot;/images/52nlp.cn/5335f7eddc1bfc94459525f21a64cccd.jpg&quot; alt=&quot;&quot;&gt;&lt;/span&gt;&lt;span style=&quot;font-family: 宋体&quot;&gt;。&lt;/span&gt;&lt;span style=&quot;font-family: 微软雅黑&quot;&gt;过程很简单，我们先找个比较容易采样的分布即proposal分布，然后从这个分布中取一个样本Z*，如果&lt;img src=&quot;/images/52nlp.cn/bebb478dcdf6559527166d7d2eeae9a7.jpg&quot; alt=&quot;&quot;&gt;大于1直接接受，如果小于1就接着算出接受率，并且从（0，1）之间取一个随机数和这个接受率做比较来决定是否接受这个样本。过会会在Metropolis-Hastings algorithm方法中具体说。下图是一个简单的例子，对高斯分布做采样，绿线是表示接受的步骤，红线表示拒绝的：&lt;/span&gt;&lt;span style=&quot;font-family: 宋体&quot;&gt;&lt;br&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/images/52nlp.cn/be6f1a0e3a7113b99b62a610cae45fc8.jpg&quot; alt=&quot;&quot;&gt;&lt;span style=&quot;font-family: 宋体;font-size: 12pt&quot;&gt;&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-size: 12pt&quot;&gt;&lt;span style=&quot;font-family: 微软雅黑&quot;&gt;讲Metropolis-Hastings方法前，我们先来回顾下马尔科夫链的性质，这个很重要。markov chains最基本的性质就是无后效性，就是这条链的下一个节点的状态由当前节点状态完全决定：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/2694fdfd3436300d514911d13a803268.jpg&quot; alt=&quot;&quot;&gt;&lt;/span&gt;&lt;span style=&quot;font-family: 宋体&quot;&gt;&lt;br&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-size: 12pt&quot;&gt;&lt;span style=&quot;font-family: 微软雅黑&quot;&gt;特定的齐次马尔科夫链可以收敛到平稳分布，也就是经过相当长的一段时间转移，收敛到的分布和初始值无关，转移核起着决定的作用。关于马尔科夫链的收敛，&lt;strong&gt;我们将介绍一个充分条件：detailed balance细致平稳条件。&lt;/strong&gt;&lt;br&gt;
先介绍两个公式，马尔科夫链节点状态的marginal distribution计算公式，由于无后效性我们可以得到&lt;img src=&quot;/images/52nlp.cn/1d6a27c28ab04731de7c087ee837c6f4.jpg&quot; alt=&quot;&quot;&gt;&lt;/span&gt;&lt;span style=&quot;font-family: 宋体&quot;&gt;&lt;br&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-size: 12pt&quot;&gt;&lt;span style=&quot;font-family: 微软雅黑&quot;&gt;上面公式的加和结合马儿科夫链的状态转移矩阵是比较容易理解。&lt;/span&gt;&lt;span style=&quot;font-family: 宋体&quot;&gt;&lt;br&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;平稳分布的定义就是下面的形式：&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/images/52nlp.cn/d78b12f5959dfd4faa56b688d0355c16.jpg&quot; alt=&quot;&quot;&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;其中&lt;img src=&quot;/images/52nlp.cn/13ef8b65acbb6c5782d1a38d05ba22c8.jpg&quot; alt=&quot;&quot;&gt;是z’到z的转移概率，上面的公式不难理解，就是转移后分布不再发生变化。&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-size: 12pt&quot;&gt;&lt;span style=&quot;font-family: 微软雅黑&quot;&gt;下面我们给出细致平稳条件的公式：&lt;img src=&quot;/images/52nlp.cn/3e73a3f73a4ae00c4525c56c919ea969.jpg&quot; alt=&quot;&quot;&gt;&lt;/span&gt;&lt;span style=&quot;font-family: 宋体&quot;&gt;&lt;br&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-size: 12pt&quot;&gt;&lt;span style=&quot;font-family: 微软雅黑&quot;&gt;满足细致平稳条件就能收敛到平稳分布，下面是推导：&lt;img src=&quot;/images/52nlp.cn/c5c385f4dadcde1a5c514aac42d46626.jpg&quot; alt=&quot;&quot;&gt;&lt;/span&gt;&lt;span style=&quot;font-family: 宋体&quot;&gt;&lt;br&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;我们把上面细致平稳条件的公式11.40代入公式11.41最左边，从左朝右推导就是平稳分布的公式11.39。细致平稳条件的好处，就是我们能控制马尔科夫链收敛到我们指定的分布&lt;img src=&quot;/images/52nlp.cn/745780e669e082ab95e5743434069663.jpg&quot; alt=&quot;&quot;&gt;。以后的Metropolis-Hastings方法及改进都是基于这个基础的。&lt;br&gt;
前面我们提到，Metropolis方法需要先选一个比较容易取样的proposal distribution，从这个分布里取样，然后通过接受率决定是否采用这个样本。一个简单的例子就是对于proposal distribution我们可以采用Gaussian centred on the current state，其实很好理解，就是上一步节点的值可以做下一步节点需要采样的proposal distribution即高斯分布的均值，这样下一步节点的状态由上一步完全决定，这就是一个马尔科夫链。马尔科夫链有了，我们怎么保证能收敛到目标分布呢？就是前面说的细致平稳条件，我们可以通过设置接受率的形式来满足这个条件。Metropolis-Hastings接受率的形式：&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/images/52nlp.cn/46700a50239af108971dd994f4bc8ce7.jpg&quot; alt=&quot;&quot;&gt;&lt;span style=&quot;font-family: 宋体;font-size: 12pt&quot;&gt;&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/images/52nlp.cn/a542e621f3bf2e87bfcb62f8a649867e.jpg&quot; alt=&quot;&quot;&gt;&lt;span style=&quot;font-size: 12pt&quot;&gt;&lt;span style=&quot;font-family: 微软雅黑&quot;&gt;来自于&lt;img src=&quot;/images/52nlp.cn/fbb5026f786fdd4a8f69ca775ae7cb31.jpg&quot; alt=&quot;&quot;&gt;，分布q便是proposal distribution。&lt;br&gt;
范涛@推荐系统(289765648) 10:49:15&lt;br&gt;
Zp 是什么？&lt;/span&gt;&lt;span style=&quot;font-family: 宋体&quot;&gt;&lt;br&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;网络上的尼采(813394698) 10:52:04&lt;br&gt;
Zp是分布中和z无关的部分。&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-size: 12pt&quot;&gt;&lt;span style=&quot;font-family: 微软雅黑&quot;&gt;为了使各位有个形象的理解，我描述一下过程，我们把&lt;img src=&quot;/images/52nlp.cn/123d877f6137540dea4f50c8e35c4ef3.jpg&quot; alt=&quot;&quot;&gt;当做高斯分布的均值，方差是固定的。然后从这个分布取一个样本就是&lt;img src=&quot;/images/52nlp.cn/1dbeae49b015e112fa68aaa8394dca06.jpg&quot; alt=&quot;&quot;&gt;，如果&lt;img src=&quot;/images/52nlp.cn/92f62a05c5a9d50f29284ea484573598.jpg&quot; alt=&quot;&quot;&gt;大于1肯定接受，如果小于1，我们便从（0，1）之间取一个随机数，和这个接受率做比较，如果接受率大于这个随机数便接受，反之便拒绝。接受率这么设就能满足细致平稳条件的原因，看这个（11.45）公式：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/e8a50f727e2d016e0ec9241fbdb6f15f.jpg&quot; alt=&quot;&quot;&gt;&lt;/span&gt;&lt;span style=&quot;font-family: 宋体&quot;&gt;&lt;br&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/images/52nlp.cn/1bd403dbc35da9c2fb9fc0eeedeb503f.jpg&quot; alt=&quot;&quot;&gt;&lt;span style=&quot;font-family: 宋体;font-size: 12pt&quot;&gt;&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;我们把接受率公式11.44代入上面的公式的左边，会推出左右两边就是细致平稳条件的形式，红框部分便是细致平稳条件公式11.40的转移核，书上的公式明显错了，上面的这个是勘误过的。&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;刚才说了proposal distribution一般采用Gaussian centred on the current state，高斯分布的方差是固定的，其实方差就是步长，如何选择步长这是一个state of the art问题，步子太小扩散太慢，步子太大，拒绝率会很高，原地踏步。书中的一个例子，当用Gaussian centred on the current state作proposal distribution时，步长设为目标高斯分布的最小标准差最合适：&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/images/52nlp.cn/d61e7cec7d48d02a32bd1649a06bb723.jpg&quot; alt=&quot;&quot;&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;&lt;br&gt;
下面讲Gibbs Sampling，Gibbs Sampling其实是每次只对一个维度的变量进行采样，固定住其他维度的变量，然后迭代，可以看做是Metropolis-Hastings的特例，它的接受率一直是1.&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/afb3bb9efe55fc209e8d6c2bd3bb67d7.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
步骤是比较容易理解的，跟上一章的变分法的有相似之处。假设有三个变量的分布&lt;img src=&quot;/images/52nlp.cn/39d35252c7d5101688c653dc77a5bc6b.jpg&quot; alt=&quot;&quot;&gt;，&lt;br&gt;
先固定住z2 z3对z1进行采样，&lt;img src=&quot;/images/52nlp.cn/fd85725b2c7551c38fae8b4d049590d8.jpg&quot; alt=&quot;&quot;&gt;；&lt;br&gt;
然后固定住z1 z3对z2进行采样，&lt;img src=&quot;/images/52nlp.cn/6fa7d4ba80f631b03b0ba44af688a1b4.jpg&quot; alt=&quot;&quot;&gt;；&lt;br&gt;
然后是Z3，&lt;img src=&quot;/images/52nlp.cn/43f041743feebf9f9fb85fbda0f7f055.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
如此迭代。&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-size: 12pt&quot;&gt;&lt;span style=&quot;font-family: 微软雅黑&quot;&gt;根据Metropolis-Hastings，它的接受率恒为1。看下面的推导：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/5cbe9b3a26b7f54360235cc295972e53.jpg&quot; alt=&quot;&quot;&gt;&lt;/span&gt;&lt;span style=&quot;font-family: 宋体&quot;&gt;&lt;br&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-size: 12pt&quot;&gt;&lt;span style=&quot;font-family: 微软雅黑&quot;&gt;因为其他维度是固定不变的，所以&lt;img src=&quot;/images/52nlp.cn/6866031857945261a263ec440bcda57b.jpg&quot; alt=&quot;&quot;&gt;，代入上式就都约去了，等于1.&lt;br&gt;
最后对于图模型采用gibbs sampling，条件概率&lt;img src=&quot;/images/52nlp.cn/f2dffc8b35ae69f3ec91ff391feebe57.jpg&quot; alt=&quot;&quot;&gt;可以根据马尔科夫毯获得，下面一个是无向图，一个是有向图，蓝色的节点是和要采样的变量有关的其他变量：&lt;/span&gt;&lt;span style=&quot;font-family: 宋体&quot;&gt;&lt;br&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/images/52nlp.cn/b41a8dcffb6a465a9daddaf99f1e9915.jpg&quot; alt=&quot;&quot;&gt;&lt;span style=&quot;font-family: 宋体;font-size: 12pt&quot;&gt;&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;关于更多的gibbs sampling的内容可以看MLAPP，里面有blocked gibbs和collapsed gibbs。&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;刚才提到Metropolis-Hastings对步长敏感，针对这个问题，下面介绍两个增加辅助变量的方法，这些方法也是满足细致平稳条件的。先介绍slice sampling，这种方法增加了一个变量U，可以根据分布的特征自动调整步长： &lt;img src=&quot;/images/52nlp.cn/fea441012b2279b7a7add118c00680ce.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;步骤很简单：在&lt;img src=&quot;/images/52nlp.cn/fa1a0f9f6a42445b34ff92d298222341.jpg&quot; alt=&quot;&quot;&gt;与&lt;img src=&quot;/images/52nlp.cn/e76e926fcdd5a41e89e1b5e5717eebba.jpg&quot; alt=&quot;&quot;&gt;之间的这段距离随机取个值U，然后通过U画个横线，然后在包含&lt;img src=&quot;/images/52nlp.cn/eb3c00a7a887dd30d35a1c89136c74b1.jpg&quot; alt=&quot;&quot;&gt;并且&lt;img src=&quot;/images/52nlp.cn/f9126f7a0e11d7840df4fe185614473e.jpg&quot; alt=&quot;&quot;&gt;这段横线对z进行随机采样，然后按这种方式迭代。图(b)为了实际中便于操作，有时还需要多出那么一段，因为我们事先不知道目标分布的具体形式，所以包含&lt;img src=&quot;/images/52nlp.cn/6d2aa7a1adb367e291d45b5bc73f48ad.jpg&quot; alt=&quot;&quot;&gt;并且&lt;img src=&quot;/images/52nlp.cn/f360c1132f5201b2bdf507123c2569ae.jpg&quot; alt=&quot;&quot;&gt;这段横线没法确定，只能朝外延伸加单位长度进行试，最后会多出来一段，这一点书上并没有介绍详细。&lt;br&gt;
下面介绍The Hybrid Monte Carlo Algorithm （Hamiltonian MCMC）：哈密顿，神童，经典力学三巨头之一，这个算法引入了哈密顿动力系统的概念，计算接受率时考虑的是系统的总能量。Hybrid Monte Carlo 定义了势能和动能两种能量，它们的和便是系统总能量哈密顿量。先看势能，分布可以写成这种形式：&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/images/52nlp.cn/2b92099fb5b39c580dead8c51756c99e.jpg&quot; alt=&quot;&quot;&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;&lt;br&gt;
E(z)便是系统的势能。&lt;br&gt;
另外增加一个变量，状态变量变化的速率：&lt;img src=&quot;/images/52nlp.cn/814297d12db74fb385879441cbe84e94.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
系统的动能便是：&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/images/52nlp.cn/d137edac3fa62e5cc47b709b150f8977.jpg&quot; alt=&quot;&quot;&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;&lt;br&gt;
总的能量便是：&lt;img src=&quot;/images/52nlp.cn/73826eb9c507562224a31264b60af43b.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;比如高斯分布的哈密顿量就可表示为：&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/images/52nlp.cn/7253cc1668ee4f62803c9cde7ce41132.jpg&quot; alt=&quot;&quot;&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;下面这个公式便是Hybrid Monte Carlo 的接受率：&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/images/52nlp.cn/55a81f9a2ddd8ff7532cf8da0ef0970e.jpg&quot; alt=&quot;&quot;&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;&lt;br&gt;
可以证明 这种接受率是满足detailed balance条件的。&lt;br&gt;
ORC(267270520) 12:14:03&lt;br&gt;
推荐一本相关的书 Introducing Monte Carlo Methods with R (use R) ，PS：R做MCMC很方便。&lt;br&gt;
赞尼采讲的很精彩，学习了，嘿嘿&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;网络上的尼采(813394698) 12:36:37&lt;br&gt;
Markov Chain Monte Carlo In Practice(Gilks)这本书也挺不错。&lt;br&gt;
红烧鱼(403774317) 12:38:06&lt;br&gt;
这本读研的时候生读过，非常实用，随书附带code&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;网络上的尼采(813394698)&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;最后需要补充的是：判断MCMC的burn-in何时收敛是个问题，koller介绍了两种方法，即同一条链上设置不同的时间窗做比较，另一种同时跑多条链然后作比较。当然也有一条链跑到黑的。&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;注：PRML读书会系列文章由 &lt;a href=&quot;http://weibo.com/dmalgorithms&quot;&gt;@Nietzsche_复杂网络机器学习&lt;/a&gt; 同学授权发布，转载请注明原作者和相关的主讲人，谢谢。&lt;/p&gt;
&lt;p&gt;PRML读书会讲稿PDF版本以及更多资源下载地址：&lt;a href=&quot;http://vdisk.weibo.com/u/1841149974&quot;&gt;http://vdisk.weibo.com/u/1841149974&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;本文链接地址：&lt;a href=&quot;http://www.52nlp.cn/prml%E8%AF%BB%E4%B9%A6%E4%BC%9A%E7%AC%AC%E5%8D%81%E4%B8%80%E7%AB%A0-sampling-methods&quot;&gt;http://www.52nlp.cn/prml读书会第十一章-sampling-methods&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 9pt&quot;&gt;&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;

											

</description>
        <pubDate>Sat, 31 Jan 2015 00:00:00 +0800</pubDate>
        <link>http://iftti.com/posts/2015/2015-01-31-prml%25e8%25af%25bb%25e4%25b9%25a6%25e4%25bc%259a%25e7%25ac%25ac%25e5%258d%2581%25e4%25b8%2580%25e7%25ab%25a0-sampling-methods-ed6b661c1.html</link>
        <guid isPermaLink="true">http://iftti.com/posts/2015/2015-01-31-prml%25e8%25af%25bb%25e4%25b9%25a6%25e4%25bc%259a%25e7%25ac%25ac%25e5%258d%2581%25e4%25b8%2580%25e7%25ab%25a0-sampling-methods-ed6b661c1.html</guid>
        
        
        <category>52nlp</category>
        
      </item>
    
      <item>
        <title>PRML读书会第八章  Graphical Models</title>
        <description>

						&lt;p style=&quot;text-align: center;background: white&quot;&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 15pt&quot;&gt;&lt;strong&gt;PRML读书会&lt;span style=&quot;color: black&quot;&gt;第八章 Graphical Models&lt;br&gt;
&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p style=&quot;text-align: center;background: white&quot;&gt;&lt;span style=&quot;color: black;font-family: 微软雅黑;font-size: 15pt&quot;&gt;&lt;strong&gt;主讲人 网神&lt;br&gt;
&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p style=&quot;text-align: center;background: white&quot;&gt;&lt;span style=&quot;color: black;font-family: 微软雅黑;font-size: 12pt&quot;&gt;&lt;strong&gt;（新浪微博: &lt;a href=&quot;http://weibo.com/ghtimaq&quot;&gt;@豆角茄子麻酱凉面&lt;/a&gt;）&lt;br&gt;
&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p style=&quot;background: white&quot;&gt;&lt;span style=&quot;color: black;font-family: 微软雅黑;font-size: 9pt&quot;&gt;网神(66707180) 18:52:10&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p style=&quot;background: white&quot;&gt;&lt;span style=&quot;color: black;font-family: 微软雅黑;font-size: 9pt&quot;&gt;今天的内容主要是：&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p style=&quot;background: white&quot;&gt;&lt;span style=&quot;color: black;font-family: 微软雅黑;font-size: 9pt&quot;&gt;1.贝叶斯网络和马尔科夫随机场的概念，联合概率分解，条件独立表示；2.图的概率推断inference。&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p style=&quot;background: white&quot;&gt;&lt;span style=&quot;color: black;font-family: 微软雅黑;font-size: 9pt&quot;&gt;图模型是用图的方式表示概率推理 ，将概率模型可视化，方便展示变量之间的关系，概率图分为有向图和无向图。有向图主要是贝叶斯网络，无向图主要是马尔科夫随机场。对两类图，prml都讲了如何将联合概率分解为条件概率，以及如何表示和判断条件依赖。&lt;br&gt;
先说贝叶斯网络，贝叶斯网络是有向图，用节点表示随机变量，用箭头表示变量之间的依赖关系。一个例子:&lt;br&gt;
&lt;/span&gt;&lt;span id=&quot;more-7603&quot;&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p style=&quot;background: white&quot;&gt;&lt;img src=&quot;/images/52nlp.cn/711dd31ad211f0e208691384f21bebbe.jpg&quot; alt=&quot;&quot;&gt;&lt;span style=&quot;color: black;font-family: 微软雅黑;font-size: 9pt&quot;&gt;&lt;br&gt;
这是一个有向无环图，这个图表示的概率模型如下:&lt;br&gt;
p(x1,x2,…x7)= &lt;img src=&quot;/images/52nlp.cn/240009b30917c75e6e8dea72da7c15f8.jpg&quot; alt=&quot;&quot;&gt;形式化一下，贝叶斯网络表示的联合分布是：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/74f8f5620ef90e699e0342640893699b.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p style=&quot;background: white&quot;&gt;&lt;span style=&quot;color: black;font-family: 微软雅黑;font-size: 9pt&quot;&gt;其中&lt;img src=&quot;/images/52nlp.cn/81e64fe6e9f426a5b1fefd362bab6542.jpg&quot; alt=&quot;&quot;&gt;是xk的所有父节点。&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p style=&quot;background: white&quot;&gt;&lt;span style=&quot;color: black;font-family: 微软雅黑;font-size: 9pt&quot;&gt;以上是贝叶斯网络将联合概率分解为条件概率的方法，比较直观易懂，就不多说了。下面说一下条件独立的表示和判断方法。条件独立是，给定a,b,c三个节点，如果p(a,b|c)=p(a|c)p(b|c)，则说给定c，a和b条件独立。当然 a, b, c也可以是三组节点，这里只以单个节点为例。用图表示，有三种情况 。&lt;br&gt;
第一种情况如图：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/dcb19345aa1d8d4667bee7cb594a98c9.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
c位于两个箭头的尾部，称作tail-to-tail，这种情况，c未知的时候，a，b是不独立的。c已知的时候，a,b条件独立。来看为什么，首先，这个图联合概率如下：&lt;br&gt;
在c未知的时候,p(a,b)如下求解:&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/cdb88b875a38905cd10f8b30a9fdfe2e.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
可以看出，无法得出:p(a,b)=p(a)p(b)，所以a,b不独立。&lt;br&gt;
如果c已知，则：&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p style=&quot;background: white&quot;&gt;&lt;img src=&quot;/images/52nlp.cn/20331242043b319984d3d085b162a187.jpg&quot; alt=&quot;&quot;&gt;&lt;span style=&quot;color: black;font-family: 微软雅黑;font-size: 9pt&quot;&gt;&lt;br&gt;
所以a,b条件独立于c。条件独立用以下符号表示：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/4ee6105a10bd19978f490fdf3db351be.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
a,b不独立的符号表示:：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/59487f132a9f73a1f28adb3a4bb993d0.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
这是图表示的条件独立的第一种形式，叫做tail-to-tail。第二种是tail-to-head，如图：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/167997121e6a661fe1e5c6f4494fbbfb.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
这种情况也是c未知时，a和b不独立。c已知时，a和b条件独立于c，推导如下：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/e54196de84b7b77e7d95a53330fcacb5.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
第三种情况是head-to-head，如图：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/f615cf610671e1a7e35d0cc8113996d5.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
这种情况反过来了，c未知时，a和b是独立的；但当c已知时，a和b不满足条件独立，&lt;br&gt;
因为：&lt;img src=&quot;/images/52nlp.cn/9125f163f71810ae944df8a9838e0612.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
计算该概率的边界概率，得&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/8c54ccb844643a221887efc4f4d5c016.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
所以a和b相互独立.&lt;br&gt;
但c已知时：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/08329d123764480718e95f9b24b560d2.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
无法得到p(a,b|c)=p(a|c)p(b|c)&lt;br&gt;
将这三种情况总结，就是贝叶斯网络的一个重要概念，D-separation，这个概念的内容就是:&lt;br&gt;
A,B,C三组节点，如果A中的任意节点与B的任意节点的所有路径上，存在以下节点，就说A和B被C阻断:&lt;br&gt;
1, A到B的路径上存在tail-to-tail或head-to-tail形式的节点，并且该节点属于C&lt;br&gt;
2. 路径上存在head-to-head的节点，并且该节点不属于C&lt;br&gt;
举个例子：&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p style=&quot;background: white&quot;&gt;&lt;img src=&quot;/images/52nlp.cn/53312848ea6cc6fa4fe45ee72a517857.jpg&quot; alt=&quot;&quot;&gt;&lt;span style=&quot;color: black;font-family: 微软雅黑;font-size: 9pt&quot;&gt;&lt;br&gt;
左边图上，节点f和节点e都不是d-separation.因为f是tail-to-tail，但f不是已知的，因此f不属于C.&lt;br&gt;
e是head-to-head，但e的子节点c是已知的，所以e也不属于C。&lt;br&gt;
speedmancs&amp;lt;speedmancs@qq.com&amp;gt; 19:23:05&lt;br&gt;
2漏了一点，该节点包括所有后继&lt;br&gt;
网神(66707180) 19:23:21&lt;br&gt;
右边图，f和e都是d-separation.理由与上面相反.对，是漏了这一点。看到这个例子才想起来，这部分大家有什么问题没？&lt;br&gt;
speedmancs&amp;lt;speedmancs@qq.com&amp;gt; 19:24:54&lt;br&gt;
这个还是抽象了一些，我之前看的prml这一章，没看懂，后来看了PGM前三章，主要看了那个学生成绩的那个例子，就明白了。姑妄记之，其实蛮不好记的。讲得挺好的，继续。&lt;br&gt;
网神(66707180) 19:27:14&lt;br&gt;
因为有了这些条件独立的规则，可以将图理解成一个filter。&lt;br&gt;
既给定一系列随机变量，其联合分布p(x1,x2…,xn)理论上可以分解成各种条件分布的乘积，但过一遍图，不满足图表示依赖关系和条件独立的分布就被过滤掉。所以图模型，用不同随机变量的连接表示各种关系，可以表示复杂的分布模型。&lt;br&gt;
接下来是马尔科夫随机场，是无向图，也叫马尔科夫网络，马尔科夫网络也有条件独立属性。&lt;br&gt;
用MRF(malkov random field)表示马尔科夫网络，MRF因为是无向的，所以不存在tail-to-tail这些概念。MRF的条件独立如图：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/078f0292ee0e167d5b57e39f2f33a3b9.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
如果A的任意节点和B的任意节点的任意路径上，都存在至少一个节点属于C&lt;br&gt;
speedmancs&amp;lt;speedmancs@qq.com&amp;gt; 19:33:16&lt;br&gt;
无向图的条件独立 比有向图简单多了。&lt;br&gt;
网神(66707180) 19:33:18&lt;br&gt;
那么A和B条件独立于C，可以理解为，如果C的节点都是已知的，就阻断了A和B的所有路径。&lt;br&gt;
网神(66707180) 19:33:49&lt;br&gt;
嗯，MRF的 概率分解就概念比较多了，不像有向图那么直观，MRF联合概率分解成条件概率。用到了clique的概念，我翻译成”团”，就是图的一个子图，子图上两两节点都有连接. 例如这个图，最大团有两个，分别是(x1,x2,x3)和(x2,x3,x4)：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/9e79837e987cd2dc6d65684894f97413.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
MRF的联合概率分解另一个概念是potential function，联合概率分解成一系列potential函数的乘积:&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/1d5657c5f5a06a5bde0c9b9191c61124.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p style=&quot;background: white&quot;&gt;&lt;img src=&quot;/images/52nlp.cn/1a6025a85cd29be822854607d38f0843.jpg&quot; alt=&quot;&quot;&gt;&lt;span style=&quot;color: black;font-family: 微软雅黑;font-size: 9pt&quot;&gt;是一个最大团的所有节点，一个potential函数&lt;img src=&quot;/images/52nlp.cn/a323fb98bb812323c011f2b2580f21d1.jpg&quot; alt=&quot;&quot;&gt;，是最大团的一个函数.&lt;br&gt;
这个函数具体的定义是依赖具体应用的，一会举个例子.&lt;br&gt;
上面式子里那个Z是normalization常量：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/2710737c2857506f56039c1fafe3dcdc.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
speedmancs&amp;lt;speedmancs@qq.com&amp;gt; 19:42:47&lt;br&gt;
这个Z很麻烦&lt;br&gt;
网神(66707180) 19:43:15&lt;br&gt;
在这个式子里&lt;img src=&quot;/images/52nlp.cn/529938c377158d69ed2a5a35791dc4d4.jpg&quot; alt=&quot;&quot;&gt;，p(x)是一系列potential函数的乘积。换一种理解方式，定义将potential函数表示成指数函数:&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/f3ef8cad75c98f0b2ee4d97a21d51902.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
这样p(x)就可以表示成一系列E(Xc)的和的指数函数，E(Xc)叫做能量函数，这么转换之后，可以将图理解成一个能量的集合，他的值等于各个最大团的能量的和.先举个例子看看potential函数和能量函数在具体应用中是什么样的，大家再讨论。&lt;br&gt;
要把噪声图片尽量还原成 原图，用图的方式表示噪声图和还原后的图，每个像素点是一个节点:&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/fbe5e6aad5e858efd74fc9405d7db1b4.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
上面那层Yi，是噪声图，紫色表示这些是已知的，是观察值。下面那层Xi是未知的，要求出Xi，使Xi作为像素值得到的图，尽量接近无噪声图片。每个xi的值，与yi相关，也与相邻的xj相关。这里边，最大团是(xi, yi)和(xi, xj)，两类最大团。&lt;br&gt;
对于(xi, yi)，选择能量函数E(xi,yi)=&lt;img src=&quot;/images/52nlp.cn/1bb01c9ebf347758bffb277296a3773a.jpg&quot; alt=&quot;&quot;&gt;；对于(xi,xj)，选择能量函数E(xi, xj)=&lt;img src=&quot;/images/52nlp.cn/bea0d1d974cb1d1328298a50269effba.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
两个能量函数的意思都是，如果xi和yi (或xi和xj)的值相同，则能量小；如果不同，则能量大.&lt;br&gt;
整个图的能量如下：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/e7573463095c637a611ffbdc9815e918.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p style=&quot;background: white&quot;&gt;&lt;img src=&quot;/images/52nlp.cn/aa64bd639bca57abb6382807b8c54fc6.jpg&quot; alt=&quot;&quot;&gt;&lt;span style=&quot;color: black;font-family: 微软雅黑;font-size: 9pt&quot;&gt;是一个偏置项&lt;br&gt;
speedmancs&amp;lt;speedmancs@qq.com&amp;gt; 19:56:55&lt;br&gt;
偏置项是一种先验吧&lt;br&gt;
网神(66707180) 19:57:28&lt;br&gt;
有了这个能量函数，接下来就是求出Xi，使得能量E(x,y)最小。求最小，书上简单说了一下，我的理解也是用梯度下降类似的方法。&lt;br&gt;
speedmancs&amp;lt;speedmancs@qq.com&amp;gt; 19:57:34&lt;br&gt;
这里表示-1的点更多吧。&lt;br&gt;
网神(66707180) 19:58:20&lt;br&gt;
对偏执项的作用，书上这么解释：Such a term has the effect of biasing the model towards pixel values that have one particular sign in preference to the other.&lt;br&gt;
speedmancs&amp;lt;speedmancs@qq.com&amp;gt; 19:59:29&lt;br&gt;
恩，对，让能量最小&lt;br&gt;
网神(66707180) 19:59:39&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p style=&quot;background: white&quot;&gt;&lt;span style=&quot;color: black;font-family: 微软雅黑;font-size: 9pt&quot;&gt;为了使E(x,y)尽量小，是尽量让xi选择-1，而不是1。E(x,y)越小，得到的图就越接近无噪声的图，因为：&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p style=&quot;background: white&quot;&gt;&lt;span style=&quot;color: black;font-family: 微软雅黑;font-size: 9pt&quot;&gt;&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/17fe077bd1cd61e2bcf88c8e3856f8e2.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
所以E(x,y)越小，p(x,y)就越大。&lt;br&gt;
η&amp;lt;liyitan2144@163.com&amp;gt; 20:01:13&lt;br&gt;
是不是可以这样看，&lt;img src=&quot;/images/52nlp.cn/02fb8032fb5a670e9c6ddb53977d247a.jpg&quot; alt=&quot;&quot;&gt;表示平滑；&lt;img src=&quot;/images/52nlp.cn/767ab9ef9b394c9e6842bb0f6f9a8546.jpg&quot; alt=&quot;&quot;&gt;表示似然。&lt;br&gt;
网神(66707180) 20:02:18&lt;br&gt;
liyitan2144说得好，&lt;img src=&quot;/images/52nlp.cn/f7a363d2fae2929c533b7eebf412dee5.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
speedmancs&amp;lt;speedmancs@qq.com&amp;gt; 20:02:33&lt;br&gt;
恩，所以这是一个 产生式模型。&lt;br&gt;
网神(66707180) 20:02:59&lt;br&gt;
有向图和无向图的概率分解 和 条件独立 都说完了.有向图和无向图是可以互相转换的，有向图转换成无向图，如果每个节点都只有少于等于1个父节点，比较简单。如果有超过1个父节点，就需要在转换之后的无向图上增加一些边，来避免都是有向图上的一些关系，这部分就不细说了。&lt;br&gt;
下面要说图的inference了。前面大家有啥要讨论的？先讨论一下吧。&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;color: #333333&quot;&gt;&lt;span style=&quot;font-family: Arial&quot;&gt;============================&lt;/span&gt;&lt;span style=&quot;font-family: 宋体&quot;&gt;讨论&lt;/span&gt;&lt;span style=&quot;font-family: Arial&quot;&gt;=================================&lt;br&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p style=&quot;background: white&quot;&gt;&lt;span style=&quot;color: black;font-family: 微软雅黑;font-size: 9pt&quot;&gt;speedmancs&amp;lt;speedmancs@qq.com&amp;gt; 20:06:45&lt;br&gt;
刚才那个例子中，那几个beta, h等参数如何得到？&lt;br&gt;
网神(66707180) 20:07:30&lt;br&gt;
那个是用迭代求解的方法，求得这几个参数，书上提到了两种方法，一种ICM,iterated conditional modes&lt;br&gt;
一种max-product方法，其中max-product方法效果比较好,在后面的inference一节里详细讲了这个方法，而ICM方法只是提了一下，原理没有细说.两种方法的效果看下图,左边是ICM的结果，右边是max-product的方法：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/1fda0c07f6f3fe74fdb1f5fd01ba140e.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p style=&quot;background: white&quot;&gt;&lt;span style=&quot;color: black;font-family: 微软雅黑;font-size: 9pt&quot;&gt;speedmancs&amp;lt;speedmancs@qq.com&amp;gt; 20:15:07&lt;br&gt;
稍微插一句，刚才那个denoise的例子，最好的那个结果是graph cut，而且那几个beta参数是事先固定了。&lt;br&gt;
η&amp;lt;liyitan2144@163.com&amp;gt; 20:16:12&lt;br&gt;
graph cut是指？&lt;br&gt;
kxkr&amp;lt;lxfkxkr@126.com&amp;gt; 20:16:30&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/777fee0478174419b50c2e7f2181b32b.jpg&quot; alt=&quot;&quot;&gt;kxkr&amp;lt;lxfkxkr@126.com&amp;gt; 20:17:18&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/4d57459d061b12b5306943839c821a92.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p style=&quot;background: white&quot;&gt;&lt;span style=&quot;color: black;font-family: 微软雅黑;font-size: 9pt&quot;&gt;网神(66707180) 20:18:11&lt;br&gt;
这个例子只是为了说明能量函数和潜函数.所以不一定是最佳方法，这两段截屏是prml上的，咋没看到捏&lt;br&gt;
kxkr&amp;lt;lxfkxkr@126.com&amp;gt; 20:19:05&lt;br&gt;
那几个参数如何得到 文中好像并没有说，只是说了ICM、graph-cut能够得到最后的去噪图像，第一段 在 figure8.30，第二个截图在 section8.4，figure8.32下面。&lt;br&gt;
网神(66707180) 20:20:15&lt;br&gt;
看到了，先不管这个吧，主要知道能量函数是啥样的就行了&lt;br&gt;
kxkr&amp;lt;lxfkxkr@126.com&amp;gt; 20:21:25&lt;br&gt;
嗯&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;color: #333333&quot;&gt;&lt;span style=&quot;font-family: Arial&quot;&gt;========================&lt;/span&gt;&lt;span style=&quot;font-family: 宋体&quot;&gt;讨论结束&lt;/span&gt;&lt;span style=&quot;font-family: Arial&quot;&gt;==============f===================&lt;br&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p style=&quot;background: white&quot;&gt;&lt;span style=&quot;color: black;font-family: 微软雅黑;font-size: 9pt&quot;&gt;网神(66707180) 20:22:24&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p style=&quot;background: white&quot;&gt;&lt;span style=&quot;color: black;font-family: 微软雅黑;font-size: 9pt&quot;&gt;接着说inference了，inference就是已知一些变量的值，求另一些变量的概率&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p style=&quot;background: white&quot;&gt;&lt;img src=&quot;/images/52nlp.cn/cbe9bf98180a14cb5045dca1794dc84d.jpg&quot; alt=&quot;&quot;&gt;&lt;span style=&quot;color: black;font-family: 微软雅黑;font-size: 9pt&quot;&gt;&lt;br&gt;
比如上图，已知y，求x的概率&lt;br&gt;
这个简单的图可以用典型的贝叶斯法则来求&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/7e71122e14cfaa88afdd58296b070111.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
对于复杂点的情况，比如链式图：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/3635e536873f2f8fc72de50279ebdd3d.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
为了求p(xn)，就是求xn的边界概率。这里都假设x的值是离散的。如果是连续的，就是积分，为了求这个边界概率，做这个累加动作，如果x的取值是k个，则要做k的(N-1)次方次计算，利用图结构，可以简化计算，这个简化方法就不讲了。&lt;br&gt;
下面讲通用的图inference的方法，就是factor graph方法，链式图或树形图，都比较好求边界概率。&lt;br&gt;
所以factor graph就是把复杂的图转换成树形图，针对树形图来求.先说一下什么是树形图，树形图有两种情况：&lt;br&gt;
1. 每个节点只有一个父节点。&lt;br&gt;
2. 如果有的节点有多个父节点，必须图上每个节点之间只有一条路径。&lt;br&gt;
这是树形图的三种情况：&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 宋体;font-size: 12pt&quot;&gt;&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/8eb9c9545ad3c13c7fad9613b7a21b60.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p style=&quot;background: white&quot;&gt;&lt;span style=&quot;color: black;font-family: 微软雅黑;font-size: 9pt&quot;&gt;对于无向图，只要无环，都可以看做树形图；对于有向图，必须每两个节点之间只有一条路径；中间那种是典型的树.右边那种多个有多个节点的叫polytree。&lt;br&gt;
这种书结构的图，都比较好求边界概率. 具体怎么求，就不说了。&lt;br&gt;
这里主要说怎么把复杂的图转换成树形图.，这种转换引入factor节点，从而将普通的图转换成factor图.&lt;br&gt;
先看个例子：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/780cb05565153f42f5c16af41ffb2745.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
对于这个有向图，p(x1,x2,x3)=p(x1)p(x2|x1)p(x3|x1,x2)，等号右边的三个概率作为三个factor&lt;br&gt;
每个factor作为一个节点，加入新的factor图中：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/b73761b2e92ab0e5ee900c85c690fced.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
上面的x1,x2,x3是原图的随机变量 ，下面的fa,fb,fc,fd是factor节点，只有随即变量节点和factor节点之间有连接，每类节点自身互不连接，每个factor连接的变量节点，是相互依赖的节点。&lt;br&gt;
kxkr&amp;lt;lxfkxkr@126.com&amp;gt; 20:42:10&lt;br&gt;
就是一个clique中的节点吧&lt;br&gt;
网神(66707180) 20:42:19&lt;br&gt;
对，严格的说，也不是。&lt;br&gt;
kxkr&amp;lt;lxfkxkr@126.com&amp;gt; 20:44:15&lt;br&gt;
对于有向图 ？&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p style=&quot;background: white&quot;&gt;&lt;span style=&quot;color: black;font-family: 微软雅黑;font-size: 9pt&quot;&gt;网神(66707180) 20:45:10&lt;br&gt;
x1,x2,x3是一个最大团. 可以只用一个factor节点，如中间那个图&lt;br&gt;
kxkr&amp;lt;lxfkxkr@126.com&amp;gt; 20:45:17&lt;br&gt;
嗯&lt;br&gt;
网神(66707180) 20:45:22&lt;br&gt;
也可以用多个factor节点，如右边图，但什么情况下用一个factor什么情况用多个factor，我没想明白&lt;br&gt;
kxkr&amp;lt;lxfkxkr@126.com&amp;gt; 20:47:10&lt;br&gt;
嗯，继续&lt;br&gt;
网神(66707180) 20:47:21&lt;br&gt;
转换成factor图后，就是树形图了，符合前面树形图的两种情况，这时候，要求一个节点或一组节点的边界概率，用一种叫做sum-product的方法，已求一个节点的边界概率为例.&lt;br&gt;
η&amp;lt;liyitan2144@163.com&amp;gt; 20:49:50&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/bce8e43c96148fe947b45673d132458d.jpg&quot; alt=&quot;&quot;&gt;虽然也对具体的应用情景不清楚，但是一个factor能表达的信息比使用多个factor能表达的信息多.&lt;br&gt;
网神(66707180) 20:51:06&lt;br&gt;
单个的factor表达的信息多，而且节点越少，计算越简单，所以是不是尽量用少的factor？max-product求单个节点的边界概率，其思想是以该节点为root。&lt;br&gt;
η&amp;lt;liyitan2144@163.com&amp;gt; 20:52:38&lt;br&gt;
但是，从设计模型的角度看，节点少了，一个节点设计的复杂程度就大了，不一定容易设计，拆解成多个factor，每个factor都很简单，设计方便。&lt;br&gt;
kxkr&amp;lt;lxfkxkr@126.com&amp;gt; 20:54:37&lt;br&gt;
文中貌似倾向于一个单个的factor&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/ab7496f79bbd3a88c5efdc07bfc7f41e.jpg&quot; alt=&quot;&quot;&gt;η&amp;lt;liyitan2144@163.com&amp;gt; 20:56:57&lt;br&gt;
这貌似是在表达一个通用的方法，和是把大的clique的factor拆解成小的多个factor没有关系。&lt;br&gt;
kxkr&amp;lt;lxfkxkr@126.com&amp;gt; 20:57:53&lt;br&gt;
拆成多个factor是不是会造成参数变多，不容易求呢，继续吧，这个有待实践。η&amp;lt;liyitan2144@163.com&amp;gt; 21:00:00&lt;br&gt;
参数应该会变多吧，不过模型其实简单了，确实有待实践，我觉得这和具体应用有关，比如在一副图像里面，如果仅仅表达”像素”的相似度，我们完全可以使用小factor。&lt;br&gt;
网神(66707180) 21:00:39&lt;br&gt;
我继续，prml这章图模型只讲了基础的概念，没讲常用的图模型实例，HMM和CRF这两大主流图模型方法还没讲，感觉不够直观.&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p style=&quot;background: white&quot;&gt;&lt;span style=&quot;color: black;font-family: 微软雅黑;font-size: 9pt&quot;&gt;我继续说inference. 转换成factor图后，求节点xi的边缘概率。把xi作为root节点,把求root边界概率理解成一个信息(message)传递的过程，从叶子节点传递概率信息到root节点.传递的规则是：&lt;br&gt;
从叶子节点开始，如果叶子是变量节点，发送1给父节点,如果叶子是factor节点，发送f(x)给父节点.&lt;br&gt;
对于非叶子节点，如果是变量节点，将其收到的message相乘，发给父节点，如果是factor节点，将其收到的message和自身f(x)相乘，然后做一个sum，发给父节点。&lt;br&gt;
举个例子：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/6abcc733a0eef13480d910dcbbf65917.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
这个图中求x3的边缘概率，message传递的过程是:&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/373711657bf8fc457039064b2160f500.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/c3a6215edc71bf7e8f686093c75d3eb4.jpg&quot; alt=&quot;&quot;&gt;中的&lt;img src=&quot;/images/52nlp.cn/76860163e1ac4afd0716cb30dc461b10.jpg&quot; alt=&quot;&quot;&gt;表示从节点x1传递到fa，最后p(x3)是等于&lt;img src=&quot;/images/52nlp.cn/6c1d66db3d4502367458e64206cbbeca.jpg&quot; alt=&quot;&quot;&gt;，因为只有一个fb节点向其传入信息，如果要求x2的边界概率,因为x2有三个节点出入信息,分别是：&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p style=&quot;background: white&quot;&gt;&lt;img src=&quot;/images/52nlp.cn/5251644e28bcc5c37856419b42209fe7.jpg&quot; alt=&quot;&quot;&gt;&lt;img src=&quot;/images/52nlp.cn/3763991ca8d8df08824cbe3afa03a122.jpg&quot; alt=&quot;&quot;&gt;&lt;img src=&quot;/images/52nlp.cn/fb058e2e7185d55c95cdfd5caab7bccc.jpg&quot; alt=&quot;&quot;&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 9pt&quot;&gt;&lt;span style=&quot;color: black&quot;&gt;&lt;br&gt;
所以p(x2)就等于这三个信息的乘积&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/1123302e409577b601ed6d165ec7ffa9.jpg&quot; alt=&quot;&quot;&gt;&lt;/span&gt;&lt;span style=&quot;color: red&quot;&gt;&lt;br&gt;
&lt;/span&gt;&lt;span style=&quot;color: black&quot;&gt;这个结果与边界分布的定义是相符的，x2的边界定义如下：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/a10834c216091c29e028a5dcefbcfb45.jpg&quot; alt=&quot;&quot;&gt;&lt;img src=&quot;/images/52nlp.cn/eaae67fffb2d9a5038ead38c5fdbbc0b.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/09d6d75acf7191132e8fc99768b0cc1e.jpg&quot; alt=&quot;&quot;&gt;通过这个，可以推导出上面的边界分布. 推导如下：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/4dde39f1d7972ae3eede98f20eba946d.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p style=&quot;background: white&quot;&gt;&lt;span style=&quot;color: black;font-family: 微软雅黑;font-size: 9pt&quot;&gt;上面就是用sum-product来求边缘分布的方法。&lt;br&gt;
不知道讲的是否明白，不明白就看书吧，一起研究&lt;img src=&quot;/images/52nlp.cn/6998144da89ee64b53db7ed2ef38f45a.jpg&quot; alt=&quot;&quot;&gt;今天就讲到这了，大家有啥问题讨论下。&lt;br&gt;
huajh7(284696304) 21:34:52&lt;br&gt;
补充几点，一是 temporal model ,如Dynamical Bayesian newtwork(DBN), plate models ，这是图模型的表达能力; 二是belief Bropagation ，包括exact 和approximation ，loopy时的收敛性; Inference包括MCMC,变分法。这是图模型在tree和graph的推理能力。三是Structure Learning ，BIC score等。这是图模型的学习能力。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;注：PRML读书会系列文章由 &lt;a href=&quot;http://weibo.com/dmalgorithms&quot;&gt;@Nietzsche_复杂网络机器学习&lt;/a&gt; 同学授权发布，转载请注明原作者和相关的主讲人，谢谢。&lt;/p&gt;
&lt;p&gt;PRML读书会讲稿PDF版本以及更多资源下载地址：&lt;a href=&quot;http://vdisk.weibo.com/u/1841149974&quot;&gt;http://vdisk.weibo.com/u/1841149974&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;本文链接地址：&lt;a href=&quot;http://www.52nlp.cn/prml%E8%AF%BB%E4%B9%A6%E4%BC%9A%E7%AC%AC%E5%85%AB%E7%AB%A0-graphical-models&quot;&gt;http://www.52nlp.cn/prml读书会第八章-graphical-models&lt;/a&gt;&lt;/p&gt;

											

</description>
        <pubDate>Sat, 31 Jan 2015 00:00:00 +0800</pubDate>
        <link>http://iftti.com/posts/2015/2015-01-31-prml%25e8%25af%25bb%25e4%25b9%25a6%25e4%25bc%259a%25e7%25ac%25ac%25e5%2585%25ab%25e7%25ab%25a0-graphical-models-7cc076ef3.html</link>
        <guid isPermaLink="true">http://iftti.com/posts/2015/2015-01-31-prml%25e8%25af%25bb%25e4%25b9%25a6%25e4%25bc%259a%25e7%25ac%25ac%25e5%2585%25ab%25e7%25ab%25a0-graphical-models-7cc076ef3.html</guid>
        
        
        <category>52nlp</category>
        
      </item>
    
      <item>
        <title>PRML读书会第九章  Mixture Models and EM</title>
        <description>

						&lt;p style=&quot;text-align: center&quot;&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 15pt&quot;&gt;&lt;strong&gt;PRML读书会第九章 Mixture Models and EM&lt;br&gt;
&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p style=&quot;text-align: center&quot;&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 15pt&quot;&gt;&lt;strong&gt;主讲人 网络上的尼采&lt;br&gt;
&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p style=&quot;text-align: center&quot;&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 12pt&quot;&gt;&lt;strong&gt;（新浪微博: &lt;a href=&quot;http://weibo.com/dmalgorithms&quot;&gt;@Nietzsche_复杂网络机器学习&lt;/a&gt;）&lt;br&gt;
&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-size: 10pt&quot;&gt;&lt;span style=&quot;font-family: 微软雅黑&quot;&gt;网络上的尼采(813394698) 9:10:56&lt;br&gt;
今天的主要内容有k-means、混合高斯模型、 EM算法。&lt;br&gt;
对于k-means大家都不会陌生，非常经典的一个聚类算法，已经50多年了，关于clustering推荐一篇不错的survey:&lt;/span&gt;&lt;br&gt;
&lt;span style=&quot;font-family: 微软雅黑&quot;&gt;Data clustering: 50 years beyond K-means。k-means表达的思想非常经典，就是对于复杂问题分解成两步不停的迭代进行逼近，并且每一步相对于前一步都是递减的。&lt;br&gt;
k-means有个目标函数 ：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/46837cdb2db9de12cf0e58aa9828e496.jpg&quot; alt=&quot;&quot;&gt;&lt;/span&gt;&lt;span style=&quot;font-family: 宋体&quot;&gt;&lt;br&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 10pt&quot;&gt;假设有k个簇，&lt;img src=&quot;/images/52nlp.cn/ec21b28673bf918acfc0ffb1a19f7d6a.jpg&quot; alt=&quot;&quot;&gt;是第k个簇的均值；每个数据点都有一个向量表示属于哪个簇，r&lt;sub&gt;nk&lt;/sub&gt;是向量的元素，如果点x&lt;sub&gt;n&lt;/sub&gt;属于第k个簇，则r&lt;sub&gt;nk&lt;/sub&gt;是1，向量的其他元素是0。&lt;br&gt;
上面这个目标函数就是各个簇的点与簇均值的距离的总和，k-means要做的就是使这个目标函数最小。 这是个NP-hard问题，k-means只能收敛到局部最优。&lt;br&gt;
&lt;/span&gt;&lt;span id=&quot;more-7668&quot;&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-size: 10pt&quot;&gt;&lt;span style=&quot;font-family: 微软雅黑&quot;&gt;算法的步骤非常简单：&lt;br&gt;
先随机选k个中心点&lt;br&gt;
第一步也就是E步把离中心点近的数据点划分到这个簇里；&lt;br&gt;
第二步M步根据各个簇里的数据点重新确定均值，也就是中心点。&lt;br&gt;
然后就是迭代第一步和第二步，直到满足收敛条件为止。&lt;br&gt;
自强&amp;lt;ccab4209211@qq.com&amp;gt; 9:29:00&lt;br&gt;
收敛是怎么判断的呀？&lt;br&gt;
网络上的尼采(813394698) 9:30:16&lt;br&gt;
不再发生大的变化。大家思考下不难得出：无论E步还是M步,目标函数都比上一步是减少的。 下面是划分两个簇的过程 ：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/7598cdf6a2910058717d1e31efb05bda.jpg&quot; alt=&quot;&quot;&gt;&lt;/span&gt;&lt;span style=&quot;font-family: 宋体&quot;&gt;&lt;br&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 10pt&quot;&gt;下面这个图说明聚类过程中目标函数单调递减，经过三轮迭代就收敛了，由于目标函数只减不增，并且有界，所以k-means是可以保证收敛的：&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/images/52nlp.cn/38ad63e87623715f4c20ecd77af98815.jpg&quot; alt=&quot;&quot;&gt;&lt;span style=&quot;font-family: 宋体;font-size: 10pt&quot;&gt;&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p style=&quot;margin-left: 84pt&quot;&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 10pt&quot;&gt; 书里还举例一个k-means对图像分割和压缩的例子：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/e678f035703211290bed1a74c9ba371a.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 10pt&quot;&gt;图像分割后，每个簇由均值来表示，每个像素只存储它属于哪个簇就行了。压缩后图像的大小是k的函数 ：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/85e721be3166977e2f1aa987f1cba99b.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p style=&quot;margin-left: 15pt&quot;&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 10pt&quot;&gt;现在讨论下k-means的性质和不足 ：&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p style=&quot;margin-left: 15pt&quot;&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 10pt&quot;&gt;首先对初值敏感 ，由于只能收敛到局部最优，初值很大程度上决定它收敛到哪里；&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p style=&quot;margin-left: 15pt&quot;&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 10pt&quot;&gt;从算法的过程可以看出，k-means对椭球形状的簇效果最好 ，不能发现任意形状的簇；&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 10pt&quot;&gt;对孤立点干扰的鲁棒性差，孤立点是异质的，可以说是均值杀手，k-means又是围绕着均值展开的，试想下，原离簇的孤立点对簇的均值的拉动作用是非常大的。&lt;br&gt;
针对这些问题后来又有了基于密度的DBSCAN算法，最近Science上发了另一篇基于密度的方法：Clustering by fast search and find of density peaks。基于密度的方法无论对clustering还是outliers detection效果都不错，和k-means不同，这些算法已经没有了目标函数，但鲁棒性强，可以发现任意形状的簇。&lt;br&gt;
另外如何自动确定k-means的k是目前研究较多的一个问题。k-means就到这里，现在一块讨论下。&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 10pt&quot;&gt;口水猫(465191936) 9:49:20&lt;br&gt;
如果对于这批数据想做k-mean 聚类，那么如果去换算距离？&lt;br&gt;
网络上的尼采(813394698) 9:49:53&lt;br&gt;
k-means一般基于欧式距离，关于距离度量是个专门的方向，点集有了度量才能有拓扑，有专门的度量学习这个方向。&lt;br&gt;
口水猫(465191936) 9:50:08&lt;br&gt;
嗯嗯 有没有一些参考意见了，k的选择可以参考coursera上的视频 选择sse下降最慢的那个拐点的k&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 10pt&quot;&gt;网络上的尼采(813394698) 9:51:41&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-size: 10pt&quot;&gt;&lt;span style=&quot;font-family: 微软雅黑&quot;&gt;关于选哪个k是最优的比较主观，有从结果稳定性来考虑的，毕竟一个算法首先要保证的是多次运行后结果相差不大，关于这方面有一篇survey:&lt;/span&gt;&lt;br&gt;
&lt;span style=&quot;font-family: 微软雅黑&quot;&gt;Clustering stability an overview。另外还有其他自动选k的方法，DP、MDL什么的。MDL是最短描述长度，从压缩的角度来看k-means；DP是狄利克雷过程，一种贝叶斯无参方法，感兴趣可以看JORDAN小组的文章。&lt;br&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 10pt&quot;&gt;我们下面讲混合高斯模型GMM，第二章我们说过，高斯分布有很多优点并且普遍存在，但是单峰函数，所以对于复杂的分布表达能力差，我们可以用多个高斯分布的线性组合来逼近这些复杂的分布。我们看下GMM的线性组合形式：&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/images/52nlp.cn/42e654495709a117c1093f0ac66a49e4.jpg&quot; alt=&quot;&quot;&gt;&lt;span style=&quot;font-family: 宋体;font-size: 10pt&quot;&gt;&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 10pt&quot;&gt;对于每个数据点是哪个分布生成的，我们假设有个Z隐变量 ，和k-means类似，对于每个数据点都有一个向量z，如果是由第k个分布生成，元素z&lt;sub&gt;k&lt;/sub&gt;=1,其他为0。&lt;br&gt;
z&lt;sub&gt;k&lt;/sub&gt;=1的概率的先验就是高斯分布前的那个系数：&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/images/52nlp.cn/3377e6ba6f505ea07bb1e3e945251f39.jpg&quot; alt=&quot;&quot;&gt;&lt;span style=&quot;font-size: 10pt&quot;&gt;&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 10pt&quot;&gt;下面是z&lt;sub&gt;k&lt;/sub&gt;=1概率的后验，由贝叶斯公式推导的：&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/images/52nlp.cn/f90fba04fd491181c63ee115401aaf4b.jpg&quot; alt=&quot;&quot;&gt;&lt;span style=&quot;font-family: 宋体;font-size: 10pt&quot;&gt;&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 10pt&quot;&gt;其实很好理解，如果没有&lt;img src=&quot;/images/52nlp.cn/f555fa502a4ad8c0ba1e6e0a05cda131.jpg&quot; alt=&quot;&quot;&gt;限制，数据点由哪个分布得出的概率大z&lt;sub&gt;k&lt;/sub&gt;=1的期望就大，但前面还有一个系数限制，所以期望形式是：&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/images/52nlp.cn/904a80fea2276aa374479de00127ed8f.jpg&quot; alt=&quot;&quot;&gt;&lt;span style=&quot;font-size: 10pt&quot;&gt;&lt;span style=&quot;font-family: 微软雅黑&quot;&gt;&lt;br&gt;
刚才有人问如何确定模型的参数，我们首先想到的就是log最大似然：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/b105469cae1f0a59218f1218b98c6917.jpg&quot; alt=&quot;&quot;&gt;&lt;/span&gt;&lt;span style=&quot;font-family: 宋体&quot;&gt;&lt;br&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-size: 10pt&quot;&gt;&lt;span style=&quot;font-family: 微软雅黑&quot;&gt;但是我们可以观察下这个目标函数，log里面有加和，求最优解是非常困难的，混合高斯和单个高斯的参数求法差别很大，如果里面有一个高斯分布坍缩成一个点，log似然函数会趋于无穷大。由于直接求解困难，这也是引入EM的原因。&lt;br&gt;
下面是GMM的图表示 ：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/ca805a375bd0ed7af66d23a8ccc38f37.jpg&quot; alt=&quot;&quot;&gt;&lt;/span&gt;&lt;span style=&quot;font-family: 宋体&quot;&gt;&lt;br&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-size: 10pt&quot;&gt;&lt;span style=&quot;font-family: 微软雅黑&quot;&gt;我们可以试想下，如果隐变量z&lt;sub&gt;n&lt;/sub&gt;是可以观测的，也就是知道哪个数据点是由哪个分布生成的，那么我们求解就会很方便，可以利用高斯分布直接得到解析解。 但关键的是z&lt;sub&gt;n&lt;/sub&gt;是隐变量，我们没法观测到。但是我们可以用它的期望来表示。 现在我们来看一下EM算法在GMM中的应用：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/3e952447b6aff2f048828e89e7640b31.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
上面是EM对GMM的E步&lt;br&gt;
我们首先对模型的参数初始化&lt;br&gt;
E步就是我们利用这些参数得z&lt;sub&gt;nk&lt;/sub&gt;的期望，这种形式我们前面已经提到了：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/393125222a26a161318325c1342c9c23.jpg&quot; alt=&quot;&quot;&gt;&lt;/span&gt;&lt;span style=&quot;font-family: 宋体&quot;&gt;&lt;br&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-size: 10pt&quot;&gt;&lt;span style=&quot;font-family: 微软雅黑&quot;&gt;现在我们有隐藏变量的期望了，由期望得新的模型参数也就是M步，高斯分布的好处就在这儿，可以推导出新参数的闭式解：&lt;img src=&quot;/images/52nlp.cn/e868fc7713f28af916be0b413a18a9f8.jpg&quot; alt=&quot;&quot;&gt;&lt;/span&gt;&lt;span style=&quot;font-family: 宋体&quot;&gt;&lt;br&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 10pt&quot;&gt;然后不断迭代E步和M步直到满足收敛条件。&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 10pt&quot;&gt;为什么要这么做，其实EM算法对我们前面提到的log最大似然目标函数：&lt;img src=&quot;/images/52nlp.cn/8fc48c3f196073406ece346bd8582008.jpg&quot; alt=&quot;&quot;&gt;是单调递增的。&lt;br&gt;
karnon(447457116) 10:39:42&lt;br&gt;
用EM来解GMM其实是有问题的，解出来的解并不是最优的。。&lt;br&gt;
网络上的尼采(813394698) 10:40:14&lt;br&gt;
嗯，这个问题最后讲。&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 10pt&quot;&gt;我们再来看EM更一般的形式：&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/images/52nlp.cn/b44ebca0a92fda2e432578bcca6a66ff.jpg&quot; alt=&quot;&quot;&gt;&lt;span style=&quot;font-size: 10pt&quot;&gt;&lt;span style=&quot;font-family: 微软雅黑&quot;&gt;是我们的目标函数，加入隐藏变量可以写成这种形式：&lt;img src=&quot;/images/52nlp.cn/85a9b6d13d21ee510462edaf9d370390.jpg&quot; alt=&quot;&quot;&gt;&lt;/span&gt;&lt;span style=&quot;font-family: 宋体&quot;&gt;&lt;br&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 10pt&quot;&gt;先初始化模型的参数，由于隐变量无法观测到，我们用参数来得到它的后验&lt;img src=&quot;/images/52nlp.cn/c4145dc79b970bc4da0511d536e839ad.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 10pt&quot;&gt;然后呢，我们通过隐藏变量的期望得到新的完整数据的最大似然函数：&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/images/52nlp.cn/683a3882cdac58430bfc03a110f6e148.jpg&quot; alt=&quot;&quot;&gt;&lt;span style=&quot;font-family: 宋体;font-size: 10pt&quot;&gt;&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-size: 10pt&quot;&gt;&lt;span style=&quot;font-family: 微软雅黑&quot;&gt;以上是E步&lt;/span&gt;&lt;span style=&quot;font-family: 宋体&quot;&gt;，&lt;/span&gt;&lt;span style=&quot;font-family: 微软雅黑&quot;&gt;M步是求这个似然函数的Q函数的最优解，也就是新的参数：&lt;img src=&quot;/images/52nlp.cn/029f14c8859e95dd4a6baae4510b42ff.jpg&quot; alt=&quot;&quot;&gt;&lt;/span&gt;&lt;span style=&quot;font-family: 宋体&quot;&gt;&lt;br&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-size: 10pt&quot;&gt;&lt;span style=&quot;font-family: 微软雅黑&quot;&gt;注意这个Q函数是包含隐变量的完整数据的似然函数，不是我们一开始的目标函数&lt;img src=&quot;/images/52nlp.cn/3198a4df5965b0bdc03d8b1d2b1b1271.jpg&quot; alt=&quot;&quot;&gt;&lt;/span&gt;&lt;span style=&quot;font-family: 宋体&quot;&gt;&lt;br&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 10pt&quot;&gt;其实求完整数据的最大似然是在逼近我们目标函数的局部最优解，这个在后面讲。&lt;br&gt;
下面这个是一般化EM算法的步骤： &lt;img src=&quot;/images/52nlp.cn/5d408efa7376e2982e2573bcc5b9f3d4.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
EM算法只所以用途广泛在于有潜在变量的场合都能用，并不局限于用在GMM上。现在我们回过头来看混合高斯模型的M步，这些得到的新参数就是Q函数的最优解：&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/images/52nlp.cn/7bb976ffbb06377117ecb49acfcdc7e7.jpg&quot; alt=&quot;&quot;&gt;&lt;span style=&quot;font-family: 宋体;font-size: 10pt&quot;&gt;&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-size: 10pt&quot;&gt;&lt;span style=&quot;font-family: 微软雅黑&quot;&gt;再思考下k-means，其实它是EM的特例，只不过是k-means对数据点的分配是硬性的，在E步每个数据点必须分配到一个簇，z里面只有一个1其他是0,而EM用的是z的期望：&lt;img src=&quot;/images/52nlp.cn/417c395286d53e86de7c5bd8543d7de5.jpg&quot; alt=&quot;&quot;&gt;&lt;/span&gt;&lt;span style=&quot;font-family: 宋体&quot;&gt;&lt;br&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-size: 10pt&quot;&gt;&lt;span style=&quot;font-family: 微软雅黑&quot;&gt;下面这个图说明k-means是EM算法特例，这与前面k-means聚类的过程的图对应：&lt;/span&gt;&lt;span style=&quot;font-family: 宋体&quot;&gt;&lt;br&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/images/52nlp.cn/369c4c7aaa8e231d1879d3dc681018e1.jpg&quot; alt=&quot;&quot;&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 10pt&quot;&gt;&lt;br&gt;
对于EM算法性质的证明最后讲，下面讲混合伯努利模型，高斯分布是针对连续的属性，伯努利是针对离散属性。混合形式：&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/images/52nlp.cn/a6983d91f6455e55e54452a870159ab7.jpg&quot; alt=&quot;&quot;&gt;&lt;span style=&quot;font-family: 宋体;font-size: 10pt&quot;&gt;&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 10pt&quot;&gt; 目标函数，log里面同样有加和：&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/images/52nlp.cn/46a87a121aedde9f18f311a15ca1b0fc.jpg&quot; alt=&quot;&quot;&gt;&lt;span style=&quot;font-size: 10pt&quot;&gt;&lt;span style=&quot;font-family: 宋体&quot;&gt;，&lt;/span&gt;&lt;span style=&quot;font-family: 微软雅黑&quot;&gt;。&lt;br&gt;
z&lt;sub&gt;nk&lt;/sub&gt;=1的期望：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/f996995179951169639a0a15e5dcef4a.jpg&quot; alt=&quot;&quot;&gt;&lt;/span&gt;&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 10pt&quot;&gt;Q函数：&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/images/52nlp.cn/9c7307b5bcc431605d934b5dc784949c.jpg&quot; alt=&quot;&quot;&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 10pt&quot;&gt;&lt;br&gt;
以上是E步，下面是M步的解，这两个：&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/images/52nlp.cn/5e93c35848ad64f7454b2507a7b78faf.jpg&quot; alt=&quot;&quot;&gt;&lt;img src=&quot;/images/52nlp.cn/96fe6ca703ed64539952382065c4e591.jpg&quot; alt=&quot;&quot;&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 10pt&quot;&gt;&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 10pt&quot;&gt;其中：&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/images/52nlp.cn/e361a37822c7dfbffc5e5c5a93cc6062.jpg&quot; alt=&quot;&quot;&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 10pt&quot;&gt;&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 10pt&quot;&gt;书里举了一个手写字聚类的例子 ，先对像素二值化 ，然后聚成3个簇：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/ada06663b0f148ef71d3b8792a8a36a6.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
这是3个簇的代表：&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/images/52nlp.cn/2db3ba32db5c8cadb786ac0fddfabbd8.jpg&quot; alt=&quot;&quot;&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 10pt&quot;&gt;&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 10pt&quot;&gt;k=1时簇的代表：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/0ac12718dee9547a9fe6c4e2f1286d14.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;color: #333333;font-family: Arial;font-size: 10pt&quot;&gt;=============================================================&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 10pt&quot;&gt;最后说下EM算法为什么能收敛到似然函数的局部最优解:&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 10pt&quot;&gt;我们的目标函数是分布&lt;img src=&quot;/images/52nlp.cn/1414bf376650d7991084242f378aca24.jpg&quot; alt=&quot;&quot;&gt;的最大log似然&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-size: 10pt&quot;&gt;&lt;span style=&quot;font-family: 微软雅黑&quot;&gt;引入潜在变量，分布表示为：&lt;img src=&quot;/images/52nlp.cn/f244d5d0eaabe1ce163db96988479a59.jpg&quot; alt=&quot;&quot;&gt;&lt;/span&gt;&lt;span style=&quot;font-family: 宋体&quot;&gt;&lt;br&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 10pt&quot;&gt;定义隐藏变量的分布为q(z)，目标函数可以表达为下面形式，这个地方是神来一笔：&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/images/52nlp.cn/0ab8563bdfb9c7320e15ca59c948db9a.jpg&quot; alt=&quot;&quot;&gt;&lt;span style=&quot;font-family: 宋体;font-size: 10pt&quot;&gt;&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-size: 10pt&quot;&gt;&lt;span style=&quot;font-family: 微软雅黑&quot;&gt;其中&lt;img src=&quot;/images/52nlp.cn/2a6f7fa880f92e0af9997c915665ec2e.jpg&quot; alt=&quot;&quot;&gt;是&lt;img src=&quot;/images/52nlp.cn/6d84d3f5481ef52a5df3648ff85f18fc.jpg&quot; alt=&quot;&quot;&gt;与q(z)的KL散度；&lt;img src=&quot;/images/52nlp.cn/c30e96d7892982975ffacadeebc3afcf.jpg&quot; alt=&quot;&quot;&gt;是q(z)的泛函形式。&lt;br&gt;
我们原来讲过根据Jensen不等式来证明KL散度是非负的，这个性质在这里发挥了作用。由于&lt;img src=&quot;/images/52nlp.cn/fa4f4f3b02293115dac180d475e553c1.jpg&quot; alt=&quot;&quot;&gt;是大于等于零的，所以&lt;img src=&quot;/images/52nlp.cn/b45384983a6c3f39edeb181aa6c481ba.jpg&quot; alt=&quot;&quot;&gt;是目标函数&lt;img src=&quot;/images/52nlp.cn/d2a8b72a1a991c24e843619168652dcc.jpg&quot; alt=&quot;&quot;&gt;的下界。&lt;img src=&quot;/images/52nlp.cn/9284d99061fd7b10adc0cdb1c73a5912.jpg&quot; alt=&quot;&quot;&gt;与目标函数什么时候相等呢？其实就是&lt;img src=&quot;/images/52nlp.cn/8ab6be979e26dde23bb1f5571d315cb4.jpg&quot; alt=&quot;&quot;&gt;等于0的时候，也就是q(z)与z的后验分布&lt;img src=&quot;/images/52nlp.cn/e58cf86ef6e20f15298e720e16245974.jpg&quot; alt=&quot;&quot;&gt;相同时，这个时候就是E步z取它的期望时候。&lt;img src=&quot;/images/52nlp.cn/ed9fe9cd163a25c7015d7d2c576abf6c.jpg&quot; alt=&quot;&quot;&gt;与目标函数相等是为了取一个比较紧的bound。&lt;br&gt;
M步就是最大化&lt;img src=&quot;/images/52nlp.cn/50c80aed6038d7de0dd64f67c310a8a6.jpg&quot; alt=&quot;&quot;&gt;，随着&lt;img src=&quot;/images/52nlp.cn/70edc6f0495ceac5a10da1dd7b7382b3.jpg&quot; alt=&quot;&quot;&gt;的增大，&lt;img src=&quot;/images/52nlp.cn/067af07d7fb1cbd0dadcc0ce7165b7d6.jpg&quot; alt=&quot;&quot;&gt;开始大于0，也就是目标函数比&lt;img src=&quot;/images/52nlp.cn/2083ed7c66c75b0a75f86ebced03a246.jpg&quot; alt=&quot;&quot;&gt;增大的幅度更大。这个过程是使目标函数一直单调递增的。下面是&lt;img src=&quot;/images/52nlp.cn/489009f24edc0393ae3e99a5de227554.jpg&quot; alt=&quot;&quot;&gt;与Q函数的关系，这也是为什么M步新参数取完整数据的最大似然解的原因：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/ec93cb053b7fe2ccdd0e154e54c8f46f.jpg&quot; alt=&quot;&quot;&gt;&lt;/span&gt;&lt;span style=&quot;font-family: 宋体&quot;&gt;&lt;br&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p style=&quot;margin-left: 15pt&quot;&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 10pt&quot;&gt;最后上一张非常形象的图，解释为什么EM能收敛到目标函数的局部最优：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/be83534220e9ff3f7423041e7e437345.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-size: 10pt&quot;&gt;&lt;span style=&quot;font-family: 微软雅黑&quot;&gt;红的曲线是目标函数；蓝的绿的曲线是两步迭代。&lt;br&gt;
咱们先看蓝的E步和M步：&lt;br&gt;
&lt;img src=&quot;/images/52nlp.cn/0772ad51bdbc2259e5de5d05d424c298.jpg&quot; alt=&quot;&quot;&gt;&lt;/span&gt;&lt;span style=&quot;font-family: 宋体&quot;&gt;&lt;br&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-family: 微软雅黑;font-size: 10pt&quot;&gt;E步时就是取z的期望的时候，这时目标函数与&lt;img src=&quot;/images/52nlp.cn/0d6b36f529709da422275ec9c84ba62d.jpg&quot; alt=&quot;&quot;&gt;相同；&lt;br&gt;
M步就是最大化&lt;img src=&quot;/images/52nlp.cn/768d61ba74934368420071b414838037.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
绿线是下一轮的迭代，EM过程中，目标函数一直是单调上升的，并且有界，所以EM能够保证收敛。但不一定能收敛到最优解，这与初始值有很大关系，试想一下，目标函数的曲线变动下，EM就有可能收敛到局部最优了。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;注：PRML读书会系列文章由 &lt;a href=&quot;http://weibo.com/dmalgorithms&quot;&gt;@Nietzsche_复杂网络机器学习&lt;/a&gt; 同学授权发布，转载请注明原作者和相关的主讲人，谢谢。&lt;/p&gt;
&lt;p&gt;PRML读书会讲稿PDF版本以及更多资源下载地址：&lt;a href=&quot;http://vdisk.weibo.com/u/1841149974&quot;&gt;http://vdisk.weibo.com/u/1841149974&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;本文链接地址：&lt;a href=&quot;http://www.52nlp.cn/prml%E8%AF%BB%E4%B9%A6%E4%BC%9A%E7%AC%AC%E4%B9%9D%E7%AB%A0-mixture-models-and-em&quot;&gt;http://www.52nlp.cn/prml读书会第九章-mixture-models-and-em&lt;/a&gt;&lt;/p&gt;

											

</description>
        <pubDate>Sat, 31 Jan 2015 00:00:00 +0800</pubDate>
        <link>http://iftti.com/posts/2015/2015-01-31-prml%25e8%25af%25bb%25e4%25b9%25a6%25e4%25bc%259a%25e7%25ac%25ac%25e4%25b9%259d%25e7%25ab%25a0-mixture-models-and-em-a7767c4f3.html</link>
        <guid isPermaLink="true">http://iftti.com/posts/2015/2015-01-31-prml%25e8%25af%25bb%25e4%25b9%25a6%25e4%25bc%259a%25e7%25ac%25ac%25e4%25b9%259d%25e7%25ab%25a0-mixture-models-and-em-a7767c4f3.html</guid>
        
        
        <category>52nlp</category>
        
      </item>
    
      <item>
        <title>PRML读书会第七章 Sparse Kernel Machines</title>
        <description>

						&lt;p style=&quot;text-align: center&quot;&gt;&lt;strong&gt;PRML&lt;/strong&gt;&lt;strong&gt;读书会第七章 Sparse Kernel Machines&lt;/strong&gt;&lt;/p&gt;
&lt;p style=&quot;text-align: center&quot;&gt;&lt;strong&gt;主讲人 网神&lt;/strong&gt;&lt;/p&gt;
&lt;p style=&quot;text-align: center&quot;&gt;&lt;strong&gt;（新浪微博: &lt;a href=&quot;http://weibo.com/ghtimaq&quot;&gt;@豆角茄子麻酱凉面&lt;/a&gt;）&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;网神(66707180) 18:59:22&lt;br&gt;
大家好，今天一起交流下PRML第7章。第六章核函数里提到，有一类机器学习算法，不是对参数做点估计或求其分布，而是保留训练样本，在预测阶段，计算待预测样本跟训练样本的相似性来做预测，例如KNN方法。&lt;br&gt;
将线性模型转换成对偶形式，就可以利用核函数来计算相似性，同时避免了直接做高维度的向量内积运算。本章是稀疏向量机，同样基于核函数，用训练样本直接对新样本做预测，而且只使用了少量训练样本，所以具有稀疏性，叫sparse kernel machine。&lt;br&gt;
本章包括SVM和RVM(revelance vector machine)两部分，首先讲SVM，支持向量机。首先看SVM用于二元分类，并先假设两类数据是线性可分的。&lt;br&gt;
二元分类线性模型可以用这个式子表示：&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-0.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7374&quot; src=&quot;/images/52nlp.cn/59499bfd873ade8f22a64988c0dea4fd.jpg&quot; alt=&quot;prml7-0&quot; width=&quot;150&quot; height=&quot;20&quot;&gt;&lt;/a&gt;。其中&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-1.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7375&quot; src=&quot;/images/52nlp.cn/9065666d1d1c75ddf12b4478768ca4f0.jpg&quot; alt=&quot;prml7-1&quot; width=&quot;50&quot; height=&quot;20&quot;&gt;&lt;/a&gt;是基函数，这些都跟第三章和第四章是一样的。&lt;br&gt;
两类数据线性可分，当&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-2.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7376&quot; src=&quot;/images/52nlp.cn/62a94e5fa5586f24b2af2b33cdc595c4.jpg&quot; alt=&quot;prml7-2&quot; width=&quot;59&quot; height=&quot;23&quot;&gt;&lt;/a&gt;时,分类结果是&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-3.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7377&quot; src=&quot;/images/52nlp.cn/865ae1ac29c14312a242a71e0941856b.jpg&quot; alt=&quot;prml7-3&quot; width=&quot;48&quot; height=&quot;24&quot;&gt;&lt;/a&gt;; &lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-4.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7378&quot; src=&quot;/images/52nlp.cn/3ca788b51e2d7807ed8d22e9964f9e3f.jpg&quot; alt=&quot;prml7-4&quot; width=&quot;57&quot; height=&quot;22&quot;&gt;&lt;/a&gt;时,分类结果&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-5.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7379&quot; src=&quot;/images/52nlp.cn/ed5c3bbafaf081ccdeeefdf846a00604.jpg&quot; alt=&quot;prml7-5&quot; width=&quot;45&quot; height=&quot;25&quot;&gt;&lt;/a&gt;;也就是对所有训练样本总是有&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-6.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7380&quot; src=&quot;/images/52nlp.cn/50c2e172008f76f4e4b32fc028486cb7.jpg&quot; alt=&quot;prml7-6&quot; width=&quot;64&quot; height=&quot;20&quot;&gt;&lt;/a&gt;.要做的就是确定决策边界y(x)=0&lt;br&gt;
为了确定决策边界&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-7.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7382&quot; src=&quot;/images/52nlp.cn/a74a09c269c26009d290ae095e28252e.jpg&quot; alt=&quot;prml7-7&quot; width=&quot;99&quot; height=&quot;19&quot;&gt;&lt;/a&gt;，SVM引入margin的概念。margin定义为决策边界y(x)到最近的样本的垂直距离。如下图所示：&lt;span id=&quot;more-7371&quot;&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-8.png&quot;&gt;&lt;img class=&quot;aligncenter wp-image-7383&quot; src=&quot;/images/52nlp.cn/abd000d9eec61f9646d15e0c67628a4a.jpg&quot; alt=&quot;prml7-8&quot; width=&quot;400&quot; height=&quot;280&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;SVM的目标是寻找一个margin最大的决策边界。 我们来看如何确定目标函数：&lt;br&gt;
首先给出一个样本点x到决策边界&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-9.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7384&quot; src=&quot;/images/52nlp.cn/075264ffb4abb01638be86e9a1dafaf4.jpg&quot; alt=&quot;prml7-9&quot; width=&quot;87&quot; height=&quot;21&quot;&gt;&lt;/a&gt;的垂直距离公式是什么，先给出答案：|y(x)|/||w||&lt;br&gt;
这个距离怎么来的，在第四章有具体介绍。看下图:&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-10.png&quot;&gt;&lt;img class=&quot;aligncenter wp-image-7385&quot; src=&quot;/images/52nlp.cn/e506e3b1c96b4b2827230dab7b899b19.jpg&quot; alt=&quot;prml7-10&quot; width=&quot;400&quot; height=&quot;316&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;图例，我们看点x到y=0的距离r是多少：&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-11.png&quot;&gt;&lt;img class=&quot;aligncenter wp-image-7386&quot; src=&quot;/images/52nlp.cn/bff6d35612a26344477e268c3afff903.jpg&quot; alt=&quot;prml7-11&quot; width=&quot;450&quot; height=&quot;135&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;上面我们得到了任意样本点x到y(x)=0的距离，要做的是最大化这个距离。&lt;br&gt;
同时，要满足条件 &lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-12.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7387&quot; src=&quot;/images/52nlp.cn/27075e97b252ba28a0ddbe0e59d21e79.jpg&quot; alt=&quot;prml7-12&quot; width=&quot;65&quot; height=&quot;22&quot;&gt;&lt;/a&gt;&lt;br&gt;
所以目标函数是:&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-13.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7388&quot; src=&quot;/images/52nlp.cn/ca03bad2ab7e36c084d2fd4cff86ceed.jpg&quot; alt=&quot;prml7-13&quot; width=&quot;242&quot; height=&quot;51&quot;&gt;&lt;/a&gt;&lt;br&gt;
求w和b，使所有样本中，与y=0距离最小的距离 最大化，整个式子就是最小距离最大化&lt;/p&gt;
&lt;p&gt;这个函数优化很复杂，需要做一个转换&lt;/p&gt;
&lt;p&gt;可以看到，对w和b进行缩放 &lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-14.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7389&quot; src=&quot;/images/52nlp.cn/9415b8de671116a2bd53ee247cbac413.jpg&quot; alt=&quot;prml7-14&quot; width=&quot;231&quot; height=&quot;16&quot;&gt;&lt;/a&gt;，距离 &lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-15.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7390&quot; src=&quot;/images/52nlp.cn/9953879babb90d559046baf5ab94eb98.jpg&quot; alt=&quot;prml7-15&quot; width=&quot;66&quot; height=&quot;44&quot;&gt;&lt;/a&gt;并不会变化&lt;br&gt;
根据这个属性，调整w和b,使到决策面最近的点满足：&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-16.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7391&quot; src=&quot;/images/52nlp.cn/87cc33b3d14b495163c72f1cfddd9682.jpg&quot; alt=&quot;prml7-16&quot; width=&quot;150&quot; height=&quot;31&quot;&gt;&lt;/a&gt;&lt;br&gt;
从而左右样本点都满足 &lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-17.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7392&quot; src=&quot;/images/52nlp.cn/46519d67b80ea6bb0f62f90f388ab49b.jpg&quot; alt=&quot;prml7-17&quot; width=&quot;157&quot; height=&quot;26&quot;&gt;&lt;/a&gt;&lt;br&gt;
这样，前面的目标函数可以变为：&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-18.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7393&quot; src=&quot;/images/52nlp.cn/c3b915490c7ceeae753a731329fa59d7.jpg&quot; alt=&quot;prml7-18&quot; width=&quot;200&quot; height=&quot;38&quot;&gt;&lt;/a&gt;&lt;br&gt;
同时满足约束条件：&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-19.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7394&quot; src=&quot;/images/52nlp.cn/8c1512eb3d8806b77c75301c22a0f7c5.jpg&quot; alt=&quot;prml7-19&quot; width=&quot;150&quot; height=&quot;30&quot;&gt;&lt;/a&gt;&lt;br&gt;
这是一个不等式约束的二次规划问题，用拉格朗日乘子法来求解&lt;br&gt;
构造如下的拉格朗日函数：&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-20.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7395&quot; src=&quot;/images/52nlp.cn/1022359da65c76cf2205affcc2927a8f.jpg&quot; alt=&quot;prml7-20&quot; width=&quot;312&quot; height=&quot;46&quot;&gt;&lt;/a&gt;&lt;br&gt;
&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-21.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7397&quot; src=&quot;/images/52nlp.cn/e89b2f43b408c06cafa1a174dcc1e925.jpg&quot; alt=&quot;prml7-21&quot; width=&quot;25&quot; height=&quot;23&quot;&gt;&lt;/a&gt;是拉格朗日乘子 ，这个函数分别对w和b求导，令导数等于0，可以得到w和b的表达式：&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-22.png&quot;&gt;&lt;img class=&quot;aligncenter wp-image-7398&quot; src=&quot;/images/52nlp.cn/f8e89966e267ab1a04b3820677410e2f.jpg&quot; alt=&quot;prml7-22&quot; width=&quot;237&quot; height=&quot;137&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;将w带入前面的拉格朗如函数L(w,b,a)，就可以消去w和b，变成a的函数&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-23.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7399&quot; src=&quot;/images/52nlp.cn/f53659a449efa280275a648b0969cef2.jpg&quot; alt=&quot;prml7-23&quot; width=&quot;35&quot; height=&quot;21&quot;&gt;&lt;/a&gt;，这个函数是拉格朗日函数的对偶函数：&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-24.png&quot;&gt;&lt;img class=&quot;aligncenter wp-image-7400&quot; src=&quot;/images/52nlp.cn/3f9d3931bdfea21d53288a111f08c37d.jpg&quot; alt=&quot;prml7-24&quot; width=&quot;400&quot; height=&quot;60&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;为什么要转换成对偶函数，主要是变形后可以借助核函数，来解决线性不可分的问题，尤其是基函数的维度特别高的情况。求解这个对偶函数，得到参数&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-21.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7397&quot; src=&quot;/images/52nlp.cn/e89b2f43b408c06cafa1a174dcc1e925.jpg&quot; alt=&quot;prml7-21&quot; width=&quot;25&quot; height=&quot;23&quot;&gt;&lt;/a&gt;，就确定了分类模型&lt;br&gt;
把 &lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-25.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7401&quot; src=&quot;/images/52nlp.cn/eb54ddf34ba921b2c8a00e9ceafb3c3c.jpg&quot; alt=&quot;prml7-25&quot; width=&quot;150&quot; height=&quot;45&quot;&gt;&lt;/a&gt; 带入 &lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-26.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7402&quot; src=&quot;/images/52nlp.cn/fc839196371d02c6158bbe4eafd8e2b7.jpg&quot; alt=&quot;prml7-26&quot; width=&quot;200&quot; height=&quot;35&quot;&gt;&lt;/a&gt;，就是用核函数表示的分类模型：&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-27.png&quot;&gt;&lt;img class=&quot;aligncenter wp-image-7403&quot; src=&quot;/images/52nlp.cn/c0583ddaa7807f4397f5480ed5573d5d.jpg&quot; alt=&quot;prml7-27&quot; width=&quot;236&quot; height=&quot;54&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;这就是最终的分类模型，完全由训练样本 &lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-28.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7404&quot; src=&quot;/images/52nlp.cn/22fc880c382e2c140db96790fcead64b.jpg&quot; alt=&quot;prml7-28&quot; width=&quot;22&quot; height=&quot;18&quot;&gt;&lt;/a&gt;，n=1…N决定。&lt;br&gt;
SVM具有稀疏性，这里面对大部分训练样本，&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-29.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7405&quot; src=&quot;/images/52nlp.cn/88189307048654b24e85a33cabdf44a7.jpg&quot; alt=&quot;prml7-29&quot; width=&quot;28&quot; height=&quot;24&quot;&gt;&lt;/a&gt;都等于0，从而大部分样本在新样本预测时都不起作用。&lt;br&gt;
我们来看看为什么大部分训练样本，&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-29.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7405&quot; src=&quot;/images/52nlp.cn/88189307048654b24e85a33cabdf44a7.jpg&quot; alt=&quot;prml7-29&quot; width=&quot;28&quot; height=&quot;24&quot;&gt;&lt;/a&gt;都等于0。这主要是由KKT条件决定的。我们从直观上看下KKT条件是怎么回事：&lt;/p&gt;
&lt;p&gt;KKT是对拉格朗日乘子法的扩展，将其从约束为等式的情况扩展为约束为不等式的情况。所以先看下约束为等式的情况：例如求函数&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-30.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7406&quot; src=&quot;/images/52nlp.cn/c3640ffd1ec7c2073078ff78d971e3b8.jpg&quot; alt=&quot;prml7-30&quot; width=&quot;66&quot; height=&quot;22&quot;&gt;&lt;/a&gt;的极大值，同时满足约束 &lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-31.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7407&quot; src=&quot;/images/52nlp.cn/a07eabf0b88fc6364eb1b1a76193843e.jpg&quot; alt=&quot;prml7-31&quot; width=&quot;78&quot; height=&quot;20&quot;&gt;&lt;/a&gt;，拉格朗日乘子法前面已经介绍，引入拉式乘子，构造拉式函数，然后求导，解除的值就是极值。这里从直观上看一下，为什么这个值就是满足条件的极值。设想取不同的z值，使&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-32.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7408&quot; src=&quot;/images/52nlp.cn/8f5459a9ee72efc02904561a35914af1.jpg&quot; alt=&quot;prml7-32&quot; width=&quot;74&quot; height=&quot;21&quot;&gt;&lt;/a&gt;，就可以得到f(x1,x2)的不同等高线，如图:&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-33.png&quot;&gt;&lt;img class=&quot;aligncenter wp-image-7409&quot; src=&quot;/images/52nlp.cn/63018771852e659a67719d5adeff95d9.jpg&quot; alt=&quot;prml7-33&quot; width=&quot;352&quot; height=&quot;299&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;构成图中的曲线，图中标记的g=c，对于这种情况，改成g-c=0就可以了.假设g与f的某些等高线相交，交点就是同时满足约束条件和目标函数的值，但不一定是极大值。。有两种相交形式，一种是穿过，一种是相切。因为穿过意味着在该条等高线内部还存在着其他等高线与g相交，新等高线与目标函数的交点的值更大。只有相切时，才可能取得最大值。因此，在极大值处，f的梯度与g的梯度是平行的，因为梯度都垂直于g或f曲线，也就是存在lamda，使得 &lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-36.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7411&quot; src=&quot;/images/52nlp.cn/c478796cb2fb8f0c194f28c281a2d46a.jpg&quot; alt=&quot;prml7-36&quot; width=&quot;84&quot; height=&quot;25&quot;&gt;&lt;/a&gt;，这个式子正是拉格朗日函数 &lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-37.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7412&quot; src=&quot;/images/52nlp.cn/d209fb106a0fc35e385536ae708c53d8.jpg&quot; alt=&quot;prml7-37&quot; width=&quot;127&quot; height=&quot;20&quot;&gt;&lt;/a&gt; 对x求导的结果。&lt;br&gt;
接下来看看约束条件为不等式的情况，例如约束为 &lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-38.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7413&quot; src=&quot;/images/52nlp.cn/3c1231e0cbc887bc8672be508765474b.jpg&quot; alt=&quot;prml7-38&quot; width=&quot;60&quot; height=&quot;27&quot;&gt;&lt;/a&gt;，先看个图：&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-39.png&quot;&gt;&lt;img class=&quot;aligncenter wp-image-7414&quot; src=&quot;/images/52nlp.cn/c1305c8cf34c0c2f93fceb97ce9d9646.jpg&quot; alt=&quot;prml7-39&quot; width=&quot;397&quot; height=&quot;304&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;图里的约束是g&amp;lt;0，不影响解释KKT条件。不等式约束分两种情况，假设极值点是x` ，当&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-40.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7415&quot; src=&quot;/images/52nlp.cn/c4bb0c8151a3292020b94669617402e4.jpg&quot; alt=&quot;prml7-40&quot; width=&quot;61&quot; height=&quot;18&quot;&gt;&lt;/a&gt;时，也就是图中左边那部分，此时该约束条件是inactive的，对于极值点的确定不起作用。因此拉格朗日函数&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-37.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7412&quot; src=&quot;/images/52nlp.cn/d209fb106a0fc35e385536ae708c53d8.jpg&quot; alt=&quot;prml7-37&quot; width=&quot;140&quot; height=&quot;22&quot;&gt;&lt;/a&gt;中，lamda等于0，极值完全由f一个人确定，相当于lamda等于0.当&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-42.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7416&quot; src=&quot;/images/52nlp.cn/be527f6fe36ee6a2f5b931c0204a6b1a.jpg&quot; alt=&quot;prml7-42&quot; width=&quot;58&quot; height=&quot;23&quot;&gt;&lt;/a&gt;时，也就是图中右边部分，极值出现在g的边界处,这跟约束条件为等式时是一样的。&lt;br&gt;
总之，对于约束条件为不等式的拉格朗日乘子法，总有 &lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-43.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7417&quot; src=&quot;/images/52nlp.cn/eb3415d9793de6c55328b15cf1d9cfbb.jpg&quot; alt=&quot;prml7-43&quot; width=&quot;77&quot; height=&quot;20&quot;&gt;&lt;/a&gt;，不是lamda等于0，就是g=0&lt;br&gt;
这个结论叫KKT条件，总结起来就是:&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-44.png&quot;&gt;&lt;img class=&quot;aligncenter wp-image-7418&quot; src=&quot;/images/52nlp.cn/b6a979e79f6c4ddf0760e8394d5afd13.jpg&quot; alt=&quot;prml7-44&quot; width=&quot;124&quot; height=&quot;117&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;再返回来看SVM的目标函数构造的拉格朗日函数：&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-45.png&quot;&gt;&lt;img class=&quot;aligncenter wp-image-7419&quot; src=&quot;/images/52nlp.cn/ae4997a1ed1100ba6197111e0c6c87c4.jpg&quot; alt=&quot;prml7-45&quot; width=&quot;400&quot; height=&quot;56&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;根据KKT条件，有 &lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-46.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7420&quot; src=&quot;/images/52nlp.cn/a2966e08f63c2648ae1b58060c6d1757.jpg&quot; alt=&quot;prml7-46&quot; width=&quot;300&quot; height=&quot;27&quot;&gt;&lt;/a&gt;，所以对于&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-47.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7421&quot; src=&quot;/images/52nlp.cn/6fed8fc0a89aa5397e0c277aa0418419.jpg&quot; alt=&quot;prml7-47&quot; width=&quot;58&quot; height=&quot;20&quot;&gt;&lt;/a&gt;大于1的那些样本点，其对应&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-48.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7422&quot; src=&quot;/images/52nlp.cn/09942740e0117eedc43b4f30bf2a2f43.jpg&quot; alt=&quot;prml7-48&quot; width=&quot;38&quot; height=&quot;23&quot;&gt;&lt;/a&gt;的都等于0。只有&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-47.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7421&quot; src=&quot;/images/52nlp.cn/6fed8fc0a89aa5397e0c277aa0418419.jpg&quot; alt=&quot;prml7-47&quot; width=&quot;58&quot; height=&quot;20&quot;&gt;&lt;/a&gt;等于1的那些样本点对保留下来，这些点就是支持向量。&lt;br&gt;
这部分大家有什么意见和问题吗？&lt;/p&gt;
&lt;p&gt;============================讨论===============================&lt;/p&gt;
&lt;p&gt;Fire(564122106) 20:01:56&lt;br&gt;
他为什么要符合KKT条件啊&lt;/p&gt;
&lt;p&gt;网神(66707180) 20:02:33&lt;br&gt;
因为只有符合KKT条件，才能有解，否则拉格朗日函数没解，我的理解是这样的&lt;br&gt;
Fire(564122106) 20:03:54&lt;br&gt;
我上次看到一个版本说只有符合KKT条件    对偶解才和原始解才相同，不知道怎么解释。&lt;br&gt;
kxkr&amp;lt;lxfkxkr@126.com&amp;gt; 20:04:18&lt;br&gt;
貌似统计学习方法 附录里面 讲了这个&lt;br&gt;
Wolf      &amp;lt;wuwja@foxmail.com&amp;gt; 20:04:19&lt;br&gt;
an为0为什么和kkt条件相关&lt;br&gt;
kxkr&amp;lt;lxfkxkr@126.com&amp;gt; 20:04:20&lt;br&gt;
不过忘记了 ，我上次看到一个版本说只有符合KKT条件，对偶解才和原始解相同。&lt;br&gt;
YYKuaiXian(335015891) 20:04:40&lt;br&gt;
Ng的讲义就是用这种说法&lt;br&gt;
苦瓜炒鸡蛋(852383636) 20:04:47&lt;br&gt;
因为大部分的样本都不是sv&lt;/p&gt;
&lt;p&gt;Wolf      &amp;lt;wuwja@foxmail.com&amp;gt; 20:05:05&lt;br&gt;
如果两类正好分布在margin上，那么所有的点都是sv&lt;br&gt;
YYKuaiXian(335015891) 20:05:40&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-60.png&quot;&gt;&lt;img class=&quot;aligncenter wp-image-7424&quot; src=&quot;/images/52nlp.cn/9272b84e4040957000ad940230cba486.jpg&quot; alt=&quot;prml7-60&quot; width=&quot;400&quot; height=&quot;225&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Wolf      &amp;lt;wuwja@foxmail.com&amp;gt; 20:05:54&lt;br&gt;
只有符合kkt条件，primary问题和due问题的解才是一样的，否则胖子里面的瘦子总比瘦子里面的胖子大。&lt;br&gt;
kxkr&amp;lt;lxfkxkr@126.com&amp;gt; 20:07:03&lt;br&gt;
这个比喻 好！&lt;br&gt;
高老头(1316103319) 20:07:08&lt;br&gt;
对偶问题和原问题是什么关系，一个问题怎么找到它的对偶问题？&lt;br&gt;
Wolf      &amp;lt;wuwja@foxmail.com&amp;gt; 20:07:19&lt;br&gt;
所以kkt条件和sv为什么大部分为0没有直接关系，sv为0个人觉得是分界面的性质决定的，分界面是一个低维流形。&lt;br&gt;
Fire(564122106) 20:08:38&lt;/p&gt;
&lt;p&gt;我也感觉sv是和样本数据性质有关的&lt;br&gt;
Wolf      &amp;lt;wuwja@foxmail.com&amp;gt; 20:09:26&lt;br&gt;
比如在二维的时候，分界面是一个线性函数，导致sv比较少，当投影到高维空间，分界面变成了一个超平面，导致sv变多了，另外，很多样本变成sv也是svm慢的一个原因。&lt;br&gt;
网神(66707180) 20:09:37&lt;br&gt;
sv本质上是svm选择的错误函数决定的，在正确一边分类边界以外的样本点，错误为0，在边界以内或在错误一边，错误大于0.&lt;br&gt;
苦瓜炒鸡蛋(852383636) 20:11:04&lt;br&gt;
sv确定的超平面 而非是超平面确定的sv&lt;br&gt;
Wolf      &amp;lt;wuwja@foxmail.com&amp;gt; 20:11:30&lt;br&gt;
sv确定的超平面 而非是超平面确定的sv，一样的，hinge为什么会导致稀疏？什么样的优化问题才有对偶问题，我也在疑问。对于一些规划问题（线性规划，二次规划）可以将求最大值（最小值）的问题转化为求最小值最大值的问题。&lt;/p&gt;
&lt;p&gt;网神(66707180) 20:12:24&lt;br&gt;
kkt是从一个侧面解释稀疏，从另一个侧面，也就是错误函数是hinge函数，也可以得出稀疏的性质。svm跟逻辑回归做对比， hinge损失导致稀疏，我们先讲下这吧，svm的错误函数可以这么写:&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-50.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7425&quot; src=&quot;/images/52nlp.cn/58b400bb1ea52597c9c89e61a0478869.jpg&quot; alt=&quot;prml7-50&quot; width=&quot;224&quot; height=&quot;78&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;其中&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-51.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7426&quot; src=&quot;/images/52nlp.cn/630bda5cc934d8c6bc726f0b6d25d3e6.jpg&quot; alt=&quot;prml7-51&quot; width=&quot;186&quot; height=&quot;31&quot;&gt;&lt;/a&gt;&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-52.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7427&quot; src=&quot;/images/52nlp.cn/87fe05cfda35e5611d09b76659ca408e.jpg&quot; alt=&quot;prml7-52&quot; width=&quot;220&quot; height=&quot;20&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;这就是hinge错误函数，图形如图中的蓝色线&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-54.png&quot;&gt;&lt;img class=&quot;aligncenter wp-image-7428&quot; src=&quot;/images/52nlp.cn/74faa7c7b7c0dcea9390786a5f924e8e.jpg&quot; alt=&quot;prml7-54&quot; width=&quot;300&quot; height=&quot;242&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;而逻辑回归的错误函数是：&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-55.png&quot;&gt;&lt;img class=&quot;alignleft wp-image-7429&quot; src=&quot;/images/52nlp.cn/b16a03e3c24e112ee235c5784ad754a7.jpg&quot; alt=&quot;prml7-55&quot; width=&quot;188&quot; height=&quot;59&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-56.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7430&quot; src=&quot;/images/52nlp.cn/4bcdc1bc37e27ad144ec193a14c5f70f.jpg&quot; alt=&quot;prml7-56&quot; width=&quot;200&quot; height=&quot;27&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;如图中的红色线，红色线跟蓝色线走势相近 ，区别是hinge函数在&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-57.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7431&quot; src=&quot;/images/52nlp.cn/7a24a34c5a297c86825dd42b3a16ec62.jpg&quot; alt=&quot;prml7-57&quot; width=&quot;226&quot; height=&quot;38&quot;&gt;&lt;/a&gt;,图中z&amp;gt;1时，错误等于0，也就是yt&amp;gt;1的那些点都不产生损失. 这个性质可以带来稀疏的解。&lt;/p&gt;
&lt;p&gt;========================讨论结束===============================&lt;/p&gt;
&lt;p&gt;我接着讲了，后面还有挺多内容，刚才说的都是两类训练样本可以完全分开的情况，比如下面这个图，采用了高斯核函数的支持向量机，可以很清楚的看到决策边界，支持向量：&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-61.png&quot;&gt;&lt;img class=&quot;aligncenter wp-image-7433&quot; src=&quot;/images/52nlp.cn/da9c89c51ae9d65242e8864904b59d78.jpg&quot; alt=&quot;prml7-61&quot; width=&quot;400&quot; height=&quot;300&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;但实际中两类数据的分布会有重叠的情况，另外也有噪音的存在，导致两类训练数据如果一定要完全分开，泛化性能会很差。因此svm引入一些机制，允许训练时一些样本被误分类.我们要修改目标函数，允许样本点位于错误的一边，但会增加一个惩罚项，其大小随着数据点到边界的距离而增大这个惩罚项叫松弛变量, slack variables，记为 &lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-62.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7434&quot; src=&quot;/images/52nlp.cn/14e411765a20156ddf4f7c929c63b25c.jpg&quot; alt=&quot;prml7-62&quot; width=&quot;25&quot; height=&quot;26&quot;&gt;&lt;/a&gt;，并且大于等于0.其中下标n=1,..,N，也就是每个训练样本对应一个&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-62.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7434&quot; src=&quot;/images/52nlp.cn/14e411765a20156ddf4f7c929c63b25c.jpg&quot; alt=&quot;prml7-62&quot; width=&quot;25&quot; height=&quot;26&quot;&gt;&lt;/a&gt;，对于位于正确的margin边界上或以内的数据点，其松弛变量 &lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-65.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7435&quot; src=&quot;/images/52nlp.cn/e498d11c1fb025eaca5b81820ecab2f3.jpg&quot; alt=&quot;prml7-65&quot; width=&quot;47&quot; height=&quot;21&quot;&gt;&lt;/a&gt;，其他样本点&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-68.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7436&quot; src=&quot;/images/52nlp.cn/6f7b57de480057c2b81ff08e10ee296c.jpg&quot; alt=&quot;prml7-68&quot; width=&quot;95&quot; height=&quot;20&quot;&gt;&lt;/a&gt;&lt;br&gt;
这样，如果样本点位于决策边界y(x)=0上, &lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-69.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7437&quot; src=&quot;/images/52nlp.cn/a395e373c005b5ea620d8bc96d54c6eb.jpg&quot; alt=&quot;prml7-69&quot; width=&quot;48&quot; height=&quot;21&quot;&gt;&lt;/a&gt;&lt;br&gt;
如果被错分，位于错误的一边, &lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-70.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7438&quot; src=&quot;/images/52nlp.cn/f4b5b7034c1467026c986e34910cb0bf.jpg&quot; alt=&quot;prml7-70&quot; width=&quot;34&quot; height=&quot;20&quot;&gt;&lt;/a&gt; ，因此目标函数的限制条件由&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-71.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7439&quot; src=&quot;/images/52nlp.cn/841479b84e99082d0a673ab4670e1eee.jpg&quot; alt=&quot;prml7-71&quot; width=&quot;136&quot; height=&quot;19&quot;&gt;&lt;/a&gt;修改为&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-73.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7440&quot; src=&quot;/images/52nlp.cn/337ff86f6a77f287bfc2db57b98c577c.jpg&quot; alt=&quot;prml7-73&quot; width=&quot;130&quot; height=&quot;20&quot;&gt;&lt;/a&gt;，目标函数修从最小化&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-751.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7441&quot; src=&quot;/images/52nlp.cn/a7187b53eae4a460bb71ed0c9760ccb2.jpg&quot; alt=&quot;prml7-75&quot; width=&quot;46&quot; height=&quot;36&quot;&gt;&lt;/a&gt;改为最小化 &lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-76.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7442&quot; src=&quot;/images/52nlp.cn/ccdc27763fe43af22fa927e0f2ed67d1.jpg&quot; alt=&quot;prml7-76&quot; width=&quot;100&quot; height=&quot;39&quot;&gt;&lt;/a&gt;，其中参数C用于控制松弛变量和margin之间的trade-off，因为对于错分的点，有&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-77.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7443&quot; src=&quot;/images/52nlp.cn/24f1d2c88b68dfd9c5d5b69e98887bfe.jpg&quot; alt=&quot;prml7-77&quot; width=&quot;45&quot; height=&quot;25&quot;&gt;&lt;/a&gt; ，所以&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-771.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7444&quot; src=&quot;/images/52nlp.cn/8f6c678cbf360d8f3de03aa192ded234.jpg&quot; alt=&quot;prml7-77&quot; width=&quot;45&quot; height=&quot;25&quot;&gt;&lt;/a&gt;是错分样本数的一个上限upper bound ，所以C相当于一个正则稀疏，控制着最小错分数和模型复杂度的trade-off.&lt;br&gt;
SVM在实际使用中，需要调整的参数很少，C是其中之一。&lt;br&gt;
看这个目标函数，可以看到，C越大，松弛变量就越倍惩罚，就会训练出越复杂的模型，来保证尽量少的样本被错分。当C趋于无穷时，每个样本点就会被模型正确分类。&lt;br&gt;
我们现在求解这个新的目标函数，加上约束条件，拉格朗日函数如下：&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-80.png&quot;&gt;&lt;img class=&quot;aligncenter wp-image-7445&quot; src=&quot;/images/52nlp.cn/222c43ac387bc14efed52cdbd5ce444d.jpg&quot; alt=&quot;prml7-80&quot; width=&quot;400&quot; height=&quot;46&quot;&gt;&lt;/a&gt;&lt;br&gt;
其中&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-81.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7446&quot; src=&quot;/images/52nlp.cn/2437b16f4196b5745efcd50bad44be3d.jpg&quot; alt=&quot;prml7-81&quot; width=&quot;22&quot; height=&quot;24&quot;&gt;&lt;/a&gt;和&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-82.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7447&quot; src=&quot;/images/52nlp.cn/0d6e5a883e8476f677f5d6ec2cfa44ab.jpg&quot; alt=&quot;prml7-82&quot; width=&quot;24&quot; height=&quot;24&quot;&gt;&lt;/a&gt;是拉式乘子，分别对w, b和{&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-83.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7448&quot; src=&quot;/images/52nlp.cn/483f19b23089cfbe2af9a2b253d634e9.jpg&quot; alt=&quot;prml7-83&quot; width=&quot;15&quot; height=&quot;21&quot;&gt;&lt;/a&gt;}求导，令导数等于0，得到w, b,&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-83.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7448&quot; src=&quot;/images/52nlp.cn/483f19b23089cfbe2af9a2b253d634e9.jpg&quot; alt=&quot;prml7-83&quot; width=&quot;18&quot; height=&quot;25&quot;&gt;&lt;/a&gt;的表示，带入L(w,b,a)，消去这些变量，得到以拉格朗日乘子为变量的对偶函数：&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-84.png&quot;&gt;&lt;img class=&quot;aligncenter wp-image-7449&quot; src=&quot;/images/52nlp.cn/07f3f2c0abd9376e0874389d3300122b.jpg&quot; alt=&quot;prml7-84&quot; width=&quot;350&quot; height=&quot;62&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;新的对偶函数跟前面的对偶函数形式相同，只有约束条件有不同。 这就是正则化的SVM。&lt;br&gt;
接下来提一下对偶函数的解法，对偶函数都是二次函数，而且是凸函数，这是svm的优势，具有全局最优解，该二次规划问题的求解难度是参数&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-81.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7446&quot; src=&quot;/images/52nlp.cn/2437b16f4196b5745efcd50bad44be3d.jpg&quot; alt=&quot;prml7-81&quot; width=&quot;22&quot; height=&quot;24&quot;&gt;&lt;/a&gt;的数量很大，等于训练样本的数量 。书上回顾了一些方法，介绍不详细，主要思想是chunking，我总结一下，总结的不一定准确:&lt;br&gt;
1.去掉&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-90.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7451&quot; src=&quot;/images/52nlp.cn/8dcc0da742e7d1767a9f8928d7d52615.jpg&quot; alt=&quot;prml7-90&quot; width=&quot;22&quot; height=&quot;28&quot;&gt;&lt;/a&gt;=0对应的核函数矩阵的行和列，将二次优化问题划分成多个小的优化问题；&lt;br&gt;
2.按固定大小划分成小的优化问题。&lt;br&gt;
3.SVM中最流行的是SMO, sequentialminimal optimization。每次只考虑两个拉格朗日乘子.&lt;br&gt;
SVM中维度灾难问题：核函数相当于高维(甚至无线维)的特征空间的内积，避免了显示的高维空间运算，貌似是避免了维度过高引起的维度灾难问题。 但实际上并没有避免。书上举了个例子 ，看这个二维多项式核函数：&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-91.png&quot;&gt;&lt;img class=&quot;aligncenter wp-image-7452&quot; src=&quot;/images/52nlp.cn/db43e9e78826635b7b4bf17d027b8aa4.jpg&quot; alt=&quot;prml7-91&quot; width=&quot;481&quot; height=&quot;88&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;这个核函数表示一个六维空间的内积。&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-92.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7453&quot; src=&quot;/images/52nlp.cn/823fa005a94902864ec3bddb104c6a3d.jpg&quot; alt=&quot;prml7-92&quot; width=&quot;46&quot; height=&quot;21&quot;&gt;&lt;/a&gt;是从输入空间到六维空间的映射.映射后，六个维度每个维度的值是由固定参数的，也就是映射后，六维特征是有固定的形式。因此，原二维数据x都被限制到了六维空间的一个nonlinear manifold中。这个manifold之外就没有数据.&lt;br&gt;
网神(66707180) 20:48:59&lt;br&gt;
大家有什么问题吗？&lt;br&gt;
高老头(1316103319) 20:49:58&lt;br&gt;
manifold是什么意思？&lt;br&gt;
网神(66707180) 20:50:26&lt;br&gt;
我的理解是空间里一个特定的区域，原空间的数据，如果采样不够均匀，映射后的空间，仍然不会均匀，不会被打散到空间的各个角落，而只会聚集在某个区域。&lt;/p&gt;
&lt;p&gt;接下来讲下SVM用于回归问题.&lt;/p&gt;
&lt;p&gt;在线性回归中，一个正则化错误函数如下：&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-93.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7454&quot; src=&quot;/images/52nlp.cn/12fae1e7969dab323d1e2146f4019498.jpg&quot; alt=&quot;prml7-93&quot; width=&quot;169&quot; height=&quot;47&quot;&gt;&lt;/a&gt;&lt;br&gt;
为了获得稀疏解，将前面的二次错误函数用&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-94.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7455&quot; src=&quot;/images/52nlp.cn/d542f7cab050485e3b45266d978885ad.jpg&quot; alt=&quot;prml7-94&quot; width=&quot;90&quot; height=&quot;14&quot;&gt;&lt;/a&gt;错误函数代替&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-95.png&quot;&gt;&lt;img class=&quot;aligncenter wp-image-7456&quot; src=&quot;/images/52nlp.cn/14493f70a42eb3fb8eebbee4c14e6f60.jpg&quot; alt=&quot;prml7-95&quot; width=&quot;347&quot; height=&quot;44&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;这个错误函数在y(x)和t的差小于时等于0.错误函数变为:&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-97.png&quot;&gt;&lt;img class=&quot;aligncenter wp-image-7457&quot; src=&quot;/images/52nlp.cn/658d5d59dcd734a6e34c685440ed3b73.jpg&quot; alt=&quot;prml7-97&quot; width=&quot;234&quot; height=&quot;58&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;我们再引入松弛变量，对每个样本，有两个松弛变量，分别对应 &lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-98.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7458&quot; src=&quot;/images/52nlp.cn/988e4963a79f026fa496dc9183924a45.jpg&quot; alt=&quot;prml7-98&quot; width=&quot;180&quot; height=&quot;21&quot;&gt;&lt;/a&gt;&lt;br&gt;
如图:&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-99.png&quot;&gt;&lt;img class=&quot;aligncenter wp-image-7459&quot; src=&quot;/images/52nlp.cn/558c4d2ada4b168fea3b29235b5cf50b.jpg&quot; alt=&quot;prml7-99&quot; width=&quot;400&quot; height=&quot;279&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;没引入松弛变量前，样本值t预测正确的条件是&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-100.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7461&quot; src=&quot;/images/52nlp.cn/e1eca48537e2c625765782fc0ce6a0bb.jpg&quot; alt=&quot;prml7-100&quot; width=&quot;120&quot; height=&quot;17&quot;&gt;&lt;/a&gt;&lt;br&gt;
引入松弛变量后，变为:&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-101.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7462&quot; src=&quot;/images/52nlp.cn/619cb53c9553b67259feba85fea72cc7.jpg&quot; alt=&quot;prml7-101&quot; width=&quot;168&quot; height=&quot;49&quot;&gt;&lt;/a&gt;&lt;br&gt;
错误函数变为：&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-102.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7463&quot; src=&quot;/images/52nlp.cn/f5671fb35dc230cd86ba5023c234c1ea.jpg&quot; alt=&quot;prml7-102&quot; width=&quot;216&quot; height=&quot;63&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;加上约束条件&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-103.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7464&quot; src=&quot;/images/52nlp.cn/21616a41882cf5e1603edffe22e031b1.jpg&quot; alt=&quot;prml7-103&quot; width=&quot;99&quot; height=&quot;21&quot;&gt;&lt;/a&gt;，就可以写出拉格朗日函数&lt;br&gt;
下面就跟前面的分类一样了.&lt;br&gt;
关于统计学习理论，书上简单提了一下PAC(probably approximately correct)和VC维，简单总结一下书上的内容：PAC的目的是理解多大的数据集可以给出好的泛化性，以及研究损失的上限。PAC里的一个关键概念是VC维，用于提供一个函数空间复杂度的度量，将PAC理论推广到了无限大的函数空间上。&lt;/p&gt;
&lt;p&gt;============================讨论===============================&lt;/p&gt;
&lt;p&gt;Fire(564122106) 21:05:56&lt;br&gt;
有哪位大神想过对svm提速的啊，svm在非线性大数据的情况下，速度还是比较慢的啊&lt;br&gt;
网神(66707180) 21:07:36&lt;br&gt;
svm分布式训练的方案研究过吗？&lt;br&gt;
Fire(564122106) 21:09:29&lt;br&gt;
没有，不过将来肯定要研究的！现在只是单机，现在有在单机的情况下，分布式进入内存的方案，有兴趣的可以看下：Selective Block Minimization for Faster Convergence of Limited Memory Large_Scale Linear Models 这个有介绍，我共享下啊。&lt;br&gt;
苦瓜炒鸡蛋(852383636) 21:11:05&lt;br&gt;
韩家炜的一个学生 提出了一个  仿照层次聚类的思想  改进的svm 速度好像挺快的&lt;/p&gt;
&lt;p&gt;Making SVMs Scalable to Large Data Sets using Hierarchical Cluster Indexing    这个就是那篇论文的题目  发在  Data Mining and Knowledge Discovery&lt;/p&gt;
&lt;p&gt;Fire(564122106) 21:16:29&lt;br&gt;
哦 我看下，我现在看的都是台湾林的&lt;br&gt;
Fire 分享文件 21:14:33&lt;br&gt;
“Selective Block Minimization for Faster Convergence of Limited Memory Large_Scale Linear Models.pdf” 下载&lt;br&gt;
苦瓜炒鸡蛋(852383636) 21:17:41&lt;br&gt;
有那个大神 在用svm做聚类，Support Vector Clustering   这篇能做  就是时间复杂度太高了&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-105.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7465&quot; src=&quot;/images/52nlp.cn/8b00c12ec4ef95c9a0f7c1bf57960f17.jpg&quot; alt=&quot;prml7-105&quot; width=&quot;47&quot; height=&quot;28&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;========================讨论结束===============================&lt;/p&gt;
&lt;p&gt;网神(66707180) 8:54:01&lt;br&gt;
咱们开始讲RVM，前面讲了SVM，SVM有一些缺点，比如输出是decision而不是概率分布，SVM是为二元分类设计的，多类别分类不太实用，虽然有不少策略可以用于多元分类，但也各有问题参数C需要人工选择，通过多次训练来调整，感觉实际应用中这些缺点不算什么大缺点，但是RVM可以避免这些缺点。&lt;br&gt;
RVM是一种贝叶斯方式的稀疏核方法，可以用于回归和分类，除了避免SVM的主要缺点，还可以更稀疏，而泛化能力不会降低 。先看RVM回归，RVM回归的模型跟前面第三章形式相同，属于线性模型，但是参数w的先验分布有所不同。&lt;br&gt;
这个不同导致了稀疏性，等下再看这个不同&lt;br&gt;
线性回归模型如下：&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-110.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7467&quot; src=&quot;/images/52nlp.cn/f523f266a47763c867e9e792251615b5.jpg&quot; alt=&quot;prml7-110&quot; width=&quot;252&quot; height=&quot;30&quot;&gt;&lt;/a&gt;&lt;br&gt;
其中&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-111.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7468&quot; src=&quot;/images/52nlp.cn/76259bb20147c8220dfa577ab2669797.jpg&quot; alt=&quot;prml7-111&quot; width=&quot;45&quot; height=&quot;22&quot;&gt;&lt;/a&gt;，是噪音的精度precision&lt;br&gt;
均值y(x)定义为：&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-112.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7469&quot; src=&quot;/images/52nlp.cn/f70baa9a0b60452b1fb9de7611e9a48d.jpg&quot; alt=&quot;prml7-112&quot; width=&quot;200&quot; height=&quot;46&quot;&gt;&lt;/a&gt;&lt;br&gt;
RVM作为一种稀疏核方法，它是如何跟核函数搭上边的，就是基函数 &lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-113.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7470&quot; src=&quot;/images/52nlp.cn/70dabe8eb195b3b1f4408ff65be9958d.jpg&quot; alt=&quot;prml7-113&quot; width=&quot;45&quot; height=&quot;22&quot;&gt;&lt;/a&gt;采用了核函数的形式&lt;br&gt;
每个核与一个训练样本对应，也就是：&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-115.png&quot;&gt;&lt;img class=&quot;aligncenter wp-image-7471&quot; src=&quot;/images/52nlp.cn/882e36bb526824c0ceaa8932563f40f3.jpg&quot; alt=&quot;prml7-115&quot; width=&quot;233&quot; height=&quot;63&quot;&gt;&lt;/a&gt;&lt;br&gt;
这个形式跟SVM用于回归的模型形式是相同的，看前面的式子(7.64)最后求得的SVM回归模型是：&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-116.png&quot;&gt;&lt;img class=&quot;aligncenter wp-image-7472&quot; src=&quot;/images/52nlp.cn/03d56aff60da5c23e3b63127c0900db0.jpg&quot; alt=&quot;prml7-116&quot; width=&quot;265&quot; height=&quot;56&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;可以看到，RVM回归和SVM回归模型相同，只是前面的系数从&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-117.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7473&quot; src=&quot;/images/52nlp.cn/e4f1a415ede76c01809c1cc27cf928e4.jpg&quot; alt=&quot;prml7-117&quot; width=&quot;87&quot; height=&quot;23&quot;&gt;&lt;/a&gt;，接下来分析如何确定RVM模型中的参数w，下面的分析过程跟任何基函数都适用，不限于核函数。&lt;br&gt;
确定w的过程可以总结为：先假设w的先验分布，一般是高斯分布；然后给出似然函数，先验跟似然函数相乘的到w的后验分布，最大化后验分布，得到参数w 。&lt;br&gt;
先看w的先验，w的先验是以0为均值，以&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-118.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7474&quot; src=&quot;/images/52nlp.cn/ba0eee69afc7e38998e601db63213782.jpg&quot; alt=&quot;prml7-118&quot; width=&quot;18&quot; height=&quot;18&quot;&gt;&lt;/a&gt;为精度的高斯分布，但是跟第三章线性回归的区别是，RVM为每个&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-119.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7475&quot; src=&quot;/images/52nlp.cn/087b5fe54ee0ecc2faec223bfc462c56.jpg&quot; alt=&quot;prml7-119&quot; width=&quot;29&quot; height=&quot;23&quot;&gt;&lt;/a&gt;分别引入一个精度&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-120.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7476&quot; src=&quot;/images/52nlp.cn/0f16ebb822624ae14bbf7efe8bcbcc83.jpg&quot; alt=&quot;prml7-120&quot; width=&quot;25&quot; height=&quot;20&quot;&gt;&lt;/a&gt;，而不是&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-119.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7475&quot; src=&quot;/images/52nlp.cn/087b5fe54ee0ecc2faec223bfc462c56.jpg&quot; alt=&quot;prml7-119&quot; width=&quot;29&quot; height=&quot;23&quot;&gt;&lt;/a&gt;所有用一个的共享的精度&lt;br&gt;
所以w的先验是：&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-121.png&quot;&gt;&lt;img class=&quot;aligncenter wp-image-7478&quot; src=&quot;/images/52nlp.cn/b84f19aa02a01d9c1749cf6223f4d591.jpg&quot; alt=&quot;prml7-121&quot; width=&quot;200&quot; height=&quot;44&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;对于线性回归模型，根据这个先验和似然函数可以得到其后验分布：&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-122.png&quot;&gt;&lt;img class=&quot;aligncenter wp-image-7479&quot; src=&quot;/images/52nlp.cn/13c37468ba3ae5e1a73640e6ed23e451.jpg&quot; alt=&quot;prml7-122&quot; width=&quot;261&quot; height=&quot;29&quot;&gt;&lt;/a&gt;&lt;br&gt;
均值和方差分别是：&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-130.png&quot;&gt;&lt;img class=&quot;aligncenter wp-image-7480&quot; src=&quot;/images/52nlp.cn/f2653512e01330750b7d07214aa8d633.jpg&quot; alt=&quot;prml7-130&quot; width=&quot;200&quot; height=&quot;58&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;这是第三章的结论，推导过程就不说了&lt;br&gt;
其中&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-132.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7481&quot; src=&quot;/images/52nlp.cn/ffd96b0166c47644ea2b087bf19152c7.jpg&quot; alt=&quot;prml7-132&quot; width=&quot;22&quot; height=&quot;20&quot;&gt;&lt;/a&gt;是NxM的矩阵，&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-134.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7482&quot; src=&quot;/images/52nlp.cn/627003b183a65c22431525dfd8d07444.jpg&quot; alt=&quot;prml7-134&quot; width=&quot;100&quot; height=&quot;25&quot;&gt;&lt;/a&gt;，A是对角矩阵&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-135.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7483&quot; src=&quot;/images/52nlp.cn/0a055de07c3b81d693fc8315d73df92c.jpg&quot; alt=&quot;prml7-135&quot; width=&quot;100&quot; height=&quot;21&quot;&gt;&lt;/a&gt;&lt;br&gt;
对于RVM，因为基函数是核函数，所以&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-136.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7484&quot; src=&quot;/images/52nlp.cn/3963b0edc13de5261b43bd01d589e212.jpg&quot; alt=&quot;prml7-136&quot; width=&quot;49&quot; height=&quot;20&quot;&gt;&lt;/a&gt;，K是NxN维的核矩阵，其元素是&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-137.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7485&quot; src=&quot;/images/52nlp.cn/956c79933e2575a0800b77c694e38cf2.jpg&quot; alt=&quot;prml7-137&quot; width=&quot;71&quot; height=&quot;25&quot;&gt;&lt;/a&gt;&lt;br&gt;
接下来需要确定超参数&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-1381.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7487&quot; src=&quot;/images/52nlp.cn/119817f3aecefed48c1e2967a3733314.jpg&quot; alt=&quot;prml7-138&quot; width=&quot;25&quot; height=&quot;20&quot;&gt;&lt;/a&gt;和&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-139.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7488&quot; src=&quot;/images/52nlp.cn/a080a04d50fbfbe08d27bfaccbdd8fbf.jpg&quot; alt=&quot;prml7-139&quot; width=&quot;24&quot; height=&quot;22&quot;&gt;&lt;/a&gt;。一个是w先验的精度，一个是线性模型p(t|x,w)的精度&lt;br&gt;
确定的方法叫做evidence approximation方法，又叫type-2 maximum likelihood，这在第三章有详细介绍，这里简单说一下思路：&lt;br&gt;
该方法基于一个假设，即两个参数是的后验分布&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-140.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7489&quot; src=&quot;/images/52nlp.cn/3ce874a70da30237da1e1f04958dbfc3.jpg&quot; alt=&quot;prml7-140&quot; width=&quot;67&quot; height=&quot;16&quot;&gt;&lt;/a&gt;是sharply peaked的 ，其中心值是&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-141.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7490&quot; src=&quot;/images/52nlp.cn/01ab2d9a82b3439cc604d3c3ba504d98.jpg&quot; alt=&quot;prml7-141&quot; width=&quot;56&quot; height=&quot;19&quot;&gt;&lt;/a&gt; ，根据贝叶斯定理，&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-145.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7491&quot; src=&quot;/images/52nlp.cn/36988089e341589716c18197a4e9b6fc.jpg&quot; alt=&quot;prml7-145&quot; width=&quot;200&quot; height=&quot;22&quot;&gt;&lt;/a&gt;，先验&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-146.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7492&quot; src=&quot;/images/52nlp.cn/81e9bf85606d52d5e498f5fd0309a224.jpg&quot; alt=&quot;prml7-146&quot; width=&quot;54&quot; height=&quot;17&quot;&gt;&lt;/a&gt;是relatively flat的，所以只要看&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-147.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7493&quot; src=&quot;/images/52nlp.cn/24c1df957dfec2c2a22e77217a53aca5.jpg&quot; alt=&quot;prml7-147&quot; width=&quot;60&quot; height=&quot;18&quot;&gt;&lt;/a&gt;，&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-141.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7490&quot; src=&quot;/images/52nlp.cn/01ab2d9a82b3439cc604d3c3ba504d98.jpg&quot; alt=&quot;prml7-141&quot; width=&quot;56&quot; height=&quot;19&quot;&gt;&lt;/a&gt;就是使的&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-147.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7493&quot; src=&quot;/images/52nlp.cn/24c1df957dfec2c2a22e77217a53aca5.jpg&quot; alt=&quot;prml7-147&quot; width=&quot;60&quot; height=&quot;18&quot;&gt;&lt;/a&gt;最大的值。&lt;br&gt;
&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-147.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7493&quot; src=&quot;/images/52nlp.cn/24c1df957dfec2c2a22e77217a53aca5.jpg&quot; alt=&quot;prml7-147&quot; width=&quot;60&quot; height=&quot;18&quot;&gt;&lt;/a&gt;是对w进行积分的边界分布：&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-149.png&quot;&gt;&lt;img class=&quot;aligncenter wp-image-7494&quot; src=&quot;/images/52nlp.cn/c31a6ca8846b2858059c7d668e338b8f.jpg&quot; alt=&quot;prml7-149&quot; width=&quot;300&quot; height=&quot;44&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;这个分布是两个高斯分布的卷积，其log最大似然函数是:&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-150.png&quot;&gt;&lt;img class=&quot;aligncenter wp-image-7495&quot; src=&quot;/images/52nlp.cn/6fc7a490aa3d156b7b6e02af6f953689.jpg&quot; alt=&quot;prml7-150&quot; width=&quot;400&quot; height=&quot;61&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;其中C是NxN矩阵，&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-151.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7496&quot; src=&quot;/images/52nlp.cn/ae86b2f1e612df3c802e51e360c75184.jpg&quot; alt=&quot;prml7-151&quot; width=&quot;120&quot; height=&quot;20&quot;&gt;&lt;/a&gt;&lt;br&gt;
&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-153.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7497&quot; src=&quot;/images/52nlp.cn/86529881251ba3be0ed87737638dd0fb.jpg&quot; alt=&quot;prml7-153&quot; width=&quot;184&quot; height=&quot;22&quot;&gt;&lt;/a&gt;这一步，以及C的值，是第三章的内容，大家看前面吧。&lt;br&gt;
我们可以通过最大化似然函数，求得&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-141.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7490&quot; src=&quot;/images/52nlp.cn/01ab2d9a82b3439cc604d3c3ba504d98.jpg&quot; alt=&quot;prml7-141&quot; width=&quot;56&quot; height=&quot;19&quot;&gt;&lt;/a&gt;，书上提到了两种方法，一种是EM，一种是直接求导迭代。前者第九章尼采已经讲了，这里看下后者。&lt;br&gt;
首先我们分别求这个log似然函数对所有参数&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-1381.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7487&quot; src=&quot;/images/52nlp.cn/119817f3aecefed48c1e2967a3733314.jpg&quot; alt=&quot;prml7-138&quot; width=&quot;25&quot; height=&quot;20&quot;&gt;&lt;/a&gt;和&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-139.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7488&quot; src=&quot;/images/52nlp.cn/a080a04d50fbfbe08d27bfaccbdd8fbf.jpg&quot; alt=&quot;prml7-139&quot; width=&quot;25&quot; height=&quot;23&quot;&gt;&lt;/a&gt;求偏导，并令偏导等于0，求得参数的表达式：&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-155.png&quot;&gt;&lt;img class=&quot;aligncenter wp-image-7498&quot; src=&quot;/images/52nlp.cn/8faf55d76c0fce515ecaa4a95a424c49.jpg&quot; alt=&quot;prml7-155&quot; width=&quot;352&quot; height=&quot;148&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;其中&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-159.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7499&quot; src=&quot;/images/52nlp.cn/bb4e0796cb82f0954e0d4b013b7ab4e7.jpg&quot; alt=&quot;prml7-159&quot; width=&quot;30&quot; height=&quot;15&quot;&gt;&lt;/a&gt;是w的后验均值m的第i个元素,&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-160.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7500&quot; src=&quot;/images/52nlp.cn/ce72b635cd9a1f7768926bb94d22f463.jpg&quot; alt=&quot;prml7-160&quot; width=&quot;20&quot; height=&quot;20&quot;&gt;&lt;/a&gt;是度量&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-161.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7503&quot; src=&quot;/images/52nlp.cn/9cfaeb20e6da4864e4feb584c969f993.jpg&quot; alt=&quot;prml7-161&quot; width=&quot;25&quot; height=&quot;22&quot;&gt;&lt;/a&gt;被样本集合影响的程度&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-162.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7501&quot; src=&quot;/images/52nlp.cn/ad588c9949a2169d6b30247144be272c.jpg&quot; alt=&quot;prml7-162&quot; width=&quot;120&quot; height=&quot;21&quot;&gt;&lt;/a&gt;&lt;br&gt;
&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-163.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7502&quot; src=&quot;/images/52nlp.cn/aa7bcc00d0e4dd971e72b53aed127fd4.jpg&quot; alt=&quot;prml7-163&quot; width=&quot;30&quot; height=&quot;19&quot;&gt;&lt;/a&gt;是w的后验方差&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-164.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7504&quot; src=&quot;/images/52nlp.cn/afab5e15f0ba7149697a3a301b4db262.jpg&quot; alt=&quot;prml7-164&quot; width=&quot;20&quot; height=&quot;23&quot;&gt;&lt;/a&gt;的对角线上的元素。&lt;br&gt;
求&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-141.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7490&quot; src=&quot;/images/52nlp.cn/01ab2d9a82b3439cc604d3c3ba504d98.jpg&quot; alt=&quot;prml7-141&quot; width=&quot;56&quot; height=&quot;19&quot;&gt;&lt;/a&gt;是一个迭代的过程：&lt;br&gt;
先选一个&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-141.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7490&quot; src=&quot;/images/52nlp.cn/01ab2d9a82b3439cc604d3c3ba504d98.jpg&quot; alt=&quot;prml7-141&quot; width=&quot;56&quot; height=&quot;19&quot;&gt;&lt;/a&gt;的初值，然后用下面这个公式得到后验的均值和方差：&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-171.png&quot;&gt;&lt;img class=&quot;aligncenter wp-image-7506&quot; src=&quot;/images/52nlp.cn/e2885e40bb51bf30f5ecdb671bed9517.jpg&quot; alt=&quot;prml7-171&quot; width=&quot;239&quot; height=&quot;70&quot;&gt;&lt;/a&gt;&lt;br&gt;
然后再同这个这个公式重新计算&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-141.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7490&quot; src=&quot;/images/52nlp.cn/01ab2d9a82b3439cc604d3c3ba504d98.jpg&quot; alt=&quot;prml7-141&quot; width=&quot;76&quot; height=&quot;25&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-175.png&quot;&gt;&lt;img class=&quot;aligncenter wp-image-7507&quot; src=&quot;/images/52nlp.cn/5a4a3bc4908f24d819384c8661e9f036.jpg&quot; alt=&quot;prml7-175&quot; width=&quot;300&quot; height=&quot;126&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;这样迭代计算，一直到到达一个人为确定的收敛条件，这就是确定&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-141.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7490&quot; src=&quot;/images/52nlp.cn/01ab2d9a82b3439cc604d3c3ba504d98.jpg&quot; alt=&quot;prml7-141&quot; width=&quot;56&quot; height=&quot;19&quot;&gt;&lt;/a&gt;的过程。&lt;br&gt;
通过计算，最后的结果中，大部分参数&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-177.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7508&quot; src=&quot;/images/52nlp.cn/b30b11aae574a5d38603ff2c65b888b6.jpg&quot; alt=&quot;prml7-177&quot; width=&quot;35&quot; height=&quot;24&quot;&gt;&lt;/a&gt;都是非常大甚至无穷大的值，从而根据w后验均值和方差的公式，其均值和方差都等于0，这样&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-178.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7509&quot; src=&quot;/images/52nlp.cn/675d3088d84f2995d9e3078543f689de.jpg&quot; alt=&quot;prml7-178&quot; width=&quot;28&quot; height=&quot;23&quot;&gt;&lt;/a&gt;的值就是0，其对应的基函数就不起作用了 ，从而达到了稀疏的目的。这就是RVM稀疏的原因。&lt;br&gt;
需要实际推导一下整个过程，才能明白为什么大部分&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-177.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7508&quot; src=&quot;/images/52nlp.cn/b30b11aae574a5d38603ff2c65b888b6.jpg&quot; alt=&quot;prml7-177&quot; width=&quot;35&quot; height=&quot;24&quot;&gt;&lt;/a&gt;都趋于无穷大。那些&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-1781.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7510&quot; src=&quot;/images/52nlp.cn/b7d8242b385352c42bbba280199fe7a9.jpg&quot; alt=&quot;prml7-178&quot; width=&quot;28&quot; height=&quot;23&quot;&gt;&lt;/a&gt;不为0的&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-180.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7511&quot; src=&quot;/images/52nlp.cn/9498be8074018ba224b5a529ccb73c62.jpg&quot; alt=&quot;prml7-180&quot; width=&quot;25&quot; height=&quot;22&quot;&gt;&lt;/a&gt;叫做relevance vectors，相当于SVM中的支持向量。需要强调，这种获得稀疏性的机制可以用于任何基函数的线性组合中。这种获得稀疏性的机制似乎非常普遍的。&lt;br&gt;
求得超参数，就可以通过下面式子得到新样本的分布：&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-181.png&quot;&gt;&lt;img class=&quot;aligncenter wp-image-7512&quot; src=&quot;/images/52nlp.cn/9108931dd540c9a70425bfe4b97bdee5.jpg&quot; alt=&quot;prml7-181&quot; width=&quot;400&quot; height=&quot;50&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;下面看一个图示：&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-182.png&quot;&gt;&lt;img class=&quot;aligncenter wp-image-7513&quot; src=&quot;/images/52nlp.cn/43915b79038ff79d36864219d57fa7a0.jpg&quot; alt=&quot;prml7-182&quot; width=&quot;350&quot; height=&quot;255&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;可以看到，其相关向量的数量比SVM少了很多，跟SVM相比的缺点是，RVM的优化函数不是凸函数，训练时间比SVM长，书上接下来专门对RVM的稀疏性进行分析，并且介绍了一种更快的求&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-141.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7490&quot; src=&quot;/images/52nlp.cn/01ab2d9a82b3439cc604d3c3ba504d98.jpg&quot; alt=&quot;prml7-141&quot; width=&quot;56&quot; height=&quot;19&quot;&gt;&lt;/a&gt;的方法：&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-185.png&quot;&gt;&lt;img class=&quot;wp-image-7514 aligncenter&quot; src=&quot;/images/52nlp.cn/452b8b8bd8be4712b220ef4bc4ac6707.jpg&quot; alt=&quot;prml7-185&quot; width=&quot;489&quot; height=&quot;273&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;我接着讲RVM分类，我们看逻辑回归分类的模型:&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-189.png&quot;&gt;&lt;img class=&quot;aligncenter wp-image-7516&quot; src=&quot;/images/52nlp.cn/903b349deaf785d0a72b68b53467f7ee.jpg&quot; alt=&quot;prml7-189&quot; width=&quot;233&quot; height=&quot;32&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-190.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7517&quot; src=&quot;/images/52nlp.cn/916df9972541b9d2bf493165933952f2.jpg&quot; alt=&quot;prml7-190&quot; width=&quot;30&quot; height=&quot;24&quot;&gt;&lt;/a&gt;是sigmoid函数，我们引入w的先验分布，跟RVM回归相同，每个&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-191.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7518&quot; src=&quot;/images/52nlp.cn/129a801cc3b69c2e26a824482e0edaed.jpg&quot; alt=&quot;prml7-191&quot; width=&quot;25&quot; height=&quot;21&quot;&gt;&lt;/a&gt;对应一个不同的精度&lt;br&gt;
这种先验叫做ARD先验，跟RVM回归相比，在求&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-147.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7493&quot; src=&quot;/images/52nlp.cn/24c1df957dfec2c2a22e77217a53aca5.jpg&quot; alt=&quot;prml7-147&quot; width=&quot;60&quot; height=&quot;18&quot;&gt;&lt;/a&gt;的分布时，不再对w进行积分。&lt;br&gt;
我们看在RVM回归时，是这么求&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-147.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7493&quot; src=&quot;/images/52nlp.cn/24c1df957dfec2c2a22e77217a53aca5.jpg&quot; alt=&quot;prml7-147&quot; width=&quot;60&quot; height=&quot;18&quot;&gt;&lt;/a&gt;的：&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-149.png&quot;&gt;&lt;img class=&quot;aligncenter wp-image-7494&quot; src=&quot;/images/52nlp.cn/c31a6ca8846b2858059c7d668e338b8f.jpg&quot; alt=&quot;prml7-149&quot; width=&quot;300&quot; height=&quot;44&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;从而得到：&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-150.png&quot;&gt;&lt;img class=&quot;aligncenter wp-image-7495&quot; src=&quot;/images/52nlp.cn/6fc7a490aa3d156b7b6e02af6f953689.jpg&quot; alt=&quot;prml7-150&quot; width=&quot;400&quot; height=&quot;61&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;在RVM分类时，因为涉用到sigmod函数,计算积分很难，具体的为什么难，在第四章4.5节有更多的介绍，我们这里用Laplace approximation来求&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-147.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7493&quot; src=&quot;/images/52nlp.cn/24c1df957dfec2c2a22e77217a53aca5.jpg&quot; alt=&quot;prml7-147&quot; width=&quot;60&quot; height=&quot;18&quot;&gt;&lt;/a&gt;的近似高斯分布，Laplace approximation我叫拉普拉斯近似，后面都写中文了。&lt;br&gt;
先看下拉普拉斯近似的原理，拉普拉斯近似的目的是找到连续变量的分布函数的高斯近似分布，也就是用高斯分布 近似模拟一个不是高斯分布的分布。&lt;br&gt;
假设一个单变量z，其分布是&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-200.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7519&quot; src=&quot;/images/52nlp.cn/13b0b6a4364a7fcd1cdc116c1193edc1.jpg&quot; alt=&quot;prml7-200&quot; width=&quot;89&quot; height=&quot;39&quot;&gt;&lt;/a&gt;，分母上的Z是归一化系数&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-201.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7520&quot; src=&quot;/images/52nlp.cn/6a78ccd59f2bfb7dc0ec6c95d591b07b.jpg&quot; alt=&quot;prml7-201&quot; width=&quot;79&quot; height=&quot;24&quot;&gt;&lt;/a&gt;，目标是找到一个可以近似p(z)的高斯分布q(z)。&lt;br&gt;
第一步是先找到p(z)的mode(众数) ，众数mode是一个统计学的概念，可以代表一组数据，不受极端数据的影响，比如可以选择中位数做一组实数的众数，对于高斯分布，众数就是其峰值。一组数据可能没有众数也可能有几个众数。&lt;br&gt;
拉普拉斯分布第一步要找到p(z)的众数&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-203.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7521&quot; src=&quot;/images/52nlp.cn/b51c879c43f7d4ba01f109d839882b98.jpg&quot; alt=&quot;prml7-203&quot; width=&quot;23&quot; height=&quot;21&quot;&gt;&lt;/a&gt;，这是p(z)的一个极大值点，可能是局部的，因为p(z)可能有多个局部极大值。在该点，一阶导数等于0，&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-205.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7523&quot; src=&quot;/images/52nlp.cn/6d697339ae20468daf9bbef89ed88641.jpg&quot; alt=&quot;prml7-205&quot; width=&quot;73&quot; height=&quot;26&quot;&gt;&lt;/a&gt;，后面再说怎么找&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-203.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7521&quot; src=&quot;/images/52nlp.cn/b51c879c43f7d4ba01f109d839882b98.jpg&quot; alt=&quot;prml7-203&quot; width=&quot;23&quot; height=&quot;21&quot;&gt;&lt;/a&gt;。找到&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-203.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7521&quot; src=&quot;/images/52nlp.cn/b51c879c43f7d4ba01f109d839882b98.jpg&quot; alt=&quot;prml7-203&quot; width=&quot;23&quot; height=&quot;21&quot;&gt;&lt;/a&gt;后，用&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-208.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7524&quot; src=&quot;/images/52nlp.cn/2ee0c0653701b0d445cc35b1742c30f0.jpg&quot; alt=&quot;prml7-208&quot; width=&quot;47&quot; height=&quot;22&quot;&gt;&lt;/a&gt;的泰勒展开来构造一个二次函数：&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-210.png&quot;&gt;&lt;img class=&quot;aligncenter wp-image-7525&quot; src=&quot;/images/52nlp.cn/f203f571e5c4b9cfcafe2a5df6aede93.jpg&quot; alt=&quot;prml7-210&quot; width=&quot;246&quot; height=&quot;47&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;其中A是f(z)在&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-203.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7521&quot; src=&quot;/images/52nlp.cn/b51c879c43f7d4ba01f109d839882b98.jpg&quot; alt=&quot;prml7-203&quot; width=&quot;23&quot; height=&quot;21&quot;&gt;&lt;/a&gt;的二阶导数再取负数。上式中，没有一阶导数部分，因为&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-203.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7521&quot; src=&quot;/images/52nlp.cn/b51c879c43f7d4ba01f109d839882b98.jpg&quot; alt=&quot;prml7-203&quot; width=&quot;23&quot; height=&quot;21&quot;&gt;&lt;/a&gt;是局部极大值，一阶导数为0，把上式两边取指数，得到：&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-221.png&quot;&gt;&lt;img class=&quot;wp-image-7526 aligncenter&quot; src=&quot;/images/52nlp.cn/619f5fa128c995cb3ddf8e6023738306.jpg&quot; alt=&quot;prml7-221&quot; width=&quot;242&quot; height=&quot;43&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;把&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-224.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7527&quot; src=&quot;/images/52nlp.cn/ab463aa0251bc608748331788cce65cf.jpg&quot; alt=&quot;prml7-224&quot; width=&quot;30&quot; height=&quot;26&quot;&gt;&lt;/a&gt;换成归一化系数，得到近似的高斯分布：&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-225.png&quot;&gt;&lt;img class=&quot;aligncenter wp-image-7528&quot; src=&quot;/images/52nlp.cn/ebec1a1327597aee0b66d316ba0a8f54.jpg&quot; alt=&quot;prml7-225&quot; width=&quot;258&quot; height=&quot;49&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;拉普拉斯分布得到的近似高斯分布的一个图示：&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-226.png&quot;&gt;&lt;img class=&quot;aligncenter wp-image-7529&quot; src=&quot;/images/52nlp.cn/8172709b41e6e3716ac7a7dd6d3f1ee1.jpg&quot; alt=&quot;prml7-226&quot; width=&quot;460&quot; height=&quot;199&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;注意，高斯近似存在的条件是，原分布的二阶导数取负数、也就是高斯近似的精确度A&amp;gt;0，也就是驻点&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-203.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7521&quot; src=&quot;/images/52nlp.cn/b51c879c43f7d4ba01f109d839882b98.jpg&quot; alt=&quot;prml7-203&quot; width=&quot;23&quot; height=&quot;21&quot;&gt;&lt;/a&gt;必须是局部极大值，f(x)在改点出的导数为负数。当z是一个M维向量时，近似方法跟单变量的不同只是二阶导数的负数A变成了MxM维的海森矩阵的负数。&lt;br&gt;
多维变量近似后的高斯分布如下：&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-250.png&quot;&gt;&lt;img class=&quot;aligncenter wp-image-7530&quot; src=&quot;/images/52nlp.cn/3297f5bbf4fb74ea4f95c105d2c749d6.jpg&quot; alt=&quot;prml7-250&quot; width=&quot;400&quot; height=&quot;58&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;A是海森矩阵的负数.mode众数&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-203.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7521&quot; src=&quot;/images/52nlp.cn/b51c879c43f7d4ba01f109d839882b98.jpg&quot; alt=&quot;prml7-203&quot; width=&quot;23&quot; height=&quot;21&quot;&gt;&lt;/a&gt;一般是通过数值优化算法来寻找的，不讲了。&lt;br&gt;
再回来看用拉普拉斯分布来近似RVM分类中的&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-147.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7493&quot; src=&quot;/images/52nlp.cn/24c1df957dfec2c2a22e77217a53aca5.jpg&quot; alt=&quot;prml7-147&quot; width=&quot;60&quot; height=&quot;18&quot;&gt;&lt;/a&gt;：&lt;br&gt;
刚才拉普拉斯分布忘了说一个公式，就是求得q(z)后，确定&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-200.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7519&quot; src=&quot;/images/52nlp.cn/13b0b6a4364a7fcd1cdc116c1193edc1.jpg&quot; alt=&quot;prml7-200&quot; width=&quot;89&quot; height=&quot;39&quot;&gt;&lt;/a&gt;中的分母，也就是归一化系数的公式：&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-252.png&quot;&gt;&lt;img class=&quot;aligncenter wp-image-7531&quot; src=&quot;/images/52nlp.cn/2acdce5f24d6d6993653bb2e690425c3.jpg&quot; alt=&quot;prml7-252&quot; width=&quot;350&quot; height=&quot;127&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;这个一会有用。先看RVM中对w的后验分布的近似，先求后验分布的mode众数，通过最大化log后验分布&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-253.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7532&quot; src=&quot;/images/52nlp.cn/2e042045c196ecdf67584bede9703287.jpg&quot; alt=&quot;prml7-253&quot; width=&quot;95&quot; height=&quot;22&quot;&gt;&lt;/a&gt;来求mode.先写出这个log后验分布：&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-255.png&quot;&gt;&lt;img class=&quot;aligncenter wp-image-7533&quot; src=&quot;/images/52nlp.cn/a892da69f67d1371b3be2be04c5de799.jpg&quot; alt=&quot;prml7-255&quot; width=&quot;461&quot; height=&quot;82&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;其中&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-135.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7483&quot; src=&quot;/images/52nlp.cn/0a055de07c3b81d693fc8315d73df92c.jpg&quot; alt=&quot;prml7-135&quot; width=&quot;98&quot; height=&quot;20&quot;&gt;&lt;/a&gt;&lt;br&gt;
最后求得的高斯近似的均值(也就是原分布的mode)和精度如下：&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-256.png&quot;&gt;&lt;img class=&quot;aligncenter wp-image-7534&quot; src=&quot;/images/52nlp.cn/69dc8c1687afafadbde2314ca9d1d02b.jpg&quot; alt=&quot;prml7-256&quot; width=&quot;227&quot; height=&quot;66&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;现在用这个w后验高斯近似来求边界似然&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-258.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7535&quot; src=&quot;/images/52nlp.cn/6267d7cf0db322c083dac5fb5bc370eb.jpg&quot; alt=&quot;prml7-258&quot; width=&quot;276&quot; height=&quot;30&quot;&gt;&lt;/a&gt;&lt;br&gt;
根据前面那个求归一化系数Z的公式Z=&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-259.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7536&quot; src=&quot;/images/52nlp.cn/d25835c1ffb0f0a38cb525a142741eda.jpg&quot; alt=&quot;prml7-259&quot; width=&quot;94&quot; height=&quot;41&quot;&gt;&lt;/a&gt;&lt;br&gt;
有:&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-260.png&quot;&gt;&lt;img class=&quot;aligncenter wp-image-7537&quot; src=&quot;/images/52nlp.cn/b50ffda19afb0e102300eef9c341a9ca.jpg&quot; alt=&quot;prml7-260&quot; width=&quot;350&quot; height=&quot;75&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;RVM这部分大量基于第二章高斯分布和第三、四两章，公式推导很多，需要前后关联才能看明白。&lt;/p&gt;
&lt;p&gt;注：PRML读书会系列文章由 &lt;a href=&quot;http://weibo.com/dmalgorithms&quot;&gt;@Nietzsche_复杂网络机器学习&lt;/a&gt; 同学授权发布，转载请注明原作者和相关的主讲人，谢谢。&lt;/p&gt;
&lt;p&gt;PRML读书会讲稿PDF版本以及更多资源下载地址：&lt;a href=&quot;http://vdisk.weibo.com/u/1841149974&quot;&gt;http://vdisk.weibo.com/u/1841149974&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;本文链接地址：&lt;a href=&quot;http://www.52nlp.cn/prml%E8%AF%BB%E4%B9%A6%E4%BC%9A%E7%AC%AC%E4%B8%83%E7%AB%A0-sparse-kernel-machines&quot;&gt;http://www.52nlp.cn/prml读书会第七章-sparse-kernel-machines&lt;/a&gt;&lt;/p&gt;

											

</description>
        <pubDate>Fri, 30 Jan 2015 00:00:00 +0800</pubDate>
        <link>http://iftti.com/posts/2015/2015-01-30-prml%25e8%25af%25bb%25e4%25b9%25a6%25e4%25bc%259a%25e7%25ac%25ac%25e4%25b8%2583%25e7%25ab%25a0-sparse-kernel-machines-9458d994d.html</link>
        <guid isPermaLink="true">http://iftti.com/posts/2015/2015-01-30-prml%25e8%25af%25bb%25e4%25b9%25a6%25e4%25bc%259a%25e7%25ac%25ac%25e4%25b8%2583%25e7%25ab%25a0-sparse-kernel-machines-9458d994d.html</guid>
        
        
        <category>52nlp</category>
        
      </item>
    
      <item>
        <title>PRML读书会第六章   Kernel Methods</title>
        <description>

						&lt;p style=&quot;text-align: center&quot;&gt;&lt;strong&gt;PRML&lt;/strong&gt;&lt;strong&gt;读书会第六章 &lt;/strong&gt; &lt;strong&gt;Kernel Methods&lt;/strong&gt;&lt;/p&gt;
&lt;p style=&quot;text-align: center&quot;&gt;&lt;strong&gt;主讲人 网络上的尼采&lt;/strong&gt;&lt;/p&gt;
&lt;p style=&quot;text-align: center&quot;&gt;&lt;strong&gt;（新浪微博:&lt;a href=&quot;http://weibo.com/dmalgorithms&quot;&gt;@Nietzsche_复杂网络机器学习&lt;/a&gt;）&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;网络上的尼采(813394698) 9:16:05&lt;/p&gt;
&lt;p&gt;今天的主要内容：Kernel的基本知识，高斯过程。边思考边打字，有点慢，各位稍安勿躁。&lt;br&gt;
机器学习里面对待训练数据有的是训练完得到参数后就可以抛弃了，比如神经网络；有的是还需要原来的训练数据比如KNN，SVM也需要保留一部分数据–支持向量。&lt;br&gt;
很多线性参数模型都可以通过dual representation的形式表达为核函数的形式。所谓线性参数模型是通过非线性的基函数的线性组合来表达非线性的东西，模型还是线性的。比如线性回归模型是y=&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-0.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7284&quot; src=&quot;/images/52nlp.cn/f69bf3291baa22562dc0307d19ce45c8.jpg&quot; alt=&quot;prml6-0&quot; width=&quot;66&quot; height=&quot;20&quot;&gt;&lt;/a&gt;，&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-1.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7285&quot; src=&quot;/images/52nlp.cn/ab655185988d45d9adad9096b0ef807a.jpg&quot; alt=&quot;prml6-1&quot; width=&quot;45&quot; height=&quot;26&quot;&gt;&lt;/a&gt;是一组非线性基函数，我们可以通过线性的模型来表达非线性的结构。&lt;/p&gt;
&lt;p&gt;核函数的形式：&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-3.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7286&quot; src=&quot;/images/52nlp.cn/f1f79680d42ebb575ec08736033b45b9.jpg&quot; alt=&quot;prml6-3&quot; width=&quot;150&quot; height=&quot;26&quot;&gt;&lt;/a&gt;，也就是映射后高维特征空间的内积可以通过原来低维的特征得到。因此kernel methods用途广泛。&lt;/p&gt;
&lt;p&gt;核函数有很多种，有平移不变的stationary kernels  &lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-4.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7287&quot; src=&quot;/images/52nlp.cn/a44fe7b7b0b5a8d89da153c34de1a511.jpg&quot; alt=&quot;prml6-4&quot; width=&quot;160&quot; height=&quot;20&quot;&gt;&lt;/a&gt;还有仅依赖欧氏距离的径向基核：&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-5.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7288&quot; src=&quot;/images/52nlp.cn/f32073eb6f8a77372ed8625b23466393.jpg&quot; alt=&quot;prml6-5&quot; width=&quot;160&quot; height=&quot;23&quot;&gt;&lt;/a&gt;&lt;span id=&quot;more-7282&quot;&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;非线性转化为线性的形式的好处不言而喻，各种变换推导、闭式解就出来了。 下面推导下线性回归模型的dual representation，有助于我们理解核函数的作用：&lt;/p&gt;
&lt;p&gt;根据最小二乘，我们得到下面的目标函数&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-7.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7289&quot; src=&quot;/images/52nlp.cn/5ecedeb9e5e4a0034dcfb76486db037b.jpg&quot; alt=&quot;prml6-7&quot; width=&quot;250&quot; height=&quot;43&quot;&gt;&lt;/a&gt;，加了L2正则。我们对w求导，令J(w)的梯度等于0，得到以下解：&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-8.png&quot;&gt;&lt;img class=&quot;aligncenter wp-image-7290&quot; src=&quot;/images/52nlp.cn/6a88ba41a438470fab044492fab637e4.jpg&quot; alt=&quot;prml6-8&quot; width=&quot;400&quot; height=&quot;63&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-9.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7291&quot; src=&quot;/images/52nlp.cn/a1bf0bc770bd32ff433b20fc8f354c99.jpg&quot; alt=&quot;prml6-9&quot; width=&quot;25&quot; height=&quot;22&quot;&gt;&lt;/a&gt;是个由基函数构成的样本矩阵，&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-10.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7292&quot; src=&quot;/images/52nlp.cn/4e3e314f21e297c9e29d20daed3ec734.jpg&quot; alt=&quot;prml6-10&quot; width=&quot;25&quot; height=&quot;29&quot;&gt;&lt;/a&gt;向量里面的元素由&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-11.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7293&quot; src=&quot;/images/52nlp.cn/38a1fe6ac08c663c397e57dcc2a980a0.jpg&quot; alt=&quot;prml6-11&quot; width=&quot;200&quot; height=&quot;40&quot;&gt;&lt;/a&gt;组成：&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-12.png&quot;&gt;&lt;img class=&quot;aligncenter wp-image-7294&quot; src=&quot;/images/52nlp.cn/379a061c6a43f73456cd481953b10745.jpg&quot; alt=&quot;prml6-12&quot; width=&quot;400&quot; height=&quot;69&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;我们把&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-13.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7295&quot; src=&quot;/images/52nlp.cn/e14ddcbd720aa06797b162982f22101a.jpg&quot; alt=&quot;prml6-13&quot; width=&quot;60&quot; height=&quot;19&quot;&gt;&lt;/a&gt;代入最初的J(w)得到：&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-14.png&quot;&gt;&lt;img class=&quot;aligncenter wp-image-7296&quot; src=&quot;/images/52nlp.cn/577d2747905ebd74ef31ed46a9af8dc2.jpg&quot; alt=&quot;prml6-14&quot; width=&quot;400&quot; height=&quot;47&quot;&gt;&lt;/a&gt;&lt;br&gt;
咱们用核矩阵K来替换&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-16.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7297&quot; src=&quot;/images/52nlp.cn/8af87d4cc48564e8cc0b4ef3d1b43f3f.jpg&quot; alt=&quot;prml6-16&quot; width=&quot;35&quot; height=&quot;21&quot;&gt;&lt;/a&gt;，其中矩阵K里面的元素是&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-17.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7298&quot; src=&quot;/images/52nlp.cn/6c70142a76a13e63359f16b9778a02a4.jpg&quot; alt=&quot;prml6-17&quot; width=&quot;200&quot; height=&quot;28&quot;&gt;&lt;/a&gt;&lt;br&gt;
于是得到&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-20.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7301&quot; src=&quot;/images/52nlp.cn/0bfacc4fba3cd57e5e2bd38cad5ce647.jpg&quot; alt=&quot;prml6-20&quot; width=&quot;250&quot; height=&quot;35&quot;&gt;&lt;/a&gt;&lt;br&gt;
然后&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-21.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7302&quot; src=&quot;/images/52nlp.cn/e5cfb4fc6d15a7a12d4dc4e05350a58b.jpg&quot; alt=&quot;prml6-21&quot; width=&quot;30&quot; height=&quot;17&quot;&gt;&lt;/a&gt;对&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-10.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7292&quot; src=&quot;/images/52nlp.cn/4e3e314f21e297c9e29d20daed3ec734.jpg&quot; alt=&quot;prml6-10&quot; width=&quot;20&quot; height=&quot;23&quot;&gt;&lt;/a&gt;求导，令其梯度等于0，得到解&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-23.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7303&quot; src=&quot;/images/52nlp.cn/cc6ac4ad7eefef631f64feb08fb598fb.jpg&quot; alt=&quot;prml6-23&quot; width=&quot;120&quot; height=&quot;28&quot;&gt;&lt;/a&gt;&lt;br&gt;
所以原来的线性回归方程就变成了&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-24.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7304&quot; src=&quot;/images/52nlp.cn/97354754bf06beb11e5116638e026e7c.jpg&quot; alt=&quot;prml6-24&quot; width=&quot;350&quot; height=&quot;30&quot;&gt;&lt;/a&gt;&lt;br&gt;
K(X)的含义：&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-25.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7305&quot; src=&quot;/images/52nlp.cn/62563682c6e5fef04e0d222d41bf2b20.jpg&quot; alt=&quot;prml6-25&quot; width=&quot;380&quot; height=&quot;17&quot;&gt;&lt;/a&gt;，上面的DUAL形式的含义非常明显，就是根据已知的的训练数据来做预测。至此原来线性回归方程的参数w消失，由核函数来表示回归方程，以上方式把基于特征的学习转换成了基于样本的学习。 这是线性回归的DUAL表示，svm等很多模型都有DUAL表示。&lt;br&gt;
80(850639048) 10:09:50&lt;br&gt;
professor 核函数其实是为了求基函数的内积对吗？&lt;br&gt;
网络上的尼采(813394698) 10:12:57&lt;/p&gt;
&lt;p&gt;如果有很多基的话维度势必会很高，计算内积的花销会很大，有些是无限维的，核函数能绕过高维的内积计算，直接用核函数得到内积。&lt;/p&gt;
&lt;p&gt;接下来看下核函数的性质及构造方法。核函数的一般形式：&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-26.png&quot;&gt;&lt;img class=&quot;aligncenter wp-image-7306&quot; src=&quot;/images/52nlp.cn/d3b6ed5c4051029b7462385fb53b1605.jpg&quot; alt=&quot;prml6-26&quot; width=&quot;400&quot; height=&quot;89&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;下面是个简单的例子说明&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-27.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7307&quot; src=&quot;/images/52nlp.cn/f555c093aeccecc4d5d2e62928ce2b7c.jpg&quot; alt=&quot;prml6-27&quot; width=&quot;120&quot; height=&quot;31&quot;&gt;&lt;/a&gt;为什么是个核函数：&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-29.png&quot;&gt;&lt;img class=&quot;aligncenter wp-image-7308&quot; src=&quot;/images/52nlp.cn/b75216a4234a0e89139156d1fb86407b.jpg&quot; alt=&quot;prml6-29&quot; width=&quot;400&quot; height=&quot;127&quot;&gt;&lt;/a&gt;&lt;br&gt;
很明显 &lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-30.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7309&quot; src=&quot;/images/52nlp.cn/f85c384e4ac7fbdaf57d135a874eaa96.jpg&quot; alt=&quot;prml6-30&quot; width=&quot;100&quot; height=&quot;26&quot;&gt;&lt;/a&gt; 是个核函数，它能写成核函数的一般形式。&lt;/p&gt;
&lt;p&gt;核函数的一个充分必要定理也就是mercer定理：核矩阵是半正定的：&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-31.png&quot;&gt;&lt;img class=&quot;aligncenter wp-image-7310&quot; src=&quot;/images/52nlp.cn/08ad61872577230b8516fd599bc6dd10.jpg&quot; alt=&quot;prml6-31&quot; width=&quot;400&quot; height=&quot;94&quot;&gt;&lt;/a&gt;&lt;br&gt;
我们可以通过以下规则用简单的核函数来构造复杂的核函数：&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-32.png&quot;&gt;&lt;img class=&quot;aligncenter wp-image-7311&quot; src=&quot;/images/52nlp.cn/cb61ade2759454cb07e0ab649bd364fb.jpg&quot; alt=&quot;prml6-32&quot; width=&quot;400&quot; height=&quot;311&quot;&gt;&lt;/a&gt;&lt;br&gt;
过会我们讲高斯过程时再举个核函数线性组合的例子。&lt;br&gt;
介绍一个经常用到的径向基核函数，高斯核：&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-36.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7313&quot; src=&quot;/images/52nlp.cn/f1e1f9a9ccc4aec333988ee594f36597.jpg&quot; alt=&quot;prml6-36&quot; width=&quot;257&quot; height=&quot;41&quot;&gt;&lt;/a&gt;，这个核函数能把数据映射到无限维的空间：&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-37.png&quot;&gt;&lt;img class=&quot;aligncenter wp-image-7314&quot; src=&quot;/images/52nlp.cn/22f4ca139b80c2651078e16a93f789ab.jpg&quot; alt=&quot;prml6-37&quot; width=&quot;400&quot; height=&quot;96&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;中间&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-38.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7315&quot; src=&quot;/images/52nlp.cn/e9aa6627eb2c2084062cbfcfe9149dc8.jpg&quot; alt=&quot;prml6-38&quot; width=&quot;100&quot; height=&quot;31&quot;&gt;&lt;/a&gt;可以展开成无限维的，然后核函数可以表示成内积的形式。&lt;br&gt;
内积的含义就是表示相似性，所以核函数还有其他的用法。比如我们可以通过生成模型来构造核。&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-39.png&quot;&gt;&lt;img class=&quot;aligncenter wp-image-7316&quot; src=&quot;/images/52nlp.cn/757b9f7cb880a47a3e8bdc21441a077c.jpg&quot; alt=&quot;prml6-39&quot; width=&quot;349&quot; height=&quot;52&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;两个变量的概率都很高相似性就越大，其实这样做就是映射到一维的内积。&lt;br&gt;
我们可以引入离散的隐变量：&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-40.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7317&quot; src=&quot;/images/52nlp.cn/5a319aae49956cf73bc228c1b37dd284.jpg&quot; alt=&quot;prml6-40&quot; width=&quot;243&quot; height=&quot;51&quot;&gt;&lt;/a&gt;&lt;br&gt;
连续的隐变量：&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-41.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7318&quot; src=&quot;/images/52nlp.cn/b83bb1452d305932a5dc7796048ae797.jpg&quot; alt=&quot;prml6-41&quot; width=&quot;274&quot; height=&quot;49&quot;&gt;&lt;/a&gt;&lt;br&gt;
举个这样做有啥用的例子，我们可以用来比较HMM由同一条隐马尔科夫链生成的两条序列的相似性：&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-42.png&quot;&gt;&lt;img class=&quot;aligncenter wp-image-7319&quot; src=&quot;/images/52nlp.cn/21c737d26ac53f73854646e6e9759784.jpg&quot; alt=&quot;prml6-42&quot; width=&quot;400&quot; height=&quot;150&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;网络上的尼采(813394698) 10:40:34&lt;br&gt;
接下来讲我们今天的重点Gaussian Processes&lt;br&gt;
牧云(1106207961) 10:41:02&lt;/p&gt;
&lt;p&gt;数据海洋(1009129701) 10:41:14&lt;br&gt;
我先再理解，理解这些。&lt;br&gt;
网络上的尼采(813394698) 10:42:41&lt;br&gt;
Gaussian Processes是贝叶斯学派的一个大杀器，用处很广。不同于参数模型，Gaussian Processes认为函数在函数空间里存在一个先验分布。&lt;/p&gt;
&lt;p&gt;高斯过程和很多模型是等价的：ARMA (autoregressive moving average) models, Kalman filters, radial basis function networks ，还有特定情况下的神经网络。&lt;br&gt;
现在我们从贝叶斯线性回归自然的引出高斯过程：&lt;br&gt;
前面我们提到的线性回归的形式 &lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-43.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7321&quot; src=&quot;/images/52nlp.cn/b32754c949b65ed446adbd979014a435.jpg&quot; alt=&quot;prml6-43&quot; width=&quot;100&quot; height=&quot;25&quot;&gt;&lt;/a&gt;&lt;br&gt;
贝叶斯方法为参数加了一个高斯分布 &lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-44.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7322&quot; src=&quot;/images/52nlp.cn/aea8d22b11fe9bea31975f9b22aa8149.jpg&quot; alt=&quot;prml6-44&quot; width=&quot;150&quot; height=&quot;31&quot;&gt;&lt;/a&gt;&lt;br&gt;
大家发现了没有，这样做直接导致了函数有个预测分布，并且也是高斯的，因为方程是线性的并且参数是高斯分布。线性的东西和高斯分布总是不分家的。&lt;br&gt;
我们定义向量：y，&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-45.png&quot;&gt;&lt;img class=&quot;aligncenter wp-image-7323&quot; src=&quot;/images/52nlp.cn/a3426d2ea69a33712460e6303cf268f2.jpg&quot; alt=&quot;prml6-45&quot; width=&quot;400&quot; height=&quot;35&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;y就是个多元的高斯分布。&lt;br&gt;
它的每一维 &lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-46.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7324&quot; src=&quot;/images/52nlp.cn/69dd688dfe3fc8cf50b1ad5a35baf217.jpg&quot; alt=&quot;prml6-46&quot; width=&quot;45&quot; height=&quot;22&quot;&gt;&lt;/a&gt;都是个高斯分布，这也是高斯过程的由来。&lt;br&gt;
y可以表示为 &lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-47.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7325&quot; src=&quot;/images/52nlp.cn/aa074df6fea20ee1c73806a03b34ce36.jpg&quot; alt=&quot;prml6-47&quot; width=&quot;67&quot; height=&quot;19&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-48.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7326&quot; src=&quot;/images/52nlp.cn/6e5b4ef8924c0d17716965b7a864a8bf.jpg&quot; alt=&quot;prml6-48&quot; width=&quot;300&quot; height=&quot;26&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-49.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7327&quot; src=&quot;/images/52nlp.cn/af732bfb85ee71047a35d203f431d357.jpg&quot; alt=&quot;prml6-49&quot; width=&quot;23&quot; height=&quot;18&quot;&gt;&lt;/a&gt;是基函数组成的样本矩阵。&lt;br&gt;
高斯过程可以由均值和协方差矩阵完全决定。由于w的均值是0，所以我们也认为高斯过程的均值是0，&lt;br&gt;
剩下的就是根据定义求它的协方差矩阵，很自然地就得出来了：&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-50.png&quot;&gt;&lt;img class=&quot;aligncenter wp-image-7328&quot; src=&quot;/images/52nlp.cn/d04dfd4d8e9c60289cc6d93364c8498b.jpg&quot; alt=&quot;prml6-50&quot; width=&quot;400&quot; height=&quot;85&quot;&gt;&lt;/a&gt;&lt;br&gt;
矩阵K里的元素&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-51.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7329&quot; src=&quot;/images/52nlp.cn/7f0d72a944951fc5ff152df038399c50.jpg&quot; alt=&quot;prml6-51&quot; width=&quot;277&quot; height=&quot;44&quot;&gt;&lt;/a&gt;都是核函数的形式。&lt;br&gt;
选用什么样的核函数也是问题，下面的图是对采用高斯核和指数核的高斯过程的取样，一共取了5条，可以看到两者的区别：&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-52.png&quot;&gt;&lt;img class=&quot;aligncenter wp-image-7330 size-full&quot; src=&quot;/images/52nlp.cn/490aeda99d6a1243f8260aafd93629f7.jpg&quot; alt=&quot;prml6-52&quot; width=&quot;441&quot; height=&quot;129&quot;&gt;&lt;/a&gt;&lt;br&gt;
接下来我们就用GP来做回归 ：&lt;br&gt;
我们观测的目标值是包含噪音的，噪音是高斯分布。&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-60.png&quot;&gt;&lt;img class=&quot;aligncenter wp-image-7332&quot; src=&quot;/images/52nlp.cn/395e3305130c3c96c0536ac22789cd6b.jpg&quot; alt=&quot;prml6-60&quot; width=&quot;186&quot; height=&quot;54&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;那么根据线性高斯模型的性质，&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-65.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7334&quot; src=&quot;/images/52nlp.cn/f35c82cdae8826e9de9ec51bc7632b4e.jpg&quot; alt=&quot;prml6-65&quot; width=&quot;180&quot; height=&quot;31&quot;&gt;&lt;/a&gt;，其中&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-66.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7335&quot; src=&quot;/images/52nlp.cn/bd6593d1b905edb5b58ce88dc9ef9cea.jpg&quot; alt=&quot;prml6-66&quot; width=&quot;26&quot; height=&quot;29&quot;&gt;&lt;/a&gt;是噪音的参数&lt;br&gt;
对于向量&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-63.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7336&quot; src=&quot;/images/52nlp.cn/69aaf5f00cbda42782600e349c08eb1f.jpg&quot; alt=&quot;prml6-63&quot; width=&quot;120&quot; height=&quot;23&quot;&gt;&lt;/a&gt;和向量&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-64.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7337&quot; src=&quot;/images/52nlp.cn/bb0bb0128ea9e7ac88813d40bca1edb7.jpg&quot; alt=&quot;prml6-64&quot; width=&quot;120&quot; height=&quot;23&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-68.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7338&quot; src=&quot;/images/52nlp.cn/64a6cddece36f9c2c75feed536c507b2.jpg&quot; alt=&quot;prml6-68&quot; width=&quot;150&quot; height=&quot;30&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;咱们前面说过了，&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-61.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7333&quot; src=&quot;/images/52nlp.cn/b9cc209c7df789950e8be3e79ea22f1c.jpg&quot; alt=&quot;prml6-61&quot; width=&quot;30&quot; height=&quot;25&quot;&gt;&lt;/a&gt;可以表示为&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-62.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7339&quot; src=&quot;/images/52nlp.cn/202c2aa03a886e372b958d82cbd119f5.jpg&quot; alt=&quot;prml6-62&quot; width=&quot;120&quot; height=&quot;29&quot;&gt;&lt;/a&gt;&lt;br&gt;
所以marginal distribution：&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-69.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7340&quot; src=&quot;/images/52nlp.cn/a60c5157a752ec3ef8a5f745ddbf3a7f.jpg&quot; alt=&quot;prml6-69&quot; width=&quot;266&quot; height=&quot;40&quot;&gt;&lt;/a&gt;&lt;br&gt;
其中矩阵C的元素&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-70.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7341&quot; src=&quot;/images/52nlp.cn/9bcb289464ee4ab65a1c50559c9eacbf.jpg&quot; alt=&quot;prml6-70&quot; width=&quot;255&quot; height=&quot;38&quot;&gt;&lt;/a&gt;，&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-72.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7342&quot; src=&quot;/images/52nlp.cn/5f7741d35e88fc5e971ffce2de50f2d8.jpg&quot; alt=&quot;prml6-72&quot; width=&quot;35&quot; height=&quot;25&quot;&gt;&lt;/a&gt;是单位矩阵的元素，其实就是把&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-73.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7343&quot; src=&quot;/images/52nlp.cn/b1dd9869e453a6ee6346fb7bdf5d8f40.jpg&quot; alt=&quot;prml6-73&quot; width=&quot;38&quot; height=&quot;29&quot;&gt;&lt;/a&gt;加在了矩阵K的对角线上。这个不难理解，一开始&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml7-75.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7344&quot; src=&quot;/images/52nlp.cn/33a6889bc19d94b121c13f81b9f2206f.jpg&quot; alt=&quot;prml7-75&quot; width=&quot;100&quot; height=&quot;25&quot;&gt;&lt;/a&gt;，都是高斯的，协方差是两者的相加，噪音每次取都是独立的，所以只在协方差矩阵对角线上有。&lt;br&gt;
现在确定下用什么核的问题，举个例子，下面这个核函数用了高斯核，线性核，以及常数的线性组合，这样做是为了更灵活，过会再讲如何确定里面的这些超参：&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-77.png&quot;&gt;&lt;img class=&quot;aligncenter wp-image-7345&quot; src=&quot;/images/52nlp.cn/f51b2f19f4d5de3ae43c364017e966f6.jpg&quot; alt=&quot;prml6-77&quot; width=&quot;400&quot; height=&quot;63&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;下图是不同的超参对高斯过程的影响：&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-78.png&quot;&gt;&lt;img class=&quot;aligncenter wp-image-7346&quot; src=&quot;/images/52nlp.cn/229007048f4f801ab005dcba861c8442.jpg&quot; alt=&quot;prml6-78&quot; width=&quot;400&quot; height=&quot;292&quot;&gt;&lt;/a&gt;&lt;br&gt;
解决了核函数的问题，我们再回来，通过前面的结论，不难得出 &lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-79.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7347&quot; src=&quot;/images/52nlp.cn/cb40489f63f1ee13bbc9525edc10ee13.jpg&quot; alt=&quot;prml6-79&quot; width=&quot;180&quot; height=&quot;31&quot;&gt;&lt;/a&gt;&lt;br&gt;
如何确定矩阵&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-80.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7348&quot; src=&quot;/images/52nlp.cn/a0fc15383d796ea863529f05733c3f53.jpg&quot; alt=&quot;prml6-80&quot; width=&quot;43&quot; height=&quot;23&quot;&gt;&lt;/a&gt;呢，其实我们在原来矩阵&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-81.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7349&quot; src=&quot;/images/52nlp.cn/f436e5f2e52babca3b9dfd95cc620f58.jpg&quot; alt=&quot;prml6-81&quot; width=&quot;31&quot; height=&quot;19&quot;&gt;&lt;/a&gt;的基础上补上就行。&lt;/p&gt;
&lt;p&gt;k和c比较容易理解：&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-82.png&quot;&gt;&lt;img class=&quot;aligncenter wp-image-7350&quot; src=&quot;/images/52nlp.cn/47224f22088bfc96df440d2472333d3e.jpg&quot; alt=&quot;prml6-82&quot; width=&quot;400&quot; height=&quot;21&quot;&gt;&lt;/a&gt; &lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-83.png&quot;&gt;&lt;img class=&quot;aligncenter wp-image-7351&quot; src=&quot;/images/52nlp.cn/d4f9a61ceed77129c7d54fedc8aa2a4a.jpg&quot; alt=&quot;prml6-83&quot; width=&quot;300&quot; height=&quot;43&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;咱们的最终目标就是得&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-85.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7352&quot; src=&quot;/images/52nlp.cn/c5a6dde91d487e32d1b23bc2995c6c0c.jpg&quot; alt=&quot;prml6-85&quot; width=&quot;68&quot; height=&quot;26&quot;&gt;&lt;/a&gt;，由于这两个都是高斯分布，用第二章条件高斯分布的公式套一下，其中均值是0：&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-89.png&quot;&gt;&lt;img class=&quot;aligncenter wp-image-7353&quot; src=&quot;/images/52nlp.cn/855da7e7d64275144bdb626d0dbdd9f5.jpg&quot; alt=&quot;prml6-89&quot; width=&quot;400&quot; height=&quot;85&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;就会得到&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-85.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7352&quot; src=&quot;/images/52nlp.cn/c5a6dde91d487e32d1b23bc2995c6c0c.jpg&quot; alt=&quot;prml6-85&quot; width=&quot;68&quot; height=&quot;26&quot;&gt;&lt;/a&gt;的均值和方差：&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-86.png&quot;&gt;&lt;img class=&quot;aligncenter wp-image-7354&quot; src=&quot;/images/52nlp.cn/3dc90e5709a3e604915a009dd472ee1a.jpg&quot; alt=&quot;prml6-86&quot; width=&quot;400&quot; height=&quot;77&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;可以看出均值和方差都是&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-90.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7356&quot; src=&quot;/images/52nlp.cn/ae1c4222dfa5812b4da4352f1219485c.jpg&quot; alt=&quot;prml6-90&quot; width=&quot;46&quot; height=&quot;23&quot;&gt;&lt;/a&gt;的函数，我们做预测时用均值就行了。&lt;br&gt;
最后一个问题就是高斯过程是由它的协方差矩阵完全决定的，我们如何学习矩阵里面的超参呢？包括我们刚才提到的核函数里面的参数以及噪音的参数。&lt;br&gt;
其实由于高斯分布的原因，我们可以方便的利用log最大似然：&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-91.png&quot;&gt;&lt;img class=&quot;aligncenter wp-image-7357&quot; src=&quot;/images/52nlp.cn/738fc937451fb62e97e8026ec381f768.jpg&quot; alt=&quot;prml6-91&quot; width=&quot;400&quot; height=&quot;52&quot;&gt;&lt;/a&gt;&lt;br&gt;
求最优解时可以用共轭梯度等方法，梯度：&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-92.png&quot;&gt;&lt;img class=&quot;aligncenter wp-image-7358&quot; src=&quot;/images/52nlp.cn/0d2f291815e0ecba579bf2c662925ad4.jpg&quot; alt=&quot;prml6-92&quot; width=&quot;400&quot; height=&quot;55&quot;&gt;&lt;/a&gt;&lt;br&gt;
到这里，用高斯过程做回归就结束了。&lt;br&gt;
有了做回归的基础，咱们再看下如何做分类。&lt;br&gt;
类似逻辑回归，加个sigmoid函数就能做分类了：&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-93.png&quot;&gt;&lt;img class=&quot;aligncenter wp-image-7359&quot; src=&quot;/images/52nlp.cn/a78912791a05ada28cc8bcc1c0b70347.jpg&quot; alt=&quot;prml6-93&quot; width=&quot;400&quot; height=&quot;84&quot;&gt;&lt;/a&gt;&lt;br&gt;
分类与回归不同的是&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-94.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7360&quot; src=&quot;/images/52nlp.cn/2a1f0d69da48b0734e0aefa7bdc3ab67.jpg&quot; alt=&quot;prml6-94&quot; width=&quot;247&quot; height=&quot;47&quot;&gt;&lt;/a&gt;是个伯努利分布。&lt;br&gt;
&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-95.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7361&quot; src=&quot;/images/52nlp.cn/d5ff2c5d39d8b8b75d1c149e5801abd6.jpg&quot; alt=&quot;prml6-95&quot; width=&quot;258&quot; height=&quot;47&quot;&gt;&lt;/a&gt;这里还和前面一样。&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-96.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7362&quot; src=&quot;/images/52nlp.cn/feb3771c2915b0b4b715843cd138540a.jpg&quot; alt=&quot;prml6-96&quot; width=&quot;265&quot; height=&quot;40&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;对于二分类问题，最后我们要得到是：&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-97.png&quot;&gt;&lt;img class=&quot;aligncenter wp-image-7363&quot; src=&quot;/images/52nlp.cn/40677fabc142bad32874e16247fd246e.jpg&quot; alt=&quot;prml6-97&quot; width=&quot;400&quot; height=&quot;45&quot;&gt;&lt;/a&gt;&lt;br&gt;
但是这个积分是不容易求的，&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-98.png&quot;&gt;&lt;img class=&quot;alignnone size-full wp-image-7364&quot; src=&quot;/images/52nlp.cn/5204d7f71f3a349bdedd346e39b1f8fb.jpg&quot; alt=&quot;prml6-98&quot; width=&quot;15&quot; height=&quot;14&quot;&gt;&lt;/a&gt;是伯努利分布，&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-99.png&quot;&gt;&lt;img class=&quot;alignnone size-full wp-image-7365&quot; src=&quot;/images/52nlp.cn/ef0a4125c6e5c6c41938ecc2119106fc.jpg&quot; alt=&quot;prml6-99&quot; width=&quot;31&quot; height=&quot;19&quot;&gt;&lt;/a&gt;是高斯分布，不是共轭的。求积分的方法有很多，可以用MCMC，也可以用变分的方法。书上用的是Laplace approximation。&lt;/p&gt;
&lt;p&gt;今天就到这里吧，我去吃饭，各位先讨论下吧。&lt;/p&gt;
&lt;p&gt;上面Gaussian Processes的公式推导虽然有点多，但都是高斯分布所以并不复杂，并且GP在算法实现上也不难。&lt;/p&gt;
&lt;p&gt;另外给大家推荐一个机器学习视频的网站，http://blog.videolectures.net/100-most-popular-machine-learning-talks-at-videolectures-net/ 里面有很多牛人比如Jordan的talks，第一个视频就是剑桥的David MacKay讲高斯过程，他的一本书 Information Theory, Inference and Learning Algorithms也很出名。&lt;/p&gt;
&lt;p&gt;两栖动物(9219642) 14:35:09&lt;br&gt;
&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-49.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7327&quot; src=&quot;/images/52nlp.cn/af732bfb85ee71047a35d203f431d357.jpg&quot; alt=&quot;prml6-49&quot; width=&quot;23&quot; height=&quot;18&quot;&gt;&lt;/a&gt;是个由基函数构成的矩阵，向量a里面的元素由&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-100.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7366&quot; src=&quot;/images/52nlp.cn/0b3feaa488279e746c4575090732914c.jpg&quot; alt=&quot;prml6-100&quot; width=&quot;200&quot; height=&quot;40&quot;&gt;&lt;/a&gt;组成。&lt;br&gt;
&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-9.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7291&quot; src=&quot;/images/52nlp.cn/a1bf0bc770bd32ff433b20fc8f354c99.jpg&quot; alt=&quot;prml6-9&quot; width=&quot;25&quot; height=&quot;22&quot;&gt;&lt;/a&gt;的维度是基函数的个数，an的维度是样本的个数把?&lt;br&gt;
网络上的尼采(813394698) 14:36:44&lt;br&gt;
对&lt;br&gt;
两栖动物(9219642) 14:37:48&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-101.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7367&quot; src=&quot;/images/52nlp.cn/523a3f38b98b6933e0365f4be6a203f4.jpg&quot; alt=&quot;prml6-101&quot; width=&quot;381&quot; height=&quot;60&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;哪这2个怎么后来乘在一起了？维度不是不一样吗？&lt;/p&gt;
&lt;p&gt;网络上的尼采(813394698) 14:49:01&lt;br&gt;
@两栖动物 &lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-9.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7291&quot; src=&quot;/images/52nlp.cn/a1bf0bc770bd32ff433b20fc8f354c99.jpg&quot; alt=&quot;prml6-9&quot; width=&quot;25&quot; height=&quot;22&quot;&gt;&lt;/a&gt;不是方阵，可以相乘&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml6-102.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7368&quot; src=&quot;/images/52nlp.cn/ae02b63ed922acd2861fe1f99f26a016.jpg&quot; alt=&quot;prml6-102&quot; width=&quot;354&quot; height=&quot;29&quot;&gt;&lt;/a&gt;&lt;br&gt;
明白了吧，另外这个由基函数表示的样本矩阵只在推导里存在。&lt;br&gt;
两栖动物(9219642) 14:56:08&lt;br&gt;
明白了，谢谢&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;注：PRML读书会系列文章由 &lt;a href=&quot;http://weibo.com/dmalgorithms&quot;&gt;@Nietzsche_复杂网络机器学习&lt;/a&gt; 同学授权发布，转载请注明原作者和相关的主讲人，谢谢。&lt;/p&gt;
&lt;p&gt;PRML读书会讲稿PDF版本以及更多资源下载地址：&lt;a href=&quot;http://vdisk.weibo.com/u/1841149974&quot;&gt;http://vdisk.weibo.com/u/1841149974&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;本文链接地址：&lt;a href=&quot;http://www.52nlp.cn/prml%E8%AF%BB%E4%B9%A6%E4%BC%9A%E7%AC%AC%E5%85%AD%E7%AB%A0-kernel-methods&quot;&gt;http://www.52nlp.cn/prml读书会第六章-kernel-methods&lt;/a&gt;&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;

											

</description>
        <pubDate>Wed, 28 Jan 2015 00:00:00 +0800</pubDate>
        <link>http://iftti.com/posts/2015/2015-01-28-prml%25e8%25af%25bb%25e4%25b9%25a6%25e4%25bc%259a%25e7%25ac%25ac%25e5%2585%25ad%25e7%25ab%25a0-kernel-methods-70bdfcff9.html</link>
        <guid isPermaLink="true">http://iftti.com/posts/2015/2015-01-28-prml%25e8%25af%25bb%25e4%25b9%25a6%25e4%25bc%259a%25e7%25ac%25ac%25e5%2585%25ad%25e7%25ab%25a0-kernel-methods-70bdfcff9.html</guid>
        
        
        <category>52nlp</category>
        
      </item>
    
      <item>
        <title>.NET中使用Redis</title>
        <description>

        &lt;!-- div style=&quot;margin-bottom: 10px;&quot;&gt;
            &lt;script language=javascript&gt;
                var randomNumLuobo = Math.round(Math.random()*1);
                var imageLuobo=new Array(2);
                imageLuobo[0]=&quot;http://jbcdn2.b0.upaiyun.com/2014/11/luobo-620x60.png&quot;;
                imageLuobo[1]=&quot;http://jbcdn2.b0.upaiyun.com/2014/11/luobo-620x60.png&quot;;
                var imageUrlLuobo=imageLuobo[randomNumLuobo];
                var urlsLuobo=new Array(2);
                urlsLuobo[0]=&quot;http://www.luobo360.com&quot;;
                urlsLuobo[1]=&quot;http://www.luobo360.com&quot;;
                var urlLuobo = urlsLuobo[randomNumLuobo];
                var adHTML = &quot;&lt;a href=&#39;&quot;+urlLuobo+&quot;&#39; target=&#39;_blank&#39;&gt;&lt;img src=&#39;&quot;+imageUrlLuobo+&quot;&#39; border=&#39;0&#39;&gt;&lt;/a&gt;&quot;;
                document.write(adHTML);
            &lt;/script&gt;
        &lt;/div --&gt;

        &lt;span style=&quot;display:block;margin-bottom:10px;&quot;&gt;&lt;/span&gt;
		
&lt;p&gt;Redis是一个&lt;a href=&quot;http://redis.io/topics/whos-using-redis&quot; target=&quot;_blank&quot;&gt;用的比较广泛&lt;/a&gt;的Key/Value的内存数据库，&lt;a href=&quot;http://minidb-wordpress.stor.sinaapp.com/uploads/2014/04/%E5%86%85%E5%AD%98%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8@sina.pdf&quot; target=&quot;_blank&quot;&gt;新浪微博&lt;/a&gt;、Github、&lt;a href=&quot;http://highscalability.com/blog/2011/3/3/stack-overflow-architecture-update-now-at-95-million-page-vi.html&quot; target=&quot;_blank&quot;&gt;StackOverflow&lt;/a&gt; 等大型应用中都用其作为缓存，Redis的官网为&lt;a href=&quot;http://redis.io/&quot; target=&quot;_blank&quot;&gt;http://redis.io/&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;最近项目中需要使用Redis，这里简单记录一下Redis的安装，以及如何在.NET中使用Redis。&lt;/p&gt;
&lt;h1&gt;Redis安装与启动&lt;/h1&gt;
&lt;h2&gt;1. 下载Redis&lt;/h2&gt;
&lt;p&gt;Redis本身没有提供Windows版本的，并且在Windows上也不太稳定，一般都将其部署到Linux环境下，Redis可以在其官网上&lt;a href=&quot;http://redis.io/download&quot; target=&quot;_blank&quot;&gt;下载&lt;/a&gt;， &lt;a href=&quot;https://github.com/MSOpenTech/redis&quot; target=&quot;_blank&quot;&gt;MSOpenTech&lt;/a&gt;中提供了Windows版本，这里为了学习安装这一版本。&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/8442a72d1a503a94e3181aee55772d51.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;点击跳转到Github后，直接点击Zip下载。下载后根据自己计算机的版本选择32位或者64位进行安装。我将64位的解压后放到D:\Redis文件夹下，同时将文件夹内的redis.conf也拷贝到该目录下，这个是redis的配置信息：&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/525f9938da14bc0aa2d67d6a84a3c8ed.jpg&quot;&gt;&lt;/p&gt;
&lt;h2&gt;2. 启动Redis&lt;/h2&gt;
&lt;p&gt;在Windows下面启用Redis和启动MogoDB一样，需要使用命令行启动，首先定位到该目录，运行如下命令：&lt;/p&gt;
&lt;pre class=&quot;brush: text; gutter: true&quot;&gt;D:\Redis&amp;gt;redis-server.exe redis.conf&lt;/pre&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/672c6a847ff1a98a8031bd2371b987e7.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;因为是在本机运行的，这里要注意端口号，同时要保持端口不要关闭。&lt;/p&gt;
&lt;p&gt;当然您也可以将&lt;a href=&quot;http://stackoverflow.com/questions/6476945/how-do-i-run-redis-on-windows&quot; target=&quot;_blank&quot;&gt;Redis作为Windows服务&lt;/a&gt;在后台一直开启。&lt;/p&gt;
&lt;h2&gt;3. 使用&lt;/h2&gt;
&lt;p&gt;现在再开一个控制台应用程序连接之前启动的Redis，如下：&lt;/p&gt;
&lt;pre class=&quot;brush: text; gutter: true&quot;&gt;D:\Redis&amp;gt;redis-cli.exe -h 172.16.147.121 -p 6379&lt;/pre&gt;
&lt;p&gt;其中 –h后面是本机的ip地址，后面的是端口。&lt;/p&gt;
&lt;p&gt;然后就可以执行set 给key为city赋值:&lt;/p&gt;
&lt;pre class=&quot;brush: text; gutter: true&quot;&gt;redis 172.16.147.121:6379&amp;gt; set city Shanghai&lt;/pre&gt;
&lt;p&gt;通过get可以获取指定key为city的值了。&lt;/p&gt;
&lt;pre class=&quot;brush: text; gutter: true&quot;&gt;redis 172.16.147.121:6379&amp;gt; get city&lt;/pre&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/b69e2c032d26cc069445a6697f433fcf.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;同时，在我们往redis上写数据的时候，Redis服务也会定时的往文件中写数据&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/66680418c64422276fe3a5e200b492db.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;这里仅简单的介绍了get和set命令，更多命令可以查看 &lt;a href=&quot;http://redis.io/commands&quot; target=&quot;_blank&quot;&gt;http://redis.io/commands&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;.初探Redis&lt;/h1&gt;
&lt;h2&gt;下载ServiceStack.Redis&lt;/h2&gt;
&lt;p&gt;和MongoDB一样，在.NET中使用Redis其实也是使用第三方驱动，&lt;a href=&quot;http://redis.io/clients&quot; target=&quot;_blank&quot;&gt;官网推荐&lt;/a&gt;的是使用&lt;a href=&quot;https://github.com/ServiceStack/ServiceStack.Redis&quot; target=&quot;_blank&quot;&gt;ServiceStack.Redis&lt;/a&gt; 下载后解压得到如下dll&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/8ef5ca9374ebd86324497ab3b6063073.jpg&quot;&gt;&lt;/p&gt;
&lt;h4&gt;.NET项目中使用Redis&lt;/h4&gt;
&lt;p&gt;新建一个Console程序，引用上一步骤解压的四个dll。&lt;/p&gt;
&lt;p&gt;做一个简单的例子，在.NET中获取之前我们设置的city的值。&lt;/p&gt;
&lt;pre class=&quot;brush: csharp; gutter: true&quot;&gt;class Program
{
    static RedisClient redisClient = new RedisClient(&quot;172.16.147.121&quot;, 6379);//redis服务IP和端口
    static void Main(string[] args)
    {
        Console.WriteLine(redisClient.Get&amp;lt;string&amp;gt;(&quot;city&quot;));
        Console.ReadKey();
    }
}&lt;/pre&gt;
&lt;p&gt;首先通过 static RedisClient redisClient = new RedisClient(“172.16.147.121″, 6379);&lt;/p&gt;
&lt;p&gt;建立连接 ，然后就可以直接用redisClient里面的Get方法获取 key为city的值了。&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/e70c120d4b93ac160cd466b9aaf86d9a.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;在前面的命令行中，我们网city中存入了Shanghai，现在我们获取到了这个值。&lt;/p&gt;
&lt;p&gt;ServerStack中有很多方法可以在.NET中调用，其类结构图如下：&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/d487cbe53eef2bda29e9f008e0714d72.jpg&quot;&gt;&lt;/p&gt;
&lt;h1&gt;总结&lt;/h1&gt;


        
        &lt;!-- BEGIN #author-bio --&gt;


&lt;!-- END #author-bio --&gt;
	

</description>
        <pubDate>Wed, 28 Jan 2015 00:00:00 +0800</pubDate>
        <link>http://iftti.com/posts/2015/2015-01-28-83821-cb2ee3ffb.html</link>
        <guid isPermaLink="true">http://iftti.com/posts/2015/2015-01-28-83821-cb2ee3ffb.html</guid>
        
        
        <category>jobbole</category>
        
      </item>
    
      <item>
        <title>PRML读书会第五章  Neural Networks</title>
        <description>

						&lt;p style=&quot;text-align: center&quot;&gt;&lt;strong&gt;PRML&lt;/strong&gt;&lt;strong&gt;读书会第五章 Neural Networks&lt;/strong&gt;&lt;/p&gt;
&lt;p style=&quot;text-align: center&quot;&gt;&lt;strong&gt;主讲人 网神&lt;/strong&gt;&lt;/p&gt;
&lt;p style=&quot;text-align: center&quot;&gt;&lt;strong&gt;（新浪微博:&lt;a href=&quot;http://weibo.com/ghtimaq&quot;&gt;@豆角茄子麻酱凉面&lt;/a&gt;）&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;网神(66707180) 18:55:06&lt;/p&gt;
&lt;p&gt;那我们开始了啊，前面第3,4章讲了回归和分类问题，他们应用的主要限制是维度灾难问题。今天的第5章神经网络的内容：&lt;br&gt;
1. 神经网络的定义&lt;br&gt;
2. 训练方法：error函数，梯度下降，后向传导&lt;br&gt;
3. 正则化：几种主要方法，重点讲卷积网络&lt;/p&gt;
&lt;p&gt;书上提到的这些内容今天先不讲了，以后有时间再讲：BP在Jacobian和Hessian矩阵中求导的应用；&lt;br&gt;
混合密度网络；贝叶斯解释神经网络。&lt;/p&gt;
&lt;p&gt;首先是神经网络的定义，先看一个最简单的神经网络，只有一个神经元：&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml5-0.png&quot;&gt;&lt;img class=&quot;aligncenter wp-image-7237&quot; src=&quot;/images/52nlp.cn/5f5d8a997f06dc1f5c2179b2ab219ddf.jpg&quot; alt=&quot;prml5-0&quot; width=&quot;400&quot; height=&quot;179&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;这个神经元是一个以x1,x2,x3和截距1为输入的运算单元，其输出是：&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml5-1.png&quot;&gt;&lt;img class=&quot;aligncenter wp-image-7238&quot; src=&quot;/images/52nlp.cn/01e9dc088a9e751f673279fe3aecde5f.jpg&quot; alt=&quot;prml5-1&quot; width=&quot;350&quot; height=&quot;22&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;其中函数f成为”激活函数” , activation function.激活函数根据实际应用确定，经常选择sigmoid函数.如果是sigmoid函数，这个神经元的输入-输出的映射就是一个logistic回归问题。&lt;/p&gt;
&lt;p&gt;&lt;span id=&quot;more-7236&quot;&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;神经网络就是将许多个神经元连接在一起组成的网络，如图：&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml5-2.png&quot;&gt;&lt;img class=&quot;aligncenter wp-image-7239&quot; src=&quot;/images/52nlp.cn/69eca26b0c0bdc05eeedf171565501c1.jpg&quot; alt=&quot;prml5-2&quot; width=&quot;400&quot; height=&quot;361&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;神经网络的图跟后面第八章图模型不同， 图模型中，每个节点都是一个随机变量，符合某个分布。神经网络中的节点是确定的值，由与其相连的节点唯一确定。&lt;/p&gt;
&lt;p&gt;上图这个神经网络包含三层，左边是输入层，x1…xd节点是输入节点， x0是截距项。最右边是输出层，y1,…,yk是输出节点. 中间是隐藏层hidden level, z1,…,zM是隐藏节点，因为不能从训练样本中观察到他们的值，所以叫隐藏层。&lt;/p&gt;
&lt;p&gt;可以把神经网络描述为一系列的函数转换。首先，构造M个输入节点的线性组合：&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml5-5.png&quot;&gt;&lt;img class=&quot;aligncenter wp-image-7240&quot; src=&quot;/images/52nlp.cn/0b61129441c5ff88b94bbabf5821d43f.jpg&quot; alt=&quot;prml5-5&quot; width=&quot;200&quot; height=&quot;59&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;上式中，j=1,…,M，对应M个隐藏节点. 上标(1)表示这是第一层的参数。wji是权重weight,wj0是偏置.  aj 叫做activation. 中文叫激活吧，感觉有点别扭。&lt;/p&gt;
&lt;p&gt;把activation aj输入到一个非线性激活函数h(.) &lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml5-6.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7241&quot; src=&quot;/images/52nlp.cn/d602aff92ee5efe3f71eaacda1c36c75.jpg&quot; alt=&quot;prml5-6&quot; width=&quot;78&quot; height=&quot;20&quot;&gt;&lt;/a&gt;就得到了隐藏节点的值zj。&lt;/p&gt;
&lt;p&gt;在这个从输入层到隐藏层的转换中，这个激活函数不能是线性的，接下来，将隐藏节点的值线性组合得到输出节点的activation：&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml5-8.png&quot;&gt;&lt;img class=&quot;aligncenter wp-image-7242&quot; src=&quot;/images/52nlp.cn/02674cd20d6f5097f315a7e2cdd1fe62.jpg&quot; alt=&quot;prml5-8&quot; width=&quot;200&quot; height=&quot;61&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;每个ak输入一个输出层激活函数&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml5-9.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7243&quot; src=&quot;/images/52nlp.cn/9cce7d2ade857e08d21b7b2e4808c31d.jpg&quot; alt=&quot;prml5-9&quot; width=&quot;86&quot; height=&quot;20&quot;&gt;&lt;/a&gt;，就得到了输出值。&lt;/p&gt;
&lt;p&gt;这个从隐藏层到输出层的激活函数σ,根据不同应用，有不同的选择，例如回归问题的激活函数是identity函数,既y = a.分类问题的激活函数选择sigmoid函数，multiclass分类选择是softmax函数.&lt;/p&gt;
&lt;p&gt;把上面各阶段的计算连起来，神经网络整个的计算过程就是：&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml5-11.png&quot;&gt;&lt;img class=&quot;aligncenter wp-image-7244&quot; src=&quot;/images/52nlp.cn/4f2892e794b058fd56e1e31409983032.jpg&quot; alt=&quot;prml5-11&quot; width=&quot;400&quot; height=&quot;64&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;上面计算过程是以2层神经网络为例。实际使用的NN可能有多层，记得听一个报告现在很火的deep learning的隐藏层有5-9层.这个计算过程forward propagation，从前向后计算。与此相反，训练时，会用back propagation从后向前来计算偏导数。&lt;/p&gt;
&lt;p&gt;神经网络有很强的拟合能力，可以拟合很多种的曲线，这个图是书上的一个例子，对四种曲线的拟合：&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml5-13.png&quot;&gt;&lt;img class=&quot;aligncenter wp-image-7245&quot; src=&quot;/images/52nlp.cn/41907d61532018241e181a77e026cb64.jpg&quot; alt=&quot;prml5-13&quot; width=&quot;400&quot; height=&quot;233&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;第一部分神经网络的定义就这么多，大家有没有什么问题啊？&lt;/p&gt;
&lt;p&gt;============================讨论=================================&lt;/p&gt;
&lt;p&gt;阳阳(236995728) 19:20:43&lt;/p&gt;
&lt;p&gt;输入层到隐藏层使用的激活函数与隐藏层到输出层的激活函数是否要一致？&lt;/p&gt;
&lt;p&gt;网神(66707180) 19:21:11&lt;/p&gt;
&lt;p&gt;两层激活函数不一定一致，输入层到隐藏层 经常是sigmoid函数。 而隐藏层到输出层，根据应用不同，选择不同.&lt;/p&gt;
&lt;p&gt;牧云(1106207961) 19:22:30&lt;/p&gt;
&lt;p&gt;一般的算法，比如神经网络，都有分类和拟合的功能，分类和拟合有共同点吗？为什么能拟合，这个问题我没有找到地方清晰的解释。&lt;/p&gt;
&lt;p&gt;独孤圣者(303957511) 19:23:47&lt;/p&gt;
&lt;p&gt;拟合和分类，在我看来实际上是一样的，分类无非是只有2个或多个值的拟合而已。2个值的拟合，被称为二值分类&lt;/p&gt;
&lt;p&gt;ant/wxony(824976094) 19:24:30&lt;/p&gt;
&lt;p&gt;不是吧，svm做二值分类，就不是以拟合为目标。二值拟合可以用做分类。&lt;/p&gt;
&lt;p&gt;独孤圣者(303957511) 19:29:42&lt;/p&gt;
&lt;p&gt;可以作为概率来理解吧，我个人认为啊，sigmoid函数，是将分类结果做一个概率的计算。&lt;/p&gt;
&lt;p&gt;阳阳(236995728) 19:24:07&lt;/p&gt;
&lt;p&gt;输入层到隐藏层 经常是sigmoid函数 那么也就是特征值变成了【0-1】之间的数了？&lt;/p&gt;
&lt;p&gt;网神(66707180) 19:25:19&lt;/p&gt;
&lt;p&gt;是的，我认为是.机器学习经常需要做特征归一化，也是把特征归一化到[0，1]范围。当然这里不只是归一化。&lt;/p&gt;
&lt;p&gt;阳阳(236995728) 19:26:56&lt;/p&gt;
&lt;p&gt;这里应该不是归一化的作用@网神&lt;/p&gt;
&lt;p&gt;网神(66707180) 19:27:33&lt;/p&gt;
&lt;p&gt;嗯，不是归一化，我觉得神经网络的这些做法，没有一个科学的解释。不像SVM每一步都有严谨的理论。&lt;/p&gt;
&lt;p&gt;罗杰兔(38900407) 19:29:00&lt;/p&gt;
&lt;p&gt;参数W,b是算出来的，还是有些技巧得到的。因为常听人说，神经网络难就难在调试参数上。&lt;/p&gt;
&lt;p&gt;阳阳(236995728) 19:29:11&lt;/p&gt;
&lt;p&gt;我觉得神经网络也是在学习新的特征 而这些特征比较抽象难于理解@牧云 @网神&lt;/p&gt;
&lt;p&gt;网神(66707180) 19:29:48&lt;/p&gt;
&lt;p&gt;对，我理解隐藏层的目的就是学习特征&lt;/p&gt;
&lt;p&gt;阳阳(236995728) 19:30:24&lt;/p&gt;
&lt;p&gt;是的 但是这些特征难于理解较抽象@网神&lt;/p&gt;
&lt;p&gt;网神(66707180) 19:30:47&lt;/p&gt;
&lt;p&gt;最后一个隐藏层到输出层之间，就是做分类或回归。前面的不同隐藏层，则是提取特征。&lt;/p&gt;
&lt;p&gt;独孤圣者(303957511) 19:30:52&lt;/p&gt;
&lt;p&gt;隐藏层的目的就是学习特征，这个表示赞同&lt;/p&gt;
&lt;p&gt;网神(66707180) 19:31:33&lt;/p&gt;
&lt;p&gt;后面讲到卷积网络时，可以很明显感受到，隐藏层的目的就是学习特征。&lt;/p&gt;
&lt;p&gt;阳阳(236995728) 19:31:59&lt;/p&gt;
&lt;p&gt;但是我不理解为什么隐藏层学习到的特征值介于【0-1】之间是合理的？是归一化的作用？&lt;/p&gt;
&lt;p&gt;牧云(1106207961) 19:32:47&lt;/p&gt;
&lt;p&gt;归一化映射&lt;/p&gt;
&lt;p&gt;Wilbur_中博(1954123) 19:33:59&lt;/p&gt;
&lt;p&gt;要归一化也是对每一个输入变量做吧。。各个输入变量都组合在一起了，归一化干嘛。我理解就是一个非线性变换，让神经网络具有非线性能力。sigmoid之外，其他非线性变换用的也蛮多的。和归一化没什么关系。不一定要在[0, 1]之间。&lt;/p&gt;
&lt;p&gt;独孤圣者(303957511) 19:39:50&lt;/p&gt;
&lt;p&gt;这个解释我很同意，tanh函数也经常替换sigmoid函数，在神经网络中普遍使用。&lt;/p&gt;
&lt;p&gt;网神(66707180) 19:39:59&lt;/p&gt;
&lt;p&gt;嗯，让NN具有非线性能力是主要原因之一。&lt;/p&gt;
&lt;p&gt;牧云(1106207961) 19:40:03&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml5-14.png&quot;&gt;&lt;img class=&quot;aligncenter wp-image-7246&quot; src=&quot;/images/52nlp.cn/e2a90aef450857c61c2b9a8001f6cd13.jpg&quot; alt=&quot;prml5-14&quot; width=&quot;400&quot; height=&quot;208&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;这个1要的干嘛&lt;/p&gt;
&lt;p&gt;网神(66707180) 19:40:18&lt;/p&gt;
&lt;p&gt;这个是偏置项，y = wx+b， 那个+1就是为了把b 合入w,变成　ｙ＝ｗｘ&lt;/p&gt;
&lt;p&gt;罗杰兔(38900407) 19:41:36&lt;/p&gt;
&lt;p&gt;Ng好象讲b是截距&lt;/p&gt;
&lt;p&gt;网神(66707180) 19:43:16&lt;/p&gt;
&lt;p&gt;截距和偏执项是一个意思。 放在坐标系上，b就是截距&lt;/p&gt;
&lt;p&gt;阳阳(236995728) 19:40:33&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml5-15.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7247&quot; src=&quot;/images/52nlp.cn/5c97d2d6365820bfb405c8bea3506ff9.jpg&quot; alt=&quot;prml5-15&quot; width=&quot;300&quot; height=&quot;40&quot;&gt;&lt;/a&gt;它可以逼近任意的非线性函数吗@网神&lt;/p&gt;
&lt;p&gt;网神(66707180) 19:42:13&lt;/p&gt;
&lt;p&gt;书上没说可以逼近任意曲线，说的是逼近很多曲线&lt;/p&gt;
&lt;p&gt;阳阳(236995728) 19:42:58&lt;/p&gt;
&lt;p&gt;它到底是怎么逼近的啊@网神  我想看看多个函数叠加的？&lt;/p&gt;
&lt;p&gt;网神(66707180) 19:44:30&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml5-16.png&quot;&gt;&lt;img class=&quot;aligncenter wp-image-7248&quot; src=&quot;/images/52nlp.cn/b5f9b4ee6f7d0c10f18c7721db314c9e.jpg&quot; alt=&quot;prml5-16&quot; width=&quot;400&quot; height=&quot;297&quot;&gt;&lt;/a&gt;&lt;br&gt;
就是多个函数叠加。这个图上那个抛物线就是最终的曲线，由另外三条曲线叠加而成，另外那三条，每条是一个隐藏节点生成的曲线。&lt;/p&gt;
&lt;p&gt;阳阳(236995728) 19:45:05&lt;/p&gt;
&lt;p&gt;这三条曲线用的函数形式是不是一样的&lt;/p&gt;
&lt;p&gt;zeno(117127143) 19:45:31&lt;/p&gt;
&lt;p&gt;理解nn怎么逼近xor函数，就知道怎么非线性的了&lt;/p&gt;
&lt;p&gt;HX(458728037) 19:45:56&lt;/p&gt;
&lt;p&gt;问一句，你现在讲的是BP网络吧？&lt;/p&gt;
&lt;p&gt;网神(66707180) 19:46:07&lt;/p&gt;
&lt;p&gt;是BP网络&lt;/p&gt;
&lt;p&gt;========================讨论结束===============================&lt;/p&gt;
&lt;p&gt;网神(66707180) 19:46:59&lt;/p&gt;
&lt;p&gt;接下来是神经网络的训练。神经网络训练的套路与第三，四章相同：确定目标变量的分布函数和似然函数，&lt;br&gt;
取负对数，得到error函数，用梯度下降法求使error函数最小的参数w和b。&lt;/p&gt;
&lt;p&gt;NN的应用有：回归、binary分类、K个独立binary分类、multiclass分类。不同的应用决定了不同的激活函数、不同的目标变量的条件分布，不同的error函数.&lt;/p&gt;
&lt;p&gt;首先，对于回归问题，输出变量t的条件概率符合高斯分布：&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml5-17.png&quot;&gt;&lt;img class=&quot;aligncenter wp-image-7250&quot; src=&quot;/images/52nlp.cn/59bd65aeaf585bee965e56fbe0373975.jpg&quot; alt=&quot;prml5-17&quot; width=&quot;300&quot; height=&quot;41&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;因此，给定一个训练样本集合，其似然函数是：&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml5-18.png&quot;&gt;&lt;img class=&quot;aligncenter wp-image-7251&quot; src=&quot;/images/52nlp.cn/d11e985e7e3f13e99987bd6dd2af40ac.jpg&quot; alt=&quot;prml5-18&quot; width=&quot;300&quot; height=&quot;59&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;error函数是对似然函数取 负对数，得到：&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml5-19.png&quot;&gt;&lt;img class=&quot;aligncenter wp-image-7252&quot; src=&quot;/images/52nlp.cn/7d51aa630ff270995d89f6e7e26529cb.jpg&quot; alt=&quot;prml5-19&quot; width=&quot;300&quot; height=&quot;59&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;最小化这个error函数，只需要最小化第一项，所以回归问题的errro函数定义是：&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml5-20.png&quot;&gt;&lt;img class=&quot;aligncenter wp-image-7253&quot; src=&quot;/images/52nlp.cn/f9318e1cc0e785d85d40ee1c263ca6b5.jpg&quot; alt=&quot;prml5-20&quot; width=&quot;300&quot; height=&quot;58&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;回归问题的隐藏层到输出层的激活函数是identity函数，也就是 &lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml5-21.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7254&quot; src=&quot;/images/52nlp.cn/de4427cf9ee819098348e17004226616.jpg&quot; alt=&quot;prml5-21&quot; width=&quot;70&quot; height=&quot;23&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;ak就是对隐藏层节点的线性组合：&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml5-22.png&quot;&gt;&lt;img class=&quot;aligncenter wp-image-7255&quot; src=&quot;/images/52nlp.cn/624d58f619525abfee731c1c3098e806.jpg&quot; alt=&quot;prml5-22&quot; width=&quot;300&quot; height=&quot;83&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;这样有了error函数，就可以同梯度下降法求极值点.&lt;/p&gt;
&lt;p&gt;再说说binary分类和multiclass分类的error函数：&lt;/p&gt;
&lt;p&gt;binary分类的目标变量条件分布式伯努利分布：&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml5-24.png&quot;&gt;&lt;img class=&quot;aligncenter wp-image-7256&quot; src=&quot;/images/52nlp.cn/82de8ac138cddff7f3570f79acf1f326.jpg&quot; alt=&quot;prml5-24&quot; width=&quot;300&quot; height=&quot;42&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;这样，通过对似然函数取负对数，得到error函数如下：&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml5-25.png&quot;&gt;&lt;img class=&quot;aligncenter wp-image-7257&quot; src=&quot;/images/52nlp.cn/b61f91a5a35a40611cd8ca2ec8b2388e.jpg&quot; alt=&quot;prml5-25&quot; width=&quot;300&quot; height=&quot;49&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;另外binary分类，隐藏层到输出层的激活函数是sigmoid函数。对multiclass问题，也就是结果是K个互斥的类别中的一个.类似思路可以得到其error函数.这里就不说了。书上都有.&lt;/p&gt;
&lt;p&gt;知道了error函数，和激活函数，这里先做一个计算，为后面的BP做准备，将E(w)对激活ak求导，可得：&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml5-28.png&quot;&gt;&lt;img class=&quot;aligncenter wp-image-7258&quot; src=&quot;/images/52nlp.cn/b57391a8ca28a913dfc8a5c889430156.jpg&quot; alt=&quot;prml5-28&quot; width=&quot;200&quot; height=&quot;73&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;这里有意思的是，不管是回归、binary分类还是multiclass分类，尽管其error函数不同，这个求导得出的结果都是yk-tk&lt;/p&gt;
&lt;p&gt;回归问题这个求导很明显，下式中，y = a, 所以对a求导就是 y – t.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml5-29.png&quot;&gt;&lt;img class=&quot;aligncenter wp-image-7259&quot; src=&quot;/images/52nlp.cn/8cffa8a59d8cf9b5b60c1253d71f3b39.jpg&quot; alt=&quot;prml5-29&quot; width=&quot;300&quot; height=&quot;67&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;其他错误函数，需要推导一下，这里就不推了。有了error函数，用梯度下降法求其极小值点。&lt;/p&gt;
&lt;p&gt;因为输入层到隐藏层的激活函数是非线性函数，所以error函数是非凸函数，所以可能存在很多个局部极值点。&lt;/p&gt;
&lt;p&gt;两栖动物(9219642) 20:07:40&lt;/p&gt;
&lt;p&gt;你这里说的回归问题是用神经网络拟合随机变量？&lt;/p&gt;
&lt;p&gt;网神(66707180) 20:07:56&lt;/p&gt;
&lt;p&gt;是的，就是做线性回归要做的事.只是用神经网络的方法，可以拟合更复杂的曲线。这是我的理解。&lt;/p&gt;
&lt;p&gt;梯度下降法的思路就是对error函数求导，然后按下式对参数作调整：&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml5-30.png&quot;&gt;&lt;img class=&quot;aligncenter wp-image-7260&quot; src=&quot;/images/52nlp.cn/65092f33c1edb779b609dd078b5368a7.jpg&quot; alt=&quot;prml5-30&quot; width=&quot;300&quot; height=&quot;46&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;一直到导数为0，就找到了极值点.&lt;/p&gt;
&lt;p&gt;这是基本思路，实际上很多算法，如共轭梯度法，拟牛顿法等，可以更有效的寻找极值点。因为梯度下降是机器学习中求极值点的常用方法，这些算法不是本章的内容，就不讲了，有专门的章节讲这些算法。&lt;/p&gt;
&lt;p&gt;这里要讲的是error函数对参数w的求导方法，也就是back propagation后向传导法。&lt;br&gt;
可以高效的计算这个导数。&lt;/p&gt;
&lt;p&gt;这个导数的计算利用的 求导数的链式法则， &lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml5-31.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7261&quot; src=&quot;/images/52nlp.cn/655d67bf9874274f04ff2b3619f8ca49.jpg&quot; alt=&quot;prml5-31&quot; width=&quot;150&quot; height=&quot;47&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;把E对w的求导，转换成E对activation a的求导 和 a 对w 的求导. 这个地方因为涉及的式子比较多，我在纸上写了一下，我整个贴上吧：&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml5-32.png&quot;&gt;&lt;img class=&quot;aligncenter wp-image-7262&quot; src=&quot;/images/52nlp.cn/fff486cfd234de2d3b996fa2401695a7.jpg&quot; alt=&quot;prml5-32&quot; width=&quot;500&quot; height=&quot;500&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;大家看看，可以讨论一下&lt;/p&gt;
&lt;p&gt;阳阳(236995728) 20:21:30&lt;/p&gt;
&lt;p&gt;error function是负对数似然函数吧&lt;/p&gt;
&lt;p&gt;网神(66707180) 20:21:38&lt;/p&gt;
&lt;p&gt;是的&lt;/p&gt;
&lt;p&gt;天上的月亮(785011830) 20:22:35&lt;/p&gt;
&lt;p&gt;更新hidden层的w，为什么利用偏导的chain rule呢？&lt;/p&gt;
&lt;p&gt;网神(66707180) 20:22:49&lt;/p&gt;
&lt;p&gt;最主要的结论就是：&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml5-35.png&quot;&gt;&lt;img class=&quot;wp-image-7264 aligncenter&quot; src=&quot;/images/52nlp.cn/fe565148104f8710845b82cb0e70777a.jpg&quot; alt=&quot;prml5-35&quot; width=&quot;200&quot; height=&quot;55&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;对于输出层的节点，&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml5-36.png&quot;&gt;&lt;img class=&quot;alignnone size-full wp-image-7265&quot; src=&quot;/images/52nlp.cn/6714e07da34f52a08de77926917e96f3.jpg&quot; alt=&quot;prml5-36&quot; width=&quot;31&quot; height=&quot;31&quot;&gt;&lt;/a&gt; &lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml5-37.png&quot;&gt;&lt;img class=&quot;alignnone size-full wp-image-7266&quot; src=&quot;/images/52nlp.cn/3bbc3d38c72005e29effce3cc2c8de17.jpg&quot; alt=&quot;prml5-37&quot; width=&quot;54&quot; height=&quot;31&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;对于隐藏层的节点，&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml5-38.png&quot;&gt;&lt;img class=&quot;alignnone size-full wp-image-7267&quot; src=&quot;/images/52nlp.cn/f2d9b21802a0479863c600368176b125.jpg&quot; alt=&quot;prml5-38&quot; width=&quot;31&quot; height=&quot;31&quot;&gt;&lt;/a&gt; &lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml5-39.png&quot;&gt;&lt;img class=&quot;alignnone size-full wp-image-7268&quot; src=&quot;/images/52nlp.cn/331afcc36f88be8020ea588423d37c20.jpg&quot; alt=&quot;prml5-39&quot; width=&quot;99&quot; height=&quot;31&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;E对隐藏层节点w的导数，通过上式，就由输出层节点的导数 求得了.&lt;/p&gt;
&lt;p&gt;有了上面求偏导数的方法，整个训练过程就是：&lt;br&gt;
1.输入一个样本Xn,根据forward propagation得到每个hidden单元和output单元的激活值a.&lt;br&gt;
2.计算每个output单元的偏导数&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml5-40.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7269&quot; src=&quot;/images/52nlp.cn/ba37c4a89b918434852c7844a6b91054.jpg&quot; alt=&quot;prml5-40&quot; width=&quot;30&quot; height=&quot;27&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;3.根据反向传导公式，计算每个hidden单元的&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml5-42.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7270&quot; src=&quot;/images/52nlp.cn/d5a1794198eb53534d07efc036d49700.jpg&quot; alt=&quot;prml5-42&quot; width=&quot;25&quot; height=&quot;30&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;4.计算偏导数。&lt;br&gt;
5.用一定的步长更新参数w。&lt;br&gt;
循环往复，一直找到满意的w。&lt;/p&gt;
&lt;p&gt;独孤圣者(303957511) 20:32:09&lt;/p&gt;
&lt;p&gt;BP时，是不是每个样本都要反馈一次？&lt;/p&gt;
&lt;p&gt;网神(66707180) 20:33:32&lt;/p&gt;
&lt;p&gt;应该是每个样本都反馈一次，batch也是.batch形式的每个样本反馈一次，将每个样本的偏导数求和，如下式，得到所有样本的偏导数，然后做一次参数w的调整.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml5-43.png&quot;&gt;&lt;img class=&quot;aligncenter wp-image-7271&quot; src=&quot;/images/52nlp.cn/7fed0a1cd8c4608ee8418b644c78d892.jpg&quot; alt=&quot;prml5-43&quot; width=&quot;250&quot; height=&quot;93&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;接下来就讲正则化，神经网络参数多，训练复杂，所以正则化方法也被研究的很多，书上讲了很多种的正则化方法. 第一个，就是在error函数上，加上正则项。&lt;br&gt;
第三章回归的正则项如下：&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml5-45.png&quot;&gt;&lt;img class=&quot;aligncenter wp-image-7272&quot; src=&quot;/images/52nlp.cn/f4833c1e6ecfbe0bf8d60c2d1fc84ac5.jpg&quot; alt=&quot;prml5-45&quot; width=&quot;350&quot; height=&quot;45&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;后面那一项对过大的w做惩罚。&lt;/p&gt;
&lt;p&gt;但是对神经网络，正则项需要满足 一个缩放不变性，所以正则项的形式如下：&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml5-48.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7273 size-full&quot; src=&quot;/images/52nlp.cn/c6e00bbc43df0c771b5cf5072bd952bd.jpg&quot; alt=&quot;prml5-48&quot; width=&quot;84&quot; height=&quot;25&quot;&gt;&lt;/a&gt; &lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml5-49.png&quot;&gt;&lt;img class=&quot;alignnone wp-image-7274 size-full&quot; src=&quot;/images/52nlp.cn/a695d5c9e88933c020488d4676ee6a01.jpg&quot; alt=&quot;prml5-49&quot; width=&quot;107&quot; height=&quot;28&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;这个形式怎么推导出来的，大家看书吧，这里不说了。&lt;/p&gt;
&lt;p&gt;书上讲到的第二种正则化方法是early stop，我理解的思路就是 在训练集上每调整一次w, 得到新的参数，就在验证集上验证一下。 开始在验证集上的error是逐渐下降的，后来就不下降了，会上升。那么在开始上升之前，就停止训练，那个参数就是最佳情况。&lt;/p&gt;
&lt;p&gt;忘了说一个正则化注意对象了，就是隐藏节点的数量M。这个M是预先人为确定的，他的大小决定了整个模型的参数多少。所以是影响 欠拟合还是过拟合的 关键因素。&lt;/p&gt;
&lt;p&gt;monica(909117539) 20:42:38&lt;/p&gt;
&lt;p&gt;开始上升是出现了过拟合么？节点数量和层数都是怎样选择的呀？&lt;/p&gt;
&lt;p&gt;网神(66707180) 20:42:56&lt;/p&gt;
&lt;p&gt;开始上升就是过拟合了，是的，在训练集上，error还是下降的，但验证集上已经上升了。节点数量M是人为确定，我想这个数怎么定，是NN调参的重点。&lt;/p&gt;
&lt;p&gt;monica(909117539) 20:44:36&lt;/p&gt;
&lt;p&gt;：）&lt;/p&gt;
&lt;p&gt;网神(66707180) 20:44:38&lt;/p&gt;
&lt;p&gt;ML牛的人，叫做 调得一手好参，我觉得这里最能体现。&lt;/p&gt;
&lt;p&gt;这个图是不同的M值对拟合情况的影响：&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml5-50.png&quot;&gt;&lt;img class=&quot;aligncenter wp-image-7275&quot; src=&quot;/images/52nlp.cn/81bd8fea05133d290cf9bc4f6a6b194b.jpg&quot; alt=&quot;prml5-50&quot; width=&quot;400&quot; height=&quot;178&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;另外，因为NN又不是凸函数，导致train更麻烦&lt;/p&gt;
&lt;p&gt;monica(909117539) 20:47:01&lt;/p&gt;
&lt;p&gt;嗯，这个能理解，前面模型太简单，欠拟合了，后面模型太复杂，就过拟合了。&lt;/p&gt;
&lt;p&gt;网神(66707180) 20:47:08&lt;/p&gt;
&lt;p&gt;需要尝试不同的M值，对于每一个M值，又需要train多次，每次随即初始化w的值，然后尝试找到不同的局部极值点。&lt;/p&gt;
&lt;p&gt;上面说了三种正则化方法。接下来，因为神经网络在图像识别中，应用广泛，所以图像识别对正则化有些特殊的需求，就是 平移、缩放、旋转不变性。不知道这个需求在其他应用中，是否存在，例如NLP中，反正书上都是以图像处理为例讲的.&lt;/p&gt;
&lt;p&gt;这些不变形，就是图像中的物体位置移动、大小变化、或者一定程度的旋转， 对ouput层的结果应该没有影响。&lt;/p&gt;
&lt;p&gt;这里主要讲卷积神经网络。这个东东好像很有用，我看在Ng的deep learning教程中也重点讲了，当图像是大图时，如100×100像素以上，卷积网络可以大大减少参数w的个数.&lt;/p&gt;
&lt;p&gt;卷积网络的input层的unit是图像的像素值.卷积网络的hidden层由卷积层和子采样层组成，如图：&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml5-51.png&quot;&gt;&lt;img class=&quot;aligncenter wp-image-7276&quot; src=&quot;/images/52nlp.cn/7c575040a39db2b44e223556d185b94f.jpg&quot; alt=&quot;prml5-51&quot; width=&quot;400&quot; height=&quot;191&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;一个卷及网络可能包括多个卷积层和子采样层，如下图有两对卷积/子采样层：&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml5-53.png&quot;&gt;&lt;img class=&quot;aligncenter wp-image-7277&quot; src=&quot;/images/52nlp.cn/ca52f3497bae2dfa1c0ea0085ce5aaf1.jpg&quot; alt=&quot;prml5-53&quot; width=&quot;400&quot; height=&quot;203&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;卷积层的units被划分成多个planes，每个plane是一个feature map，一个feature map里的一个unit只连接input层的一个小区域，一个feature map里的所有unit使用相同的weight值。&lt;br&gt;
卷积层的作用可以认为是提取特征。每个plane就是一个特征滤波器，通过卷积的方式提取图像的一种特征，过滤掉不是本特征的信息。比如提取100种特征，就会有100个plane，上图中，提取6个特征，第二层有6个平面。&lt;br&gt;
一个plane里面的一个unit与图像的一个子区域相连，unit的值表示从该区域提取的一个特征值。共有多少个子区域，plane里就有多少个unit。&lt;br&gt;
以上图为例，输入图像是32×32像素，所以input层有32×32个unit. 取子区域的大小为4×4，，那么共有28×28个子区域。每个子区域提取得到6个特征，对应第二层有6个plane， 每个plane有28×28个unit.&lt;/p&gt;
&lt;p&gt;这样一个plane里的一个unit与输入层的4×4个unit相连，有16个权重weight值，这个plane里的其他unit都公用16个weight值.&lt;br&gt;
这样，6个平面，共有16×6=96个weight参数和6个bias参数.&lt;/p&gt;
&lt;p&gt;下面说子采样层，上面说道，6个平面，每个平面有28×28个unit，所以第二层共有6x28x28个unit. 这是从图像中提取出来的特征，作为后一层的输入，用于分类或回归。&lt;br&gt;
但实际中，6个特征远远不够，图像也不一定只有32×32像素。假如从96×96像素中提取400个特征，子区域还是4×4，则第二层的unit会有 400 x (96-4) x (96-4) = 300多万。 超过300多万输入的分类器很难训练，也容易过拟合。&lt;br&gt;
这就通过子采样来减少特征。子采样就是把卷积层的一个平面划分成几部分，每个部分取最大值或平均值，作为子采样层的一个unit值.如下图：&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://www.52nlp.cn/wp-content/uploads/2015/01/prml5-55.png&quot;&gt;&lt;img class=&quot;aligncenter wp-image-7278&quot; src=&quot;/images/52nlp.cn/ef8bfaf29c84f1bab66b473c40e473bd.jpg&quot; alt=&quot;prml5-55&quot; width=&quot;400&quot; height=&quot;201&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;左边表示卷积层的一个plane，右边是这个plane的子采样结果。一个格子表示一个unit。&lt;br&gt;
把左边整个plane分成4部分，互相不重合，每个部分取最大值或平均值，作为右边子采样层的一个unit。这样子采样层一个plane只有4个unit。&lt;/p&gt;
&lt;p&gt;通过卷积和子采样：得到的特征作为下一层分类或回归的输入。&lt;br&gt;
主要好处：&lt;br&gt;
a. 这样输入层的各种变形在后面就会变得不敏感。如果有多对 卷积/子采样，每一对就将不变形更深入一层。&lt;br&gt;
b. 减少参数的个数。就这么多了，大家慢慢看。&lt;/p&gt;
&lt;p&gt;阳阳(236995728) 21:01:30&lt;/p&gt;
&lt;p&gt;关于卷积神经网络的材料有吗？@网神&lt;/p&gt;
&lt;p&gt;阿邦(1549614810) 21:01:44&lt;/p&gt;
&lt;p&gt;问个问题，cnn怎么做的normalization？&lt;/p&gt;
&lt;p&gt;网神(66707180) 21:03:26&lt;/p&gt;
&lt;p&gt;卷积网络的材料，我看了书上的，还有Andrew Ng的deep learning里讲的，另外网上搜了一篇文章&lt;/p&gt;
&lt;p&gt;这里我贴下链接。&lt;/p&gt;
&lt;p&gt;罗杰兔(38900407) 21:04:19&lt;/p&gt;
&lt;p&gt;不大明白为什么可以采样，怎么保证采样得到的信息正好是我们需要的&lt;/p&gt;
&lt;p&gt;阿邦(1549614810) 21:05:01&lt;/p&gt;
&lt;p&gt;采样是max pooling，用于不变性&lt;/p&gt;
&lt;p&gt;网神(66707180) 21:05:02&lt;/p&gt;
&lt;p&gt;http://ibillxia.github.io/blog/2013/04/06/Convolutional-Neural-Networks/&lt;/p&gt;
&lt;p&gt;牧云(1106207961) 21:05:02&lt;/p&gt;
&lt;p&gt;采样是随机的吗？&lt;/p&gt;
&lt;p&gt;网神(66707180) 21:06:40&lt;/p&gt;
&lt;p&gt;传了个deep learning教程。这是Andrew Ng上课的notes, 网上一些人众包翻译的，我觉得翻译的不错。里面讲了神经网络、卷积网络，和其他深度网络的东西。&lt;/p&gt;
&lt;p&gt;采样不是随机的。&lt;/p&gt;
&lt;p&gt;牧云(1106207961) 21:08:04&lt;/p&gt;
&lt;p&gt;卷积提取的特征具有稳定性吗&lt;/p&gt;
&lt;p&gt;阿邦(1549614810) 21:08:56&lt;/p&gt;
&lt;p&gt;据说要数据量很大才可以&lt;/p&gt;
&lt;p&gt;网神(66707180) 21:09:03&lt;/p&gt;
&lt;p&gt;卷积层和子采样层主要是 解决 不变性问题 和减少参数w数量问题，所以可以起到泛化作用.&lt;br&gt;
注：PRML读书会系列文章由 &lt;a href=&quot;http://weibo.com/dmalgorithms&quot;&gt;@Nietzsche_复杂网络机器学习&lt;/a&gt; 同学授权发布，转载请注明原作者和相关的主讲人，谢谢。&lt;/p&gt;
&lt;p&gt;PRML读书会讲稿PDF版本以及更多资源下载地址：&lt;a href=&quot;http://vdisk.weibo.com/u/1841149974&quot;&gt;http://vdisk.weibo.com/u/1841149974&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;本文链接地址：&lt;a href=&quot;http://www.52nlp.cn/prml%E8%AF%BB%E4%B9%A6%E4%BC%9A%E7%AC%AC%E4%BA%94%E7%AB%A0-neural-networks&quot;&gt;http://www.52nlp.cn/prml读书会第五章-neural-networks&lt;/a&gt;&lt;/p&gt;

											

</description>
        <pubDate>Tue, 27 Jan 2015 00:00:00 +0800</pubDate>
        <link>http://iftti.com/posts/2015/2015-01-27-prml%25e8%25af%25bb%25e4%25b9%25a6%25e4%25bc%259a%25e7%25ac%25ac%25e4%25ba%2594%25e7%25ab%25a0-neural-networks-ba4f5a2ec.html</link>
        <guid isPermaLink="true">http://iftti.com/posts/2015/2015-01-27-prml%25e8%25af%25bb%25e4%25b9%25a6%25e4%25bc%259a%25e7%25ac%25ac%25e4%25ba%2594%25e7%25ab%25a0-neural-networks-ba4f5a2ec.html</guid>
        
        
        <category>52nlp</category>
        
      </item>
    
      <item>
        <title>SQL Server调优系列玩转篇二（如何利用汇聚联合提示（Hint）引导语句运行）</title>
        <description>

        &lt;!-- div style=&quot;margin-bottom: 10px;&quot;&gt;
            &lt;script language=javascript&gt;
                var randomNumLuobo = Math.round(Math.random()*1);
                var imageLuobo=new Array(2);
                imageLuobo[0]=&quot;http://jbcdn2.b0.upaiyun.com/2014/11/luobo-620x60.png&quot;;
                imageLuobo[1]=&quot;http://jbcdn2.b0.upaiyun.com/2014/11/luobo-620x60.png&quot;;
                var imageUrlLuobo=imageLuobo[randomNumLuobo];
                var urlsLuobo=new Array(2);
                urlsLuobo[0]=&quot;http://www.luobo360.com&quot;;
                urlsLuobo[1]=&quot;http://www.luobo360.com&quot;;
                var urlLuobo = urlsLuobo[randomNumLuobo];
                var adHTML = &quot;&lt;a href=&#39;&quot;+urlLuobo+&quot;&#39; target=&#39;_blank&#39;&gt;&lt;img src=&#39;&quot;+imageUrlLuobo+&quot;&#39; border=&#39;0&#39;&gt;&lt;/a&gt;&quot;;
                document.write(adHTML);
            &lt;/script&gt;
        &lt;/div --&gt;

        &lt;span style=&quot;display:block;margin-bottom:10px;&quot;&gt;&lt;/span&gt;
		
&lt;p&gt;&lt;strong&gt;前言&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;上一篇我们分析了查询Hint的用法，作为调优系列的最后一个玩转模块的第一篇。有兴趣的可以点击查看：&lt;a class=&quot;postTitle2&quot; id=&quot;cb_post_title_url&quot; href=&quot;http://blog.jobbole.com/83429/&quot;&gt;SQL Server调优系列玩转篇（如何利用查询提示（Hint）引导语句运行）&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;本篇继续玩转模块的内容，同样，还是希望扎实掌握前面一系列的内容，才进入本模块的内容分析。&lt;/p&gt;
&lt;p&gt;闲言少叙，进入本篇的内容。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;技术准备&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;数据库版本为SQL Server2012，利用微软的以前的案例库（Northwind）进行分析，部分内容也会应用微软的另一个案例库AdventureWorks。&lt;/p&gt;
&lt;p&gt;相信了解SQL Server的朋友，对这两个库都不会太陌生。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;误区纠正&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;在开始本篇文章主题内容之前，先纠正一些关于新手对于数据库调优的误区。也希望在日常应用解决问题的时候，切记道听途说，人云亦云，毛爷爷说过的：&lt;span style=&quot;color: #ff0000&quot;&gt;实践是检验真理的唯一标准。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;color: #000000&quot;&gt;两个误区：&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;color: #000000&quot;&gt;1、当在查询计划中发现了表扫描（Table Scan），就仿佛找到了病根一样，就想搞掉它，因为很多过来人都说过这种方式是性能很烂，而搞掉它的方式就是上索引，而且认为有了索引的就会快很多。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;color: #000000&quot;&gt;2、SQL Server语句优化就是创建索引，而创建索引就更简单了，找找查询语句，看看Where条件后…有几个筛选条件，创建几个非聚集索引就可以。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;color: #000000&quot;&gt;来看第一个问题：关于表扫描（Table Scan）是否真像传说的性能那么差劲！&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;color: #000000&quot;&gt;首先，我们知道比较查询语句性能的优越性，无非就几个关键指标：运行IO、运行时间、消耗：CPU、Memory、编译时间等。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;color: #000000&quot;&gt;来看，以下语句，完全相同的表结构、表数据，不同的是一张表是堆表，一张是加了聚集索引的表，我们来开启两个回话进行测试比较。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;color: #000000&quot;&gt;堆表查询：&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&quot;brush: sql; gutter: true&quot;&gt;SET STATISTICS IO ON
--新建个测试表
SELECT * INTO NewOrders FROM Orders
--先清空缓存数据
DBCC DROPCLEANBUFFERS
GO
SELECT * FROM NewOrders
SET STATISTICS IO OFF&lt;/pre&gt;
&lt;p&gt;&lt;span style=&quot;color: #000000&quot;&gt;存在聚集索引的测试表查询：&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&quot;brush: sql; gutter: true&quot;&gt;SET STATISTICS IO ON
--新建个测试表
SELECT * INTO NewCLOrders FROM Orders
--添加聚集索引
CREATE CLUSTERED INDEX CL_OrderID ON NewCLOrders (OrderID desc)
GO
--先清空缓存数据
DBCC DROPCLEANBUFFERS
GO
SELECT * FROM NewCLOrders
SET STATISTICS IO OFF&lt;/pre&gt;
&lt;p&gt;&lt;span style=&quot;color: #000000&quot;&gt;这样对比的原因是：很多人认为数据库优化的方式就是加上索引，并且认为查找（Seek）就比扫描好（Scan）。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;color: #000000&quot;&gt;这样可以肯定的堆表采用的为表扫描（Table Scan），而后者则就是通过聚集索引扫描（Index Scan）&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;color: #000000&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/b5afa7fd9af6de90abfd73552dbc8f39.jpg&quot; width=&quot;317&quot; height=&quot;165&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/a45e75d3a866e6b940f33a8b659583f9.jpg&quot;&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;color: #000000&quot;&gt;先来看IO的两者对比：&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;color: #000000&quot;&gt;先来看看堆表的IO信息&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;color: #000000&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/3beabbc7b2f9fb34a1cb7518c0d6bd73.jpg&quot; width=&quot;792&quot; height=&quot;233&quot;&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;堆表的表现：逻辑读取20次，预读2次，这里预读次数的多少其实是影响性能的重要指标，因为它是直接从磁盘中读取，所以性能最差，当然SQL Server此处采用并行处理，而且第一次读取数之后就缓存到内存中，防止再次的磁盘交互。&lt;/p&gt;
&lt;p&gt;再来看聚集索引表的IO信息&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/8afd71c237bcd4813cf504254b6fe434.jpg&quot; width=&quot;803&quot; height=&quot;226&quot;&gt;&lt;/p&gt;
&lt;p&gt;采用聚集索引的表，逻辑IO为23次，预读飙升至22次。&lt;/p&gt;
&lt;p&gt;相比而言：&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;background-color: #00ffff&quot;&gt;相同的查询语句，堆表的查询逻辑读取次数为20次、预读2次，没有物理读…，而用聚集索引的表逻辑读为23次、预读22次！还有统计信息的不准确导致的物理读取1次！….所以相比堆表的SCAN是不是性能好很多。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;color: #000000&quot;&gt;当然，要再深入点分析，其实这两者不同的原因很简单：采用了聚集索引的表因为其存储的结构（B-Tree）的方式，所以逻辑IO肯定要多3，因为从索引根节点至叶子节点，也就是需要经过三个索引页，才能获取到数据页。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;color: #000000&quot;&gt;而预读的差距这么大的原因也是同样的原因：&lt;span style=&quot;color: #ff0000&quot;&gt;从堆表中获取数据是一段连续的数据页（确切的说是一次连续读取64个数据页&amp;lt;512KB&amp;gt;），而这时所有加上索引的表做不到的！通过索引只能依次读取一个数据页（8KB）,这也是索引的局限性。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;color: #000000&quot;&gt;关于查询计划的逻辑读、预读、物理读等IO详细逻辑信息，可以参照我前面的文章，分析的很详细：&lt;a class=&quot;postTitle2&quot; id=&quot;cb_post_title_url&quot; href=&quot;http://blog.jobbole.com/82831/&quot;&gt;SQL Server调优系列进阶篇（查询语句运行几个指标值监测）&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;接着我们再对比下执行时间，相信这个也是更为关注的：&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/f1f730dc4ebb46f6ea1e68b29d4fd4e2.jpg&quot; width=&quot;522&quot; height=&quot;325&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/2fe7639bbadf84bdc206c0332bad69cc.jpg&quot; width=&quot;470&quot; height=&quot;287&quot;&gt;&lt;/p&gt;
&lt;p&gt;看明白了吧，获取完全相同的数据量，堆表执行的时候耗时157毫秒，并且分析和编译没有占用时间，这个很简单，因为它是堆表，根本不需要根据统计信息进行优化和选择；而加上聚集索引的表就不一样了，需要根据索引的统计信息对T-SQL语句进行优化和编译，而这足足耗费了79毫秒，然后执行的时候还需要更多的预读IO,还有如果优化器没有优化到位的时候，还要造成额外的物理IO，所以它总耗费了298毫秒…&lt;/p&gt;
&lt;p&gt;在我的测试表中只有八百多行的数据中就产生了如此的差距值..如果数据量多的话…性能就堪忧了….&lt;/p&gt;
&lt;p&gt;关于CPU和内存值我就不截图了……上面我们分析了加了聚集索引了，就产生了查询优化器一系列的过程…而编译就是需要CPU资源的…..&lt;/p&gt;
&lt;p&gt;通过上面的结果，我要表达的是：&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;color: #ff0000&quot;&gt;&lt;strong&gt;首先，在我们所看到的查询计划中，不要一看到表扫描，就感觉这个运算符是很慢的，或者是很耗时的。更有甚者看到了就感觉问题出现在这上面，并且为很多人所唾弃为“万恶的表扫描”….&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;color: #ff0000&quot;&gt;&lt;strong&gt;其次，请记住，在SQL Server中你所看到的任何一个运算符，都是在目前你所设定的环境中基本是最好的….更没有那个运算符好与那个运算符烂一说…诸如偏执的认为哈希连接就比嵌套循环要快…索引查找就比索引扫描要好等问题…..我们要做的就是合适的场景运用合适的处理方式，最优的顺应SQL Server性能。&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;color: #ff0000&quot;&gt;&lt;strong&gt;再次，经过上述了问题的分析，也不要陷入另外一个极端的误区：表扫描就要比聚集索引扫描好！后续的文章中我会给你展示聚集索引比表扫描好的用处…在SQL Server的世界中，只有你真正的触摸的本质，才不会迷茫…才会看清一切所谓的教条调优都非绝对！&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;关于第二个问题的误区，其实是很多人的误区，误认为了非聚集索引的强大性，误以为在列中加上了索引就可以充分应用。本篇就不纠正了，可以参照我前面的文章，相信看完了基本也就懂了非聚集索引的利弊项，连接：&lt;a class=&quot;postTitle2&quot; id=&quot;cb_post_title_url&quot; href=&quot;http://blog.jobbole.com/83068/&quot;&gt;SQL Server调优系列进阶篇（如何索引调优）&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;一、GROUP 提示 （Hints）&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;继续咱们本篇文章的内容，上一篇我们分析了查询的几个重要的Hints,本篇文章我们来看分组提示，分组查询也是我们在写T-SQL语句经常用到的，关于分组的运算符也有两个：Order Group和Hash Group。其实关于排序一直也是数据库中最为头疼的运算。这个运算符也是消耗比较大的，相当的耗内存，如果数据量较大的话，SQL Server处理的方式也是通过哈希算法进行优化。&lt;/p&gt;
&lt;p&gt;当然，关于分组查询运算符分解，看以参照基础篇中的文章：&lt;a class=&quot;postTitle2&quot; id=&quot;homepage1_HomePageDays_DaysList_ctl00_DayList_TitleUrl_0&quot; href=&quot;http://blog.jobbole.com/81182/&quot; target=&quot;_blank&quot;&gt;SQL Server调优系列基础篇（常用运算符总结）&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;来看个例子：&lt;/p&gt;
&lt;pre class=&quot;brush: sql; gutter: true&quot;&gt;SELECT CustomerID,MAX(OrderDate) 
FROM Orders
GROUP BY CustomerID&lt;/pre&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/294191b76410466954014662fdbbbdd7.jpg&quot; width=&quot;725&quot; height=&quot;176&quot;&gt;&lt;/p&gt;
&lt;p&gt;上面的查询语句，我们想获取出每个订单的最大订单日期。&lt;/p&gt;
&lt;p&gt;通过查询计划我们可以推测出肯定在CustomerID列存在索引，这样SQL Serer能直接利用这个进行排序，但是即便如此消耗还是飙升到56%….然后通过再加上一个流聚合计算出最大订单日期。&lt;/p&gt;
&lt;p&gt;当然，此方式也是SQL Server认为的一种最优方式，但是如果数据量多的话，此种方式将会造成内存严重的消耗。&lt;/p&gt;
&lt;p&gt;所以，我们可以采用GROUP Hint进行提示，将其更改为Hash 分组..代码如下：&lt;/p&gt;
&lt;pre class=&quot;brush: sql; gutter: true&quot;&gt;SELECT CustomerID,MAX(OrderDate) 
FROM Orders
GROUP BY CustomerID
&amp;lt;span style=&quot;color: #ff0000;&quot;&amp;gt;OPTION(HASH GROUP)&amp;lt;/span&amp;gt;&lt;/pre&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/f920ce8735716da40dc56da9399520c9.jpg&quot; width=&quot;714&quot; height=&quot;187&quot;&gt;&lt;/p&gt;
&lt;p&gt;当然，此处可能并不是一个最优的方式，只是为了演示，但是如果基础数据量增大的话，我也相信SQL Server会自动的更改为哈希匹配的方式进行。&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;二&lt;strong&gt;、组合提示 （Hints）&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;大部分情况下，我们所写的T-SQL语句并不是简单的，有很多的各种嵌套查询进行，如果这种查询语句，我们的提示（Hints）就可能不是单一的。&lt;/p&gt;
&lt;p&gt;我们来看如此方式该如何进行指导。先来看个简单的例子：&lt;/p&gt;
&lt;pre class=&quot;brush: sql; gutter: true&quot;&gt;SELECT O.OrderID
FROM Customers C JOIN Orders O
JOIN Employees E
ON O.EmployeeID=E.EmployeeID
ON C.CustomerID=O.CustomerID
WHERE C.City=N&#39;London&#39; AND E.City=N&#39;London&#39;
OPTION(FORCE ORDER,HASH JOIN)&lt;/pre&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/c9be08de44613b92fda379bdbcdd4f7e.jpg&quot; width=&quot;793&quot; height=&quot;315&quot;&gt;&lt;/p&gt;
&lt;p&gt;不仅仅如此，我们还可以手动给查询语句写查询计划。&lt;/p&gt;
&lt;p&gt;也就是我们自己写的XML查询计划，让T-SQL语句就按照我们自定义的查询计划去进行，当然，这是大招了，我们留在最后使用。&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;strong&gt;参考文献&lt;/strong&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;微软联机丛书&lt;a href=&quot;http://msdn.microsoft.com/zh-cn/library/ms191158(SQL.90).aspx&quot; target=&quot;_blank&quot;&gt;逻辑运算符和物理运算符引用&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;参照书籍《SQL.Server.2005.技术内幕》系列&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;结语&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;此篇文章先到此吧，关于SQL Server调优工具Hint的使用还有很多内容，后续依次介绍，有兴趣的童鞋可以提前关注。&lt;/p&gt;
&lt;p&gt;有问题可以留言或者私信，随时恭候有兴趣的童鞋加入SQL SERVER的深入研究。共同学习，一起进步。&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;文章最后给出前面几篇的连接，以下内容基本涵盖我们日常中所写的查询运算的分解以及调优内容项，皆为原创……..&lt;/p&gt;
&lt;p&gt;第一个基础模块注重基础内容的掌握，共分7篇文章完成，内容涵盖一系列基础运算算法，详细分析了如何查看执行计划、掌握执行计划优化点，并一一列举了日常我们平常所写的T-SQL语句所会应用的运算符:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a id=&quot;cb_post_title_url&quot; href=&quot;http://blog.jobbole.com/81176/&quot; target=&quot;_blank&quot;&gt;SQL Server调优系列基础篇&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a id=&quot;homepage1_HomePageDays_DaysList_ctl00_DayList_TitleUrl_0&quot; href=&quot;http://blog.jobbole.com/81182/&quot; target=&quot;_blank&quot;&gt;SQL Server调优系列基础篇（常用运算符总结）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a id=&quot;cb_post_title_url&quot; href=&quot;http://blog.jobbole.com/81184/&quot; target=&quot;_blank&quot;&gt;SQL Server调优系列基础篇（联合运算符总结）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a id=&quot;cb_post_title_url&quot; href=&quot;http://blog.jobbole.com/81186/&quot; target=&quot;_blank&quot;&gt;SQL Server调优系列基础篇（并行运算总结）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a id=&quot;cb_post_title_url&quot; href=&quot;http://blog.jobbole.com/81189/&quot; target=&quot;_blank&quot;&gt;SQL Server调优系列基础篇（并行运算总结篇二）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://blog.jobbole.com/81193/&quot; target=&quot;_blank&quot;&gt;SQL Server调优系列基础篇（索引运算总结）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://blog.jobbole.com/82459/&quot;&gt;SQL Server调优系列基础篇（子查询运算总结）&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;第二个进阶模块注重SQL Server执行T-SQL语句的时候一些内幕解析，共分为5篇文章完成，其中包括：查询优化器的运行方式、运行时几个优化指标值检测，统计信息、利用索引等一系列内容。通过这块内容让我们了解SQL Server为我们所写的T-SQL语句如何进行优化及运行的。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;http://blog.jobbole.com/82467/&quot;&gt;SQL Server调优系列进阶篇（查询优化器的运行方式）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://blog.jobbole.com/82831/&quot;&gt;SQL Server调优系列进阶篇（查询语句运行几个指标值监测）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://blog.jobbole.com/82845/&quot;&gt;SQL Server调优系列进阶篇（深入剖析统计信息）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://blog.jobbole.com/83068/&quot;&gt;SQL Server调优系列进阶篇（如何索引调优）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://blog.jobbole.com/83346/&quot;&gt;SQL Server调优系列进阶篇（如何维护数据库索引）&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;第三个玩转模块重点跟进特定的问题进行特定的提示（Hints)，基于前两个模块进行的分析。&lt;/p&gt;
&lt;p&gt;&lt;a class=&quot;postTitle2&quot; id=&quot;cb_post_title_url&quot; href=&quot;http://blog.jobbole.com/83429/&quot;&gt;SQL Server调优系列玩转篇（如何利用查询提示（Hint）引导语句运行）&lt;/a&gt;&lt;/p&gt;


        
        &lt;!-- BEGIN #author-bio --&gt;


&lt;!-- END #author-bio --&gt;
	

</description>
        <pubDate>Mon, 26 Jan 2015 00:00:00 +0800</pubDate>
        <link>http://iftti.com/posts/2015/2015-01-26-83597-cf094ab82.html</link>
        <guid isPermaLink="true">http://iftti.com/posts/2015/2015-01-26-83597-cf094ab82.html</guid>
        
        
        <category>jobbole</category>
        
      </item>
    
      <item>
        <title>Proxygen：Facebook开源的 C++ HTTP 框架</title>
        <description>

        &lt;!-- div style=&quot;margin-bottom: 10px;&quot;&gt;
            &lt;script language=javascript&gt;
                var randomNumLuobo = Math.round(Math.random()*1);
                var imageLuobo=new Array(2);
                imageLuobo[0]=&quot;http://jbcdn2.b0.upaiyun.com/2014/11/luobo-620x60.png&quot;;
                imageLuobo[1]=&quot;http://jbcdn2.b0.upaiyun.com/2014/11/luobo-620x60.png&quot;;
                var imageUrlLuobo=imageLuobo[randomNumLuobo];
                var urlsLuobo=new Array(2);
                urlsLuobo[0]=&quot;http://www.luobo360.com&quot;;
                urlsLuobo[1]=&quot;http://www.luobo360.com&quot;;
                var urlLuobo = urlsLuobo[randomNumLuobo];
                var adHTML = &quot;&lt;a href=&#39;&quot;+urlLuobo+&quot;&#39; target=&#39;_blank&#39;&gt;&lt;img src=&#39;&quot;+imageUrlLuobo+&quot;&#39; border=&#39;0&#39;&gt;&lt;/a&gt;&quot;;
                document.write(adHTML);
            &lt;/script&gt;
        &lt;/div --&gt;

        &lt;span style=&quot;display:block;margin-bottom:10px;&quot;&gt;&lt;/span&gt;
		
&lt;p&gt;我们在这里激动的宣布开源我们的 &lt;a href=&quot;https://code.facebook.com/projects/676603015770415/&quot;&gt;Proxygen&lt;/a&gt;，一个 C++ HTTP库的集合，连同一个简单易用的HTTP 服务器。除了HTTP/1.1之外，Proxygen还支持SPDY 3 和 SPDY/3.1，目前正在添加对HTTP/2的支持。&lt;/p&gt;
&lt;p&gt;Proxygen 的设计初衷并不是为了代替Apache 和nginx， 后两者都注重于使用C 语言提供一个极具弹性的HTTP 服务器，提供高的性能，但是配置工作非常复杂； 而我们则更关注构建一个高性能的C++ HTTP框架，该框架无论在服务器代码端还是客户代码端都配备了合理的预设值（默认不需要进行复杂配置），使其可以轻松嵌入到Facebook提供Web服务的现有应用中。 我们想帮助更多人构建和部署高性能的C++  HTTP服务器，我们相信Proxygen是一个很棒的框架。&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/416c980cf0336e715fcdf7c762da078a.jpg&quot;&gt;&lt;/p&gt;
&lt;h3&gt;&lt;strong&gt;背景&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;Proxygen一开始是一个可自定义配置，高性能的的HTTP(S)反向代理和负载平衡产品， 我们最开始是打算把Proxygen打造为一个软件库用来代理，这也是名字的由来， 但是 Proxygen 慢慢演变，看到已经有很多相同功能的软件已经存在（Apache Nginx HAProxy varnish 等）我们走了其他的方向。&lt;/p&gt;
&lt;p&gt;为什么我们要打造自己的HTTP 技术堆栈&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;能够与他们现有的基础设施和工具集成（Thrift、ODS），&lt;/li&gt;
&lt;li&gt;代码重用 , 创建一个可以供不同的内部项目（Haystack、HHVM和负载均衡器等）使用的事件驱动库,现在我们有很多内部的系统都是建立在Proxygen代码之上，包括  Haystack HHVM 我们的负载平衡器， 还有一些移动基础架构，Proxygen 提供了一个支持HTTP/2协议的平台&lt;/li&gt;
&lt;li&gt;扩展，我们尝试着使用其他已经存在的技术堆栈，但是它们已经存在很长时间了，一个已经积重难返的HTTP基础架构已经不能跟上我们的扩展迭代需求了&lt;/li&gt;
&lt;li&gt;
&lt;div&gt;  一些既有的HTTP服务器还缺少很多特性，如：SPDY、WebSockets、HTTP/1.1 (keep-alive)、TLS false start和特定的负载调度算法&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Proxygen项目发起于2011年，发起者一群对HTTP在Facebook的使用充满激情的工程师，从那时起，该项目就一直被一个三四个人的小组维护。在这个小组之外，还有很多内部的代码贡献者也加入到这个项目，自从项目开始以来，主要的里程碑包括：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;2011 –  Proxygen 开发部门开始谈论产品走的流量&lt;/li&gt;
&lt;li&gt;2012 –  加入了 SPDY/2的支持， 开始内部公开测试&lt;/li&gt;
&lt;li&gt;2013 – SPDY/3  开发测试迭代&lt;/li&gt;
&lt;li&gt;2014 –  完成  SPDY/3.1 rollout, 开始HTTP/2 的 开发&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;还有其他的里程碑可以参考这里 &lt;a href=&quot;https://www.facebook.com/l.php?u=https%3A%2F%2Fgithub.com%2Ffacebook%2Fproxygen&amp;amp;h=9AQHXOkgL&amp;amp;s=1&quot; target=&quot;_blank&quot; rel=&quot;nofollow&quot;&gt;code&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;在该项目演进并在生产环境中测试了数年之后，我们到了一个可以将它开源的时间了&lt;/p&gt;
&lt;h3&gt;&lt;strong&gt;架构&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;Proxygen用到了下列概念：transaction（事务）、session（会话）、codec（编解码器）和handler（处理器）。事务表示的是在客户端和服务器之间交换的请求-响应对。这类相关的事务所组成的序列就是一个会话。编解码器负责将来自线路上的字节解析为对象，并将其与事务关联起来。消息最终传递给处理器进行真正地处理。  这个设置允许我们不用重写代码而支持新的 协议像  SPDY 和 HTTP/2&lt;/p&gt;
&lt;p&gt;Proxygen 很是依赖最新的C++特新， 并且依赖 Thrift 和Folly 底层网络库和数据抽象， 我们使用了很多高级语法避免对大的对象像 body 缓存，和 header representations 进行多余拷贝，同时避免典型的一些坑像内存泄露， 另外，使用了 非阻塞IO和linux epoll技术，我们能够创建一个高效的服务器，&lt;/p&gt;
&lt;h3&gt;&lt;strong&gt;HTTP 服务器&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;如果你需要一个快速上手和事件驱动的服务器，那么我们的服务框架是一个很好的选择，只要一些很少的配置，你就可以使用了，参考这个demo:  &lt;a href=&quot;https://www.facebook.com/l.php?u=https%3A%2F%2Fgithub.com%2Ffacebook%2Fproxygen%2Fblob%2Fmaster%2Fproxygen%2Fhttpserver%2Fsamples%2Fecho%2FEchoServer.cpp&amp;amp;h=9AQHXOkgL&amp;amp;s=1&quot; target=&quot;_blank&quot; rel=&quot;nofollow&quot;&gt;echo server example&lt;/a&gt; ，  我们进行了在 32 logical core Intel(R) Xeon(R) CPU E5-2670 @ 2.60GHz with 16 GiB of RAM,* *从一个到八个线程做了基准测试， 我们的测试结果：&lt;/p&gt;
&lt;pre&gt;    # Load test client parameters:
    # Twice as many worker threads as server
    # 400 connections open simultaneously
    # 100 requests per connection
    # 60 second test
    # Results reported are the average of 10 runs for each test
    # simple GETs, 245 bytes of request headers, 600 byte response (in memory)
    # SPDY/3.1 allows 10 concurrent requests per connection&lt;/pre&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;/images/jobbole.com/0a8eeeba42b7442aefc1f9e57491c7bc.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;虽然 echo 服务器相对于真实的web 服务器来说很简单， 但是基准测试可以反映出解析一下二进制协议像SPDY 和HTTP/2 的效率指标，&lt;/p&gt;
&lt;h3&gt;&lt;strong&gt;影响&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;Proxygen 允许我们快速开发我们需要的特性并推出产品， 我们可以很快的看到结果， 举个例子，我们对升级 HPACK 编码压缩很是感兴趣， 但是我们还不知到有多少HTTP/2 客户端已经部署了， 并且HTTP/2 自己也在大幅度的开发中，  Proxygen允许我们自己实现HPACK，在SPDY上面使用，我们把它部署在移动客户端，和我们的服务器上进行模拟， 这个中真实的部署允许我们快速的理解性能和数据处理效率，其他的例子， 内部配置系统正一点一点变的更好，Proxygen 做的最积极的影响是给我们带来了快速迭代我们的HTTP 基础设施的能力&lt;/p&gt;
&lt;h3&gt;&lt;strong&gt;开源&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;Proxygen 现在正在快速的迭代中， 如果你对高性能网络代码，和现代C++感兴趣，我们欢迎你提交 pull requests&lt;/p&gt;
&lt;p&gt;我们欢迎并拥抱开源，一直寻求机会分享我们的经验开源我们的软件。目前流量小组已经开源了 &lt;a href=&quot;https://code.facebook.com/projects/1410559149202582/fbthrift/&quot;&gt;Thrift&lt;/a&gt; 和 &lt;a href=&quot;https://code.facebook.com/projects/676603015770415/&quot;&gt;Proxygen&lt;/a&gt;,  两个Facebook重要的基础网络架构组建。我们希望这些组件，能够构建其他的系统，并也希望这些新系统有一天也能开源出来&lt;em&gt;&lt;br&gt;
&lt;/em&gt;&lt;/p&gt;

        
        &lt;!-- BEGIN #author-bio --&gt;

&lt;div id=&quot;author-bio&quot;&gt;
	
	&lt;h3 class=&quot;widget-title&quot;&gt;
	关于作者： &lt;a href=&quot;http://blog.jobbole.com/author/sunbiaobiao/&quot;&gt;sunbiaobiao&lt;/a&gt;
	&lt;/h3&gt;
	&lt;div class=&quot;alignleft&quot;&gt;
		&lt;a href=&quot;http://blog.jobbole.com/author/sunbiaobiao/&quot;&gt;
			&lt;img src=&quot;/images/jobbole.com/b08c91623786667ce9053369297def6d.jpg&quot; alt=&quot;sunbiaobiao&quot; width=&quot;50&quot; height=&quot;50&quot; class=&quot;photo&quot;&gt;		&lt;/a&gt;
	&lt;/div&gt;
	&lt;p&gt;（新浪微博：&lt;a href=&quot;http://weibo.com/u/3274148937&quot;&gt;@sunbiao将军&lt;/a&gt;）&lt;/p&gt;
	&lt;p&gt;
		&lt;a style=&quot;text-decoration: none;&quot; href=&quot;http://blog.jobbole.com/author/sunbiaobiao/&quot;&gt;查看sunbiaobiao的更多文章 &amp;gt;&amp;gt; &lt;/a&gt;
	&lt;/p&gt;
	&lt;div class=&quot;clear&quot;&gt;&lt;/div&gt;
	
&lt;/div&gt;

&lt;!-- END #author-bio --&gt;
	

</description>
        <pubDate>Mon, 26 Jan 2015 00:00:00 +0800</pubDate>
        <link>http://iftti.com/posts/2015/2015-01-26-83496-58579500e.html</link>
        <guid isPermaLink="true">http://iftti.com/posts/2015/2015-01-26-83496-58579500e.html</guid>
        
        
        <category>jobbole</category>
        
      </item>
    
      <item>
        <title>使用蒲公英来做iOS测试应用的分发</title>
        <description>
&lt;p&gt;&lt;img src=&quot;/images/devtang.com/1a256aa67dc18e3648e97c581b2fce3b.jpg&quot;&gt;&lt;/p&gt;

&lt;h2&gt;前言&lt;/h2&gt;

&lt;p&gt;我在 &lt;a href=&quot;http://blog.devtang.com/blog/2012/02/16/apply-daily-build-in-ios-project/&quot;&gt;《给 iOS 工程增加 Daily Build》&lt;/a&gt; 一文中介绍过如何用命令行生成 IPA 文件以及通过 &lt;code&gt;itms-services&lt;/code&gt; 协议 来让用户可以直接在网页上安装测试应用。但是这种方法虽然有效，但是还是比较麻烦，因为：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;开发者需要自己写相关的 Build 脚本。&lt;/li&gt;
&lt;li&gt;开发者需要自己搭建好应用下载的服务器。&lt;/li&gt;
&lt;li&gt;如果要做得更友好，开发者还需要部署 CDN 服务、增加扫描二维码下载等功能。&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;所以本文给大家推荐一个叫 “&lt;a href=&quot;http://www.pgyer.com/&quot;&gt;蒲公英&lt;/a&gt;” 的免费服务，帮助开发者能够方便地来做 iOS 应用的测试分发工作。蒲公英于 2014 年 7 月正式上线，我自己使用过一段时间，还是挺满意的。&lt;/p&gt;

&lt;p&gt;下面我们就来看看，如何使用 “&lt;a href=&quot;http://www.pgyer.com/&quot;&gt;蒲公英&lt;/a&gt;” 来进行测试版本应用的分发工作。&lt;/p&gt;

&lt;h2&gt;生成 IPA 文件&lt;/h2&gt;

&lt;p&gt;生成应用的 IPA 文件可以使用命令行 &lt;code&gt;xcodebuild exportArchive -exportFormat ipa&lt;/code&gt;来完成，也可以使用 Xcode 提供的相应功能。相信大部分同学应该都用过，我就简单介绍一下。&lt;/p&gt;

&lt;p&gt;首先将编译的目标机器设置成 “iOS Device”，然后点击”Product”–&amp;gt;“Archive”，Archive 成功之后，就可以在 Xcode 的 Organizer 中看到相应的文件。&lt;/p&gt;

&lt;p&gt;接下来点击 Organizer 中的 “Export” 按钮，如下图所示：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/devtang.com/44f0f6ff8108f7476b7d7d8e28a65c9f.jpg&quot;&gt;&lt;/p&gt;

&lt;p&gt;在接下来的弹出界面中选择 “Save for Ad Hoc Deployment”。如下图所示：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/devtang.com/3fac266f234e06bdaa126856bdeef322.jpg&quot;&gt;&lt;/p&gt;

&lt;p&gt;选择完成后点 Next，Xcode 会自动将测试设备的签名信息附加上，并将相应的 IPA 文件导出。&lt;/p&gt;

&lt;h2&gt;上传到蒲公英&lt;/h2&gt;

&lt;p&gt;如果是第一次使用蒲公英，我们需要访问 &lt;a href=&quot;http://www.pgyer.com/user/register&quot;&gt;蒲公英的网址&lt;/a&gt; 进行注册。注册主要是为了保护测试应用的安全和设置相应的权限（例如设置下载密码）。&lt;/p&gt;

&lt;p&gt;注册之后，点击&lt;a href=&quot;http://www.pgyer.com/&quot;&gt;蒲公英网站首页&lt;/a&gt;的 “发布应用”，然后选择之前生成的 IPA 文件即可完成上传 IPA 的过程。蒲公英对上传速度进行了很大程度的优化，上传应用速度非常快，下图是我测试上传猿题库的过程（上传 18M 的 IPA 文件大概花了半分钟时间，平均速度应该超过了 500K 每秒）：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/devtang.com/29271389429122bb8c7e9fa749dda6e4.jpg&quot;&gt;&lt;/p&gt;

&lt;p&gt;上传成功后，蒲公英会自动分析应用的 Bundle ID 信息，将其在 App Store 上的应用介绍和截图获取下来。然后你可以选择设置一个 “安装密码”，以避免一些越狱用户非法安装你的应用。对于小范围试用的应用，建议也不要将其发布到 “应用广场”。如下所示：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/devtang.com/328a40b1c9355aedfd84f8d29b3fa6bc.jpg&quot;&gt;&lt;/p&gt;

&lt;p&gt;设置好所有需要的信息后，点击发布应用，蒲公英会生成一个应用分发的网页。如下图所示：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/devtang.com/0566de06a5c0a8ab9ac86d09d2ae14d2.jpg&quot;&gt;&lt;/p&gt;

&lt;p&gt;你可以把这个网页发给你的测试用户:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;如果用户在手机上打开这个页面，则可以直接点击 “安装按钮” 来一键下载和安装测试应用。&lt;/li&gt;
&lt;li&gt;如果用户在电脑上打开这个应用，则可以用扫描二维码的方式来一键下载和安装测试应用。&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;一些小技巧&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;有企业证书的同学，如果将 IPA 在本地用企业证书签名，则可以把蒲公英当作一个企业应用发布渠道，省去部署分发服务器的烦恼，但最好设置安装密码，以免被苹果认为滥用企业证书。&lt;/li&gt;
&lt;li&gt;嫌每次打开蒲公英的网页太麻烦？蒲公英提供了 &lt;a href=&quot;http://www.pgyer.com/apps&quot;&gt;Mac 客户端&lt;/a&gt;，所有上传和设置操作都可以在客户端上直接进行。&lt;/li&gt;
&lt;/ul&gt;


</description>
        <pubDate>Thu, 22 Jan 2015 00:00:00 +0800</pubDate>
        <link>http://iftti.com/posts/2015/2015-01-22-pgy-usage-guide-608dcee6f.html</link>
        <guid isPermaLink="true">http://iftti.com/posts/2015/2015-01-22-pgy-usage-guide-608dcee6f.html</guid>
        
        
        <category>devtang</category>
        
      </item>
    
      <item>
        <title>JRuby 调用 maxmind-java 测试</title>
        <description>

  
  &lt;div style=&quot;background-color: #FFF;&quot;&gt;
    &lt;p&gt;GeoIP 是一个非常有用的信息，也是使用 ELKstack 时一般都会加上的过滤器插件。不过 geoip 插件的性能，有些时候却会成为整个系统的瓶颈。另一个问题，则是 GeoIP 数据文件的准确度，在国内比较头疼。即使你有一个自己处理出来的准确度较高的 IP 库，GeoIP 也没有提供现成的修改数据文件内容的工具。这个时候，MaxMind 公司的 GeoIP2 就进入我的视线了。&lt;/p&gt;
&lt;p&gt;GeoIP2 在字段上比 GeoIP 更丰富。而且还提供了 &lt;a href=&quot;https://metacpan.org/pod/MaxMind::DB::Writer&quot;&gt;MaxMind::DB::Writer&lt;/a&gt; 库方便使用者自己生成 GeoIP2 数据文件！感谢&lt;a href=&quot;http://weibo.com/345198426&quot;&gt;@纯白色燃烧&lt;/a&gt;童鞋用&lt;a href=&quot;http://blog.yikuyiku.com/?p=4144&quot;&gt;自己的 CPAN 库成功倒逼&lt;/a&gt; MaxMind 公司。&lt;/p&gt;
&lt;p&gt;据@纯白色燃烧 介绍，GeoIP2 比 GeoIP 有六到七倍的性能提升。不过他是在 C 平台下，使用 libmaxminddb 库做的测试，而 logstash 是 JRuby 平台，所以我们需要的是验证如何在 JRuby 上使用 GeoIP2，以及跟 GeoIP 的性能对比。&lt;/p&gt;
&lt;p&gt;在 JRuby 上用模块，有两种方式，一种是纯 Ruby 实现，一种是纯 Java 实现。MaxMind 提供了纯 Java 实现，社区另外有一个纯 Ruby 实现的库。下面开始测试。&lt;/p&gt;
&lt;h2 id=&quot;section&quot;&gt;准备工作&lt;/h2&gt;
&lt;p&gt;首先需要准备环境。安装 JRuby，纯 Ruby 实现的 maxminddb 库；然后下载 GeoIP2 数据文件，下载 Java 实现的 MaxMind-Java 库。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sudo port install jruby
sudo jgem install maxminddb
wget https://geolite.maxmind.com/download/geoip/database/GeoLite2-City.mmdb.gz
gzip -d GeoLite2-City.mmdb.gz
wget https://github.com/maxmind/GeoIP2-java/releases/download/v2.1.0/geoip2-2.1.0-with-dependencies.zip
unzip geoip2-2.1.0-with-dependencies.zip
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&quot;section-1&quot;&gt;测试程序&lt;/h2&gt;
&lt;p&gt;准备就绪，然后就是如何测试的问题了。为了贴近 logstash 运行环境，我扒拉了一下 logstash 最核心的 &lt;code&gt;pipeline.rb&lt;/code&gt; 文件，简化出来了一个测试程序。相当于是 &lt;code&gt;logstash -w 20 -e &#39;input {generator {}} filter {geoip{}} output {null{}}&lt;/code&gt; 的效果：&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-ruby&quot; data-lang=&quot;ruby&quot;&gt;&lt;span class=&quot;c1&quot;&gt;#!/usr/bin/env jruby&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;require&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;geoip&quot;&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;require&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;maxminddb&quot;&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;require&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;thread&quot;&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;require&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;java&quot;&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# 测试数据&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ip&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&#39;202.106.0.20&#39;&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# 加载 maxmind-java 的所有 jar 包&lt;/span&gt;
&lt;span class=&quot;no&quot;&gt;Dir&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;/Users/raochenlin/geoip2-2.1.0/lib/*.jar&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;each&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;jar&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;require&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;jar&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# 导入关键性的 java 类&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;com&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;maxmind&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;geoip2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DatabaseReader&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;java&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;net&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;InetAddress&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# 这个原生的 java 写法是：&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;#   File database = new File(&quot;/Users/raochenlin/GeoLite2-City.mmdb&quot;)&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;#   DatabaseReader reader = new DatabaseReader.Builder(database).build()&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# 之前对 java 不太懂，想直接 import Builder 进来&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# 其实 Builder 是DatabaseReader 类里的静态类(public final static class)，不能直接 import&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;database&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;java&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;io&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;File&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;new&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;/Users/raochenlin/GeoLite2-City.mmdb&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;vi&quot;&gt;@reader&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;DatabaseReader&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;no&quot;&gt;Builder&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;new&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;database&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;build&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# 纯 Ruby 实现的库&lt;/span&gt;
&lt;span class=&quot;vi&quot;&gt;@db&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;MaxMindDB&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;new&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&#39;/Users/raochenlin/GeoLite2-City.mmdb&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# 老的 GeoIP 库，需要制定不同的数据文件类型，这部分直接抄自 logstash 源码&lt;/span&gt;
&lt;span class=&quot;vi&quot;&gt;@geo&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;GeoIP&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;new&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&#39;/Users/raochenlin/Downloads/logstash-1.4.2/vendor/geoip/GeoLiteCity.dat&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;vi&quot;&gt;@geoip_type&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;case&lt;/span&gt; &lt;span class=&quot;vi&quot;&gt;@geo&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;database_type&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;when&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;GeoIP&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;no&quot;&gt;GEOIP_CITY_EDITION_REV0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;GeoIP&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;no&quot;&gt;GEOIP_CITY_EDITION_REV1&lt;/span&gt;
  &lt;span class=&quot;ss&quot;&gt;:city&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;when&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;GeoIP&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;no&quot;&gt;GEOIP_COUNTRY_EDITION&lt;/span&gt;
  &lt;span class=&quot;ss&quot;&gt;:country&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;when&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;GeoIP&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;no&quot;&gt;GEOIP_ASNUM_EDITION&lt;/span&gt;
  &lt;span class=&quot;ss&quot;&gt;:asn&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;when&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;GeoIP&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;no&quot;&gt;GEOIP_ISP_EDITION&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;GeoIP&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;no&quot;&gt;GEOIP_ORG_EDITION&lt;/span&gt;
  &lt;span class=&quot;ss&quot;&gt;:isp&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;raise&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;RuntimeException&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;This GeoIP database is not currently supported&quot;&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# 开始 logstash 流程&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# 创建从 input 到 filter 的缓冲队列，固定大小 20&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# SizedQueue 是 thread 库导入的&lt;/span&gt;
&lt;span class=&quot;vi&quot;&gt;@input_to_filter&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;SizedQueue&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;new&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;20&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# 具体的 geoip 过滤器线程&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;geoworker&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;begin&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;while&lt;/span&gt; &lt;span class=&quot;kp&quot;&gt;true&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;ip&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;vi&quot;&gt;@input_to_filter&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pop&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# GeoIP 查询方法&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;#            data = @geo.send(@geoip_type, ip)&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;#            puts data.to_hash[:city_name]&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# MaxMind-java 查询方法，注意传入的是 InetAddress 对象&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;vi&quot;&gt;@reader&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;city&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;no&quot;&gt;InetAddress&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;getByName&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;#            puts data.getCountry().getName()&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# maxminddb 查询方法&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;#            data = @db.lookup(ip)&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;#            puts data.country.name&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# 定义 input 线程，传入一百万次 IP 到缓冲队列&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;lines_num&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1000000&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;input&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;Thread&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;do&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;lines_num&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;times&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;each&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;do&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;
        &lt;span class=&quot;vi&quot;&gt;@input_to_filter&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;push&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# IP 发送完毕，计算每秒处理的速率&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;end_time&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;Time&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;now&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to_f&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1000&lt;/span&gt;
    &lt;span class=&quot;nb&quot;&gt;puts&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lines_num&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1000&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;end_time&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;vi&quot;&gt;@start_time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# 定义 filter 线程，启动 20 个&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;arr&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;20&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;times&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;collect&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;do&lt;/span&gt;
    &lt;span class=&quot;no&quot;&gt;Thread&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;do&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;geoworker&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# 记录开始时间，运行定义好的各线程&lt;/span&gt;
&lt;span class=&quot;vi&quot;&gt;@start_time&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;Time&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;now&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to_f&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1000&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;input&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;join&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;arr&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;each&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;join&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id=&quot;section-2&quot;&gt;测试结果&lt;/h2&gt;
&lt;p&gt;在一百万次查询的测试中，结果如下：&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;geoip worker 的查询 qps 是：6038.902610617599&lt;/li&gt;
  &lt;li&gt;maxminddb worker 的查询 qps 是：4621.093443130513&lt;/li&gt;
  &lt;li&gt;maxmind-java worker 的查询 qps 是：27943.88867154753&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;可见，对于这部分有性能要求的，完全可以改用 &lt;code&gt;maxmind-java&lt;/code&gt; 库，可以数倍提高。&lt;/p&gt;
    &lt;hr&gt;
    
    &lt;hr&gt;
  &lt;!-- JiaThis Button BEGIN --&gt;
&lt;div class=&quot;jiathis_style&quot;&gt;
&lt;span class=&quot;jiathis_txt&quot;&gt;分享到：&lt;/span&gt;
&lt;a class=&quot;jiathis_button_tsina&quot;&gt;新浪微博&lt;/a&gt;
&lt;a class=&quot;jiathis_button_weixin&quot;&gt;微信&lt;/a&gt;
&lt;a class=&quot;jiathis_button_renren&quot;&gt;人人网&lt;/a&gt;
&lt;a class=&quot;jiathis_button_ydnote&quot;&gt;有道云笔记&lt;/a&gt;
&lt;a class=&quot;jiathis_button_gmail&quot;&gt;Gmail邮箱&lt;/a&gt;
&lt;a class=&quot;jiathis_button_twitter&quot;&gt;Twitter&lt;/a&gt;
&lt;a class=&quot;jiathis_button_googleplus&quot;&gt;Google+&lt;/a&gt;
&lt;a class=&quot;jiathis_button_hi&quot;&gt;百度空间&lt;/a&gt;
&lt;a class=&quot;jiathis_button_fb&quot;&gt;Facebook&lt;/a&gt;
&lt;a class=&quot;jiathis_button_douban&quot;&gt;豆瓣&lt;/a&gt;
&lt;a href=&quot;http://www.jiathis.com/share?uid=1589850&quot; class=&quot;jiathis jiathis_txt jiathis_separator jtico jtico_jiathis&quot; target=&quot;_blank&quot;&gt;更多&lt;/a&gt;
&lt;a class=&quot;jiathis_counter_style&quot;&gt;&lt;/a&gt;
&lt;/div&gt;
&lt;script type=&quot;text/javascript&quot;&gt;
var jiathis_config={
	data_track_clickback:true,
	summary:&quot;&quot;,
	ralateuid:{
		&quot;tsina&quot;:&quot;1035836154&quot;
	},
	shortUrl:false,
	hideMore:false
}
&lt;/script&gt;
&lt;script type=&quot;text/javascript&quot; src=&quot;http://v3.jiathis.com/code/jia.js?uid=1589850&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt;
&lt;!-- JiaThis Button END --&gt;
&lt;!-- UY BEGIN --&gt;


&lt;!-- UY END --&gt;
  &lt;/div&gt;

</description>
        <pubDate>Thu, 22 Jan 2015 00:00:00 +0800</pubDate>
        <link>http://iftti.com/posts/2015/2015-01-22-logstash-maxmind-java-964fb7c87.html</link>
        <guid isPermaLink="true">http://iftti.com/posts/2015/2015-01-22-logstash-maxmind-java-964fb7c87.html</guid>
        
        
        <category>chenlinux</category>
        
      </item>
    
      <item>
        <title>扩展 Zabbix Web 页面功能</title>
        <description>

  
  &lt;div style=&quot;background-color: #FFF;&quot;&gt;
    &lt;p&gt;zabbix 是目前非常流行的一个开源监控系统。虽然核心代码是 C 的，却通过 PHP 的 web 端提供了非常方便的界面和 RPC 接口。可以看到很多讲如何通过 RPC 接口自动化 zabbix 操作的文章。不过，如果你想做的事情正好没有现成的接口或者界面，怎么办呢？这时候就感谢 zabbix 的后端是用的 MySQL 数据库了，这意味着我们可以很方便的扩展 Zabbix 页面和接口的功能。&lt;/p&gt;
&lt;p&gt;打个比方：&lt;strong&gt;我们一般都会按照 hostgroup 给某个 item 做一个 summary 汇总，然后针对 summary 的值来做报警。但是收到报警的时候，怎么能快速的知道这个 group 里是哪些 host 情况相对更严重呢？&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;zabbix 页面和接口，都没有提供这种信息查看方式。所以，我们需要自动动手，实现这个功能。&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://weibo.com/spider4k&quot;&gt;@南非蜘蛛&lt;/a&gt; 的 &lt;a href=&quot;https://github.com/spider4k/zatree&quot;&gt;zatree 项目&lt;/a&gt;，解决了跟这个类似的问题。它的着手点是：针对 hostgroup 查看 graph，通过 graph 完成肉眼查看对比和 item 值的排序。但是，单个 graph 上可能就需要加载很多 item 信息。在 hostgroup 较大，或者单 host 监控项较多的情况下，zatree 直接就因为获取过多信息得不到 MySQL 响应变得无法正常访问了。&lt;/p&gt;
&lt;p&gt;我的思路是：&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;获取 hostgroup 列表供选择；&lt;/li&gt;
  &lt;li&gt;根据选择的 hostgroup 获取 item 列表；&lt;/li&gt;
  &lt;li&gt;根据选择的 hostgroup 和 item 获取全部 host 的 lastvalue 并排序；&lt;/li&gt;
  &lt;li&gt;排序后的 host 应该提供 history 的 graph 查看链接；&lt;/li&gt;
  &lt;li&gt;尽可能借用 zabbix-web 的界面。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;这其中的第 1、3 步都是有现成的 API 的，直接用 hostgroup.get 和 item.get 即可。主要说说第 2、5 步。&lt;/p&gt;
&lt;h2 id=&quot;api&quot;&gt;新增 API&lt;/h2&gt;
&lt;p&gt;前面说了，API 扩展其实就是通过 MySQL 操作完成。这里通过已知 groupid 获取 item 列表，放到 MySQL 里其实就是一行 select 语句：&lt;code&gt;SELECT DISTINCT key_ FROM items WHERE hostid IN (SELECT hostid FROM hosts_groups WHERE groupid=&#39;1&#39;);&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;而要实现在界面上，最简单的方式，参考 &lt;code&gt;include/items.inc.php&lt;/code&gt; 里的 &lt;em&gt;get_item_by_hostid&lt;/em&gt; 方法，可以定义函数如下：&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-php&quot; data-lang=&quot;php&quot;&gt;&lt;span class=&quot;x&quot;&gt;function get_items_by_groupid($groupid) {&lt;/span&gt;
&lt;span class=&quot;x&quot;&gt;        $items = array();&lt;/span&gt;
&lt;span class=&quot;x&quot;&gt;        $sql = &#39;SELECT DISTINCT key_ FROM items WHERE hostid IN (&#39; .&lt;/span&gt;
&lt;span class=&quot;x&quot;&gt;                       &#39;SELECT hostid FROM hosts_groups WHERE groupid=&#39; . zbx_dbstr($groupid) .&lt;/span&gt;
&lt;span class=&quot;x&quot;&gt;                       &#39;)&#39;;&lt;/span&gt;
&lt;span class=&quot;x&quot;&gt;        $db_items = DBselect($sql);&lt;/span&gt;
&lt;span class=&quot;x&quot;&gt;        while ($item = DBfetch($db_items)) {&lt;/span&gt;
&lt;span class=&quot;x&quot;&gt;                array_push($items, $item[&#39;key_&#39;]);&lt;/span&gt;
&lt;span class=&quot;x&quot;&gt;        }&lt;/span&gt;
&lt;span class=&quot;x&quot;&gt;        return $items;&lt;/span&gt;
&lt;span class=&quot;x&quot;&gt;}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;这就可用了。&lt;/p&gt;
&lt;p&gt;不过这个函数你只能在 require 了 items.inc.php 的 PHP 页面里使用，不能暴露成 RPC 接口。&lt;/p&gt;
&lt;h3 id=&quot;rpc-&quot;&gt;修改为 RPC 接口&lt;/h3&gt;
&lt;p&gt;首先要简介一下 zabbix 的 RPC 接口是怎么传递的：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;api_jsonrpc.php
|-&amp;gt; api/rpc/class.cjsonrpc.php
    |-&amp;gt; api/rpc/class.czbxrpc.php
        |-&amp;gt; include/classes/api/API.php
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;在 API.php 中，通过 &lt;code&gt;getObjectClassName&lt;/code&gt; 方法，在本文件里的 &lt;code&gt;$classMap&lt;/code&gt; 获取对象的类名。&lt;/p&gt;
&lt;p&gt;所以，添加一个接口，分为几步：&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;实现新的类；&lt;/li&gt;
  &lt;li&gt;在 API.php 的 &lt;code&gt;$classMap&lt;/code&gt; 里添加对应键值对；&lt;/li&gt;
  &lt;li&gt;在 API.php 中添加返回对应类的方法(这步是为了能在其他代码里用 &lt;code&gt;API::Item()&lt;/code&gt; 这样的调用方式)。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;好，第一步，创建 &lt;code&gt;api/classes/CItemByGroup.php&lt;/code&gt; 文件，内容如下：&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-php&quot; data-lang=&quot;php&quot;&gt;&lt;span class=&quot;cp&quot;&gt;&amp;lt;?php&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;CItemByGroup&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;extends&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;CZBXAPI&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;function&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;get&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$groupid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
                &lt;span class=&quot;nv&quot;&gt;$items&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;
                &lt;span class=&quot;nv&quot;&gt;$sql&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&#39;SELECT DISTINCT key_ FROM items WHERE hostid IN (&#39;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;
                               &lt;span class=&quot;s1&quot;&gt;&#39;SELECT hostid FROM hosts_groups WHERE groupid=&#39;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;zbx_dbstr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$groupid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;
                               &lt;span class=&quot;s1&quot;&gt;&#39;)&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
                &lt;span class=&quot;nv&quot;&gt;$db_items&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;DBselect&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$sql&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
                &lt;span class=&quot;k&quot;&gt;while&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$item&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;DBfetch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$db_items&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
                        &lt;span class=&quot;nb&quot;&gt;array_push&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$items&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$item&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&#39;key_&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]);&lt;/span&gt;
                &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
                &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$items&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;第二步，添加 &lt;code&gt;$classMap&lt;/code&gt; 键值对，内容如下：&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-php&quot; data-lang=&quot;php&quot;&gt;&lt;span class=&quot;x&quot;&gt;&#39;itembygroup&#39; =&amp;gt; &#39;CItemByGroup&#39;,&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;第三步，添加对应方法，内容如下：&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-php&quot; data-lang=&quot;php&quot;&gt;&lt;span class=&quot;x&quot;&gt;/**&lt;/span&gt;
&lt;span class=&quot;x&quot;&gt;         * @return CItemByGroup&lt;/span&gt;
&lt;span class=&quot;x&quot;&gt;         */&lt;/span&gt;
&lt;span class=&quot;x&quot;&gt;        public static function ItemByGroup() {&lt;/span&gt;
&lt;span class=&quot;x&quot;&gt;                return self::getObject(&#39;itembygroup&#39;);&lt;/span&gt;
&lt;span class=&quot;x&quot;&gt;        }&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;这样，之前页面中直接使用 &lt;code&gt;get_items_by_groupid($groupid)&lt;/code&gt; 的代码，就可以改写成：&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-php&quot; data-lang=&quot;php&quot;&gt;&lt;span class=&quot;x&quot;&gt;$items = API::ItemByGroup()-&amp;gt;get($groupid);&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;而在其他程序里，则可以用过 &lt;strong&gt;itembygroup.get&lt;/strong&gt; 这个 RPC 接口获取相同结果了。&lt;/p&gt;
&lt;h2 id=&quot;zabbix-web--helper-&quot;&gt;Zabbix Web 的布局和各种 helper 函数&lt;/h2&gt;
&lt;p&gt;zatree 项目中完全自己写了整个页面，所以像授权啊、返回其他页啊都比较麻烦。所以我们尽量了解一下 zabbix web 本身是怎么写的页面，把数据融合到整体风格里面去。&lt;/p&gt;
&lt;p&gt;其实 zabbix web 页面布局非常简单。主要分为三部分：&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;include/page_header.php&lt;/li&gt;
  &lt;li&gt;new CWidget&lt;/li&gt;
  &lt;li&gt;include/page_footer.php&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;header 和 footer 是很顾名思义的。不过 &lt;code&gt;page_header.php&lt;/code&gt; 里，通过 &lt;code&gt;include/menu.inc.php&lt;/code&gt; 的 &lt;code&gt;zbx_construct_menu()&lt;/code&gt; 方法，会校验访问者的权限。&lt;/p&gt;
&lt;h3 id=&quot;section&quot;&gt;新增页面授权&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;menu.inc.php&lt;/code&gt; 也很简单，跟前面 api 类似，也是一个大变量来控制菜单和页面的权限，这个变量叫 &lt;code&gt;$ZBX_MENU&lt;/code&gt;。&lt;code&gt;$ZBX_MENU&lt;/code&gt; 数组存放的，就是 zabbix web 顶部菜单大家看到的那几个标签，Monitoring、Report 等等。如果打算把页面加在顶部菜单上，那么就直接添加一个元素到 &lt;code&gt;$ZBX_MENU&lt;/code&gt; 数组，如下：&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-php&quot; data-lang=&quot;php&quot;&gt;&lt;span class=&quot;x&quot;&gt;&#39;sort&#39; =&amp;gt; array(&lt;/span&gt;
&lt;span class=&quot;x&quot;&gt;                &#39;label&#39;                 =&amp;gt; _(&#39;Sort&#39;),&lt;/span&gt;
&lt;span class=&quot;x&quot;&gt;                &#39;user_type&#39;             =&amp;gt; USER_TYPE_ZABBIX_USER,&lt;/span&gt;
&lt;span class=&quot;x&quot;&gt;                &#39;default_page_id&#39;       =&amp;gt; 0,&lt;/span&gt;
&lt;span class=&quot;x&quot;&gt;                &#39;force_disable_all_nodes&#39;=&amp;gt; true,&lt;/span&gt;
&lt;span class=&quot;x&quot;&gt;                &#39;pages&#39; =&amp;gt; array(&lt;/span&gt;
&lt;span class=&quot;x&quot;&gt;                        array(&lt;/span&gt;
&lt;span class=&quot;x&quot;&gt;                                &#39;url&#39; =&amp;gt; &#39;sort.php&#39;,&#39;label&#39; =&amp;gt; _(&#39;Sort&#39;)&lt;/span&gt;
&lt;span class=&quot;x&quot;&gt;                        )&lt;/span&gt;
&lt;span class=&quot;x&quot;&gt;                )&lt;/span&gt;
&lt;span class=&quot;x&quot;&gt;         ),&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;如果打算加到到次级菜单，比如放到 Monitoring 下面，那么找到 &lt;code&gt;view&lt;/code&gt; 元素(其 label 为 “Monitoring”)，在其 &lt;code&gt;pages&lt;/code&gt; 数组里加上即可：&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-php&quot; data-lang=&quot;php&quot;&gt;&lt;span class=&quot;x&quot;&gt;&#39;pages&#39; =&amp;gt; array(&lt;/span&gt;
&lt;span class=&quot;x&quot;&gt;			...&lt;/span&gt;
&lt;span class=&quot;x&quot;&gt;                        array(&lt;/span&gt;
&lt;span class=&quot;x&quot;&gt;                                &#39;url&#39; =&amp;gt; &#39;sort.php&#39;,&lt;/span&gt;
&lt;span class=&quot;x&quot;&gt;                                &#39;label&#39; =&amp;gt; _(&#39;Sort&#39;),&lt;/span&gt;
&lt;span class=&quot;x&quot;&gt;                        )&lt;/span&gt;
&lt;span class=&quot;x&quot;&gt;                )&lt;/span&gt;
&lt;span class=&quot;x&quot;&gt;        ),&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id=&quot;cwidget-&quot;&gt;CWidget 及其他组件&lt;/h3&gt;
&lt;p&gt;zabbix 虽然没有使用特别明确的 MVC 框架，倒也不用大家到处自己去拼接输出 HTML 代码，它已经实现了很多 helper 函数。&lt;/p&gt;
&lt;p&gt;比如：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;group 和 item 的选择器，可以用 &lt;code&gt;CComboBox()&lt;/code&gt; 生成；&lt;/li&gt;
  &lt;li&gt;页面交互的表单，可以用 &lt;code&gt;CForm()&lt;/code&gt; 生成；&lt;/li&gt;
  &lt;li&gt;数据展示的表格，可以用 &lt;code&gt;CTableInfo()&lt;/code&gt; 生成；&lt;/li&gt;
  &lt;li&gt;history graph 的链接，可以用 &lt;code&gt;CLink()&lt;/code&gt; 生成；&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;然后，&lt;code&gt;CTableInfo()&lt;/code&gt; 可以 &lt;code&gt;-&amp;gt;addRow()&lt;/code&gt;；&lt;code&gt;CForm()&lt;/code&gt;、 &lt;code&gt;CComboBox()&lt;/code&gt; 和 &lt;code&gt;CWidget()&lt;/code&gt; 都可以 &lt;code&gt;-&amp;gt;addItem()&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;把各种元素都添加到 CWidget 里以后，调用 &lt;code&gt;-&amp;gt;show()&lt;/code&gt; 方法即可。&lt;/p&gt;
&lt;p&gt;此外，还提供有 &lt;code&gt;check_fields&lt;/code&gt;, &lt;code&gt;get_request&lt;/code&gt;, &lt;code&gt;validate_sort_and_sortorder&lt;/code&gt;, &lt;code&gt;getPageSortOrder&lt;/code&gt;, &lt;code&gt;make_sorting_header&lt;/code&gt; 和 &lt;code&gt;order_result&lt;/code&gt; 等方法帮助处理请求参数和数据表格展示。&lt;/p&gt;
&lt;p&gt;最后效果如下：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/images/chenlinux.com/7064af717c9a728d7622ec37c4c72cd5.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    &lt;hr&gt;
    
    &lt;hr&gt;
  &lt;!-- JiaThis Button BEGIN --&gt;
&lt;div class=&quot;jiathis_style&quot;&gt;
&lt;span class=&quot;jiathis_txt&quot;&gt;分享到：&lt;/span&gt;
&lt;a class=&quot;jiathis_button_tsina&quot;&gt;新浪微博&lt;/a&gt;
&lt;a class=&quot;jiathis_button_weixin&quot;&gt;微信&lt;/a&gt;
&lt;a class=&quot;jiathis_button_renren&quot;&gt;人人网&lt;/a&gt;
&lt;a class=&quot;jiathis_button_ydnote&quot;&gt;有道云笔记&lt;/a&gt;
&lt;a class=&quot;jiathis_button_gmail&quot;&gt;Gmail邮箱&lt;/a&gt;
&lt;a class=&quot;jiathis_button_twitter&quot;&gt;Twitter&lt;/a&gt;
&lt;a class=&quot;jiathis_button_googleplus&quot;&gt;Google+&lt;/a&gt;
&lt;a class=&quot;jiathis_button_hi&quot;&gt;百度空间&lt;/a&gt;
&lt;a class=&quot;jiathis_button_fb&quot;&gt;Facebook&lt;/a&gt;
&lt;a class=&quot;jiathis_button_douban&quot;&gt;豆瓣&lt;/a&gt;
&lt;a href=&quot;http://www.jiathis.com/share?uid=1589850&quot; class=&quot;jiathis jiathis_txt jiathis_separator jtico jtico_jiathis&quot; target=&quot;_blank&quot;&gt;更多&lt;/a&gt;
&lt;a class=&quot;jiathis_counter_style&quot;&gt;&lt;/a&gt;
&lt;/div&gt;
&lt;script type=&quot;text/javascript&quot;&gt;
var jiathis_config={
	data_track_clickback:true,
	summary:&quot;&quot;,
	ralateuid:{
		&quot;tsina&quot;:&quot;1035836154&quot;
	},
	shortUrl:false,
	hideMore:false
}
&lt;/script&gt;
&lt;script type=&quot;text/javascript&quot; src=&quot;http://v3.jiathis.com/code/jia.js?uid=1589850&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt;
&lt;!-- JiaThis Button END --&gt;
&lt;!-- UY BEGIN --&gt;


&lt;!-- UY END --&gt;
  &lt;/div&gt;

</description>
        <pubDate>Wed, 21 Jan 2015 00:00:00 +0800</pubDate>
        <link>http://iftti.com/posts/2015/2015-01-21-extends-zabbix-web-2adffb2e1.html</link>
        <guid isPermaLink="true">http://iftti.com/posts/2015/2015-01-21-extends-zabbix-web-2adffb2e1.html</guid>
        
        
        <category>chenlinux</category>
        
      </item>
    
      <item>
        <title>用MeCab打造一套实用的中文分词系统</title>
        <description>

						&lt;p&gt;&lt;a href=&quot;https://code.google.com/p/mecab/&quot;&gt;MeCab&lt;/a&gt;是一套日文分词（形态分析）和词性标注系统（Yet Another Part-of-Speech and Morphological Analyzer）， &lt;a href=&quot;http://www.52nlp.cn/author/rickjin&quot;&gt;rick&lt;/a&gt;曾经在这里分享过&lt;a href=&quot;https://code.google.com/p/mecab/&quot;&gt;MeCab&lt;/a&gt;的官方文档中文翻译: &lt;a href=&quot;http://www.52nlp.cn/?p=5371&quot;&gt;日文分词器 Mecab 文档&lt;/a&gt;，这款日文分词器基于&lt;a href=&quot;http://www.52nlp.cn/?p=3378&quot;&gt;条件随机场&lt;/a&gt;打造，有着诸多优点，譬如代码基于C++实现，基本内嵌CRF++代码，词典检索的算法和数据结构均使用双数组Double-Array，性能优良，并通过SWIG提供多种语言调用接口，可扩展性和通用性都非常不错：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;mecab (http://mecab.sourceforge.net/) 是奈良先端科学技術大学院的工藤拓开发的日文分词系统， 该作者写过多个 machine learning 方面的软件包， 最有名的就是 CRF++， 目前该作者在 google@Japan 工作。&lt;/p&gt;
&lt;p&gt;mecab 是基于CRF 的一个日文分词系统，代码使用 c++ 实现， 基本上内嵌了 CRF++ 的代码， 同时提供了多种脚本语言调用的接口(python, perl， ruby 等).整个系统的架构采用通用泛化的设计，用户可以通过配置文件定制CRF训练中需要使用的特征模板。 甚至， 如果你有中文的分词语料作为训练语料，可以在该架构下按照其配置文件的规范定制一个中文的分词系统。&lt;/p&gt;
&lt;p&gt;日文NLP 界有几个有名的开源分词系统， Juman, Chasen, Mecab.   Juman 和 Chasen 都是比较老的系统了， Mecab 系统比较新， 在很多方面都优于 Juman 和 Chasen, mecab 目前开发也比较活跃。 Mecab 虽然使用 CRF 实现， 但是解析效率上确相当高效， 据作者的介绍， Mecab 比基于 HMM 的 Chasen 的解析速度要快。 笔者在一台 Linux 机器上粗略测试过其速度，将近达到 2MB/s， 完全达到了工程应用的需求， 该系统目前在日文 NLP 界被广泛使用。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;我们曾经介绍过一个非常初级的CRF中文分词实现方案：&lt;a href=&quot;http://www.52nlp.cn/?p=6339&quot;&gt;中文分词入门之字标注法4&lt;/a&gt;，基于CRF++实现了一个Toy级别的CRF中文分词系统，但是还远远不够。在仔细看过这篇日文分词系统MeCab的中文文档并亲测之后，不得不赞这真是一个理想的CRF分词系统，除了上述所说的优点之外，在使用上它还支持Nbest输出，多种输出格式，全切分模式，系统词典和用户词典定制等等，难怪这套分词系统在日本NLP界被广泛使用。&lt;/p&gt;
&lt;p&gt;MeCab的诸多优点以及它的通用性一直深深吸引着我，但是除了日文资料，相关的中文或英文资料相当匮乏，曾经尝试过基于MeCab的中文翻译文档以及代码中测试用例中的例子来训练一套中文分词系统，但是第一次以失败告终。这几天，由于偶然的因素又一次捡起了MeCab，然后通过Google及Google翻译发现了这篇日文文章《&lt;a href=&quot;http://www.onaneet.org/blog/archives/4020&quot;&gt;MeCabで中国語の形態素解析（分かち書き）をしてみる&lt;/a&gt;》，虽其是日语所写，但是主旨是通过MeCab构造一套中文（貌似是繁体）形态（中文分词+词性标注）分析系统，给了我很大的帮助。所以接下来，我会基于这篇文章的提示以及rick翻译文档中第八节“从原始词典/语料库做参数估计”的参考，同时结合&lt;a href=&quot;http://www.52nlp.cn/?p=2885&quot;&gt;backoff2005&lt;/a&gt;中微软研究院的中文分词语料来训练一套极简的中文分词系统，至于MeCab的相关介绍及安装使用请参考 &lt;a href=&quot;http://www.52nlp.cn/?p=5371&quot;&gt;日文分词器 Mecab 文档&lt;/a&gt;，这里不再赘述。以下是我在Mac OS下的操作记录，同理可推广制Linux下，至于Windows下，请自行测试。一些中文分词的背景知识可参考这里过往的相关文章: &lt;a href=&quot;http://www.52nlp.cn/category/word-segmentation&quot;&gt;中文分词&lt;/a&gt;。&lt;br&gt;
&lt;span id=&quot;more-6932&quot;&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;0、首先在和SIGHAN backoff2005的相关语料icwb2-data的同层次目录里建立一个msr_mecab_test目录：&lt;/p&gt;
&lt;p&gt;mkdir msr_mecab_test&lt;br&gt;
cd msr_mecab_test&lt;/p&gt;
&lt;p&gt;然后在这个目录下分别建立三个子目录：seed, final, script:&lt;/p&gt;
&lt;p&gt;mkdir seed&lt;br&gt;
mkdir final&lt;br&gt;
mkdir script&lt;/p&gt;
&lt;p&gt;1、准备Seed 词典&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;MeCab 的词典是 CSV 格式的. Seed 词典和用于发布的词典的格式基本上是相同的 .&lt;/p&gt;
&lt;p&gt;以下是词典的词条条目的例子 .&lt;/p&gt;
&lt;p&gt;進学校,0,0,0,名詞,一般,*,*,*,*,進学校,シンガクコウ,シンガクコー&lt;br&gt;
梅暦,0,0,0,名詞,一般,*,*,*,*,梅暦,ウメゴヨミ,ウメゴヨミ&lt;br&gt;
気圧,0,0,0,名詞,一般,*,*,*,*,気圧,キアツ,キアツ&lt;br&gt;
水中翼船,0,0,0,名詞,一般,*,*,*,*,水中翼船,スイチュウヨクセン,スイチューヨクセン&lt;br&gt;
前面4个字段是必须的,&lt;/p&gt;
&lt;p&gt;表层形(词条本身)&lt;br&gt;
左连接状态号&lt;br&gt;
右连接状态号&lt;br&gt;
cost&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;我们的词典源来自于icwb2-data/gold/msr_training_words.utf8，由于不含词性等其他信息，这里我们提供一个非常简单的Seed 词条格式，如下所示：&lt;/p&gt;
&lt;div class=&quot;codecolorer-container text default&quot; style=&quot;overflow:auto;white-space:nowrap;width:620px;&quot;&gt;&lt;table cellspacing=&quot;0&quot; cellpadding=&quot;0&quot;&gt;&lt;tbody&gt;&lt;tr&gt;
&lt;td class=&quot;line-numbers&quot;&gt;&lt;div&gt;1&lt;br&gt;2&lt;br&gt;3&lt;br&gt;4&lt;br&gt;5&lt;br&gt;6&lt;br&gt;7&lt;br&gt;8&lt;br&gt;9&lt;br&gt;10&lt;br&gt;
&lt;/div&gt;&lt;/td&gt;
&lt;td&gt;&lt;div class=&quot;text codecolorer&quot;&gt;１１２３项,0,0,0,0,0,0&lt;br&gt;
义演,0,0,0,0,0,0&lt;br&gt;
佳酿,0,0,0,0,0,0&lt;br&gt;
沿街,0,0,0,0,0,0&lt;br&gt;
老理,0,0,0,0,0,0&lt;br&gt;
三四十岁,0,0,0,0,0,0&lt;br&gt;
解波,0,0,0,0,0,0&lt;br&gt;
统建,0,0,0,0,0,0&lt;br&gt;
蓓蕾,0,0,0,0,0,0&lt;br&gt;
李佑生,0,0,0,0,0,0&lt;/div&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/div&gt;
&lt;p&gt;这里提供一个python脚本用于制作seed词典: &lt;a href=&quot;https://github.com/panyang/yuzhen_nlp_edu_tools/blob/master/CLPT/WordSegmentation/MeCab/script/make_mecab_seed_data.py&quot;&gt;make_mecab_seed_data.py&lt;/a&gt;:&lt;/p&gt;
&lt;div class=&quot;codecolorer-container text default&quot; style=&quot;overflow:auto;white-space:nowrap;width:620px;height:450px;&quot;&gt;&lt;table cellspacing=&quot;0&quot; cellpadding=&quot;0&quot;&gt;&lt;tbody&gt;&lt;tr&gt;
&lt;td class=&quot;line-numbers&quot;&gt;&lt;div&gt;1&lt;br&gt;2&lt;br&gt;3&lt;br&gt;4&lt;br&gt;5&lt;br&gt;6&lt;br&gt;7&lt;br&gt;8&lt;br&gt;9&lt;br&gt;10&lt;br&gt;11&lt;br&gt;12&lt;br&gt;13&lt;br&gt;14&lt;br&gt;15&lt;br&gt;16&lt;br&gt;17&lt;br&gt;18&lt;br&gt;19&lt;br&gt;20&lt;br&gt;21&lt;br&gt;22&lt;br&gt;23&lt;br&gt;24&lt;br&gt;
&lt;/div&gt;&lt;/td&gt;
&lt;td&gt;&lt;div class=&quot;text codecolorer&quot;&gt;#!/usr/bin/env python&lt;br&gt;
# -*- coding: utf-8 -*-&lt;br&gt;
# Author: 52nlpcn@gmail.com&lt;br&gt;
# Copyright 2015 @ YuZhen Technology&lt;br&gt;
&lt;br&gt;
import codecs&lt;br&gt;
import sys&lt;br&gt;
&lt;br&gt;
def make_mecab_seed_data(input_file, output_file):&lt;br&gt;
    input_data = codecs.open(input_file, &#39;r&#39;, &#39;utf-8&#39;)&lt;br&gt;
    output_data = codecs.open(output_file, &#39;w&#39;, &#39;utf-8&#39;)&lt;br&gt;
    for line in input_data.readlines():&lt;br&gt;
        word = line.strip()&lt;br&gt;
        output_data.write(word+ &quot;,0,0,0,0,0,0\n&quot;)&lt;br&gt;
    input_data.close()&lt;br&gt;
    output_data.close()&lt;br&gt;
&lt;br&gt;
if __name__ == &#39;__main__&#39;:&lt;br&gt;
    if len(sys.argv) &amp;lt; 3:&lt;br&gt;
        print &quot;pls use: python make_mecab_seed_data.py input output&quot;&lt;br&gt;
        sys.exit()&lt;br&gt;
    input_file = sys.argv[1]&lt;br&gt;
    output_file = sys.argv[2]&lt;br&gt;
    make_mecab_seed_data(input_file, output_file)&lt;/div&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/div&gt;
&lt;p&gt;可以将这个脚本拷贝到script目录下，然后进入到seed目录，执行 python ../script/make_mecab_seed_data.py ../../icwb2-data/gold/msr_training_words.utf8 msr_words.csv 即可得到MeCab所需的Seed词典。&lt;/p&gt;
&lt;p&gt;2、准备配置文件&lt;br&gt;
在seed目录下准备5个最基础的配置文件，分别是dicrc, char.def, unk.def, rewrite.def, feature.def , 这5个配置文件我主要参考自上述那篇日文文章，略作修改，其具体解释可参考中文翻译文档中的详细描述：&lt;/p&gt;
&lt;p&gt;1) &lt;a href=&quot;https://github.com/panyang/yuzhen_nlp_edu_tools/blob/master/CLPT/WordSegmentation/MeCab/seed/dicrc&quot;&gt;dicrc&lt;/a&gt;: 该文件中设定词典的各种动作的，以下为最小配置：&lt;/p&gt;
&lt;div class=&quot;codecolorer-container text default&quot; style=&quot;overflow:auto;white-space:nowrap;width:620px;&quot;&gt;&lt;table cellspacing=&quot;0&quot; cellpadding=&quot;0&quot;&gt;&lt;tbody&gt;&lt;tr&gt;
&lt;td class=&quot;line-numbers&quot;&gt;&lt;div&gt;1&lt;br&gt;2&lt;br&gt;3&lt;br&gt;4&lt;br&gt;5&lt;br&gt;
&lt;/div&gt;&lt;/td&gt;
&lt;td&gt;&lt;div class=&quot;text codecolorer&quot;&gt;cost-factor = 800&lt;br&gt;
bos-feature = BOS/EOS,*,*,*,*,*,*,*,*&lt;br&gt;
eval-size = 6&lt;br&gt;
unk-eval-size = 4&lt;br&gt;
config-charset = UTF-8&lt;/div&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/div&gt;
&lt;p&gt;2) &lt;a href=&quot;https://github.com/panyang/yuzhen_nlp_edu_tools/blob/master/CLPT/WordSegmentation/MeCab/seed/char.def&quot;&gt;char.def&lt;/a&gt;: 定义未登陆词处理的文件. 通常日语的词法分析是基于字符的种类处理未登陆词, Mecab 中哪个文字属于哪个字符种类, 用户可以进行细致的指定; 对于每个字符类别， 需要采用哪种未登陆词的识别处理,也可以进行详细的定义。&lt;/p&gt;
&lt;div class=&quot;codecolorer-container text default&quot; style=&quot;overflow:auto;white-space:nowrap;width:620px;&quot;&gt;&lt;table cellspacing=&quot;0&quot; cellpadding=&quot;0&quot;&gt;&lt;tbody&gt;&lt;tr&gt;
&lt;td class=&quot;line-numbers&quot;&gt;&lt;div&gt;1&lt;br&gt;2&lt;br&gt;3&lt;br&gt;4&lt;br&gt;5&lt;br&gt;6&lt;br&gt;7&lt;br&gt;8&lt;br&gt;9&lt;br&gt;10&lt;br&gt;
&lt;/div&gt;&lt;/td&gt;
&lt;td&gt;&lt;div class=&quot;text codecolorer&quot;&gt;DEFAULT        0 1 0  # DEFAULT is a mandatory category!&lt;br&gt;
SPACE          0 1 0&lt;br&gt;
CJK            0 0 2&lt;br&gt;
&lt;br&gt;
# SPACE&lt;br&gt;
0x0020 SPACE  # DO NOT REMOVE THIS LINE,  0x0020 is reserved for SPACE&lt;br&gt;
0x00D0 SPACE&lt;br&gt;
0x0009 SPACE&lt;br&gt;
0x000B SPACE&lt;br&gt;
0x000A SPACE&lt;/div&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/div&gt;
&lt;p&gt;3) &lt;a href=&quot;https://github.com/panyang/yuzhen_nlp_edu_tools/blob/master/CLPT/WordSegmentation/MeCab/seed/unk.def&quot;&gt;unk.def&lt;/a&gt;: 用于未登陆词的词典。&lt;/p&gt;
&lt;div class=&quot;codecolorer-container text default&quot; style=&quot;overflow:auto;white-space:nowrap;width:620px;&quot;&gt;&lt;table cellspacing=&quot;0&quot; cellpadding=&quot;0&quot;&gt;&lt;tbody&gt;&lt;tr&gt;
&lt;td class=&quot;line-numbers&quot;&gt;&lt;div&gt;1&lt;br&gt;2&lt;br&gt;3&lt;br&gt;
&lt;/div&gt;&lt;/td&gt;
&lt;td&gt;&lt;div class=&quot;text codecolorer&quot;&gt;DEFAULT,0,0,0,unk,*,*&lt;br&gt;
SPACE,0,0,0,unk,*,*&lt;br&gt;
CJK,0,0,0,unk,*,*&lt;/div&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/div&gt;
&lt;p&gt;4) &lt;a href=&quot;https://github.com/panyang/yuzhen_nlp_edu_tools/blob/master/CLPT/WordSegmentation/MeCab/seed/rewrite.def&quot;&gt;rewrite.def&lt;/a&gt;: 定义从特征列到内部状态特征列的转换映射。&lt;/p&gt;
&lt;div class=&quot;codecolorer-container text default&quot; style=&quot;overflow:auto;white-space:nowrap;width:620px;&quot;&gt;&lt;table cellspacing=&quot;0&quot; cellpadding=&quot;0&quot;&gt;&lt;tbody&gt;&lt;tr&gt;
&lt;td class=&quot;line-numbers&quot;&gt;&lt;div&gt;1&lt;br&gt;2&lt;br&gt;3&lt;br&gt;4&lt;br&gt;5&lt;br&gt;6&lt;br&gt;7&lt;br&gt;8&lt;br&gt;
&lt;/div&gt;&lt;/td&gt;
&lt;td&gt;&lt;div class=&quot;text codecolorer&quot;&gt;[unigram rewrite]&lt;br&gt;
*,*,*   \$1,\$2,\$3&lt;br&gt;
&lt;br&gt;
[left rewrite]&lt;br&gt;
*,*,*   \$1,\$2,\$3&lt;br&gt;
&lt;br&gt;
[right rewrite]&lt;br&gt;
*,*,*   \$1,\$2,\$3&lt;/div&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/div&gt;
&lt;p&gt;5) &lt;a href=&quot;https://github.com/panyang/yuzhen_nlp_edu_tools/blob/master/CLPT/WordSegmentation/MeCab/seed/feature.def&quot;&gt;feature.def&lt;/a&gt;: 该文件中定义了从内部状态的素生列中抽取CRF的素生列的模板&lt;/p&gt;
&lt;div class=&quot;codecolorer-container text default&quot; style=&quot;overflow:auto;white-space:nowrap;width:620px;&quot;&gt;&lt;table cellspacing=&quot;0&quot; cellpadding=&quot;0&quot;&gt;&lt;tbody&gt;&lt;tr&gt;
&lt;td class=&quot;line-numbers&quot;&gt;&lt;div&gt;1&lt;br&gt;2&lt;br&gt;3&lt;br&gt;4&lt;br&gt;5&lt;br&gt;6&lt;br&gt;7&lt;br&gt;8&lt;br&gt;9&lt;br&gt;10&lt;br&gt;11&lt;br&gt;12&lt;br&gt;13&lt;br&gt;14&lt;br&gt;15&lt;br&gt;16&lt;br&gt;17&lt;br&gt;18&lt;br&gt;19&lt;br&gt;
&lt;/div&gt;&lt;/td&gt;
&lt;td&gt;&lt;div class=&quot;text codecolorer&quot;&gt;UNIGRAM W0:%F[6]&lt;br&gt;
UNIGRAM W1:%F[0]/%F[6]&lt;br&gt;
UNIGRAM W2:%F[0],%F?[1]/%F[6]&lt;br&gt;
UNIGRAM W3:%F[0],%F[1],%F?[2]/%F[6]&lt;br&gt;
UNIGRAM W4:%F[0],%F[1],%F[2],%F?[3]/%F[6]&lt;br&gt;
&lt;br&gt;
UNIGRAM T0:%t&lt;br&gt;
UNIGRAM T1:%F[0]/%t&lt;br&gt;
UNIGRAM T2:%F[0],%F?[1]/%t&lt;br&gt;
UNIGRAM T3:%F[0],%F[1],%F?[2]/%t&lt;br&gt;
UNIGRAM T4:%F[0],%F[1],%F[2],%F?[3]/%t&lt;br&gt;
&lt;br&gt;
BIGRAM B00:%L[0]/%R[0]&lt;br&gt;
BIGRAM B01:%L[0],%L?[1]/%R[0]&lt;br&gt;
BIGRAM B02:%L[0]/%R[0],%R?[1]&lt;br&gt;
BIGRAM B03:%L[0]/%R[0],%R[1],%R?[2]&lt;br&gt;
BIGRAM B04:%L[0],%L?[1]/%R[0],%R[1],%R?[2]&lt;br&gt;
BIGRAM B05:%L[0]/%R[0],%R[1],%R[2],%R?[3]&lt;br&gt;
BIGRAM B06:%L[0],%L?[1]/%R[0],%R[1],%R[2],%R?[3]&lt;/div&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/div&gt;
&lt;p&gt;3、准备训练语料&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;用于训练的数据， 和 MeCab 的输出是相同格式的，&lt;/p&gt;
&lt;p&gt;太郎    名詞,固有名詞,人名,名,*,*,太郎,タロウ,タロー&lt;br&gt;
は      助詞,係助詞,*,*,*,*,は,ハ,ワ&lt;br&gt;
花子    名詞,固有名詞,人名,名,*,*,花子,ハナコ,ハナコ&lt;br&gt;
が      助詞,格助詞,一般,*,*,*, が,ガ,ガ&lt;br&gt;
好き    名詞,形容動詞語幹,*,*,*,*, 好き,スキ,スキ&lt;br&gt;
だ      助動詞,*,*,*, 特殊・ダ,基本形,だ,ダ,ダ&lt;br&gt;
.       記号,句点,*,*,*,*, . , . , .&lt;br&gt;
EOS&lt;br&gt;
焼酎    名詞,一般,*,*,*,*,焼酎,ショウチュウ,ショーチュー&lt;br&gt;
好き    名詞,形容動詞語幹,*,*,*,*,好き,スキ,スキ&lt;br&gt;
の      助詞,連体化,*,*,*,*, の,ノ,ノ&lt;br&gt;
親父    名詞,一般,*,*,*,*,親父,オヤジ,オヤジ&lt;br&gt;
.       記号,句点,*,*,*,*, . , . , .&lt;br&gt;
EOS&lt;br&gt;
…&lt;br&gt;
Tab 键分隔的第一个部分为词条表层文字, 随后是CSV 格式的特征列, 句子结束标志为 只包行EOS的行.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;我们的中文分词训练语料来源于icwb2-data/training/msr_training.utf8 ，和词典格式一样，我们提供一份格式非常简单的用于MeCab训练的分词语料，如下所示：&lt;/p&gt;
&lt;div class=&quot;codecolorer-container text default&quot; style=&quot;overflow:auto;white-space:nowrap;width:620px;&quot;&gt;&lt;table cellspacing=&quot;0&quot; cellpadding=&quot;0&quot;&gt;&lt;tbody&gt;&lt;tr&gt;
&lt;td class=&quot;line-numbers&quot;&gt;&lt;div&gt;1&lt;br&gt;2&lt;br&gt;3&lt;br&gt;4&lt;br&gt;5&lt;br&gt;6&lt;br&gt;7&lt;br&gt;8&lt;br&gt;9&lt;br&gt;10&lt;br&gt;
&lt;/div&gt;&lt;/td&gt;
&lt;td&gt;&lt;div class=&quot;text codecolorer&quot;&gt;“ 0,0,0,0,0,0&lt;br&gt;
人们  0,0,0,0,0,0&lt;br&gt;
常 0,0,0,0,0,0&lt;br&gt;
说 0,0,0,0,0,0&lt;br&gt;
生活  0,0,0,0,0,0&lt;br&gt;
是 0,0,0,0,0,0&lt;br&gt;
一 0,0,0,0,0,0&lt;br&gt;
部 0,0,0,0,0,0&lt;br&gt;
教科书   0,0,0,0,0,0&lt;br&gt;
， 0,0,0,0,0,0&lt;/div&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/div&gt;
&lt;p&gt;同样提供一个可用于语料格式转换的脚本：&lt;a href=&quot;https://github.com/panyang/yuzhen_nlp_edu_tools/blob/master/CLPT/WordSegmentation/MeCab/script/make_mecab_train_data.py&quot;&gt;make_mecab_train_data.py&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;codecolorer-container text default&quot; style=&quot;overflow:auto;white-space:nowrap;width:620px;height:450px;&quot;&gt;&lt;table cellspacing=&quot;0&quot; cellpadding=&quot;0&quot;&gt;&lt;tbody&gt;&lt;tr&gt;
&lt;td class=&quot;line-numbers&quot;&gt;&lt;div&gt;1&lt;br&gt;2&lt;br&gt;3&lt;br&gt;4&lt;br&gt;5&lt;br&gt;6&lt;br&gt;7&lt;br&gt;8&lt;br&gt;9&lt;br&gt;10&lt;br&gt;11&lt;br&gt;12&lt;br&gt;13&lt;br&gt;14&lt;br&gt;15&lt;br&gt;16&lt;br&gt;17&lt;br&gt;18&lt;br&gt;19&lt;br&gt;20&lt;br&gt;21&lt;br&gt;22&lt;br&gt;23&lt;br&gt;24&lt;br&gt;25&lt;br&gt;26&lt;br&gt;27&lt;br&gt;
&lt;/div&gt;&lt;/td&gt;
&lt;td&gt;&lt;div class=&quot;text codecolorer&quot;&gt;#!/usr/bin/env python&lt;br&gt;
# -*- coding: utf-8 -*-&lt;br&gt;
# Author: 52nlpcn@gmail.com&lt;br&gt;
# Copyright 2015 @ YuZhen Technology&lt;br&gt;
&lt;br&gt;
import codecs&lt;br&gt;
import sys&lt;br&gt;
&lt;br&gt;
def make_mecab_train_data(input_file, output_file):&lt;br&gt;
    input_data = codecs.open(input_file, &#39;r&#39;, &#39;utf-8&#39;)&lt;br&gt;
    output_data = codecs.open(output_file, &#39;w&#39;, &#39;utf-8&#39;)&lt;br&gt;
    for line in input_data.readlines():&lt;br&gt;
        word_list = line.strip().split()&lt;br&gt;
        if len(word_list) == 0: continue&lt;br&gt;
        for word in word_list:&lt;br&gt;
            output_data.write(word+ &quot;\t0,0,0,0,0,0\n&quot;)&lt;br&gt;
        output_data.write(&quot;EOS\n&quot;)&lt;br&gt;
    input_data.close()&lt;br&gt;
    output_data.close()&lt;br&gt;
&lt;br&gt;
if __name__ == &#39;__main__&#39;:&lt;br&gt;
    if len(sys.argv) &amp;lt; 3:&lt;br&gt;
        print &quot;pls use: python make_mecab_train_data.py input output&quot;&lt;br&gt;
        sys.exit()&lt;br&gt;
    input_file = sys.argv[1]&lt;br&gt;
    output_file = sys.argv[2]&lt;br&gt;
    make_mecab_train_data(input_file, output_file)&lt;/div&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/div&gt;
&lt;p&gt;可将其拷贝到script目录下，然后执行 python ../script/make_mecab_train_data.py ../../icwb2-data/training/msr_training.utf8 corpus 即可。&lt;/p&gt;
&lt;p&gt;4、生成训练用的二进制词典&lt;/p&gt;
&lt;p&gt;好了，目前为止，我们需要训练用的词典，配置文件及训练语料已准备就绪：&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;seed 词典(CSV 格式文件集合)&lt;br&gt;
所有的配置文件 (char.def, unk.def, rewrite.def, feature.def)&lt;br&gt;
训练用的数据 (文件名: corpus)&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;在seed目录下运行以下命令， 生成学习用的二进制词典:&lt;/p&gt;
&lt;p&gt;/usr/local/libexec/mecab/mecab-dict-index&lt;/p&gt;
&lt;p&gt;也可以通过 -d,  -o 选项指定输入输出目录来运行该命令&lt;br&gt;
/usr/local/libexec/mecab/mecab-dict-index -d \$WORK/seed -o \$WORK/seed&lt;br&gt;
-d: 包含seed 词典和配置文件的目录(缺省为当前目录)&lt;br&gt;
-o: 训练用二进制词典的输出目录(缺省为当前目录)&lt;/p&gt;
&lt;p&gt;执行过程如下：&lt;/p&gt;
&lt;div class=&quot;codecolorer-container text default&quot; style=&quot;overflow:auto;white-space:nowrap;width:620px;&quot;&gt;&lt;table cellspacing=&quot;0&quot; cellpadding=&quot;0&quot;&gt;&lt;tbody&gt;&lt;tr&gt;
&lt;td class=&quot;line-numbers&quot;&gt;&lt;div&gt;1&lt;br&gt;2&lt;br&gt;3&lt;br&gt;4&lt;br&gt;5&lt;br&gt;6&lt;br&gt;7&lt;br&gt;8&lt;br&gt;9&lt;br&gt;10&lt;br&gt;11&lt;br&gt;
&lt;/div&gt;&lt;/td&gt;
&lt;td&gt;&lt;div class=&quot;text codecolorer&quot;&gt;./pos-id.def is not found. minimum setting is used&lt;br&gt;
reading ./unk.def ... 3&lt;br&gt;
emitting double-array: 100% |###########################################| &lt;br&gt;
./model.def is not found. skipped.&lt;br&gt;
./pos-id.def is not found. minimum setting is used&lt;br&gt;
reading ./msr_words.csv ... 88119&lt;br&gt;
emitting double-array: 100% |###########################################| &lt;br&gt;
./matrix.def is not found. minimum setting is used.&lt;br&gt;
reading ./matrix.def ... 1x1&lt;br&gt;
&lt;br&gt;
done!&lt;/div&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/div&gt;
&lt;p&gt;5、CRF模型参数训练&lt;/p&gt;
&lt;p&gt;在seed目录下执行如下命令，训练CRF模型：&lt;br&gt;
/usr/local/libexec/mecab/mecab-cost-train -c 1.0 corpus model&lt;/p&gt;
&lt;p&gt;可以使用 -d 参数指定使用的词典&lt;br&gt;
/usr/local/libexec/mecab/mecab-cost-train -d \$WORK/seed -c 1.0 \$WORK/seed/corpus \$WORK/seed/model&lt;br&gt;
-d: 包含训练用二进制词典的目录(缺省为当前目录)&lt;br&gt;
-c: CRF的超参数(hyper-parameter)&lt;br&gt;
-f: 特征频率的阈值&lt;br&gt;
-p NUM: 实行NUM个并行训练 (缺省为1)&lt;br&gt;
corpus: 训练数据文件名&lt;br&gt;
model: 输出CRF参数文件名&lt;/p&gt;
&lt;p&gt;这个训练过程需要一会儿时间，最终的输出大致如下：&lt;/p&gt;
&lt;div class=&quot;codecolorer-container text default&quot; style=&quot;overflow:auto;white-space:nowrap;width:620px;height:450px;&quot;&gt;&lt;table cellspacing=&quot;0&quot; cellpadding=&quot;0&quot;&gt;&lt;tbody&gt;&lt;tr&gt;
&lt;td class=&quot;line-numbers&quot;&gt;&lt;div&gt;1&lt;br&gt;2&lt;br&gt;3&lt;br&gt;4&lt;br&gt;5&lt;br&gt;6&lt;br&gt;7&lt;br&gt;8&lt;br&gt;9&lt;br&gt;10&lt;br&gt;11&lt;br&gt;12&lt;br&gt;13&lt;br&gt;14&lt;br&gt;15&lt;br&gt;16&lt;br&gt;17&lt;br&gt;18&lt;br&gt;19&lt;br&gt;20&lt;br&gt;21&lt;br&gt;22&lt;br&gt;23&lt;br&gt;24&lt;br&gt;25&lt;br&gt;26&lt;br&gt;27&lt;br&gt;28&lt;br&gt;29&lt;br&gt;30&lt;br&gt;31&lt;br&gt;32&lt;br&gt;33&lt;br&gt;34&lt;br&gt;35&lt;br&gt;36&lt;br&gt;37&lt;br&gt;
&lt;/div&gt;&lt;/td&gt;
&lt;td&gt;&lt;div class=&quot;text codecolorer&quot;&gt;adding virtual node: 0,0,0,0,0,0&lt;br&gt;
adding virtual node: 0,0,0,0,0,0&lt;br&gt;
adding virtual node: 0,0,0,0,0,0&lt;br&gt;
adding virtual node: 0,0,0,0,0,0&lt;br&gt;
...&lt;br&gt;
Number of sentences: 86918&lt;br&gt;
Number of features:  28&lt;br&gt;
eta:                 0.00005&lt;br&gt;
freq:                1&lt;br&gt;
eval-size:           6&lt;br&gt;
unk-eval-size:       4&lt;br&gt;
threads:             1&lt;br&gt;
charset:             EUC-JP&lt;br&gt;
C(sigma^2):          1.00000&lt;br&gt;
&lt;br&gt;
iter=0 err=0.29595 F=0.94870 target=2267078.26396 diff=1.00000&lt;br&gt;
iter=1 err=0.13623 F=0.97665 target=1056367.13470 diff=0.53404&lt;br&gt;
iter=2 err=0.13849 F=0.97610 target=1005496.50043 diff=0.04816&lt;br&gt;
iter=3 err=0.14630 F=0.97388 target=924449.25300 diff=0.08060&lt;br&gt;
iter=4 err=0.13693 F=0.97643 target=891815.15638 diff=0.03530&lt;br&gt;
iter=5 err=0.13537 F=0.97672 target=869032.52748 diff=0.02555&lt;br&gt;
iter=6 err=0.11850 F=0.98127 target=854787.02218 diff=0.01639&lt;br&gt;
iter=7 err=0.10803 F=0.98411 target=845031.70611 diff=0.01141&lt;br&gt;
iter=8 err=0.08712 F=0.98848 target=838863.46990 diff=0.00730&lt;br&gt;
iter=9 err=0.07940 F=0.99001 target=835481.49751 diff=0.00403&lt;br&gt;
iter=10 err=0.07276 F=0.99114 target=833719.13204 diff=0.00211&lt;br&gt;
iter=11 err=0.06556 F=0.99263 target=833462.32905 diff=0.00031&lt;br&gt;
iter=12 err=0.06569 F=0.99258 target=831886.20533 diff=0.00189&lt;br&gt;
iter=13 err=0.06568 F=0.99259 target=831739.11465 diff=0.00018&lt;br&gt;
iter=14 err=0.06559 F=0.99262 target=831643.59710 diff=0.00011&lt;br&gt;
iter=15 err=0.06531 F=0.99266 target=831599.69205 diff=0.00005&lt;br&gt;
iter=16 err=0.06502 F=0.99274 target=831544.40251 diff=0.00007&lt;br&gt;
iter=17 err=0.06480 F=0.99279 target=831518.14668 diff=0.00003&lt;br&gt;
iter=18 err=0.06475 F=0.99280 target=831504.33361 diff=0.00002&lt;br&gt;
iter=19 err=0.06470 F=0.99281 target=831502.92263 diff=0.00000&lt;br&gt;
&lt;br&gt;
Done! writing model file ...&lt;/div&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/div&gt;
&lt;p&gt;6、生成用于发布的词典&lt;/p&gt;
&lt;p&gt;在seed目录下执行：&lt;/p&gt;
&lt;p&gt;/usr/local/libexec/mecab/mecab-dict-gen -o ../final -m model&lt;/p&gt;
&lt;div class=&quot;codecolorer-container text default&quot; style=&quot;overflow:auto;white-space:nowrap;width:620px;&quot;&gt;&lt;table cellspacing=&quot;0&quot; cellpadding=&quot;0&quot;&gt;&lt;tbody&gt;&lt;tr&gt;
&lt;td class=&quot;line-numbers&quot;&gt;&lt;div&gt;1&lt;br&gt;2&lt;br&gt;3&lt;br&gt;4&lt;br&gt;5&lt;br&gt;6&lt;br&gt;7&lt;br&gt;8&lt;br&gt;9&lt;br&gt;10&lt;br&gt;11&lt;br&gt;12&lt;br&gt;13&lt;br&gt;14&lt;br&gt;
&lt;/div&gt;&lt;/td&gt;
&lt;td&gt;&lt;div class=&quot;text codecolorer&quot;&gt;model is not a binary model. reopen it as text mode...&lt;br&gt;
reading ./unk.def ... 3&lt;br&gt;
reading ./msr_words.csv ... 88119&lt;br&gt;
emitting ../final/left-id.def/ ../final/right-id.def&lt;br&gt;
emitting ../final/unk.def ... 3&lt;br&gt;
emitting ../final/msr_words.csv ... 88119&lt;br&gt;
emitting matrix      : 100% |###########################################| &lt;br&gt;
emitting matrix      : 133% |###########################################        copying ./char.def to ../final/char.def&lt;br&gt;
copying ./rewrite.def to ../final/rewrite.def&lt;br&gt;
copying ./dicrc to ../final/dicrc&lt;br&gt;
copying ./feature.def to ../final/feature.def&lt;br&gt;
copying model to ../final/model.def&lt;br&gt;
&lt;br&gt;
done!&lt;/div&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/div&gt;
&lt;p&gt;也可以使用 -d -o 选项指定词典&lt;br&gt;
/usr/local/libexec/mecab/mecab-dict-gen -o \$WORK/final -d \$WORK/seed -m \$WORK/seed/model&lt;br&gt;
-d: 包含seed 词典和配置文件的目录(缺省为当前目录)&lt;br&gt;
-o: 用于发布的词典的输出目录&lt;br&gt;
-m: CRF 的参数文件&lt;br&gt;
用于发布的词典， 必须输出到和 seed 词典不同的目录，通常把包含发布词典的 final 目录打包之后用于发布 .&lt;/p&gt;
&lt;p&gt;7、生成用于解析器的词典&lt;br&gt;
进入到final目录下&lt;br&gt;
cd ../final&lt;br&gt;
执行&lt;br&gt;
/usr/local/libexec/mecab/mecab-dict-index&lt;/p&gt;
&lt;div class=&quot;codecolorer-container text default&quot; style=&quot;overflow:auto;white-space:nowrap;width:620px;&quot;&gt;&lt;table cellspacing=&quot;0&quot; cellpadding=&quot;0&quot;&gt;&lt;tbody&gt;&lt;tr&gt;
&lt;td class=&quot;line-numbers&quot;&gt;&lt;div&gt;1&lt;br&gt;2&lt;br&gt;3&lt;br&gt;4&lt;br&gt;5&lt;br&gt;6&lt;br&gt;7&lt;br&gt;8&lt;br&gt;9&lt;br&gt;
&lt;/div&gt;&lt;/td&gt;
&lt;td&gt;&lt;div class=&quot;text codecolorer&quot;&gt;reading ./unk.def ... 3&lt;br&gt;
emitting double-array: 100% |###########################################| &lt;br&gt;
./pos-id.def is not found. minimum setting is used&lt;br&gt;
reading ./msr_words.csv ... 88119&lt;br&gt;
emitting double-array: 100% |###########################################| &lt;br&gt;
reading ./matrix.def ... 3x3&lt;br&gt;
emitting matrix      : 100% |###########################################| &lt;br&gt;
&lt;br&gt;
done!&lt;/div&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/div&gt;
&lt;p&gt;也可以使用 -d -o 选项指定词典&lt;br&gt;
usr/local/libexec/mecab/mecab-dict-index -d \$WORK/final -o \$WORK/final&lt;br&gt;
-d: 包含seed 词典和配置文件的目录(缺省为当前目录)&lt;br&gt;
-o: 用于解析器的二进制词典的输出目录(缺省为当前目录)&lt;/p&gt;
&lt;p&gt;至此，MeCab中文分词所需的词典和模型文件准备就绪，都在final目录下，可以测试一下，回到上一层目录（cd ..)，然后执行&lt;/p&gt;
&lt;p&gt;    mecab -d ./final/&lt;/p&gt;
&lt;p&gt;让MeCab加载中文分词所需的相关词典文件，待加载完毕，输入一行中文句子：&lt;/p&gt;
&lt;p&gt;    扬帆远东做与中国合作的先行&lt;/p&gt;
&lt;p&gt;回车后我们得到逐行的分词结果，和输入的训练文件格式相似：&lt;/p&gt;
&lt;p&gt;扬帆	0,0,0&lt;br&gt;
远东	0,0,0&lt;br&gt;
做	0,0,0&lt;br&gt;
与	0,0,0&lt;br&gt;
中国	0,0,0&lt;br&gt;
合作	0,0,0&lt;br&gt;
的	0,0,0&lt;br&gt;
先行	0,0,0&lt;br&gt;
EOS&lt;/p&gt;
&lt;p&gt;如果想得到NBest的输出结果，可以这样执行：&lt;/p&gt;
&lt;p&gt;mecab -d ./final/ -N2&lt;br&gt;
扬帆远东做与中国合作的先行&lt;br&gt;
扬帆	0,0,0&lt;br&gt;
远东	0,0,0&lt;br&gt;
做	0,0,0&lt;br&gt;
与	0,0,0&lt;br&gt;
中国	0,0,0&lt;br&gt;
合作	0,0,0&lt;br&gt;
的	0,0,0&lt;br&gt;
先行	0,0,0&lt;br&gt;
EOS&lt;br&gt;
扬帆	0,0,0&lt;br&gt;
远东	0,0,0&lt;br&gt;
做	0,0,0&lt;br&gt;
与	0,0,0&lt;br&gt;
中	0,0,0&lt;br&gt;
国	0,0,0&lt;br&gt;
合作	0,0,0&lt;br&gt;
的	0,0,0&lt;br&gt;
先行	0,0,0&lt;br&gt;
EOS&lt;/p&gt;
&lt;p&gt;如果想得到单行的分词结果，可以这样执行：&lt;/p&gt;
&lt;p&gt;mecab -d ./final/ -O wakati&lt;br&gt;
扬帆远东做与中国合作的先行&lt;br&gt;
扬帆 远东 做 与 中国 合作 的 先行 &lt;/p&gt;
&lt;p&gt;如果想直接对文件分词，可以这样执行：&lt;/p&gt;
&lt;p&gt;mecab -d ./final/ INPUT -o OUTPUT&lt;/p&gt;
&lt;p&gt;基于以上信息，我们可以利益backoff2005的评估脚本来评估本次分词的结果，首先利益mecab对msr的测试文件进行中文分词：&lt;/p&gt;
&lt;p&gt;mecab -d ./final/ -O wakati ../icwb2-data/testing/msr_test.utf8 -o msr_test.mecab&lt;/p&gt;
&lt;p&gt;然后用测试脚本来评估MeCab的中文分词结果文件msr_test.mecab的准确率和召回率，执行：&lt;/p&gt;
&lt;p&gt;../icwb2-data/scripts/score ../icwb2-data/gold/msr_training_words.utf8 ../icwb2-data/gold/msr_test_gold.utf8 msr_test.mecab &amp;gt; msr_test.mecab.score&lt;/p&gt;
&lt;p&gt;最终的结果如下：&lt;/p&gt;
&lt;div class=&quot;codecolorer-container text default&quot; style=&quot;overflow:auto;white-space:nowrap;width:620px;&quot;&gt;&lt;table cellspacing=&quot;0&quot; cellpadding=&quot;0&quot;&gt;&lt;tbody&gt;&lt;tr&gt;
&lt;td class=&quot;line-numbers&quot;&gt;&lt;div&gt;1&lt;br&gt;2&lt;br&gt;3&lt;br&gt;4&lt;br&gt;5&lt;br&gt;6&lt;br&gt;7&lt;br&gt;8&lt;br&gt;9&lt;br&gt;10&lt;br&gt;11&lt;br&gt;12&lt;br&gt;13&lt;br&gt;14&lt;br&gt;
&lt;/div&gt;&lt;/td&gt;
&lt;td&gt;&lt;div class=&quot;text codecolorer&quot;&gt;=== SUMMARY:&lt;br&gt;
=== TOTAL INSERTIONS:   4984&lt;br&gt;
=== TOTAL DELETIONS:    474&lt;br&gt;
=== TOTAL SUBSTITUTIONS:    4828&lt;br&gt;
=== TOTAL NCHANGE:  10286&lt;br&gt;
=== TOTAL TRUE WORD COUNT:  106873&lt;br&gt;
=== TOTAL TEST WORD COUNT:  111383&lt;br&gt;
=== TOTAL TRUE WORDS RECALL:    0.950&lt;br&gt;
=== TOTAL TEST WORDS PRECISION: 0.912&lt;br&gt;
=== F MEASURE:  0.931&lt;br&gt;
=== OOV Rate:   0.026&lt;br&gt;
=== OOV Recall Rate:    0.000&lt;br&gt;
=== IV Recall Rate: 0.976&lt;br&gt;
### msr_test.mecab  4984    474 4828    10286   106873  111383  0.950   0.912   0.931   0.026   0.000   0.976&lt;/div&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/div&gt;
&lt;p&gt;我们得到一个准确率91.2%，召回率95%，F值为93.1%的中文分词器。当然，这只是一个初步测试，还有许多工作要做，譬如添加词典，添加语料，设计特征模板等等。&lt;/p&gt;
&lt;p&gt;我们再说一下如何在Python中调用MeCab进行中文分词，其他各种语言的绑定的安装方法参见 perl/README, ruby/README, python/README, java/README），写得很清楚。首先下载mecab-python-0.996.tar.gz，一路解压安装，安装成功后打开python解释器，这里使用ipython:&lt;/p&gt;
&lt;div class=&quot;codecolorer-container text default&quot; style=&quot;overflow:auto;white-space:nowrap;width:620px;&quot;&gt;&lt;table cellspacing=&quot;0&quot; cellpadding=&quot;0&quot;&gt;&lt;tbody&gt;&lt;tr&gt;
&lt;td class=&quot;line-numbers&quot;&gt;&lt;div&gt;1&lt;br&gt;2&lt;br&gt;3&lt;br&gt;4&lt;br&gt;5&lt;br&gt;6&lt;br&gt;7&lt;br&gt;8&lt;br&gt;9&lt;br&gt;10&lt;br&gt;11&lt;br&gt;
&lt;/div&gt;&lt;/td&gt;
&lt;td&gt;&lt;div class=&quot;text codecolorer&quot;&gt;In [1]: import sys&lt;br&gt;
&lt;br&gt;
In [2]: import MeCab&lt;br&gt;
&lt;br&gt;
In [3]: m = MeCab.Tagger(&quot;-d ./final/ -O wakati&quot;)&lt;br&gt;
&lt;br&gt;
In [4]: m.parse(&quot;扬帆远东做与中国合作的先行&quot;)&lt;br&gt;
Out[4]: &#39;\xe6\x89\xac\xe5\xb8\x86 \xe8\xbf\x9c\xe4\xb8\x9c \xe5\x81\x9a \xe4\xb8\x8e \xe4\xb8\xad\xe5\x9b\xbd \xe5\x90\x88\xe4\xbd\x9c \xe7\x9a\x84 \xe5\x85\x88\xe8\xa1\x8c \n&#39;&lt;br&gt;
&lt;br&gt;
In [5]: print m.parse(&quot;扬帆远东做与中国合作的先行&quot;)&lt;br&gt;
扬帆 远东 做 与 中国 合作 的 先行&lt;/div&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/div&gt;
&lt;p&gt;最后再说一下之所以说实用，并不是说马上给出一个实用的中文分词系统，而是在这个通用的分词框架下，我们可以做很多有趣的事情和测试，最终定制属于我们自己的实用的中文分词和词性标注系统，也欢迎大家一起来探索。&lt;/p&gt;
&lt;p&gt;注：原创文章，转载请注明出处“&lt;a href=&quot;http://www.52nlp.cn&quot;&gt;我爱自然语言处理&lt;/a&gt;”：&lt;a href=&quot;http://www.52nlp.cn&quot;&gt;www.52nlp.cn&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;本文链接地址：&lt;a href=&quot;http://www.52nlp.cn/?p=6932&quot;&gt;http://www.52nlp.cn/用mecab打造一套实用的中文分词系统&lt;/a&gt;&lt;/p&gt;

											

</description>
        <pubDate>Wed, 21 Jan 2015 00:00:00 +0800</pubDate>
        <link>http://iftti.com/posts/2015/2015-01-21-%25e7%2594%25a8mecab%25e6%2589%2593%25e9%2580%25a0%25e4%25b8%2580%25e5%25a5%2597%25e5%25ae%259e%25e7%2594%25a8%25e7%259a%2584%25e4%25b8%25ad%25e6%2596%2587%25e5%2588%2586%25e8%25af%258d%25e7%25b3%25bb%25e7%25bb%259f-9ed734afb.html</link>
        <guid isPermaLink="true">http://iftti.com/posts/2015/2015-01-21-%25e7%2594%25a8mecab%25e6%2589%2593%25e9%2580%25a0%25e4%25b8%2580%25e5%25a5%2597%25e5%25ae%259e%25e7%2594%25a8%25e7%259a%2584%25e4%25b8%25ad%25e6%2596%2587%25e5%2588%2586%25e8%25af%258d%25e7%25b3%25bb%25e7%25bb%259f-9ed734afb.html</guid>
        
        
        <category>52nlp</category>
        
      </item>
    
      <item>
        <title>Working at Netflix</title>
        <description>

&lt;p&gt;I&#39;ve been at Netflix now for several months, and have found it to be an amazing place to work. What has surprised me most is the culture, how different it is to other companies, and how well it works.&lt;/p&gt;

&lt;p&gt;In this post I&#39;ll describe my experience at Netflix: starting with recruiting, the culture, my work as a performance architect, and finally our mission. I&#39;m excited by what we are doing as a company, and I hope this can (or can continue to) inspire positive changes across our industry.&lt;/p&gt;

&lt;p&gt;I hope to also address the questions I&#39;m frequently asked nowadays, including: “Is the culture deck true?&quot;, and “How is performance work at Netflix?&quot;.&lt;/p&gt;

&lt;p&gt;No one at Netflix asked me to write this, and these are my own opinions. &lt;/p&gt;

&lt;h2&gt;Recruiting&lt;/h2&gt;

&lt;p&gt;My first direct experience with Netflix was the hiring process. Netflix recruiting is outstanding. I was scheduled quickly and with the right people: my would-be manager, co-workers, and higher management. The focus was on finding out if we were a good fit for each other.&lt;/p&gt;

&lt;p&gt;I had had quite different experiences interviewing with some other companies. One major tech company was not just initially slow to interview, but focused the interviews on topics unrelated to my expected role. I was told that the hiring process had known issues, which couldn&#39;t be fixed. Really? If hiring is broken, then what else is broken? And who would my co-workers be, who passed an unrelated interview? This was my first exposure to the company, and it showed a culture of &quot;stupid things happen, we know they’re stupid, and they can&#39;t be fixed&quot;. (I don&#39;t even think the recruiters were to blame; this was a company problem.)&lt;/p&gt;

&lt;p&gt;The question of compensation was also handled differently at Netflix. In many hiring negotiations, both parties try to bluff their way around this topic. With Netflix, I was encouraged to find out my market worth and discuss it with them, so that we could both agree on what constituted a good salary. They also did their own research by collecting data from candidates and other companies throughout the hiring process, to ensure the offer was top of market. I think some other companies are terrified of staff discovering what they are really worth! But this openness and honesty was characteristic of the Netflix recruiting process.&lt;/p&gt;

&lt;p&gt;As part of the recruiting process, I was encouraged to study the &lt;a href=&quot;http://www.slideshare.net/reed2001/culture-1798664&quot;&gt;culture deck&lt;/a&gt;. I did, and found it attractive.&lt;/p&gt;

&lt;h2&gt;Culture&lt;/h2&gt;

&lt;p&gt;Many companies talk about about how great their culture is, but this is more aspirational than reality. After joining the company, the real culture is learned by word of mouth, or trial and error. However, with Netflix, the culture deck is true. I think its emphasis in the recruitment stage reinforces this, as everyone learns how to act before they walk in the door.&lt;/p&gt;

&lt;p&gt;One of the key principles in the culture is &quot;freedom and responsibility&quot;: you have the freedom to do the right thing – provided you take responsibility. Management’s role is to provide context and help, not be an obstacle. This is awesome: I have a long list of projects I want to do that will help Netflix, and I&#39;m in an environment that helps me do it.&lt;/p&gt;

&lt;p&gt;But how does this work? It&#39;s not freedom gone wild. You could introduce a new technology, for example, provided you plan who will service and maintain it, how it&#39;s debugged when it fails, how fault tolerance works, etc. Freedom and responsibility.&lt;/p&gt;

&lt;p&gt;This works because Netflix hires professionals: people who have good judgement to start with and who use freedom wisely. People with self-discipline. This doesn&#39;t mean we&#39;re always right: freedom and responsibility includes having the courage and curiosity to try risky projects, when it makes sense to do so, as these can lead to important innovations. The focus is always on making a positive impact for the company.&lt;/p&gt;

&lt;p&gt;Apart from professionalism, Netflix also aims to hire high performers. People who are self-driven, highly productive, and who also work well with others. This means no &quot;brilliant jerks&quot;, who are explicitly not welcome (this has been described as a polite version of the &quot;no asshole rule&quot; [&lt;a href=&quot;http://bobsutton.typepad.com/my_weblog/2009/09/netflix-culture-an-amazing-slideshow.html&quot;&gt;1&lt;/a&gt;]).&lt;/p&gt;

&lt;p&gt;There&#39;s much more in the culture deck. I may be able to help explain it by describing what I think Netflix is &lt;strong&gt;not&lt;/strong&gt;: &lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;It&#39;s not the kind of company where known stupid things happen that can&#39;t be fixed (like the flawed recruiting process, or the antics parodied by Dilbert). &lt;/li&gt;
&lt;li&gt;It&#39;s not the kind of company where managers or departments hate each other, and use their roles to conduct political warfare.&lt;/li&gt;
&lt;li&gt;It&#39;s not the kind of company where bad mistakes are frequently made, and no one is held accountable. &lt;/li&gt;
&lt;li&gt;It&#39;s not the kind of company where all effort is on fire-fighting, and little on fire-proofing.&lt;/li&gt;
&lt;li&gt;It&#39;s not the kind of company that locks its engineers in the basement, for fear of them being poached.&lt;/li&gt;
&lt;li&gt;It&#39;s not a company where an unhealthy work/life balance is either encouraged or necessary.&lt;/li&gt;
&lt;li&gt;It&#39;s not a company suffering Not Invented Here syndrome, No Bad News syndrome, or groupthink.&lt;/li&gt;
&lt;li&gt;And it&#39;s not a company where waste or insanity run rampant. &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;It&#39;s like... a company run by adults instead of children! (Adultlike behavior has even been described previously as a tenet of hiring: &quot;hire, reward, and tolerate only fully formed adults&quot; [&lt;a href=&quot;https://hbr.org/2014/01/how-netflix-reinvented-hr/ar/1&quot;&gt;2&lt;/a&gt;].)&lt;/p&gt;

&lt;p&gt;While the Netflix culture attracted me, it doesn&#39;t suit everyone, as described on &lt;a href=&quot;http://www.slideshare.net/reed2001/culture-1798664/38&quot;&gt;slide 38&lt;/a&gt;. And that&#39;s ok. Part of being open (and honest) about our culture is that it helps people self-select.&lt;/p&gt;

&lt;h2&gt;Performance Engineering&lt;/h2&gt;

&lt;p&gt;There are a few companies doing important work in performance, and Netflix is one of them. With over 50 million subscribers, the largest cloud environment, and handling over a third of the US Internet traffic at night, there are numerous opportunities for performance engineering. This involves not just applying existing practices, but developing the state of the art.&lt;/p&gt;

&lt;p&gt;I&#39;m working on many technologies: AWS, Linux, FreeBSD, Java, Node.js, Perl, Python, Cassandra, Nginx, ftrace, perf_events, and eBPF to name a few. There are many technologies we&#39;ve developed ourselves, which are typically open sourced, including the rxNetty reactive framework, Atlas for performance monitoring, and (coming up) Vector for instance analysis. There&#39;s also hardware performance work (for the FreeBSD appliances) and capacity planning.&lt;/p&gt;

&lt;p&gt;My day-to-day work includes resolving immediate issues of poor performance, and short- and long-term projects. We&#39;re using Linux on our cloud, where advanced performance analysis tools have historically been lacking. I have the freedom to figure out what best to do, which has included developing ftrace and perf_events performance analysis tools, for short-term wins (&lt;a href=&quot;https://github.com/brendangregg/perf-tools&quot;&gt;perf-tools&lt;/a&gt;); Java hotspot hacking, for short- and long-term wins; and eBPF testing, for long-term wins. I&#39;ll be putting some time into the other tracers (SystemTap, ktap, LTTng), to find wins from them, too.&lt;/p&gt;

&lt;p&gt;The FreeBSD Open Connect Appliances, our CDN which streams the actual content, are amazing to work on. FreeBSD provides the most advanced performance analysis environment, including many standard tools as well as pmcstat and DTrace (I summarized these in my MeetBSDCA 2014 talk: &lt;a href=&quot;http://www.slideshare.net/brendangregg/meetbsd2014-performance-analysis&quot;&gt;slides&lt;/a&gt;, &lt;a href=&quot;https://www.youtube.com/watch?v=uvKMptfXtdo&quot;&gt;video&lt;/a&gt;). I was delighted to recently develop &lt;a href=&quot;http://www.brendangregg.com/blog/2014-10-31/cpi-flame-graphs.html&quot;&gt;CPI flame graphs&lt;/a&gt; on the OCAs, using pmcstat.&lt;/p&gt;

&lt;p&gt;Other specific work I&#39;ve been posting here on my blog. In particular, &lt;a href=&quot;/blog/2014-09-27/from-clouds-to-roots.html&quot;&gt;From Clouds to Roots&lt;/a&gt; shows the full performance analysis process for our cloud, from cloud-wide to instance-level tools. I&#39;ve found analysis of Linux instances to be easier than I’d feared, thanks to ftrace and perf_events being built into the kernel. The biggest challenge has been determining low-level CPU behavior in the cloud, without current access to CPU performance counters. However, I&#39;ve made some progress using &lt;a href=&quot;http://www.brendangregg.com/blog/2014-09-15/the-msrs-of-ec2.html&quot;&gt;MSRs instead&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I&#39;m part of a great team, Performance and Reliability Engineering. I&#39;m not just applying my skills, but learning more from other colleagues, and developing those skills in our environment. We&#39;re also expanding our team, and hiring &lt;a href=&quot;http://jobs.netflix.com/jobs.php?id=NFX01535&quot;&gt;performance&lt;/a&gt; and &lt;a href=&quot;http://jobs.netflix.com/jobs.php?id=NFX01887&quot;&gt;systems&lt;/a&gt; engineering roles (if interested, contact &lt;a href=&quot;https://twitter.com/coburnw&quot;&gt;Coburn&lt;/a&gt;, our manager).&lt;/p&gt;

&lt;h2&gt;Mission&lt;/h2&gt;

&lt;p&gt;Netflix’s mission is to change how entertainment is consumed worldwide, by building a good product that people choose to buy. It&#39;s exciting: we are pioneering the modern age of entertainment, and taking on all the technical and political challenges that this involves.&lt;/p&gt;

&lt;p&gt;We aren&#39;t building a technical facade, where the real mission is sell the company. We aren&#39;t winning by unsavory sales, legal, or marketing tactics. And our customers are not unwittingly themselves the product (we don&#39;t read their private emails). We want to win by making a product so good that people choose to buy it. We&#39;re an honest company.&lt;/p&gt;

&lt;h2&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;When I joined Netflix, I expected many aspects to be excellent: the technical challenges, my colleagues, the compensation, the mission, and the ambition of the company – and they are. What surprised me most by its excellence was the culture. It&#39;s made me think differently about our industry: Netflix is proving that company culture doesn&#39;t just have to be accepted: it can be engineered to be positive.&lt;/p&gt;

&lt;p&gt;In this post I described my opinions of Netflix after ten months at the company. I&#39;m motivated to write because I think the topic of positive company cultures is worth discussing. For more about the Netflix culture, apart from the culture deck, you can read &lt;a href=&quot;https://hbr.org/2014/01/how-netflix-reinvented-hr/ar/1&quot;&gt;how Netflix reinvented HR&lt;/a&gt;, &lt;a href=&quot;http://blog.slideshare.net/2014/06/11/inside-the-netflix-culture-deck/&quot;&gt;behind the slides&lt;/a&gt;, and &lt;a href=&quot;http://firstround.com/article/The-woman-behind-the-Netflix-Culture-doc&quot;&gt;the woman behind the Netflix culture doc&lt;/a&gt;. Maybe you can experience this culture directly (we are hiring), or perhaps more of our industry can adopt or engineer their own positive cultures.&lt;/p&gt;


</description>
        <pubDate>Tue, 20 Jan 2015 00:00:00 +0800</pubDate>
        <link>http://iftti.com/posts/2015/2015-01-20-working-at-netflix.html-88c8793b0.html</link>
        <guid isPermaLink="true">http://iftti.com/posts/2015/2015-01-20-working-at-netflix.html-88c8793b0.html</guid>
        
        
        <category>brendangregg</category>
        
      </item>
    
      <item>
        <title>我是如何完成《iOS开发进阶》的编写的？</title>
        <description>
&lt;h2&gt;前言&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/tangqiaoboy/iOS-Pro&quot;&gt;《iOS 开发进阶》&lt;/a&gt; 终于出版了，在架构师峰会的签售会上，首批 100 本图书在签售的两天很快就卖光了，前几天首先在&lt;a href=&quot;http://product.china-pub.com/3770871&quot;&gt;互动出版社&lt;/a&gt;上架的首批图书也在一天内卖缺货了，能得到读者这么热烈地支持，我还是很惊喜。&lt;a href=&quot;http://segmentfault.com/blog/devlevelup&quot;&gt;技术人攻略&lt;/a&gt; 的创始人张兰说：“你应该写一篇总结”。我自己也想好好的整理一下这个出版的过程，给大家分享一下此书的写作过程。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/devtang.com/efa33762d11a3e02ff60c7183d53fe95.jpg&quot;&gt;&lt;/p&gt;

&lt;h2&gt;选择出版社&lt;/h2&gt;

&lt;p&gt;去年年底，电子工业出版社博文视点的张春雨老师在微博上私信我，问我愿不愿意写一本 iOS 方面的技术书籍。其实在这之前已经有将近 5 个出版社的老师联系过我，但是我都因为创业太忙没有答应。后来有一次差点答应过的一个出版社老师，但因为她坚持要我提供 Word 版本的书稿以及一些沟通上的问题，我最终在签合同前终止了合作。&lt;/p&gt;

&lt;p&gt;张老师是足够开明大度的，在写作时间和内容上给了我足够的自由度，前期几乎没有干涉过文稿内容和写作进度（不过后来发现其实是他比较忙，没空管我）。后期也仅仅是对文章表达可能有歧义的地方提建议。这种散养式的风格非常适合我，因为我从来就是一个比较自律，以及喜欢按自己节奏做事的人。&lt;/p&gt;

&lt;h2&gt;写作格式和工具&lt;/h2&gt;

&lt;p&gt;我之前在某个出版社老师的要求下，曾经也尝试在 Mac 下用 Word 写作。但由于 Mac 下 Word 的糟糕体验以及对代码糟糕的排版支持，那完全就是一段不堪回首的痛苦过程。&lt;/p&gt;

&lt;p&gt;而张春雨老师鼓励我用任何喜欢的工具写作，这一点是我同意合作的很大原因。我的博客都是用 markdown 语法来完成的，所以我一开始就打算用 markdown 来写作。&lt;/p&gt;

&lt;p&gt;在软件上，我一开始选择 Mou 来写作，但是发现 Mou 对于篇幅超过 1 万字的文章，预览渲染会非常卡，CPU 长期达到 100% 占用。我的最新款 Macbook Air 的风扇呼呼狂响，让我不得不终止了 Mou 的进程。&lt;/p&gt;

&lt;p&gt;之后有一段时间我就用 Evernote 或 Sublime text 来写作，它们麻烦之处就是 Evernote 写完之后还需要把图片转成 markdown 的链接。而
Sublime text 的问题就是无法方便地预览最后效果。&lt;/p&gt;

&lt;p&gt;这一切的不便在我发现 &lt;a href=&quot;https://www.gitbook.com/&quot;&gt;gitbook&lt;/a&gt; 后被改变。&lt;a href=&quot;https://www.gitbook.com/&quot;&gt;gitbook&lt;/a&gt; 是一个专业的为 markdown 格式写作者提供的图书编写工具。它甚至集成了在线销售相关的功能。它也支持将书稿导出成 PDF、HTML 或 ePub 格式。&lt;/p&gt;

&lt;p&gt;以下是 gitbook 的编辑界面（最左边是目录，中间是 markdown 源码，最右边是预览）：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/devtang.com/748004371c2bd3c5de79d8633ff70eb7.jpg&quot;&gt;&lt;/p&gt;

&lt;p&gt;我用 &lt;a href=&quot;https://www.gitbook.com/&quot;&gt;gitbook&lt;/a&gt; 完成了此书后半部分的编写。gitbook 有时候会出现两个中文字符在显示时重叠的问题，不过整体体验比 Mou 和 Evernote 要好多了。唯一的一次事故是 gitbook 对中文支持不太好，如果章节名不带因为的话，gitbook 会错误的地将章节覆盖到父级的图书目录，造成其他章节的内容被覆盖。我幸运的将书稿内容保存在了 Dropbox 目录下，所以恢复出来了。&lt;/p&gt;

&lt;h2&gt;配图和示例代码&lt;/h2&gt;

&lt;p&gt;对于写作中需要的示意图截图，简单的我就直接用 QQ 截图了，复杂的我使用的是 &lt;a href=&quot;http://www.clarify-it.com/&quot;&gt;Clarify&lt;/a&gt;，它的标记功能比较丰富（不过中文支持比较差）。另外一些类图或架构图，我就直接使用 keynote 来做了。&lt;/p&gt;

&lt;p&gt;考虑到技术书籍通常配图都比较少，所以我这次尝试，那时用比较多的配图来做辅助说明。另外我尽量把图书的涉及代码都放到网上，仅仅留一些关键的代码片段以节省篇幅。&lt;/p&gt;

&lt;h2&gt;如何安排时间和控制进度&lt;/h2&gt;

&lt;p&gt;由于有过写作的经验，所以我预先就估计好了这会是一个一年左右的工作。实际上它确实花了一年时间。&lt;/p&gt;

&lt;p&gt;我希望把这个当做一个自己的 “创业项目” 来做，看自己能否执行好一个较长远的规划。我把图书编写拆分成每一个章节的写作工作，然后希望用 1~2 周完成一个章节。这样的好处是每周都可以看到明确的进展和产出。&lt;/p&gt;

&lt;p&gt;我具体在做的时候是尝试用 Scrum 的方式来安排自己周末的时间。简单来说就是把书稿的内容进行拆分，然后对于细分到每一个章节的内容进行估分。接着我的每一个周末的目标就是把当周安排的章节写作完成。&lt;/p&gt;

&lt;p&gt;当然，人都是需要休息的。所以周末两天我通常不会安排满。我会安排一整天用来写作，而另外一天做小量的修改和休息。如果有加班，就暂停写作。&lt;/p&gt;

&lt;p&gt;虽然我没有跑过马拉松，但我感觉写作的体验和跑马拉松类似。前期的起跑和最后的冲刺都比较激动，但是中途长达一年的写作过程却是孤独，枯燥和寂寞的。这个时候，需要自己的坚持和自我鼓励。&lt;/p&gt;

&lt;h2&gt;关于电子版&lt;/h2&gt;

&lt;p&gt;该书我专门保留了电子版的版权，我本来希望把书放在多看上销售，主要是因为我个人就是多看的重度用户，而且我知道多看对电子书有加密保护。但是后来联系多看阅读时才发现他们已经不和个人作者合作了，他们建议我将版权授权给出版社。这让我感觉非常失望。多看阅读这种作法使得它无法颠覆传统出版社的渠道，另外如果竞争对手允许个人作者的话，多看阅读还可能失去不少优秀的图书的发行机会，进而影响它的电子书平台的地位。&lt;/p&gt;

&lt;p&gt;在网上抱怨了此事以后，有一位朋友说可以协助我在豆瓣阅读发布此书。所以顺利的话，稍后会有豆瓣阅读的电子书版本销售。如果豆瓣阅读的出版进行得顺利，我也会将电子版发布在百度阅读、淘宝阅读等其它电子书平台上。&lt;/p&gt;

&lt;h2&gt;关于作品收入&lt;/h2&gt;

&lt;p&gt;技术书籍的写作通常都是不怎么挣钱的。那拿我的书来说，估计版税一共会有2万块钱。而写作此书大概花掉了我将近一年的周末时间。如果是要专门为了挣钱的话，可以有非常多其它的性价比更高的，更轻松的方式。&lt;/p&gt;

&lt;p&gt;但是，如果可以重来，我还是会选择写这本书。传递知识的乐趣和成就感还是比挣钱更有意思一些。也希望有更多的一线开发者投入到写作分享的行列中。&lt;/p&gt;

&lt;h2&gt;一些小结&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;推荐用 markdown 格式配合 &lt;a href=&quot;https://www.gitbook.com/&quot;&gt;gitbook&lt;/a&gt; 写作。&lt;/li&gt;
&lt;li&gt;推荐用 Scrum 的方式安排进度。&lt;/li&gt;
&lt;li&gt;推荐用 &lt;a href=&quot;http://www.clarify-it.com/&quot;&gt;Clarify&lt;/a&gt; 作配图标记。&lt;/li&gt;
&lt;li&gt;书稿放在 &lt;a href=&quot;https://www.dropbox.com/&quot;&gt;Dropbox&lt;/a&gt; 下自动同步会非常安全。当然，你需要学会科学上网。&lt;/li&gt;
&lt;li&gt;如果想在 Mac 下写作，那还是找一个不需要 Word 方式审稿的出版社吧，与我合作的电子工业出版社就是这样。博文视点的张春雨老师和刘芸老师(微信号:ly15201392806)都非常不错。&lt;/li&gt;
&lt;li&gt;从交稿到出版至少也有 1 个月时间，做好心理准备，前期不要宣传太猛，我这回从签售会到网上能够买到，将近过了一个月。我由于不懂事，签售时宣传得太狠，结果弄得跟饥饿营销似的。&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;写作者的未来&lt;/h2&gt;

&lt;p&gt;未来互联网的发展会使得传播更加容易，所以我相信：好的内容会更容易获得读者，出版社和书商作为渠道的力量会越来越弱。到那个时候，或许写作真正的能成为很多人的职业，并给他们提供不菲的收入。&lt;/p&gt;

&lt;p&gt;到那个时候，我希望我能够成为其中的一员，不但可以随意地选择工作时间和地点，还可以享受传递知识的乐趣和成就感，希望这一天能够早日到来。&lt;/p&gt;

</description>
        <pubDate>Sun, 11 Jan 2015 00:00:00 +0800</pubDate>
        <link>http://iftti.com/posts/2015/2015-01-11-how-to-finish-ios-pro-book-9ed433ee7.html</link>
        <guid isPermaLink="true">http://iftti.com/posts/2015/2015-01-11-how-to-finish-ios-pro-book-9ed433ee7.html</guid>
        
        
        <category>devtang</category>
        
      </item>
    
      <item>
        <title>给 Kibana3 添加脚本化字段支持</title>
        <description>

  
  &lt;div style=&quot;background-color: #FFF;&quot;&gt;
    &lt;p&gt;Kibana4 中确实有不少让人眼前一亮的新特性，但是整体框架和使用思路上的重构实在让人较难上手。所以，把一些有需要的特性，port 回目前更稳定的 Kibana3 就有必要了。好在去年在自己 fork 中已经做了很多铺垫，包括一些基础库的版本更新。这些特性基本都只需要几行代码的变动就可以实现。&lt;/p&gt;
&lt;p&gt;从上次写博客介绍的 uniq histogram 去重统计功能后，这段时间又添加了两个功能。&lt;/p&gt;
&lt;h2 id=&quot;table-&quot;&gt;table 的数据导出&lt;/h2&gt;
&lt;p&gt;kibana3 已经带有 &lt;a href=&quot;https://github.com/eligrey/FileSaver.js&quot;&gt;filesaver.js&lt;/a&gt;，所以加一个 &lt;code&gt;exportAsCsv&lt;/code&gt; 函数即可。要点在于怎么给 table panel 右上角那排小按钮加上一个新图标。&lt;/p&gt;
&lt;p&gt;我之前说过，kibana3 代码划分的很细致，每个 panel 都固定只需要提供 editor.html，module.html，module.js 三个文件即可。panel 本身的框架，是不用关心的。因为这部分代码，在 &lt;code&gt;app/directives/kibanaPanel.js&lt;/code&gt; 中。这次我们想修改 panel 外围的样式，就需要来看这个的代码了。最关键的部分在这里：&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-javascript&quot; data-lang=&quot;javascript&quot;&gt;&lt;span class=&quot;s1&quot;&gt;&#39;&amp;lt;span ng-repeat=&quot;task in panelMeta.modals&quot; class=&quot;row-button extra&quot; ng-show=&quot;task.show&quot;&amp;gt;&#39;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;
              &lt;span class=&quot;s1&quot;&gt;&#39;&amp;lt;span bs-modal=&quot;task.partial&quot; class=&quot;pointer&quot;&amp;gt;&amp;lt;i &#39;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;
                &lt;span class=&quot;s1&quot;&gt;&#39;bs-tooltip=&quot;task.description&quot; ng-class=&quot;task.icon&quot; class=&quot;pointer&quot;&amp;gt;&amp;lt;/i&amp;gt;&amp;lt;/span&amp;gt;&#39;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;
            &lt;span class=&quot;s1&quot;&gt;&#39;&amp;lt;/span&amp;gt;&#39;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;也就是说，它会读取你在 module.js 里定义的 &lt;code&gt;$scope.panelMeta.modals&lt;/code&gt; 数组，然后依次显示。那么就好办了，在我们 table/module.js 里定义下就好了：&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-javascript&quot; data-lang=&quot;javascript&quot;&gt;&lt;span class=&quot;nx&quot;&gt;$scope&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;panelMeta&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
       &lt;span class=&quot;nx&quot;&gt;modals&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;
         &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
          &lt;span class=&quot;nx&quot;&gt;description&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;Export&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
          &lt;span class=&quot;nx&quot;&gt;icon&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;icon-download-alt&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
          &lt;span class=&quot;nx&quot;&gt;partial&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;app/panels/table/export.html&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
          &lt;span class=&quot;nx&quot;&gt;show&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;$scope&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;panel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;exportable&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;为了跟其他的比如 inspector, editor 图标行为一致，这里又新增了一个 &lt;code&gt;$scope.panel.exportable&lt;/code&gt; 变量。而这也带来一个问题：之前已经存在的 dashboard，他们的 schema 里是没有这个变量的，所以即便使用带有这个特性的 kibana 打开老 dashboard，依然看不到导出按钮。这时候，可以手动修改一下 schema 的 JSON 内容，添加上一行 &lt;a href=&quot;https://github.com/chenryn/kibana-authorization/blob/master/src/app/dashboards/logstash.json#L138&quot;&gt;&lt;code&gt;&quot;exportable&quot;: true&lt;/code&gt;&lt;/a&gt;，也可以点击 panel 上的 dup 复制按钮，复制出来的 panel 会读取默认变量设置，就会出现导出按钮了。然后删掉原 panel ，保存 dashboard 即可。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;注意&lt;/strong&gt;：导出的数据只是 table 里的内容，这只是一个 js 功能。不要把它理解成调用 scroll API 获取 Elasticsearch 集群里的全部数据。&lt;/p&gt;
&lt;h2 id=&quot;scriptfield-&quot;&gt;scriptField 聚合&lt;/h2&gt;
&lt;p&gt;Kibana4beta3 的另一个重要特性，是可以预定义一段 script 为 scriptedField，然后在搜索、聚合的时候可以当做普通 field 一样使用这个 scriptedField。示例见官方博客说明(可以直接看&lt;a href=&quot;http://chenlinux.com/2014/12/19/kibana-4-beta-3-now-more-filtery/&quot;&gt;我的翻译&lt;/a&gt;)。至于 script 本身能在 Elasticsearch 里做些什么，之前博客里也写过&lt;a href=&quot;http://chenlinux.com/2014/11/27/elasticsearch-scripts-aggregations/&quot;&gt;两个小示例&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;&lt;em&gt;动态 script 功能在 ES 1.4 之前是因为安全问题被建议关闭的。1.4 开始加入了沙箱功能，才这么大胆的使用。&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;我印象中 script field 应该是不能保存在 mapping 里的，于是稍微看了一下 kibana4 的代码，疑似是另外用一个索引来存储这个信息。&lt;em&gt;不确保是这样，kibana4 的代码比 kibana3 难懂多了。&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;kibana3 整个界面结构跟 kibana4 不一样，没有单独的字段管理页面，而是通过 &lt;code&gt;app/services/fields.js&lt;/code&gt; 提供了 &lt;code&gt;fields.list&lt;/code&gt; 在各个 panel 的 editor.html 里做 &lt;code&gt;bs-typeahead&lt;/code&gt;。所以，如果完整的思路 port 回来，应该是写一个 &lt;em&gt;app/services/scriptFields.js&lt;/em&gt; 来提供 scriptedField 的增删改查，然后还要自己写个页面来提供操作界面。&lt;/p&gt;
&lt;p&gt;作为页面手残党，我迅速决定放弃这个思路，选择一个更简单的方式来完成类似目的：直接在最常用的 terms panel 里提供输入 script 字符串的功能，反正每个 dashboard 最后会固化成 JSON 的。而且其他 panel 应该不太会用到这个功能(如果要在 table 里也实现，改动又稍大了。Kibana4 里我猜测应该是直接返回勾选的 fields，这个接口是支持 script 的；Kibana3 里则是返回全部字段，然后在 js 里完成的表格字段选择性展示)。&lt;/p&gt;
&lt;p&gt;terms panel 中对类似情况就有示例在。这里本是有个 &lt;code&gt;tmode&lt;/code&gt; 参数，用来选择是用 termsFacet 还是 termstatsFacet API。照葫芦画瓢，我新加了一个 &lt;code&gt;fmode&lt;/code&gt; 参数，用来选择是普通字段(“normal”)还是脚本字段(“script”)：&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-html&quot; data-lang=&quot;html&quot;&gt;&lt;span class=&quot;nt&quot;&gt;&amp;lt;div&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;class=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;editor-option&quot;&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;ng-show=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;panel.fmode == &#39;script&#39;&quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&lt;/span&gt;
        &lt;span class=&quot;nt&quot;&gt;&amp;lt;label&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;class=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;small&quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&lt;/span&gt;ScriptField&lt;span class=&quot;nt&quot;&gt;&amp;lt;/label&amp;gt;&lt;/span&gt;
        &lt;span class=&quot;nt&quot;&gt;&amp;lt;input&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;type=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;text&quot;&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;class=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;input-large&quot;&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;ng-model=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;panel.script&quot;&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;ng-change=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;set_refresh(true)&quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&lt;/span&gt;
      &lt;span class=&quot;nt&quot;&gt;&amp;lt;/div&amp;gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;然后在生成 request 的时候，做一下判断：&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-javascript&quot; data-lang=&quot;javascript&quot;&gt;&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;$scope&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;panel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;fmode&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;===&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&#39;script&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
          &lt;span class=&quot;nx&quot;&gt;terms_facet&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;scriptField&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;$scope&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;panel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;script&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;这就 OK 了~&lt;/p&gt;
&lt;p&gt;接下来另一个难点：&lt;strong&gt;terms panel 是支持点击生成 filtering 过滤条件的。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;显然 filtering 里没有 script 的支持。filtering 的功能都出自 &lt;code&gt;app/services/filterSrv.js&lt;/code&gt; 服务。其中 &lt;code&gt;toEjsObj&lt;/code&gt; 方法调用不同的 Elastic.js 的 Filter 方法。在这里面可以看到原本 terms 的是怎么生成的：&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-javascript&quot; data-lang=&quot;javascript&quot;&gt;&lt;span class=&quot;k&quot;&gt;case&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&#39;terms&#39;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;ejs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;TermsFilter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;filter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;field&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;filter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;那么我就添加一个：&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-javascript&quot; data-lang=&quot;javascript&quot;&gt;&lt;span class=&quot;k&quot;&gt;case&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&#39;script&#39;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;ejs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;ScriptFilter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;filter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;script&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;filterSrv 支持搞定。最后一步，就是返回 terms panel 的 module.js 里完成调用。过一遍 click 关键字很容易找到 &lt;code&gt;build_search&lt;/code&gt; 方法。其中原先是这么生成过滤的：&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-javascript&quot; data-lang=&quot;javascript&quot;&gt;&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;isUndefined&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;term&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;meta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
         &lt;span class=&quot;nx&quot;&gt;filterSrv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;set&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;({&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&#39;terms&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;field&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;$scope&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;field&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;term&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;label&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
           &lt;span class=&quot;nx&quot;&gt;mandate&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;negate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;?&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&#39;mustNot&#39;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&#39;must&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)});&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;那么在这个前面判断一下：&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-javascript&quot; data-lang=&quot;javascript&quot;&gt;&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;$scope&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;panel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;fmode&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;===&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&#39;script&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;nx&quot;&gt;filterSrv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;set&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;({&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&#39;script&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;script&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;$scope&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;panel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;script&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&#39; == \&quot;&#39;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;term&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;label&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&#39;\&quot;&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
          &lt;span class=&quot;nx&quot;&gt;mandate&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;negate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;?&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&#39;mustNot&#39;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&#39;must&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)});&lt;/span&gt;
      &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;isUndefined&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;term&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;meta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;大功告成！&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/images/chenlinux.com/2335ac5d2a4c0f757b6d3fa1f3d6b87c.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    &lt;hr&gt;
    
    &lt;hr&gt;
  &lt;!-- JiaThis Button BEGIN --&gt;
&lt;div class=&quot;jiathis_style&quot;&gt;
&lt;span class=&quot;jiathis_txt&quot;&gt;分享到：&lt;/span&gt;
&lt;a class=&quot;jiathis_button_tsina&quot;&gt;新浪微博&lt;/a&gt;
&lt;a class=&quot;jiathis_button_weixin&quot;&gt;微信&lt;/a&gt;
&lt;a class=&quot;jiathis_button_renren&quot;&gt;人人网&lt;/a&gt;
&lt;a class=&quot;jiathis_button_ydnote&quot;&gt;有道云笔记&lt;/a&gt;
&lt;a class=&quot;jiathis_button_gmail&quot;&gt;Gmail邮箱&lt;/a&gt;
&lt;a class=&quot;jiathis_button_twitter&quot;&gt;Twitter&lt;/a&gt;
&lt;a class=&quot;jiathis_button_googleplus&quot;&gt;Google+&lt;/a&gt;
&lt;a class=&quot;jiathis_button_hi&quot;&gt;百度空间&lt;/a&gt;
&lt;a class=&quot;jiathis_button_fb&quot;&gt;Facebook&lt;/a&gt;
&lt;a class=&quot;jiathis_button_douban&quot;&gt;豆瓣&lt;/a&gt;
&lt;a href=&quot;http://www.jiathis.com/share?uid=1589850&quot; class=&quot;jiathis jiathis_txt jiathis_separator jtico jtico_jiathis&quot; target=&quot;_blank&quot;&gt;更多&lt;/a&gt;
&lt;a class=&quot;jiathis_counter_style&quot;&gt;&lt;/a&gt;
&lt;/div&gt;
&lt;script type=&quot;text/javascript&quot;&gt;
var jiathis_config={
	data_track_clickback:true,
	summary:&quot;&quot;,
	ralateuid:{
		&quot;tsina&quot;:&quot;1035836154&quot;
	},
	shortUrl:false,
	hideMore:false
}
&lt;/script&gt;
&lt;script type=&quot;text/javascript&quot; src=&quot;http://v3.jiathis.com/code/jia.js?uid=1589850&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt;
&lt;!-- JiaThis Button END --&gt;
&lt;!-- UY BEGIN --&gt;


&lt;!-- UY END --&gt;
  &lt;/div&gt;

</description>
        <pubDate>Tue, 06 Jan 2015 00:00:00 +0800</pubDate>
        <link>http://iftti.com/posts/2015/2015-01-06-implement-script-field-for-kibana3-633adc088.html</link>
        <guid isPermaLink="true">http://iftti.com/posts/2015/2015-01-06-implement-script-field-for-kibana3-633adc088.html</guid>
        
        
        <category>chenlinux</category>
        
      </item>
    
      <item>
        <title>2014年个人总结</title>
        <description>
&lt;p&gt;2014年对于我来说是重要的一年，因为一方面我刚好年满30岁了，另一方面我的家庭生活和工作都经历了很大的变化。值得总结的包括：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;通过分享，进一步提高了自己的技术影响力&lt;/li&gt;
&lt;li&gt;创业进一步取得成绩&lt;/li&gt;
&lt;/ol&gt;


&lt;h2&gt;技术分享&lt;/h2&gt;

&lt;p&gt;2014年，我的技术分享包括：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;在自己的博客上发布了58篇技术文章。&lt;/li&gt;
&lt;li&gt;在InfoQ网站发表了35期“iOS开发周报”。&lt;/li&gt;
&lt;li&gt;在《程序员》杂志和CSDN网站上发表文章4篇，分别是《从Facebook看移动开发的发展》、《iOS应用安全开发概述》、《WWDC2014，苹果的“软件”发布会》、《那些好用的iOS开发工具》。&lt;/li&gt;
&lt;li&gt;在InfoQ网站和《架构师》迷你书上发表文章4篇，分别是 《作为码农，我们为什么要写作》、《ReactiveCocoa – iOS开发的新框架》、《深入理解Tagged Pointer》、《专访《iOS测试指南》作者羋峮》。&lt;/li&gt;
&lt;li&gt;受朋友邀请，在深圳微信、人人网和豆瓣做了三场技术分享，分享的主题都是：《深入Objective-C对象模型》。&lt;/li&gt;
&lt;li&gt;11月2日在CSDN主办的MDCC移动开发者大会上做了一次分享，主题是：“猿题库的流量优化之路”。&lt;/li&gt;
&lt;li&gt;12月20日在InfoQ主办的ArchSummit北京上做了一次分享，主题是：“猿题库客户端的技术细节”。&lt;/li&gt;
&lt;li&gt;完成了图书&lt;a href=&quot;https://github.com/tangqiaoboy/iOS-Pro&quot;&gt;《iOS开发进阶》&lt;/a&gt;的写作，100本签售版很快卖光，不过正式出版得到2015年1月中旬。&lt;/li&gt;
&lt;li&gt;开源了两个猿题库客户端的iOS基础库：&lt;a href=&quot;https://github.com/yuantiku/YTKKeyValueStore&quot;&gt;YTKKeyValueStore&lt;/a&gt; 和 &lt;a href=&quot;https://github.com/yuantiku/YTKNetwork&quot;&gt;YTKNetwork&lt;/a&gt;，分别得到了400多和700多的star。&lt;/li&gt;
&lt;li&gt;微信公共帐号：iOSDevTips 发表了将近100篇推送，得到了10000多的粉丝。我的微博 &lt;a href=&quot;http://www.weibo.com/tangqiaoboy&quot;&gt;@唐巧_boy&lt;/a&gt; 分享了上百条技术内容，得到了13000多的粉丝。&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;年初的时候我还会怀疑自己的影响力，但现在已经习惯了成为iOS界的“大V”了。微博上分享技术类信息转发常常超过50次，也会常常被人@提醒来请教问题。&lt;/p&gt;

&lt;p&gt;那我是如何树立起自己的技术影响力的呢？这其实主要是通过写博客的方式。我没有想到，从我2010年底开始学iOS开发，到现在短短4年时间，通过博客我能够获得这么大的影响力。现在我也看到越来越多的人加入到技术分享的行列中，用原创的技术博客给整个社区带来知识的分享，同时收获自己的成长和影响力。&lt;/p&gt;

&lt;h2&gt;关于创业&lt;/h2&gt;

&lt;p&gt;我们今年顺利拿到了&lt;a href=&quot;http://tech.sina.com.cn/i/2014-07-22/11209510273.shtml&quot;&gt;C轮1500万美元的融资&lt;/a&gt;，估值达到1.25亿美元。我自己的创业感悟就是觉得决策团队非常牛逼，另外大家的执行力都非常强。我今年除了做日常的iOS开发外，还承担了一些别的事情，包括：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;指导了一位iOS开发零基础的实习生，现在他能够独立的进行开发，非常高兴看着他取得这么大的进步。&lt;/li&gt;
&lt;li&gt;指导了一位实习生完成了Latex渲染引擎在移动端的移植（但其实主要是他的工作出色），这个工作使得我们的客户端在显示公式上和市面上所有同类应用相比具有决定性优势。&lt;/li&gt;
&lt;li&gt;承担了校园招聘的组织工作，大家都被出面试题搞得焦头烂额的，不过最终我们还是搞定了，也收获了不少很有潜力的应届生。&lt;/li&gt;
&lt;li&gt;开始负责小猿搜题这个项目，开始更多地思考产品方面的东西，更多的沟通工作，也开始为更多事情焦虑。&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;负责小猿搜题项目对我是一个新的挑战，之前我作为一个iOS程序员，基本上都只需要专注于技术层面上的实现。而现在我不但需要参与产品的讨论，也需要做更多的组织沟通工作。我们团队仍然保持着强大的执行力，小猿搜题从7月底立项到9月底上线只经历了短短2个月时间。而我们的评测数据显示，我们在搜索质量上毫不逊于竞争对手。但我们需要改进的事情还有很多，希望小猿搜题的用户量和活跃度能够超过猿题库，成为又一个拥有海量初高中生用户的产品。&lt;/p&gt;

&lt;h2&gt;读书&lt;/h2&gt;

&lt;p&gt;今年为了更加深入的掌握Swift的函数式编程特性，学习了Scala语言以及coursea上的Funtional Programming相关的课程，不过仍然没有找到感觉。我感觉可能后面多写一些Swift程序才能有深入的理解。&lt;/p&gt;

&lt;p&gt;今年也读了不少产品的书，包括《我的互联网方法论》、《思考的技术》、《失控》、《定位》等。&lt;/p&gt;

&lt;h2&gt;个人Milestone&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;完成 《iOS开发进阶》的写作&lt;/li&gt;
&lt;li&gt;创业完成C轮融资，开始负责小猿搜题项目&lt;/li&gt;
&lt;li&gt;有了宝宝&lt;/li&gt;
&lt;/ul&gt;


</description>
        <pubDate>Thu, 01 Jan 2015 00:00:00 +0800</pubDate>
        <link>http://iftti.com/posts/2015/2015-01-01-2014-summary-2d5e03e4c.html</link>
        <guid isPermaLink="true">http://iftti.com/posts/2015/2015-01-01-2014-summary-2d5e03e4c.html</guid>
        
        
        <category>devtang</category>
        
      </item>
    
      <item>
        <title>Linux Page Cache Hit Ratio</title>
        <description>

&lt;p&gt;A recent Linux performance regression turned out to be caused by a difference in the page cache hit ratio: what was caching very well on the older system was caching poorly on the newer one. So how do you measure the page cache hit ratio directly?&lt;/p&gt;

&lt;p&gt;How about a tool like this?:&lt;/p&gt;

&lt;pre&gt;
# &lt;b&gt;./cachestat 1&lt;/b&gt;
Counting cache functions... Output every 1 seconds.
    HITS   MISSES  DIRTIES    RATIO   BUFFERS_MB   CACHE_MB
     210      869        0    19.5%            2        209
     444     1413        0    23.9%            8        210
     471     1399        0    25.2%           12        211
     403     1507        3    21.1%           18        211
     967     1853        3    34.3%           24        212
     422     1397        0    23.2%           30        212
[...]
&lt;/pre&gt;

&lt;p&gt;This not only shows the size of the buffer and page cache, but also activity statistics. I&#39;ve added cachestat to my &lt;a href=&quot;https://github.com/brendangregg/perf-tools&quot;&gt;perf-tools&lt;/a&gt; collection on github.&lt;/p&gt;

&lt;h2&gt;Longer Example&lt;/h2&gt;

&lt;p&gt;Here is some sample output followed by the workload that caused it:&lt;/p&gt;

&lt;pre&gt;
# &lt;b&gt;./cachestat -t&lt;/b&gt;
Counting cache functions... Output every 1 seconds.
TIME         HITS   MISSES  DIRTIES    RATIO   BUFFERS_MB   CACHE_MB
08:28:57      415        0        0   100.0%            1        191
08:28:58      411        0        0   100.0%            1        191
08:28:59      362       97        0    78.9%            0          8
08:29:00      411        0        0   100.0%            0          9
08:29:01      775    20489        0     3.6%            0         89
08:29:02      411        0        0   100.0%            0         89
08:29:03     6069        0        0   100.0%            0         89
08:29:04    15249        0        0   100.0%            0         89
08:29:05      411        0        0   100.0%            0         89
08:29:06      411        0        0   100.0%            0         89
08:29:07      411        0        3   100.0%            0         89
[...]
&lt;/pre&gt;

&lt;p&gt;I used the -t option to include the TIME column, to make describing the output easier.&lt;/p&gt;

&lt;p&gt;The workload was:&lt;/p&gt;

&lt;pre&gt;
# echo 1 &amp;gt; /proc/sys/vm/drop_caches; sleep 2; cksum 80m; sleep 2; cksum 80m
&lt;/pre&gt;

&lt;p&gt;At 8:28:58, the page cache was dropped by the first command, which can be seen by the drop in size for &quot;CACHE_MB&quot; (page cache size) from 191 Mbytes to 8.&lt;/p&gt;

&lt;p&gt;After a 2 second sleep, a cksum command was issued at 8:29:01, for an 80 Mbyte file (called &quot;80m&quot;), which caused a total of ~20,400 misses (&quot;MISSES&quot; column), and the page cache size to grow by 80 Mbytes. Each page is 4 Kbytes, so 20k x 4k == 80 Mbytes. The hit ratio during the uncached read dropped to 3.6%.&lt;/p&gt;

&lt;p&gt;Finally, after another 2 second sleep, at 8:29:03 the cksum command was run a second time, this time hitting entirely from cache (the statistics spanning two output rows).&lt;/p&gt;

&lt;h2&gt;How It Works&lt;/h2&gt;

&lt;p&gt;I was curious to see whether ftrace, which is built into the Linux kernel, could measure cache activity, since ftrace function profiling provides efficient in-kernel counts. Systems can have a very high rate of cache activity, so we need to be careful to consider the overhead of any instrumentation.&lt;/p&gt;

&lt;p&gt;While ftrace function profiling is cheap, its capabilities are also limited. It can count kernel function calls by-CPU, and show average latency, but that&#39;s all. (It is the same facility used by funccount from &lt;a href=&quot;https://github.com/brendangregg/perf-tools&quot;&gt;perf-tools&lt;/a&gt;.) I can&#39;t, for example, use it with an advanced filter to match on function arguments or return values. It will only work if I deduce cache activity from kernel function calls alone.&lt;/p&gt;

&lt;p&gt;For the kernels I&#39;m studying (3.2 and 3.13), here are the four kernel functions I&#39;m profiling to measure cache activity:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;mark_page_accessed() for measuring cache accesses&lt;/li&gt;
&lt;li&gt;mark_buffer_dirty() for measuring cache writes&lt;/li&gt;
&lt;li&gt;add_to_page_cache_lru() for measuring page additions&lt;/li&gt;
&lt;li&gt;account_page_dirtied() for measuring page dirties&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;mark_page_accessed() shows total cache accesses, and add_to_page_cache_lru() shows cache insertions (so does add_to_page_cache_locked(), which even includes a tracepoint, but doesn&#39;t fire on later kernels). I thought for a second that these two were sufficient: assuming insertions are misses, I have misses and total accesses, and can calculate hits.&lt;/p&gt;

&lt;p&gt;The problem is that accesses and insertions also happens for writes, dirtying cache data. So the other two kernel functions help tease this apart (remember, I only have function call rates to work with here). mark_buffer_dirty() is used to see which of the accesses were for writes, and account_page_dirtied() to see which of the insertions were for writes.&lt;/p&gt;

&lt;p&gt;It is possible that the kernel functions I&#39;m using have been renamed (or are different logically) for your kernel version, and this script will not work as-is. I was hoping to use fewer than four functions, to make this more maintainable, but I didn&#39;t find a smaller set that worked for the workloads I tested.&lt;/p&gt;

&lt;p&gt;If cachestat starts breaking too much for my kernel versions, I may rewrite it to use SystemTap or perf_events, which allow filtering.&lt;/p&gt;

&lt;h2&gt;Warnings&lt;/h2&gt;

&lt;p&gt;Instrumenting cache activity does cost some overhead, and this tool might slow your target system by 2% or so. Higher if you stress-test the cache. It also uses dynamic tracing of kernel functions, which could cause kernel freezes or panics, depending on your kernel version. Test before use.&lt;/p&gt;

&lt;p&gt;The statistics should also be treated as best-effort. There may be some error margin depending on the frequency of unusual workload types, not properly matched by these four kernel functions. Test with a known workload to get confidence it will work for the intended target.&lt;/p&gt;

&lt;h2&gt;The Problem&lt;/h2&gt;

&lt;p&gt;When I encountered the earlier Linux performance regression, I didn&#39;t have cachestat. We had spotted a high rate of disk I/O, which led me to investigate the cause and work my way back to cache misses. I did this using custom ftrace and perf_events commands, measuring the rate of kernel functions and their call stacks.&lt;/p&gt;

&lt;p&gt;While I got the job done, I wanted a better way for next time, which led to cachestat.&lt;/p&gt;

&lt;h2&gt;Other Techniques&lt;/h2&gt;

&lt;p&gt;I&#39;ve found a few ways people commonly study the page cache hit ratio on Linux:&lt;/p&gt;

&lt;ul&gt;
&lt;p&gt;A) Study the page cache miss rate by using iostat(1) to monitor disk reads, and assume these are cache misses, and not, for example, O_DIRECT. The miss rate is usually a more important metric than the ratio anyway, since misses are proportional to application pain. Also use free(1) to see the cache sizes.&lt;/p&gt;

&lt;p&gt;B) Drop the page cache (echo 1 &amp;gt; /proc/sys/vm/drop_caches), and measure how much performance gets worse! I love the use of a negative experiment, but this is of course a painful way to shed some light on cache usage.&lt;/p&gt;

&lt;p&gt;C) Use sar(1) and study minor and major faults. I don&#39;t think this works (eg, regular I/O).&lt;/p&gt;

&lt;p&gt;D) Use the &lt;a href=&quot;https://sourceware.org/systemtap/wiki/WSCacheHitRate&quot;&gt;cache-hit-rate.stp&lt;/a&gt; SystemTap script, which is number two in an Internet search for Linux page cache hit ratio.  It instruments cache access high in the stack, in the VFS interface, so that reads to any file system or storage device can be seen. Cache misses are measured via their disk I/O. This also misses some workload types (some are mentioned in &quot;Lessons&quot; on that page), and calls ratios &quot;rates&quot;.&lt;/p&gt;
&lt;/ul&gt;

&lt;p&gt;I would have tried the SystemTap approach myself to begin with, but it can miss types including mmap&#39;d reads and other kernel sources. For example, here&#39;s a call stack for mark_page_accessed() (a cache read), showing that we got here via a write() syscall:&lt;/p&gt;

&lt;pre&gt;
          dd-30425 [000] 6788093.150288: mark_page_accessed: (mark_page_accessed+0x0/0x60)
          dd-30425 [000] 6788093.150291: &lt;stack trace&gt;
 =&amp;gt; __getblk
 =&amp;gt; __bread
 =&amp;gt; ext3_get_branch
 =&amp;gt; ext3_get_blocks_handle
 =&amp;gt; ext3_get_block
 =&amp;gt; __block_write_begin
 =&amp;gt; ext3_write_begin
 =&amp;gt; generic_perform_write
 =&amp;gt; generic_file_buffered_write
 =&amp;gt; __generic_file_aio_write
 =&amp;gt; generic_file_aio_write
 =&amp;gt; do_sync_write
 =&amp;gt; vfs_write
 =&amp;gt; sys_write
 =&amp;gt; system_call_fastpath
&lt;/stack&gt;&lt;/pre&gt;

&lt;p&gt;It&#39;s reading file system metadata. This example uses ftrace, via my kprobe tool (&lt;a href=&quot;https://github.com/brendangregg/perf-tools&quot;&gt;perf-tools&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;My preferred technique would be to modify the kernel to instrument page cache activity. Eg, either:&lt;/p&gt;

&lt;ul&gt;
&lt;p&gt;E) Apply Keiichi&#39;s &lt;a href=&quot;https://lkml.org/lkml/2011/7/18/326&quot;&gt;pagecache monitoring&lt;/a&gt; kernel patch, which provides tracepoints for cache instrumentation, and tools with awesome capabilities: not just system-wide ratios, but also per-process and per-file. I&#39;d like this to be in mainline.&lt;/p&gt;

&lt;p&gt;F) Develop another kernel patch to add cache hit/miss statistics to /proc/meminfo.&lt;/p&gt;
&lt;/ul&gt;

&lt;p&gt;And then, there&#39;s the approach I used myself for the issue: dynamic tracing of file system and disk I/O functions using ftrace and perf_events.&lt;/p&gt;

&lt;h2&gt;pcstat&lt;/h2&gt;

&lt;p&gt;If you&#39;re interested in page cache activity, you should also like &lt;a href=&quot;https://github.com/tobert/pcstat&quot;&gt;pcstat&lt;/a&gt;, by Al Tobey, which uses mincore (or fincore), to see size how much files are present in the page cache. It&#39;s pretty awesome.&lt;/p&gt;

&lt;h2&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;Hopefully in the future the kernel will provide an easy way to measure page cache activity, be it from /proc or tracepoints. In the meantime, I have cachestat which works for my kernel versions. Its current implementation is brittle, and may not work well on other versions without modifications, so its greatest value may be &lt;a href=&quot;http://dtrace.org/blogs/brendan/2013/05/27/the-greatest-tool-that-never-worked-har/&quot;&gt;showing what can be done&lt;/a&gt; with a little effort.&lt;/p&gt;


</description>
        <pubDate>Wed, 31 Dec 2014 00:00:00 +0800</pubDate>
        <link>http://iftti.com/posts/2014/2014-12-31-linux-page-cache-hit-ratio.html-deae9b8e2.html</link>
        <guid isPermaLink="true">http://iftti.com/posts/2014/2014-12-31-linux-page-cache-hit-ratio.html-deae9b8e2.html</guid>
        
        
        <category>brendangregg</category>
        
      </item>
    
  </channel>
</rss>
